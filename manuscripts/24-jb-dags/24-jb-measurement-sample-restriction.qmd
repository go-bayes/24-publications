---
title: 'The Weirdest Causal Inferences in the World'
abstract: |
  The human sciences, like all sciences, should seek generalisation where generalisations may be found. For this reason, and for ethical reasons, it is desirable to sample more broadly than 'Western, Educated, Industrialised, Rich, and Democratic' (WEIRD) societies. However, restriction of the target population is sometimes necessary. For example, we would not recruit young children into studies on elderly care. Under which conditions is unrestricted sampling desirable and undesirable? Here, we use causal directed acyclic graphs (causal DAGs) to clarify structural features of bias that recur in measurement error bias, target population restriction bias at the start of a study, and target population restriction bias at the end of a study. We define any study that exhibits any one of these biases, or standard confounding bias, as **weird** (**w**rongly **e**stimated inferences due to **i**nappropriate **r**estriction and **d**istortion), clarifying why the first step in comparative study design is to mitigate 'weirdness.' We discuss the challenges in avoiding weirdness and explain how workflows for causal inference provide the preflight checklists needed for ambitious, effective, and safe comparative cultural research.
  
  **KEYWORDS**: *Causal Inference*; *Comparative*; *Cross-Cultural*; *DAGs*; *Experiments*; *Longitudinal*;  *Measurement Error Bias**; **Selection Bias*; *Target Validity*; *WEIRD*
author: 
  - name: Joseph A. Bulbulia
    affiliation: Victoria University of Wellington, New Zealand
    orcid: 0000-0002-5861-2056
    email: joseph.bulbulia@vuw.ac.nz
    corresponding: no
editor_options: 
  chunk_output_type: console
format:
  pdf:
    sanitise: true
    keep-tex: true
    link-citations: true
    colorlinks: true
    documentclass: article
    classoption: [single column]
    lof: false
    lot: false
    geometry:
      - top=30mm
      - left=25mm
      - heightrounded
      - headsep=22pt
      - headheight=11pt
      - footskip=33pt
      - ignorehead
      - ignorefoot
    template-partials: 
      - /Users/joseph/GIT/templates/quarto/title.tex
    header-includes:
      - \input{/Users/joseph/GIT/latex/latex-for-quarto.tex}
date: last-modified
execute:
  echo: true
  warning: false
  include: true
  eval: true
fontfamily: libertinus
bibliography: /Users/joseph/GIT/templates/bib/references.bib
csl: /Users/joseph/GIT/templates/csl/camb-a.csl
---


```{r}
#| label: load-libraries
#| echo: false
#| include: false
#| eval: true

## WARNING SET THIS PATH TO YOUR DATA ON YOUR SECURE MACHINE. 
# pull_path <-
#   fs::path_expand(
#     #'/Users/joseph/v-project\ Dropbox/Joseph\ Bulbulia/00Bulbulia\ Pubs/DATA/nzavs_refactor/nzavs_data_23'
#     '/Users/joseph/Library/CloudStorage/Dropbox-v-project/Joseph\ Bulbulia/00Bulbulia\ Pubs/DATA/nzavs-current/r-data/nzavs_data_qs'
#   )
# 


push_mods <-  fs::path_expand(
  '/Users/joseph/Library/CloudStorage/Dropbox-v-project/data/nzvs_mods/24/church-prosocial-v7'
)


#tinytext::tlmgr_update()

# WARNING:  COMMENT THIS OUT. JB DOES THIS FOR WORKING WITHOUT WIFI
#source('/Users/joseph/GIT/templates/functions/libs2.R')
# # WARNING:  COMMENT THIS OUT. JB DOES THIS FOR WORKING WITHOUT WIFI
# source('/Users/joseph/GIT/templates/functions/funs.R')

#ALERT: UNCOMMENT THIS AND DOWNLOAD THE FUNCTIONS FROM JB's GITHUB

# source(
#   'https://raw.githubusercontent.com/go-bayes/templates/main/functions/experimental_funs.R'
# )
# 
# source(
#   'https://raw.githubusercontent.com/go-bayes/templates/main/functions/funs.R'
# )

# check path:is this correct?  check so you know you are not overwriting other directors
#push_mods

# for latex graphs
# for making graphs
library('tinytex')
library('extrafont')
library('tidyverse')
library('kableExtra')
#devtools::install_github('go-bayes/margot')
library(margot)
loadfonts(device = 'all')
```

## Introduction  {#id-sec-intro} 

Human scientists ask and answer questions. To anchor answers in facts, we collect data.

Most publishing human scientists work in what Joseph Henrich, Steven Heine, and Ara Norenzayan have termed 'WEIRD' societies: 'Western, Educated, Industrialised, Rich, and Democratic Societies' [@henrich2010weirdest]. Unsurprisingly, WEIRD samples are over-represented in human science datasets [@sears1986college; @arnett2008neglected]. Henrich et al. illustrate how WEIRD samples differ from non-WEIRD samples in areas such as spatial cognition and perceptions of fairness, while showing continuities in basic emotion recognition, positive self-views, and motivation to punish anti-social behaviour. Because science seeks generalisation wherever it can, Henrich et al. urge that sampling from non-WEIRD populations is desirable.

Recently, a host of institutional diversity and inclusion initiatives have been developed that commend researchers to obtain data from global samples. In my view, the motivation for these mission statements is ethically laudable. The injunction for a broader science of humanity also accords with institutional missions. For example, the scientific mission of the American Psychological Association (APA) is 'to promote the advancement, communication, and application of psychological science and knowledge to benefit society and improve lives.' The APA does not state that it wants to understand and benefit only North Atlantic Societies.[^1] It is therefore tempting to use such a mission statement as an ideal by which to evaluate the samples used in human scientific research.

[^1]: https://www.apa.org/pubs/authors/equity-diversity-inclusion

Suppose we agree that promoting a globally diverse science makes ethical sense. Does the sampling of globally diverse populations always advance this ideal? It is easy to find examples in which restricting our source population makes better scientific sense. Suppose we are interested in the psychological effects of restorative justice among victims of violent crime. Here, it would make little scientific sense to sample from a population that has not experienced violent crime. Nor would it make ethical sense. The scientific question, which may have important ethical implications, is not served by casting a wider net. Suppose our interest were to investigate the health effects of calorie restriction. It might be unethical to include children or the elderly. It makes little sense to investigate the psychological impact of vasectomy in biological females or hysterectomy in biological males.

In the cases we just considered, the scientific questions pertained to a sub-sample of the human population and so could be sensibly restricted (refer also to @gachter2010, @machery2010). However, even for questions that relate to all of humanity, sampling from all of humanity might be undesirable. For example, if we were interested in the effects of a vaccine on disease, sampling from one population might be as good as sampling from all. Sampling from one population might spare time and expense, which come with opportunity costs. We might conclude that sampling universally, where unnecessary, is wasteful and unethical. 

We might agree with our mission statements in judging that ethical aspirations must guide research at every phase. More fundamentally, we cannot assess the bandwidth of human diversity from the armchair, without empirical study, and this is a motivation to investigate. Yet, mistaking our aspirations for sampling directives risks wasteful science. Because waste carries opportunity costs, wasteful science is unethical science.

I present these examples to remind ourselves of the importance of addressing questions of sampling in relation to its context.

During the past twenty years, causal data science, also known as 'causal inference' or 'CI', has enabled tremendous clarity for questions of research design and analysis [@richardson2014causal]. Here, we examine how workflows developed from causal inference clarify threats and opportunities for comparative human research. Put differently, causal inference helps us to clarify the assumptions under which restriction is desirable and when it is not. Not all questions are causal, of course. However, because manifest associations in a dataset may not be evidence of *association* in the world, even those who seek descriptive understanding benefit from causal inferential workflows [@vansteelandt2022a].

In the remainder of the introduction, I review causal directed acyclic graphs (causal DAGs). Readers familiar with causal diagrams may skip this section. Because causal diagrams encode causal assumptions, we will use the terms 'structural' and 'causal' synonymously. I encourage readers unfamiliar with causal directed acyclic graphs to develop familiarity before proceeding: [@hernan2024WHATIF, chapter 6; @neal2020introduction; @mcelreath2020, chapters 5, 6; @pearl2009a; @barrett2021; @bulbulia2023].

[Part 1](#id-sec-1) uses causal diagrams to clarify five structural features of measurement-error bias. Understanding measurement error bias is essential in all research, especially in comparative human science, where it casts a long shadow.

[Part 2](#id-sec-2) examines structural sources of bias arising from attrition and non-response, also known as 'right-censoring' or simply 'censoring'. Censoring may lead to restriction of the sample population at baseline. If the sample population at baseline is the target sample, censoring may lead to bias.

[Part 3](#id-sec-3) considers biases that arise at the start of a study when there is a mismatch between the sample population and the target population. When there is restriction of the target population in the sample population at baseline, this may bias results. I focus on structural threats to inference when the sample population is (1) too restrictive (for example, too WEIRD) and (2) insufficiently restrictive (WEIRD sampling leads to bias).

The importance of causal inference for comparative research has been discussed in several recent studies [@deffner2022; @bulbulia2022]. Here, I focus on challenges arising from structural features of (1) measurement error bias, (2) target population restriction bias from censoring, and (3) target population restriction bias at baseline. Causal directed acyclic graphs clarify these recurring structural motifs of measurement error bias. Understanding structural commonalities in these biases equips investigators to better plan their research.

We begin with a brief overview of causal inference, causal diagrams, and our terminology.

### What is Causality?

To quantify a causal effect, we must contrast the world as it is – in principle, observable – with the world as it might have been – in principle, not observable.

Consider a binary treatment variable $A \in \{0,1\}$ representing the randomised administration of a vaccine to individuals $i$ in the set $\{1, 2, \ldots, n\}$. $A_i = 1$ denotes vaccine administration, and $A_i = 0$ denotes no vaccine. The potential outcomes for each individual are $Y_i(0)$ and $Y_i(1)$, representing outcomes yet to be realised before administration. Thus, they are called 'potential' or 'counterfactual' outcomes. For an individual $i$, we define a causal effect as the contrast between the outcome observed under one intervention level and the outcome observed under another. This contrast, for the $i^{th}$ individual, can be expressed on the difference scale as:

$$
\text{Individual Treatment Effect} = Y_i(1) - Y_i(0)
$$

where the 'Individual Treatment Effect' is the difference in a predefined measure $Y$ between receiving and not receiving the treatment. $Y_i(1) - Y_i(0) \neq 0$ denotes a causal effect of $A$ on $Y$ for individual $i$ on the difference scale. Similarly, $\frac{Y_i(1)}{Y_i(0)} \neq 1$ denotes a causal effect of treatment $A$ for individual $i$ on the risk ratio scale. These quantities cannot be computed from observational data for any individual $i$. The inability to observe individual-level causal effects is the *Fundamental Problem of Causal Inference* [@rubin1976; @holland1986]. This problem has long puzzled philosophers [@hume1902; @lewis1973]. However, although individual causal effects are generally unobservable, we can sometimes recover average causal effects by treatment group.


### How We Obtain Average Causal Effect Estimates from Ideally Conducted Randomised Experiments

The Average Treatment Effect (ATE) measures the difference in outcomes between treated and control groups as follows:

$$
\text{Average Treatment Effect} = \mathbb{E}[Y(1)] - \mathbb{E}[Y(0)]
$$

Here, $\mathbb{E}[Y(1)]$ and $\mathbb{E}[Y(0)]$ represent the average outcome for the target population if *everyone* in the population were subjected to the treatment and control conditions, respectively.

In a randomised experiment, we estimate these averages assuming that the sample population matches the target population. We do this by considering the average observed and unobserved outcomes under the treatment conditions:

$$
\text{ATE} = \left(\mathbb{E}[Y(1) | A = 1] + \mathbb{E}[Y(1) | A = 0]\right) - \left(\mathbb{E}[Y(0) | A = 0] + \mathbb{E}[Y(0) | A = 1]\right)
$$

Effective randomisation ensures that potential outcomes are similarly distributed across both groups. Thus, any differences in the averages of the treatment groups can be attributed to the treatment. Therefore, in an ideally conducted randomised experiment, the average outcomes are expected to be equal across different treatment conditions for the population from which the sample is drawn:

$$
\widehat{\mathbb{E}}[Y(0) | A = 1] = \widehat{\mathbb{E}}[Y(0) | A = 0], \quad \widehat{\mathbb{E}}[Y(1) | A = 1] = \widehat{\mathbb{E}}[Y(1) | A = 0]
$$

This provides an unbiased estimate of the Average Treatment Effect:

$$
\widehat{\text{ATE}} = \widehat{\mathbb{E}}[Y | A = 1] - \widehat{\mathbb{E}}[Y | A = 0]
$$

Randomised controlled experiments are powerful because they evenly distribute potential explanatory factors across treatment groups. We cannot observe individual causal effects for Alice or Bob. However, for the groups to which they have been randomised, we recover average causal effects by a process of inference by elimination: randomisation rules out alternative explanations.

Note that in the context of our imagined experiment, $\widehat{\text{ATE}}$ applies to the population from which the experimental participants were drawn and is calculated on the difference scale. A more explicit notation would define this effect estimate by referencing its scale and population: $\widehat{\text{ATE}}^{a'-a}_{\text{S}}$, where $a'-a$ denotes the difference scale, and $S$ denotes the source population. We will return to this point in [Part 2](#id-sec-2) and [Part 3](#id-sec-3), but it is important to build intuition early that in causal inference we must specify: (1) the causal effect of interest; (2) a scale of contrast; and (3) a target population for whom a causal effect estimate is meant to generalise.


### Three Fundamental Assumptions For Causal Inference

An observational study aims to estimate the average treatment effects without researchers controlling treatments or randomising treatment assignments. We can consistently estimate counterfactual contrasts only under strict assumptions. Three fundamental assumptions are required to obtain the counterfactual quantities required to compute causal contrasts from observational data.

#### Assumption 1. Causal Consistency

Causal consistency states that the observed outcome for each individual under the treatment they actually received is equal to their potential outcome under that treatment. This means if an individual $i$ received treatment $A_i = 1$, their observed outcome $Y_i$ is the same as their potential outcome under treatment, denoted as $Y_i(1)$. Similarly, if they did not receive the treatment ($A_i = 0$), their observed outcome is the same as their potential outcome without treatment, denoted as $Y_i(0)$, such that:

$$
Y_i = A_i \cdot Y_i(1) + (1 - A_i) \cdot Y_i(0)
$$

where: 

- $Y_i$ is the observed outcome for individual $i$; 
- $A_i$ is the treatment status for individual $i$, with $A_i = 1$ indicating treatment received and $A_i = 0$ indicating no treatment; 
- $Y_i(1)$ and $Y_i(0)$ are the potential outcomes for individual $i$ under treatment and no treatment, respectively.

The causal consistency assumption is necessary to link the theoretical concept of potential outcomes — the target quantities of interest — with observable data (see @bulbulia2023a).

#### Assumption 2. Conditional Exchangeability (or Ignorability)

Conditional exchangeability states that given a set of measured covariates $L$, the potential outcomes are independent of the treatment assignment. Once we control for $L$, the treatment assignment $A$ is as good as random with respect to the potential outcomes:

$$
Y(a) \coprod A | L
$$

where:

- $Y(a)$ represents the potential outcomes for a particular treatment level $a$.
- $\coprod$ denotes conditional independence.
- $A$ represents the treatment levels to be contrasted.
- $L$ represents the measured covariates.

Under the conditional exchangeability assumption, any differences in outcomes between treatment groups can be attributed to the treatment. This assumption requires that all confounding variables affecting both the treatment assignment $A$ and the potential outcomes $Y(a)$ are measured and included in $L$ (For further clarification, see [Appendix A](#id-app-a)).

#### Assumption 3. Positivity

The positivity assumption requires that every individual in the population has a non-zero probability of receiving each treatment level, given their covariates. Formally,

$$
0 < Pr(A = a | L = l) < 1, \quad \forall a \in A, \, \forall l \in L \, \text{ such that } \, Pr(L = l) > 0
$$

where: 

- $A$ is the treatment or exposure variable. 
- $L$ is a vector of covariates. 
- $a$ and $l$ represent specific values of treatment and covariates, respectively.

The positivity assumption, essential for valid treatment effect estimates, requires that every individual has a non-zero chance of receiving each treatment across all covariates in $L$. In practice, verifying this assumption faces two main challenges:
a. **Data sparsity**: Certain covariate combinations are rare or unobserved in the data, making it difficult to empirically confirm positivity for these groups.
b. **Model dependence**: Researchers rely on statistical models to estimate treatment probabilities for all covariate patterns due to data sparsity. However, these assessments are only as reliable as the models used, which may be subject to misspecification or inaccuracies. For a discussion of causal assumption in relation to target validity, refer to @imai2008misunderstandings.



### Terminology

To avoid terminology confusion, we define the meanings of our terms:

- **Unit/individual**: An entity, such as an object, person, or culture. We will use the term 'individual' instead of the more general term 'unit'. Think 'row' in one's dataset.

- **Variable**: A feature of an individual, transient or permanent. 'John was sleepy but is no longer.' 'Alice was born 30 November.'

- **Treatment**: Equivalent to 'exposure', an event that might change a variable. 'John was sleepy; we intervened with coffee; he is wide awake.' 'Alice was born in November, nothing can change that.' The 'cause'.

- **Outcome**: The response variable or 'effect'. In causal inference, we contrast 'potential' or 'counterfactual outcomes'. In observational or 'real-world' studies where treatments are not randomised, the assumptions for obtaining contrasts of counterfactual outcomes are typically much stronger than in randomised controlled experiments.

- **Confounding**: A state where the treatment and outcome share a common cause and no adjustment is made to remove the non-causal association, or where the treatment and outcome share a common effect, and adjustment is made for this common effect, or when the effect of the treatment on the outcome is mediated by a variable which is conditioned upon. In each case, the observed association will not reflect a causal association. Causal directed acyclic graphs clarify strategies for confounding control.

- **Measurement**: A recorded trace of a variable, such as a column in one's dataset.

- **Measurement error**: A misalignment between the true state of a variable and its recorded state. 'Alice was born on 30 November; records were lost, and her birthday was recorded as 1 December.'

- **Population**: An abstraction from statistics, denoting the set of all individuals defined by certain features. John belongs to the set of all individuals who ignore instructions.

- **Super-population**: An abstraction, the population of all possible individuals of a given kind. John and Alice belong to a super-population of hominins.

- **Restricted population**: Population $p$ is restricted relative to another population $P$ if the individuals $p \in P$ share some but not all features of $P$. 'The living' is a restriction of hominins.

- **Target population**: A restriction of the super-population whose features interest investigators. An investigator who defines their interests is a member of the population of 'good investigators'.

- **Source population**: The population from which the study's sample is drawn. Investigators wanted to recruit from a general population but recruited from the pool of first-year university psychology students.

- **Baseline sample population**: The abstract set of individuals from which the units in one's study at treatment assignment belong, e.g. 'the set of all first-year university psychology students who might end up in this study'. To simplify, we will consider the baseline population as the *source population*.

- **Selection into the sample**: The process by which individuals are included in a population or sample. Selection occurs and is under investigator control when a target population is defined from a super-population or when investigators apply eligibility criteria for inclusion in the analytic sample. Selection into the sample is often out of the investigator's control. Investigators might aspire to answer questions about all of humanity but find themselves limited to undergraduate samples. Investigators might sample from a source population but recover an analytic sample that differs from it in ways they cannot measure, such as mistrust of scientists. There is typically attrition of an analytic sample over time, and this is not typically fully within investigator control. Because the term 'selection' has different meanings in different areas of human science, we will speak of 'target population restriction at the start of study.' Note that to evaluate this bias, it is important for investigators to state a causal effect of interest with respect to *the full data* that includes the counterfactual quantities for the treatments to be compared in a clearly defined target population where all members of the target population are exposed to each level of treatment to be contrasted [@westreich2017].

- **Censored sample population**: The population from which the censored units are drawn. Censoring is uninformative if there is no treatment effect for everyone in the baseline population (the sharp causal null hypothesis). Censoring is informative if there is an effect of the treatment, and this effect varies in at least one stratum of the baseline population. Note that uninformative censoring does not ensure valid inference for the target population even when valid inference is ensured for the baseline population. If the baseline population differs in the distribution of those features that modify the effect of the treatment, and no correction is applied, unbiased effect estimates for the baseline population will nevertheless be biased for the target population in at least one measure of effect [@greenland2009commentary; @lash2020]. We call this bias from censoring 'target population restriction at the end of study.' Note again that to evaluate this bias, the causal effect of interest must be stated with respect to *the full data* that includes the counterfactual quantities for the treatments to be compared in a clearly defined target population where all members of the target population are exposed to each level of treatment to be contrasted [@westreich2017].

- **Target population restriction bias**: bias occurs when the distribution of effect modifiers in the sample population differs from that in the target population, either at the start, at the end, or throughout the study. Here we consider: *target population restriction bias at the start of study* and *target population restriction bias at the end of study.*  If this bias occurs at the start of the study, it will generally occur at the end of the study (and at intervals between), except by accident. We require validity to be non-accidental.  

- **Generalisability**: A study's findings generalise to a target population if the effects observed in the study group at the end of study are also valid for the target population for structurally valid reasons (i.e., non-accidentally), see [Appendix B](#id-app-b)

- **Transportability:** When the study sample is not drawn from the target population, we cannot directly generalise the findings. However, we can transport the estimated causal effect from the source population to the target population under certain assumptions. This involves adjusting for differences in the distributions of effect modifiers between the two populations. The closer the source population is to the target population, the more plausible the transportability assumptions and the less we need to rely on complex adjustment methods see [Appendix B](#id-app-b).

- **Marginal effect**: Synonym for the average treatment effect — always relative to some population investigators specify.

- **Intention-to-treat effect**: The marginal effect of random treatment assignment.

- **Per-protocol effect**: The effect of adherence to a randomly assigned treatment assignment if adherence were perfect [@hernan2017per]. We have no guarantee that the intention-to-treat effect will be the same as the per-protocol effect. A safe assumption is that:
$$
\widehat{ATE}_{\text{target}}^{\text{Per-Protocol}} \ne \widehat{ATE}_{\text{target}}^{\text{Intention-to-Treat}}
$$
  When evaluating evidence for causality, in addition to specifying their causal contrast, effect measure, and target population, investigators should specify whether they are estimating an intention-to-treat or per-protocol effect [@hernán2004; @tripepi2007].

- **WEIRD**: a sample of 'Western, Educated, Industrialized, Rich, and Democratic Societies' [@henrich2010weirdest].

- **weird**:  (**w**rongly **e**stimated inferences due to **i**nappropriate **r**estriction and **d**istortion) a causal effect estimate that is not valid for the target population, either from confounding bias, measurement error bias, target population restriction at the start of study, or target population restriction at the end of study (noting these biases typically overlap -- for example, often measurement error bias leads to confounding.)


Note that our terminology differs in causal inference for the concepts we have defined here (refer to @dahabreh2021study; @imai2008misunderstandings; @cole2010generalizing; @westreich2017transportability). A clear decomposition of key concepts needed to assess generalisability — or what we call 'target validity' — is given in @imai2008misunderstandings. For a less technical, pragmatically useful discussion, refer to @stuart2018generalizability.

### Graphical Conventions

- **$A$**: Denotes the 'treatment' or 'exposure' — a random variable, 'the cause.'

- **$Y$**: Denotes the outcome or response, measured at the end of the study. $Y$ is the 'effect'.

- **$L$**: Denotes a measured confounder or set of confounders.

- **$U$**: Denotes an unmeasured confounder or confounders.

- $\mathbf{\mathcal{R}}$: Denotes randomisation to treatment condition $\big(\mathcal{R} \rightarrow A\big)$.

- **Node**: Represents characteristics or features of units within a population on a causal diagram — that is, a 'variable.' In causal directed acyclic graphs (DAGs), we draw nodes with respect to the *target population*, which is the population for whom investigators seek causal inferences [@suzuki2020]. Time-indexed node: $X_t$ denotes relative chronology.

- **Edge without an Arrow** ($\association$): path of association, causality not asserted.

- **Red Edge without an Arrow** ($\associationred$): confounding path: ignores arrows to clarify statistical dependencies. 

- **Arrow** ($\rightarrow$): Denotes a causal relationship from the node at the base of the arrow (a 'parent') to the node at the tip of the arrow (a 'child'). In causal DAGs, it is conventional to refrain from drawing an arrow from treatment to outcome to avoid asserting a causal path from $A$ to $Y$ because we aim to ascertain whether causality can be identified for this path. All other nodes and paths — including the absence of nodes and paths — are typically assumed.

- **Red Arrow** ($\rightarrowred$): Denotes a path of non-causal association between the treatment and outcome. Despite the arrows, this path is associational and may flow against time.

- **Open Blue Arrow** ($\rightarrowblue$): Denotes effect modification, which occurs when the effect of treatment varies within levels of a covariate. We do not assess the causal effect of the effect modifier on the outcome, recognising that it may be incoherent to consider intervening on the effect modifier. However, if the distribution of effect modifiers in the sample population differs from that in the target population, then at least one measure of causal effect will differ.

- **Boxed Variable** $\big(\boxed{X}\big)$: Denotes conditioning or adjustment for $X$. 

- **Red-Boxed Variable** $\big(\boxedred{X}\big)$: Highlights the source of confounding bias from adjustment.

- **Dashed Circle** $\big(\circledotted{X}\big)$: Denotes no adjustment is made for a variable (implied for unmeasured confounders).

- **$\mathcal{G}$**: Names a causal directed acyclic graph.


### Causal Directed Acyclic Graphs (causal DAGs)

In the 1990s, Judea Pearl showed that we can evaluate causal dependencies using observable probability distributions [@pearl1995; @pearl2009a]. He also demonstrated that causal directed acyclic graphs (causal DAGs) clarify the conditional dependencies among variables [@pearl1995]. Based on assumptions about causal structure, researchers can identify causal effects from joint distributions of observed data.


::: {#tbl-terminologygeneral}
```{=latex}
\terminologydirectedgraph
```
Elements of Causal Graphs 
:::


Pearl developed graphical rules known as d-separation [@pearl1995]:

- **Fork rule** ($B \leftarrowNEW \boxed{A} \rightarrowNEW C$): $B$ and $C$ are independent when conditioned on $A$ ($B \coprod C \mid A$).
- **Chain rule** ($A \rightarrowNEW \boxed{B} \rightarrowNEW C$): Conditioning on $B$ blocks the path between $A$ and $C$ ($A \coprod C \mid B$).
- **Collider rule** ($A \rightarrowNEW \boxed{C} \leftarrowNEW B$): $A$ and $B$ are independent until conditioned on $C$, which introduces dependence ($A \cancel{\coprod} B \mid C$).

These rules lead to the backdoor criterion and 'backdoor adjustment' theorem, which provide algorithms for identifying causal effects based on the structural assumptions encoded in a causal DAG [@pearl1995].

Consider the following graphs:

- **$\mathcal{G}_1$**: If $A$ and $B$ are not causally related and share no common causes, $A$ and $B$ will not be statistically related.
- **$\mathcal{G}_2$**: If $A$ causes $B$, and they share no common causes or their common causes are accounted for, $A$ and $B$ will be statistically related.
- **$\mathcal{G}_3$**: If $A$ causes $B$ and $A$ causes $C$, then conditioning on $A$ allows us to estimate the effect of $B$ on $C$.
- **$\mathcal{G}_4$**: If $A$ causes $B$ and $B$ causes $C$, conditioning on $B$ obscures the true causal effect of $A$ on $C$, making $A$ independent of $C$.
- **$\mathcal{G}_5$**: If $A$ causes $C$ and $B$ causes $C$, conditioning on $C$ associates $A$ and $B$, despite no direct causal effect.

If we assume that the variables in the graph correspond to Structural Causal Models, all causal relationships can be defined by the elementary structures presented above.


### Effect-Modification on Causal Directed Acyclic Graphs

The primary function of a causal directed acyclic graph is to clarify relations of conditional independence [@tbl-terminologygeneral] for the purposes of causal identification. We have noted that modifying a causal effect within one or more strata of the target population opens the possibility for biased average treatment effect estimates when the distribution of these effect modifiers differs in the sample population. 

We do not generally represent non-linearities in causal directed acyclic graphs, which are tools for obtaining relationships of conditional and unconditional independence from assumed structural relationships encoded in a causal diagram that may lead to a non-causal treatment/outcome association.

[@tbl-terminologygeneral] presents our convention for highlighting a relationship of effect modification in settings where (1) we assume no confounding of treatment and outcome and (2) there is effect modification such that the effect of $A$ on $Y$ differs in at least one stratum of the target population.

::: {#tbl-terminologygeneral}
```{=latex}
\terminologyeffectmodification
```
Elements of Causal Graphs 
:::

To focus on effect modification, we do not draw a causal arrow from the direct effect modifier $F$ to the outcome $Y$. This convention is specific to this article (refer to @hernan2024WHATIF, pp. 126-127, for a discussion of 'noncausal' arrows).

## Part 1: How Measurement Error Bias Makes Your Causal Inferences **weird** (**w**rongly **e**stimated inferences due to **i**nappropriate **r**estriction and **d**istortion) {#id-sec-1}

Measurements record reality, but they are not always accurate. When variables are measured with error, our results can be misleading. Every study must therefore consider how its measurements might mislead. Causal directed acyclic graphs (DAGs) can deepen understanding because, as implied by the concept of 'record', there are structural or causal properties of measurement error. Understanding these properties can greatly assist with study design, data collection, data analysis, and inference.

Measurement error can take various forms, each with distinct implications for causal inference. Causal diagrams clarify these types of measurement error bias:

- **Independent/uncorrelated**: Errors in different variables do not influence each other.
- **Independent and correlated**: Errors in different variables are related through a shared cause.
- **Dependent and uncorrelated**: Errors in one variable influence the measurement of another, but these influences are not related through a shared cause.
- **Dependent and correlated**: Errors in one variable influence the measurement of another, and these influences are related through a shared cause [@hernán2009; @vanderweele2012a].

The six examples presented in @tbl-terminologymeasurementerror illustrate structural features of measurement error bias and clarify how they can affect the accuracy of causal inferences.

::: {#tbl-terminologymeasurementerror}
```{=latex}
\terminologymeasurementerror
```
Example of measurement error bias
:::

Understanding these types of measurement error bias helps researchers design better studies, collect more accurate data, and apply analytical techniques that address measurement error bias. As we will consider in Parts 2 and 3, these four types of measurement error bias will enable us to better clarify the promise and perils of comparative research. 

### Example 1: Uncorrelated Errors under Sharp Null: No Treatment Effect

@tbl-terminologymeasurementerror $\mathcal{G}_1$ illustrates uncorrelated non-differential measurement error under the 'sharp-null,' which arises when the error terms in the exposure and outcome are independent. In this setting, the measurement error structure is not expected to produce bias.

For example, consider a study investigating the causal effect of beliefs in big Gods on social complexity in ancient societies. Imagine that societies either randomly omitted or inaccurately recorded details about their beliefs in big Gods and their social complexities. This might happen due to the varying preservation of records across cultures, unrelated to the actual beliefs or social complexities. In this scenario, the errors in historical records for beliefs in big Gods and for social complexity will be independent. Such errors may generally not introduce bias — suggesting an effect — when there is no true effect (although see @richardson2013 for edge cases). 

Generally, uncorrelated undirected errors will not be be weird **weird** (**w**rongly **e**stimated inferences due to **i**nappropriate **r**estriction and **d**istortion)

### Example 2: Uncorrelated Errors under Treatment Effect Biases True Effects toward the Null

@tbl-terminologymeasurementerror $\mathcal{G}_2$ illustrates uncorrelated non-differential measurement error, which arises when the error terms in the exposure and outcome are independent (information bias). In this setting, bias will typically attenuate a true treatment effect.

Consider again the example of a study investigating a causal effect of beliefs in big Gods on social complexity in ancient societies, where there are uncorrelated errors in the treatment and outcome. In this case, measurement error will typically make it seem that the true causal effects of beliefs in big Gods are smaller than they are, or perhaps even that such an effect is absent.


Uncorrelated undirected measurement error in the presence of a true effect leads to distortion of true causal effects, inviting **weird** results (**w**rongly **e**stimated inferences due to **i**nappropriate **r**estriction and **d**istortion)

### Example 3: Correlated Errors Non-Differential (Undirected) Measurement Errors

@tbl-terminologymeasurementerror $\mathcal{G}_3$ illustrates the structure of correlated non-differential (undirected) measurement error bias, which arises when the error terms of the treatment and outcome share a common cause.

Consider an example: imagine that societies with more sophisticated record-keeping systems tend to offer more precise and comprehensive records of both beliefs in big Gods and social complexity. In this setting, it is the record-keeping systems that give an illusion of a relationship between big Gods and social complexity. This might occur without any effect of big-God beliefs on measuring social complexity or vice versa. Nevertheless, the correlated sources of error for both the exposure and outcome may suggest causation in its absence.

Correlated non-differential measurement error invites **weird** results (**w**rongly **e**stimated inferences due to **i**nappropriate **r**estriction and **d**istortion)

### Example 4: Uncorrelated Directed Measurement Error: Exposure Affects Error of Outcome

@tbl-terminologymeasurementerror $\mathcal{G}_4$ illustrates the structure of uncorrelated differential (or directed) measurement error, where a non-causal path is opened linking the treatment, the outcome, or a common cause of the treatment and outcome.

Continuing with our previous example, imagine that beliefs in big Gods lead to inflated records of social complexity in a culture's record-keeping. This might happen because the record keepers in societies that believe in big Gods prefer societies to reflect the grandeur of their big Gods. Suppose further that cultures lacking beliefs in big Gods prefer Bacchanalian-style feasting to record-keeping. In this scenario, societies with record keepers who believe in big Gods would appear to have more social complexity than equally complex societies without such record keepers.

Uncorrelated directed measurement error bias also invites **weird** results (**w**rongly **e**stimated inferences due to **i**nappropriate **r**estriction and **d**istortion)

### Example 5: Uncorrelated Directed Error: Outcome Affects Error of Exposure

@tbl-terminologymeasurementerror $\mathcal{G}_5$ illustrates the structure of uncorrelated differential (or directed) measurement error, this time when the outcome affects the recording of the treatment that preceded the outcome.

Consider if 'history is written by the victors.' How might this affect measurement error bias? Suppose that social complexity causes beliefs in big Gods. Perhaps kings create big Gods after the image of kings. If the kings prefer a history in which big Gods were historically present, this might bias the historical record, opening a path of association that reverses the order of causation. Such results would be **weird**: (**w**rongly **e**stimated inferences due to **i**nappropriate **r**estriction and **d**istortion)

### Example 6: Directed Error: Outcome Affects Error of Exposure

@tbl-terminologymeasurementerror $\mathcal{G}_6$ illustrates the structure of correlated differential (directed) measurement error, which occurs when the exposure affects levels of already correlated error terms.

Suppose social complexity produces a flattering class of religious elites who tend to produce vainglorious depictions of kings and their dominions, and also of the extent and scope of their society's beliefs in big Gods. For example, such elites might tend to downplay widespread cultural practices of worshipping lesser gods, inflate population estimates, and overstate the range of their economies. In this scenario, the errors of the exposure and of the outcome are both correlated and differential.

Such results would be **weird**: (**w**rongly **e**stimated inferences due to **i**nappropriate **r**estriction and **d**istortion)

### Summary

In Part 1, we examined four types of measurement error bias: independent, correlated, dependent, and correlated dependent. The structural features of measurement error bias clarify not only why measurement errors threaten causal inferences but also how measurement errors make causal inferences weird. For example, inferences will be biased toward the null when measurement error is uncorrelated and undirected. 

Considerably more could be said about measurement error bias. For example, @vanderweele2012a demonstrate that, under specific conditions, we can infer the direction of a causal effect from observed associations. Specifically, if:

1. The association between the measured variables $A^{\prime}_{1}$ and $Y^{\prime}_{2}$ is positive,
2. The measurement errors for these variables are not correlated, and
3. We assume distributional monotonicity for the effect of $A$ on $Y$ (applicable when both are binary),

then a positive observed association implies a positive causal effect from $A$ to $Y$. Conversely, a negative observed association provides stronger evidence for a negative causal effect if the error terms are positively correlated than if they are independent. This conclusion relies on the assumption of distributional monotonicity for the effect of $A$ on $Y$. For now, the four elementary structures of measurement error bias will enable us to clarify the connections between the structures of measurement error bias, target population restriction bias at the end of a study, and target-restriction bias at the start of a study.

For now, we have examined how measurement error can bias causal effect estimates, exposing investigators to **weird** findings (**w**rongly **e**stimated inferences due to **i**nappropriate **r**estriction and **d**istortion). In [Part 1](#id-sec-1)we have focussed on the potential of measurement error to cause distortion. Next, we focus on structural features of bias when there is an inappropriate restriction of the target population.

## Part 2: How Target Population Restriction Bias At The End of Study Makes Your Causal Inferences weird (**w**rongly **e**stimated inferences due to **i**nappropriate **r**estriction and **d**istortion) {#id-sec-2}

Suppose the sample population at the start of a study matches the source population from which it is drawn, and furthermore that this source population aligns with the target population. Censoring, also called 'right-censoring', or attrition (and non-response), may bias causal effect estimates in one of two ways: by opening pathways of confounding bias -- distortion, or by inappropriately restricting the sample population at the end of a study so that it is no longer aligned with the target population, as it was at the start of a study.  We next consider how censoring can make a study **weird**: (**w**rongly **e**stimated inferences due to **i**nappropriate **r**estriction and **d**istortion).

::: {#tbl-terminologycensoring}
```{=latex}
\terminologycensoring
```
Five examples of censoring bias.
:::


### Example 1:  Confounding by common cause of treatment and attrition

@tbl-terminologycensoring $\mathcal{G}_1$ illustrates confounding by common cause of treatment and outcome in the censored such that the potential outcomes of the population at baseline $Y(a)$ may differ from those of the censored population at the end of study $Y'(a)$ such that $Y'(a) \neq Y(a)$. 

Suppose investigators are interested in whether religious service attendance affects volunteering. Suppose that an unmeasured variable, loyalty, affects religious service attendance, attrition, and volunteering.  The structure of this bias reveals an open backdoor path from the treatment to the outcome. 

We have encountered this bias before. The structure we observe here is one of correlated measurement errors (@tbl-terminologymeasurementerror $\mathcal{G}_3$). In this example, attrition may exacerbate measurement error bias by opening a path from $A \associationred U \associationred U_{\Delta{A}}  \associationred Y'$


The results obtained from such a study would be distorted and so **weird**: (**w**rongly **e**stimated inferences due to **i**nappropriate **r**estriction and **d**istortion). Here, distortion operates through the restriction of the target population in the sample population at the end of the study.

### Example 2: Treatment affects censoring

@tbl-terminologycensoring $\mathcal{G}_2$ illustrates confounding bias in which the treatment affects the censoring process. 


Consider a study investigating the effects of mediation on well-being. Suppose there is no treatment effect but that Buddha-like detachment increases attrition. If $\mathcal{G}_4$ faithfully represents reality, there will be no bias in the treatment effect estimate. That is, there will be no risk that attrition will induce the appearance of a causal effect in its absence. The biasing path runs: $A \associationred U_{\Delta{A\to Y}}  \associationred Y'$.

We have encountered this structural bias before. The structure we observe here is one of directed uncorrelated measurement error (@tbl-terminologymeasurementerror $\mathcal{G}_4$). Randomisation ensures no backdoor paths.  However, if the intervention affects both attrition and bias in the outcome, this may exacerbate measurement error bias.  

The results obtained from this study would be distorted and so **weird**: (**w**rongly **e**stimated inferences due to **i**nappropriate **r**estriction and **d**istortion). Here, distortion operates through the restriction of the target population in the sample population at the end of the study.



### Example 3: No treatment effect when outcome causing censoring 

@tbl-terminologycensoring $\mathcal{G}_3$ illustrates the structure of bias when there is no treatment effect yet the outcome affects censoring. 


 If $\mathcal{G}_3$ faithfully represents reality, there will be no bias in the treatment effect estimate from attrition, even if there are measurement errors in the outcome. Recall again our measurement error causal diagrams. 
 
 The structure we observe here is again familiar: it is one of undirected uncorrelated measurement error (@tbl-terminologymeasurementerror $\mathcal{G}_1$). Here, we would not expect attrition to induce or exaggerate the appearance of a causal effect in its absence, for the same reason that uncorrelated treatment errors do not produce results under the sharp null. 

The results obtained from this study are not distorted. Although the sample population is restricted, this is not a restriction of the target population—assuming the causal diagramme is drawn for the target population as it should be.


### Example 4: Treatment effect when outcome causes censoring and there is a true treatment effect

@tbl-terminologycensoring $\mathcal{G}_4$ illustrates the structure of bias when the outcome affects censoring in the presence of a treatment effect. In contrast to the previous example, here there is scope for confounding bias. 

Consider again a study investigating the effects of mediation on well-being. This time suppose that Buddha-like detachment affects attrition. As such, the investigators will observe a restricted range of effects in the sample at the end of the study compared to the sample at the start of the study.  The structure of bias in this example is not one of measurement error.  Rather, there is a confounding of the per-protocal effect of meditation on well-being.  Note that the 'intent-to-treat' effect would be unbiased if the randomisation were successful.


The results obtained from this study would be distorted but they would not necessarily be **very weird** if the treatment and the outcome shared the same sign and we assume distributional monotonicity for the effect of $A$ on $Y$. In that case, the effect of censoring on target population restriction would be reduced. Strictly speaking, however, results would be **weird**: (**w**rongly **e**stimated inferences due to **i**nappropriate **r**estriction and **d**istortion). 


### Example 5: Treatment effect and effect-modifiers differ in censored (restriction bias without confounding)


@tbl-terminologycensoring $\mathcal{G}_5$ represents a setting in which there is a true treatment effect, but the distribution of effect-modifiers -- variables that interact with the treatment -- differ among the sample at baseline and the sample at the end of study. Knowing nothing else, we might expect this setting to be standard.  Where measured variables are sufficient to predict attrition, that is, where missingness is at random, we can obtain valid estimates for a treatment effect by inverse probability of treatment weighting [@cole2008; @leyrat2021]. In this approach, the sample gives more weight to under-represented individuals owing to drop-out. As with missing data imputation, IPW with censoring weights also assumes that we can correctly model the missingness from the observed data [@shiba2021]. However, if missingness is not completely at random, then identification may be compromised [@tchetgen2017general; @malinsky2022semiparametric].   


Note that @tbl-terminologycensoring $\mathcal{G}_5$ closely resembles a measurement structure we have considered before, in **Part 1:** @tbl-terminologymeasurementerror $\mathcal{G}_2$  Replacing the unmeasured effect modifiers $\circledotted{F}$ and $U_{\Delta F}$ in 
@tbl-terminologycensoring $\mathcal{G}_5$ for $\circledotted{U_Y}$ in @tbl-terminologymeasurementerror $\mathcal{G}_2$ reveals that the unmeasured effect modification in the present setting can be viewed as an example of uncorrelated independent measurment error when there is a treatment effect (i.e. off the null.) 

Here there is no distortion. Effect estimates are valid for the end-of-study population. However the end-of-study sample population would be an inappropriate restriction of the target population (the sample at the start of study). Results here would be **weird**: (**w**rongly **e**stimated inferences due to **i**nappropriate **r**estriction and **d**istortion) because there is **i**nappropriate **r**estriction.


### Summary

In this section, we examined how right-censoring, or attrition, can lead to biased causal effect estimates. Example 5 is particularly important as it assumes no confounding but exhibits **i**nappropriate **r**estriction. When the distribution of variables that modify treatment effects differs between the sample population at the start and end of the study, the average treatment effects will likely differ, leading to biased estimates. To address this, researchers must ensure that the distribution of potential outcomes at the end of the study aligns with that of the target population. Techniques such as inverse probability weighting and multiple imputation can help mitigate this bias [refer to @bulbulia2024PRACTICAL].

Attrition is nearly inevitable. Before seeking ambitious samples that extend a species understanding, we must ensure our design is not **weird**: (**w**rongly **e**stimated inferences due to **i**nappropriate **r**estriction and **d**istortion).

Next we investigate target population restriction bias at the start of study (left-censoring), where structural motifs of measurement error bias reappear.

{{< pagebreak >}}


## Part 3: How Target Population Restriction Bias At The Start of Study Makes Your Causal Inferences weird (**w**rongly **e**stimated inferences due to **i**nappropriate **r**estriction and **d**istortion) {#id-sec-3}

In this part, we examine target-restriction bias that occurs at the start of a study. There are several failure modes. The source population from which participants are recruited might not align with the target population.  Even where there is such alignment the participants recruited into a study might not align with the source population. For simplicity, we will imagine the sample population at the start of study accurately aligns with the source population. Here, 'source bias' equates to 'target-restriction bias at the start of study.' What constitutes 'alignment'? We say the sample is unrestrictive of the target population if there are no differences between the sample and target population in the distribution of variables that modify treatment effects. Such alignment cannot be verifed with data (refer to [Appendix](#id-app-c)). 

### Target Population Restriction Bias at Baseline Can Be Collider-Restriction Bias

::: {#tbl-terminologyselectionrestrictionclassic}
```{=latex}
\terminologyselectionrestrictionclassic
```
Collider-Stratification bias at the start of a study ('M-bias')
:::

@tbl-terminologyselectionrestrictionclassic $\mathcal{G}_1$ illustrates an example of target population restriction bias at baseline in which there is collider-restriction bias.

Suppose investigators want to estimate the causal effects of regular physical activity, $A$, and heart health, $Y$, among adults visiting a network of community health centres for routine check-ups.

Suppose there are two unmeasured variables that affect selection into the study $S=1$

1. Health Awareness, $U1$, an unmeasured variable that influences both the probability of participating in the study, $\boxed{S = 1}$, and the probability of being physically active, $A$. Perhaps people with higher health awareness are more likely to (1) engage in physical activity and (2) participate in health-related studies.

2. Socioeconomic Status (SES), $U2$, an unmeasured variable that influences both the probability of participating in the study, $\boxed{S = 1}$, and heart health, $Y$. We assume that individuals with higher SES have better access to healthcare and are more likely to participate in health surveys; they also tend to have better heart health from healthy lifestyles: joining expensive gyms, juicing, long vacations, and the like.

As presented in @tbl-terminologyselectionrestrictionclassic $\mathcal{G}_1$, there is collider-restriction bias from conditioning on $S=1$:

1. **$U1$**: Because individuals with higher health awareness are more likely to be both physically active and participate in the study, the subsample over-represents physically active individuals. This overestimates the prevalence of physical activity, setting up a bias in overstating the potential benefits of physical activity on heart health in the general population.

2. **$U2$**: Because individuals with higher SES may have better heart health from SES-related factors, this opens a confounding path from physical activity and heart health through the selected sample, setting up the investigators for the potentially erroneous inference that physical activity has a greater positive impact on heart health than it actually does in the general population. The actual effect of physical activity on heart health in the general population might be less pronounced than observed.

It might seem that researchers would need to sample from the target population. However, by adjusting for health awareness or SES, or a proxy of either, researchers may block the open path. @tbl-terminologyselectionrestrictionclassic $ mathcalG_2$ presents this solution. However, this strategy will only provide an unbiased effect estimate for the population if either there is no causal effect for all strata of the selected sample (the sharp null hypothesis) or there are no interactions between the distribution of effect modifiers in the sample population and the target population. 

The next series of examples illustrate challenges to obtaining valid causal effect estimates in the presence of interactions.

### Target Population Restriction Bias at Baseline Without Collider-Restriction Bias at Baseline

::: {#tbl-terminologyselectionrestrictionbaseline}
```{=latex}
\terminologyselectionrestrictionbaseline
```
The association in the population of selected individuals differs from the causal association in the target population. Hernán calls this scenario 'selection bias off the null' [@hernán2017]. Lu et al. call this scenario 'Type 2 selection bias' [@lu2022]. We call this bias 'target population restriction bias at baseline'.
:::

### Problem 1: Target population is not WEIRD; sample population is WEIRD

@tbl-terminologyselectionrestrictionbaseline $\mathcal{G}_{1.1}$ presents a scenario with target population restriction bias at baseline. Again, when the sample we obtain at baseline differs from the target population in the distributions of variables that modify treatment effects, effect estimates may be biased, even in the absence of confounding bias. Results may be **weird** without being distorted from confounding bias.

Suppose investigators are interested in the effects of political campaigning but only sampled from their preferred political party. Results might distort results if the distribution of effect modifiers varied by party.  One such effect modifier might be 'party affiliation'.  I think that this valid worry animates the call for broader sampling in the human sciences. 

Note that we have encountered @tbl-terminologyselectionrestrictionbaseline $\mathcal{G}_{1.1}$ *twice* before. It is the same causal directed acyclic graph as we found in  @tbl-terminologycensoring $\mathcal{G}_5$.  As we did before, we may replacing the unmeasured effect modifiers $\circledotted{F}$ and $U_{\Delta F}$ for $\circledotted{U_Y}$ in @tbl-terminologymeasurementerror $\mathcal{G}_2$ and observe that we recover uncorrelated measurement error off the null (i.e. when there is a true treatment effect.)

The structural similarity suggests options might be easily overlooked. Where the distributions of treatment-effect modifiers are known and measured and where census (or other) weights are available for the distributions of effect modifiers in the target population, it may be possible to weight the sample to more closely approximate the target population parameters of interest  (refer to @stuart2015.)

Let $\widehat{ATE}_{target}$ denote the population average treatment effect for the target population. Let $\widehat{ATE}_{\text{restricted}}$ denote the average treatment effect at the end of treatment. Let $W$ denote a set of variables upon which the restricted and target populations structurally differ. We say that results *generalise* if we can ensure that:

$$
\widehat{ATE}_{target} = \widehat{ATE}_{restricted}
$$

or if there is a known function such that:

$$
ATE_{target} \approx f_W(ATE_{\text{restricted}}, W)
$$

In most cases, $f_W$ will be unknown, as it must account for potential heterogeneity of effects and unobserved sources of bias. For further discussion on this topic, see @imai2008misunderstandings; @cole2010generalizing; @stuart2018generalizability.

 @tbl-terminologyselectionrestrictionbaseline $\mathcal{G}_{1.2}$ provides a graphical representation of the solution.  
 
 Importantly, if there is considerable heterogeneity, then we might not know how to interpret the average treatment effect for the target population. In comparative research, we are generally interested in treatment heterogeneity. If we seek explicitly comparative models, we will need to ensure validity for every sample that we compare. If one stratum in the comparative study is **weird**: (**w**rongly **e**stimated inferences due to **i**nappropriate **r**estriction and **d**istortion), errors may propogate to the remainder of the comparative study. To understand such propogation we consider scenarios where explicit target population restriction at baseline through the use of clearly defined 'eligibility criteria' are desirable.

### Example 2: Target population is WEIRD; sample population is not WEIRD

@tbl-terminologyselectionrestrictionbaseline $\mathcal{G}_{2.1}$ presents a scenario where the source population does not meet eligibility criteria. Consider again the question of whether vasectomy affects a sense of meaning and purpose in life. Suppose further we want to evaluate effects in New Zealand among men over the age of 40 who have no prior history of vasectomy, and who are in relationships with heterosexual partners. The target population is a stratum of WEIRD -- WEIRDER-THAN-WEIRD we might say. We should not sample from young children, the elderly, and any who do not qualify. For many scientific questions, a narrow population is desirable. 

Note again @tbl-terminologyselectionrestrictionbaseline $\mathcal{G}_{2.1}$ is identical to @tbl-terminologycensoring $\mathcal{G}_5$ — right-censoring bias with effect modifiers in an otherwise unconfounded study. The structure is also similar to @tbl-terminologymeasurementerror $\mathcal{G}_2$ the problem is structurally that of uncorrelated measurement error ‘off the null.’ Where it is the defusion of the effect-modifiers that causes we may fix the measurement error by restricting the sample.


@tbl-terminologyselectionrestrictionbaseline $\mathcal{G}_{2.2}$ presents a solution. Ensure eligibility criteria are scientifically relevant and feasible. Sample from this eligible population. With caution, apply survey or other weights where these weights enable a closer approximation to the distributions of effect-modifiers in the target population.  Here, we avoid **weird**: (**w**rongly **e**stimated inferences due to **i**nappropriate **r**estriction and **d**istortion) by imposing greater restriction on what had been an inappropriately unrestricted target population. 


### Example 3: Correlated Measurement Error of Covariates and Outcome in the Absence of a Treatment Effect

@tbl-terminologyselectionrestrictionbaseline $\mathcal{G}_{3.1}$ considers the threats to target validity from correlated measurement errors in the target population arising from structured errors across heterogeneous stratums. For simplicity imagine the groups with structured errors are cultures. Even if the treatment is measured without error, multiple sources of error may lead to association without causation.

Suppose investigators plan a cross-cultural investigation to clarify the relationship between interventions on religious service attendance ($A$) and an outcome ($Y$) like charitable giving. They plan to obtain measures of covariates $L$ sufficient to control for confounding. Suppose the investigators observe religious attendance so that it is not measured with error [as did @shaver2021comparison], yet there is heterogeneity in the measurement of covariates $L$ and the outcome $Y$. For example, if charitable giving measures are included as baseline covariates in $L$, measurement errors at baseline will be correlated with outcome measures. Perhaps in certain cultures, religious service is under-reported (associated with witchcraft), while in others, it is over-reported. Suppose further that true covariates affect the treatment and outcome. As shown in @tbl-terminologyselectionrestrictionbaseline $\mathcal{G}_{3.1}$, multiple paths of bias are opened.

Moreover, because measurements are causally related to the phenomena they record, investigators cannot apply statistical tests to verify whether measures are recorded with error [@vanderweele2022; @vansteelandt2022]. Whether the phenomena that investigators hope to measure are functionally equivalent across cultural settings remains unknown, and can only be discovered slowly, through patient, careful work with local experts. Although big cross-cultural projects are preferred at certain science journals, including multiple cultures into a single analysis imposes considerable burdens on investigators. All sources of error must be evaluated -- and error from one culture can poison the wells of others at analysis. 

@tbl-terminologyselectionrestrictionbaseline $\mathcal{G}_{3.2}$ provides a sensible solution: restrict one's study to those cultures where causality can be identified. Democritus wrote, 'I would rather discover one cause than gain the kingdom of Persia' [@freeman1948ancilla]. Paraphrasing Democritus, we might say, 'I should rather discover one WEIRD cause than the kingdom of **weird** comparative research.'

### Example 4: Correlated Measurement Error of Effect-Modifiers for an Overly Ambitious Target Population

@tbl-terminologyselectionrestrictionbaseline $\mathcal{G}_{4.1}$ considers the threats to target validity from correlated measurement errors in the target population arising from structured errors linking measurements for the effect modifiers. Here, we discover a familiar structural bias of correlated measurement error bias @tbl-terminologymeasurementerror $\mathcal{G}_3$

Even if the treatment is randomised so that there are no open backdoor paths, and even if the treatment and outcome are measured without error, investigators will not be able to obtain valid estimates for treatment-effect heterogeneity from their data, nor will they be able to apply target-sample weights (such as census weights) to obtain valid estimates for the populations in which the measurement errors of effect modifiers are manifest.

@tbl-terminologyselectionrestrictionbaseline $\mathcal{G}_{4.2}$ suggests that where measures of effect modification are uncertain, it is best to consider settings in which the measurements are reliable — whether or not the settings are WEIRD. Moreover, in comparative settings where multiple cultures are measured, unless each is proven innocent of structural measurement error bias, it is generally best to report the results for each culture separately, without attempting comparisons. 


## Conclusions

In causal inference, we begin by specifying clear treatments and outcomes, defining the contrasts to be made for treatments at specific levels, and identifying a target population for whom results generalise. Although causal inference is gaining popularity, much work remains to address the threats posed by measurement error bias to causal inferences. These threats are particularly evident in the comparative human sciences. Here, we have considered how problems of target-population restriction at the end and beginning of a study can be approached as variations on the motifs of measurement-error bias. Using causal directed acyclic graphs, we have clarified the structural features of bias that recur in measurement-error bias, target population restriction bias at the start of study and target population restriction bias at the end of study.

We define any study that exhibits these biases as **weird** (**w**rongly **e**stimated inferences due to **i**nappropriate **r**estriction and **d**istortion).

It is laudable to seek species-level knowledge. However, before venturing into the gardens of human existence, accessible gardens must have been cultivated. A long shadow of measurement error casts its shade over every sampling design. However, the standard workflow for causal inference offers guidance for research design whether or not an the causal questions are comparative.

### Schematic Workflow For Avoiding weird Causal Inferences

We avoid **weird** (**w**rongly **e**stimated inferences due to **i**nappropriate **r**estriction and **d**istortion) inferences in comparative research in the same way that we avoid **weird** inferences in any research: 

1. **State a well-defined intervention.**
   Clearly define the treatment or exposure to ensure precise implementation and interpretation. This clarity helps standardise the analysis of interventions across different settings and times.

2. **State a well-defined outcome.**
   Specify the outcome measure so that a causal contrast is interpretable. 

3. **Clarify the target population.**
   Define the population to whom the results will generalise to ensure the relevance and applicability of the findings. 

4. **Ensure treatments to be compared satisfy causal consistency.**
   Verify that the treatments correspond to interpretable interventions (refer to @bulbulia2023).

5. **Evaluate whether treatment groups, conditional on measured covariates, are exchangeable.**
   Balancing confounding covariates across treatment levels ensures that differences between groups are 'ignorable', which is critical for internal validity.

6. **Check if the positivity assumption is satisfied.**
   Confirm that all individuals have a non-zero probability of receiving each treatment level, given their covariates. 

7. **Ensure that the measures relate to the scientific questions at hand.**
   Ensure that the data collected and the measures used directly relate to the research question. As part of this, evaluate structural features of measurement error bias. As we have considered, there are manifold possibilities for measurement error bias to obscure the phenomena under study.

8. **Consider strategies to ensure the study group measured at the end of the study represents the target population.**
   If the study population both at the beginning and end of treatment differs in the distribution of variables that modify the effect of a treatment on the outcome, the study will be biased when there is a treatment effect.

9. **Clearly communicate the reasoning, evidence, and decision-making that inform steps 1-8.**
   Provide transparent and thorough documentation of the causal inference process. This includes detailing the assumptions, methods, and decisions made throughout the study to support the validity of causal inference for the study population and the reliability of generalisations to the target population.

The demands in following this workflow in comparative research are more stringent because measurement error bias must be evaluated at every site to be compared. Heterogeneity in measurement error can open biasing paths, and the target populations may not be easily defined, sampled, or -- where the scientific question requires -- restricted. Methodologists broadly agree on these points, but they can easily forget them. We have considered how workflows for causal inference provide the essential preflight checklists needed for ambitious, effective, and safe comparative cultural research.


{{< pagebreak >}}

## Funding

This work is supported by a grant from the Templeton Religion Trust (TRT0418) and RSNZ Marsden 3721245, 20-UOA-123; RSNZ 19-UOO-090. I also received support from the Max Planck Institute for the Science of Human History. The Funders had no role in preparing the manuscript or the decision to publish it.


{{< pagebreak >}}

## References

::: {#refs}
:::

{{< pagebreak >}}


## Appendix A: Glossary {#id-app-a}


::: {#tbl-experiments}
```{=latex}
\glossaryTerms
```
Glossary
:::

{{< pagebreak >}}

## Appendix B: Generalisability and Transportability  {#id-app-b}

**Generalisability:** When a study sample is drawn randomly from the target population, we may generalise from the sample to the target population as follows:

Suppose we sample randomly from the target population, where:

-   $n_S$ denotes the size of the study sample $S$.
-   $N_T$ denotes the total size of the target population $T$.
-   $\widehat{ATE}_{n_S}$ denotes the estimated average treatment effect in the study sample $S$.
-   $ATE_{T}$ denotes the true average treatment effect in the target population $T$.

Assuming the rest of the causal inference workflow goes to plan (randomisation succeeds, there is no measurement error, no model misspecification, etc.), as the random sample size $n_S$ increases, the estimated treatment effect in the sample $S$ converges in probability to the true treatment effect in the target population $T$:

$$
\lim_{n_S \to N_T} P(|\widehat{ATE}_{n_S} - ATE_{T}| < \epsilon) = 1
$$

for any small positive value of $\epsilon$.

**Transportability:** We cannot directly generalise the findings When the study sample is not drawn from the target population. However, we can transport the estimated causal effect from the source population to the target population under certain assumptions. This involves adjusting for differences in the distributions of effect modifiers between the two populations. The closer the source population is to the target population, the more plausible the transportability assumptions are, and the less we need to rely on complex adjustment methods.

Suppose we have a study sample $n_S$ drawn from a source population $S$, and we want to estimate the average treatment effect in a target population $T$.

Define:

-   $\widehat{ATE}_{S}$ as the estimated average treatment effect in the source population $S$.
-   $\widehat{ATE}_{T}$ as the estimated average treatment effect in the target population $T$.
-   $f(n_S, R)$ as the mapping function that adjusts the estimated effect in the study sample using a set of measured covariates $R$, allowing for valid projection to the target population.

The transportability assumption is that there exists a function $f$ such that:

$$
\widehat{ATE}_{T} = f(n_S, R)
$$

Finding a suitable function $f$ is the central challenge in adjusting for sampling bias and achieving transportability [@bareinboim2013general; @westreich2017transportability; @dahabreh2019generalizing; @deffner2022].


{{< pagebreak >}}

## Appendix C: Explanation for the Difference in Marginal Effects between Censored and Uncensored Populations  {#id-app-c}

#### Definitions:

- **$A$**: Exposure variable
- **$Y$**: Outcome variable
- **$F$**: Effect modifier
- **$C$**: Indicator for the uncensored population ($C = 0$) or the censored population ($C = 1$)

#### Average Treatment Effects:

The average treatment effects for the uncensored and censored populations are defined as:

$$
\Delta_{\text{uncensored}} = \mathbb{E}[Y(a^*) - Y(a) \mid C = 0]
$$
$$
\Delta_{\text{censored}} = \mathbb{E}[Y(a^*) - Y(a) \mid C = 1]
$$

#### Potential Outcomes:

By causal consistency, potential outcomes can be expressed in terms of observed outcomes:

$$
\Delta_{\text{uncensored}} = \mathbb{E}[Y \mid A=a^*, C=0] - \mathbb{E}[Y \mid A=a, C=0]
$$
$$
\Delta_{\text{censored}} = \mathbb{E}[Y \mid A=a^*, C=1] - \mathbb{E}[Y \mid A=a, C=1]
$$

#### Law of Total Probability:

Applying the Law of Total Probability, we can weight the average treatment effects by the conditional probability of the effect modifier $F$:

$$
\Delta_{\text{uncensored}} = \sum_{f} \left\{\mathbb{E}[Y \mid A=a^*, F=f, C=0] - \mathbb{E}[Y \mid A=a, F=f, C=0]\right\} \times \Pr(F=f \mid C=0)
$$
$$
\Delta_{\text{censored}} = \sum_{f} \left\{\mathbb{E}[Y \mid A=a^*, F=f, C=1] - \mathbb{E}[Y \mid A=a, F=f, C=1]\right\} \times \Pr(F=f \mid C=1)
$$

#### Assumption of Informative Censoring:

We assume that the effect modifier $F$ has a different distribution in the censored and uncensored populations:

$$
\Pr(F=f \mid C=0) \neq \Pr(F=f \mid C=1)
$$

Under this assumption, the probability weights used to calculate the marginal effects for the uncensored and censored populations differ.

#### Effect Estimates for Censored and Uncensored Populations:

Given that $\Pr(F=f \mid C=0) \neq \Pr(F=f \mid C=1)$, we cannot guarantee that:

$$
\Delta_{\text{uncensored}} = \Delta_{\text{censored}}
$$

The equality of marginal effects between the two populations will only hold if there is a universal null effect across all units, by chance, or under specific conditions discussed by @vanderweele2007 and further elucidated by @suzuki2013counterfactual. Otherwise:

$$
\Delta_{\text{uncensored}} \ne \Delta_{\text{censored}}
$$

Furthermore, @vanderweele2012 proved that if there is effect modification of $A$ by $F$, there will be a difference in at least one scale of causal contrast, such that:

$$
\Delta^{\text{risk ratio}}_{\text{uncensored}} \ne \Delta^{\text{risk ratio}}_{\text{censored}}
$$

or

$$
\Delta^{\text{difference}}_{\text{uncensored}} \ne \Delta^{\text{difference}}_{\text{censored}}
$$

For comprehensive discussions on sampling and inference, refer to @dahabreh2019 and @dahabreh2021study.

{{< pagebreak >}}

## Appendix D: R Simulation to Clarify Why The Distribution of Effect Modifiers Matters For Estimating Treatment Effects For A Target Population  {#id-app-d}

First, we load the `stdReg` library, which obtains marginal effect estimates by simulating counterfactuals under different levels of treatment [@sjölander2016]. If a treatment is continuous, the levels can be specified.

We also load the `parameters` library, which creates nice tables [@parameters2020].

```{r}
# to obtain marginal effects
if (!requireNamespace('stdReg', quietly = TRUE)) install.packages('stdReg')
library(stdReg)

#  to view data
if (!requireNamespace('skimr', quietly = TRUE)) install.packages('skimr')
library(parameters)

# to create nice tables
if (!requireNamespace('parameters', quietly = TRUE)) install.packages('parameters')
library(parameters)
```

Next, we write a function to simulate data for the sample and target populations.

We assume the treatment effect is the same in the sample and target populations, that the coefficient for the effect modifier and the coefficient for interaction are the same, that there is no unmeasured confounding throughout the study, and that there is only selective attrition of one effect modifier such that the baseline population differs from the sample population at the end of the study.

That is: **the distribution of effect modifiers is the only respect in which the sample will differ from the target population.**

This function will generate data under a range of scenarios.[^margot]

[^margot]: See documentation in the `margot` package: @margot2024

```{r}
# function to generate data for the sample and population, 
# along with precise sample weights for the population, there are differences 
# in the distribution of the true effect modifier but no differences in the treatment effect 
# or the effect modification. all that differs between the sample and the population is 
# the distribution of effect-modifiers.

# reproducibility
set.seed(123)

# simulate the data -- you can use different parameters
data <- margot::simulate_ate_data_with_weights(
  n_sample = 10000,
  n_population = 100000,
  p_z_sample = 0.1,
  p_z_population = 0.5,
  beta_a = 1,
  beta_z = 2.5,
  noise_sd = 0.5
)

# inspect
# skimr::skim(data)
```

We have generated both sample and population data.

Next, we verify that the distributions of effect modifiers differ in the sample and in the target population:

```{r}
# obtain the generated data
sample_data <- data$sample_data
population_data <- data$population_data

# check imbalance
table(sample_data$z_sample) # type 1 is rare
table(population_data$z_population) # type 1 is common
```

The sample and population distributions differ.

Next, consider the question: 'What are the differences in the coefficients that we obtain from the study population at the end of the study, compared with those we would obtain for the target population?'

First, we obtain the regression coefficients for the sample. They are as follows:

```{r}
# model coefficients sample
model_sample  <- glm(y_sample ~ a_sample * z_sample, 
  data = sample_data)

# summary
parameters::model_parameters(model_sample, ci_method = 'wald')
```

Next, we obtain the regression coefficients for the weighted regression of the sample. Notice that the coefficients are virtually the same:

```{r}
# model the sample weighted to the population, again note that these coefficients are similar 
model_weighted_sample <- glm(y_sample ~ a_sample * z_sample, 
  data = sample_data, weights = weights)

# summary
summary(parameters::model_parameters(model_weighted_sample, 
  ci_method = 'wald'))
```

We might be tempted to infer that weighting wasn't relevant to the analysis. However, we'll see that such an interpretation would be a mistake.

Next, we obtain model coefficients for the population. Note again there is no difference -- only narrower errors owing to the large sample size.

```{r}
# model coefficients population -- note that these coefficients are very similar. 
model_population <- glm(y_population ~ a_population * z_population, 
  data = population_data)

parameters::model_parameters(model_population, ci_method = 'wald')
```

Again, there is no difference. That is, we find that all model coefficients are practically equivalent. The different distribution of effect modifiers does not result in different coefficient values for the treatment effect, the effect-modifier 'effect,' or the interaction of the effect modifier and treatment.

Consider why this is the case: in a large sample where the causal effects are invariant -- as we have simulated them to be -- we will have good replication in the effect modifiers within the sample, so our statistical model can recover the *coefficients* for the population without challenge.

However, *in causal inference, we are interested in the marginal effect of the treatment. That is, we seek an estimate for the counterfactual *contrast* in which everyone in a pre-specified population was subject to one level of treatment compared with a counterfactual condition in which everyone in a population was subject to another level of the same treatment.*

**The marginal effect estimates will typically differ When the sample population differs in the distribution of effect modifiers from the target population effect.**

To see this, we use the `stdReg` package to recover marginal effect estimates, comparing (1) the sample ATE, (2) the true oracle ATE for the population, and (3) the weighted sample ATE. We will use the outputs of the same models above. The only difference is that we will calculate marginal effects from these outputs. We will contrast a difference from an intervention in which everyone receives treatment = 0 with one in which everyone receives treatment = 1; however, this choice is arbitrary, and the general lessons apply irrespective of the estimand.

First, consider this Average Treatment Effect for the sample population:

```{r}
# What inference do we draw?  
# we cannot say the models are unbiased for the marginal effect estimates. 
# regression standardisation 
library(stdReg) # to obtain marginal effects 

# obtain sample ate
fit_std_sample <- stdReg::stdGlm(model_sample, 
  data = sample_data, X = 'a_sample')

# summary
summary(fit_std_sample, contrast = 'difference', reference = 0)
```

The treatment effect is given as a 1.06 unit change in the outcome across the sample population, with a confidence interval from 1.04 to 1.08.

Next, we obtain the true (oracle) treatment effect for the population under the same intervention:

```{r}
## note the population effect is different

# obtain true ate
fit_std_population <- stdReg::stdGlm(model_population, 
  data = population_data, X = 'a_population')

# summary
summary(fit_std_population, contrast = 'difference', reference = 0)
```

Note, the true treatment effect is a 1.25 unit change in the population, with a confidence bound between 1.24 and 1.26. This is well outside the ATE that we obtain from the sample population!

Next, consider the ATE in the weighted regression, where the sample was weighted to the target population's true distribution of effect modifiers:

```{r}
## next try weights adjusted ate where we correctly assign population weights to the sample
fit_std_weighted_sample_weights <- stdReg::stdGlm(model_weighted_sample, 
  data = sample_data, X = 'a_sample')

# this gives us the right answer
summary(fit_std_weighted_sample_weights, contrast = 'difference', reference = 0)
```

We find that we obtain the population-level causal effect estimate with accurate coverage by weighting the sample to the target population. So with appropriate weights, our results generalise from the sample to the target population.

## Lessons

- **Regression coefficients do not clarify the problem of sample/target population mismatch** — or selection bias as discussed in this manuscript.
- **Investigators should not rely on regression coefficients alone** when evaluating the biases that arise from sample attrition. This advice applies to both methods that authors use to investigate threats of bias. To implement this advice, authors must first take it themselves.
- **Observed data are generally insufficient for assessing threats**. Observed data do not clarify structural sources of bias, nor do they clarify effect-modification in the full counterfactual data condition where all receive the treatment and all do not receive the treatment (at the same level).
- **To properly assess bias, one needs access to the counterfactual outcome** — what would have happened to the missing participants had they not been lost to follow-up or had they responded. The joint distributions over 'full data' are inherently unobservable [@vanderlaan2011].
- **In simple settings, like the one we just simulated, we can address the gap between the sample and target population using methods such as modelling the censoring (e.g., censoring weighting).** However, we never know what setting we are in or whether it is simple—such modelling must be handled carefully. There is a large and growing epidemiology literature on this topic (see, for example, @li2023non).


