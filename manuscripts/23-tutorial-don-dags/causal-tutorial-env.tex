% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames,x11names}{xcolor}
%
\documentclass[
  singlecolumn]{article}

\usepackage{amsmath,amssymb}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage[]{libertinus}
\ifPDFTeX\else  
    % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\usepackage[top=30mm,left=20mm,heightrounded]{geometry}
\setlength{\emergencystretch}{3em} % prevent overfull lines
\setcounter{secnumdepth}{-\maxdimen} % remove section numbering
% Make \paragraph and \subparagraph free-standing
\ifx\paragraph\undefined\else
  \let\oldparagraph\paragraph
  \renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
  \let\oldsubparagraph\subparagraph
  \renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi


\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
% definitions for citeproc citations
\NewDocumentCommand\citeproctext{}{}
\NewDocumentCommand\citeproc{mm}{%
  \begingroup\def\citeproctext{#2}\cite{#1}\endgroup}
\makeatletter
 % allow citations to break across lines
 \let\@cite@ofmt\@firstofone
 % avoid brackets around text for \cite:
 \def\@biblabel#1{}
 \def\@cite#1#2{{#1\if@tempswa , #2\fi}}
\makeatother
\newlength{\cslhangindent}
\setlength{\cslhangindent}{1.5em}
\newlength{\csllabelwidth}
\setlength{\csllabelwidth}{3em}
\newenvironment{CSLReferences}[2] % #1 hanging-indent, #2 entry-spacing
 {\begin{list}{}{%
  \setlength{\itemindent}{0pt}
  \setlength{\leftmargin}{0pt}
  \setlength{\parsep}{0pt}
  % turn on hanging indent if param 1 is 1
  \ifodd #1
   \setlength{\leftmargin}{\cslhangindent}
   \setlength{\itemindent}{-1\cslhangindent}
  \fi
  % set entry spacing
  \setlength{\itemsep}{#2\baselineskip}}}
 {\end{list}}
\usepackage{calc}
\newcommand{\CSLBlock}[1]{\hfill\break\parbox[t]{\linewidth}{\strut\ignorespaces#1\strut}}
\newcommand{\CSLLeftMargin}[1]{\parbox[t]{\csllabelwidth}{\strut#1\strut}}
\newcommand{\CSLRightInline}[1]{\parbox[t]{\linewidth - \csllabelwidth}{\strut#1\strut}}
\newcommand{\CSLIndent}[1]{\hspace{\cslhangindent}#1}

\input{/Users/joseph/GIT/templates/latex/custom-commands.tex}
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\AtBeginDocument{%
\ifdefined\contentsname
  \renewcommand*\contentsname{Table of contents}
\else
  \newcommand\contentsname{Table of contents}
\fi
\ifdefined\listfigurename
  \renewcommand*\listfigurename{List of Figures}
\else
  \newcommand\listfigurename{List of Figures}
\fi
\ifdefined\listtablename
  \renewcommand*\listtablename{List of Tables}
\else
  \newcommand\listtablename{List of Tables}
\fi
\ifdefined\figurename
  \renewcommand*\figurename{Figure}
\else
  \newcommand\figurename{Figure}
\fi
\ifdefined\tablename
  \renewcommand*\tablename{Table}
\else
  \newcommand\tablename{Table}
\fi
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Listing}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}
\makeatother
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\@ifpackageloaded{subcaption}{}{\usepackage{subcaption}}
\makeatother
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\usepackage{bookmark}

\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same} % disable monospaced font for URLs
\hypersetup{
  pdftitle={Causal Inference in Environmental Psychology},
  pdfkeywords={DAGS, Causal
Inference, Confounding, Environmental, Psychology, Panel},
  colorlinks=true,
  linkcolor={blue},
  filecolor={Maroon},
  citecolor={Blue},
  urlcolor={Blue},
  pdfcreator={LaTeX via pandoc}}

\title{Causal Inference in Environmental Psychology}
\author{Donald W Hine \and Joseph A. Bulbulia}
\date{2024-02-01}

\begin{document}
\maketitle
\begin{abstract}
Quantifying causal effects from observational data presents substantial
challenges for environmental psychology, however, recent progress in
methods for causal inference offers hope. In this chapter, we provide an
accessible introduction Causal Diagrams, also known as Directed Acyclic
Graphs (DAGs), which are powerful tools for evaluating evidence for
causality in observational settings. We start with a conceptual
introduction to the basic framework of causal inference within which
causal diagrams find there utility. This non-mathematical introduction
helps to build intuition for what is needed for obtaining unbiased
causal effect estimates. We next offer a primer on how to construct
causal diagrams, focussing on the four elemental causal diagrams from
which more complicated causal diagrams are built. We conclude with
practical guidelines for data collection and modelling, highlighting
``tips'' and ``common pitfalls.'' This chapter will interest
environmental psychology researchers who wish to draw stronger causal
inferences from data derived from non-experimental research designs.
\end{abstract}

\subsection{Introduction}\label{introduction}

Causal inference involves using empirical observations to draw
conclusions about the causal relationships between variables or events.
It typically involves determining whether changes in one or more
independent variables are linked to changes in one or more dependent
variables, while controlling for potential sources of confounding. The
aim of causal inference is to identify cause-effect relationships with a
high level of confidence, understanding the mechanisms underlying these
relationships, while ruling out plausible alternative explanations
(\citeproc{ref-cook2002experimental}{Cook \emph{et al.} 2002})

Causal inference is sometimes characterised as a complex cognitive
process unique to humans. However, it may be more appropriate to think
of causal inference as existing throughout the plant and animal kingdoms
on a spectrum, or potentially multiple spectra, that vary in terms
conscious awareness and sophistication
(\citeproc{ref-mancuso2018revolutionary}{Mancuso 2018}). For example,
plants and microorganisms are genetically preprogrammed to recognise and
respond to environmental conditions relevant to reproduction and
survival. Birds and rodents have well-established capacities to learn,
but not to the level of complexity exhibited by larger mammals and
primates. During the past several thousand years, humans have developed
a broad range for frameworks for causal inference such as the law of
karma in Hindu, Jain, and Buddhist traditions, Aristotle's four
(material, formal, efficient, and final) causes, early work empiricism
and experimentation by Francis Bacon and John Locke, Newtonian
mechanics, as well as more contemporary conceptualisations based on
counterfactual reasoning such as those described in this chapter.

\subsection{Correlation in Psychological
Research}\label{correlation-in-psychological-research}

As taught in most introductory undergraduate psychology courses,
correlations between two more variables does not imply the presence of a
causal process. However, in most psychological research, quantifying a
causal effect requires drawing inferences based on research design,
statistical modelling, and correlations present in data.

In experimental settings, quantitative causal inferences from
associational models are made possible through randomisation, controlled
interventions, and chronologically ordered data collection. For example,
even in non-longitudinal experimental studies, manipulations always
precede the measurement of outcomes. By contrast, in observational
settings, the data researchers collect are not the products of
randomisation and control. Moreover, observational researchers are often
limited to cross-sectional data, which lack temporal order. In
observational settings, then, credible causal inference from
correlations can be, at best, indirect. The problems of inferring
causation in observational settings are widely known. For example, in
cross-sectional studies, correlations between two variables may be due
to the presence of a third variable that causes changes in one or both.
Furthermore, variables can be correlated, but the direction of causation
can be ambiguous. Variable A may cause variable B? Or B may cause A? In
the absence of an appropriate research design is may be impossible to
tell.

Despite this, many observational researchers persist in using hedging
causal language to reporting their results, offering cautious policy
recommendations couched in words such as ``might effect'', ``suggestive
of a causal link'', and ``is consistent with our hypothesis that A
causes B'' (\citeproc{ref-bulbulia2022}{Bulbulia 2022}). This is true of
our own work, and also that of many of colleagues in environmental
psychology. However, because correlations can be caused by unmeasured
variables and true causal effects, when they exist, can run in the
opposite direction of the proposed causal model, researchers need to be
extremely cautious when inferring causal conclusions.

Are there any settings in which we might we obtain valid causal
inferences from observational data? If so, by which pathways might we
pursue such inferences?

Environmental psychologists have strong motivations to ask these
questions. Among the most compelling is an interest in understanding the
possible effects of interventions on environmental attitudes and
behaviours, which is essential for offering effective policy advice.
Moreover, although randomised experiments can sometimes afford causal
understanding, experimental studies are often expensive to perform at
scale, and random assignment to treatment conditions can pose
unacceptable ethical risks. Designing and implementing interventions and
then tracking their effects over long periods are typically infeasible,
and often decisions cannot wait. Experimental control often comes at the
price of ecological validity, limiting direct applications in
non-laboratory contexts. {[}Although see MIT Poverty Action lab for
examples for examples of large scale RCTs.{]} For these reasons, the
bandwidth of psychologically interesting and practically relevant
questions that experiments may address is limited. By contrast,
observational data is abundant. If we could obtain causal insights from
observational data, the state of knowledge in environmental psychology
would be greatly accelerated and the field would be better equipped to
offer advice. Considerable progress in the methods for estimating
causation from observational data during the past 20 years offer reasons
for hope (\citeproc{ref-vanderweele2015}{VanderWeele 2015}). Although
most of this progress has occurred outside of psychology, interest
amongst psychological scientists is growing
(\citeproc{ref-mcelreath2020}{McElreath 2020};
\citeproc{ref-rohrer2018}{Rohrer 2018}). Efforts to unite methods for
causal inference with observational environmental psychology research is
both promising and timely.

We believe that rate of progress in the uptake of causal inference in
environmental psychology (and in psychological science more broadly)
will turn on how rapidly researchers understand the theoretical basis on
which methods for causal inference rely. The theoretical basis of causal
effect estimation is vital because to obtain causal effect estimates
from observational data relies on assumptions. That is, before
researchers apply statistical methods to data, they must devote
considerable attention to planning and design than is their custom.
Whether the assumptions required for causal inference are credibly met
will typically require the input of experts familiar with the questions
at hand. In some cases the data allow reasoned guesses about whether
assumptions have been satisfied. However such cases are the exception
are rare. That causal inference relies on untestable assumptions invites
many risks. On the one hand, if we persist in demanding verification
from data researchers obtained from conventional thresholds they risk
missing causal inferences that are supported by reasonable assumptions.
On the other hand, researchers might be tempted to admit implausible
assumptions without warrant. This latter risk is particularly worrying
in those social sciences where publication biases favour compelling
narratives couched in causal language, often with a one or two sentence
caveat about strong causal attributions at the end of the paper. Too few
journals are interested in studies that report uncertainty, or that
provide detailed accounts of methodological limitations. . We believe
that to accelerate progress in environmental psychology, and in the
human sciences more generally, the theoretical foundations of causal
inference must be accessible and transparently communicated. The
assumptions that underpin causal inference must be explained in a way
that builds a clear understanding about the underlying problem each
assumption addresses. Researchers must understand how these assumptions
flow on to the tasks of data collection and data organisation. For
causal inference to bring about the scientific transformation, each
practical step from assumption to data collection must also be grounded
in scientifically supported assumptions. Because the theoretical
foundations of causal inference remain unfamilar and opaque to most
psychological scientists, including those with strong backgrounds in
statistical modelling, we hope this introduction outlining the
foundations of causal inference from association data will encourage
more environmental psychologists to adopt these new conceptual
frameworks and methods.

\subsection{Overview}\label{overview}

\textbf{Part 1} introduces the ``potential outcomes'' framework of
causal inference (\citeproc{ref-hernan2023}{Hernan and Robins 2023}),
and describes the three fundamental assumptions of causal inference.
Here, we develop causal intuitions by anchoring them in the framework of
randomised experiments, which will be familiar to most readers. We shall
discover that the building intuitions for causal inference from an
understanding of experiments help demystify the assumptions on which
causal inference relies and offers clear directives for data collection.
Surprisingly, perhaps, this discussion will bring new theoretical
understanding of experiments that may be helpful to improving their
designs.

\textbf{Part 2} introduces Directed Acyclic Graphs (DAGs) -- or causal
diagrams -- as powerful tools for investigating the implications of
causal assumptions. We note that the functionality of causal diagrams
rests on three rules, and that these three rules, in turn, rest on a
robust system of mathematical proofs. Although these proofs should
inspire confidence, it will be come clear that causal inference
typically relies on strong assumptions about how causation operates in
the world. Indeed a causal diagram typically assumes ever causal
relationship except the specific causal effect to be evaluated. Thus,
not only does causal inference rely on the three general fundamental
assumptions described in part 1, causal inference inevitably relies on a
considerable scientific assumptions. For this reason, the assumptions
encoded in a causal diagram are sometimes called `structural
assumptions.' Of course, not all scientists agree about the world.
However, causal diagrams are tools by which scientists can make their
disagreements explicit. Moreover, different causal diagrams can be
constructed to reflect different assumptions. In some cases, points of
disagreement may be irrelevant to the question at hand, either because
the valid causal estimates may be obtained regardless of where one
stands, or because confounding is unavoidable. Of course the magnitude
of the causal effect under consideration -- also called the `treatment
effect' or the `exposure effect' -- cannot be assumed. If it could,
there would be no point to conducting one's study. Causal diagrams are
qualitative tools that transform difficult mathematical and statistical
problems into problems that can be readily understood by visual
inspection of the graph. Although we cannot hope to cover the full power
of causal diagrams to assist researchers in investigating causal
questions, we offer basic strategies for constructing and interpreting
causal diagrams that will be suitable for many question that arise in
environmental psychology.

\textbf{Part 3} experiments are the gold standard for quantifying the
causal effects of interventions. To investigate causality, environmental
psychologists should use experiments wherever doing so is possible.
Unfortunately it is impractical or unethical to perform experiments for
many fundamental questions in environmental psychology. For this reason,
questions about whether environmental psychologists should use
experiments are often framed as questions about internal and external
validity. In Part 3, we use causal diagrams to examine how threats to
internal validity arise for experimental designs irrespective of
external validity. Widespread confusion about confounding in experiments
is particularly worrying when experimentals assume that randomisation
studies in large samples avoid problems of confounding. Here, we
demonstrate how methods for causal inference in observational settings
are very much relevant for experimentalists. We hope to provide readers
of this volume with an understanding of why methods for causal should be
routinely taught as part of experimental methodology courses.

\subsection{Part 1: Introducing the Potential Outcomes Framework of
Causal
Inference}\label{part-1-introducing-the-potential-outcomes-framework-of-causal-inference}

The potential outcomes framework was introduced by Jerzy Neyman in his
work on agricultural experiments and their effectiveness
(\citeproc{ref-neyman1923}{Neyman 1923}). Half a century later, Harvard
statistician Donald Rubin expanded this framework for causal inference
in non-experimental contexts (\citeproc{ref-rubin1976}{Rubin 1976}).
Later, Jamie Robins further developed it to address confounding in
scenarios with multiple treatments and time-varying factors
(\citeproc{ref-robins1986}{Robins 1986}). A key concept in this
framework is `counterfactual outcomes.''

To understand the role of counterfactual outcomes in causal inference,
let's use a personal crossroads scenario. Suppose you are finishing
university and deciding between graduate school and an industry job. You
get accepted into your dream graduate program at the University of
Canterbury and start planning your move to Christchurch, New Zealand.
Then, you receive a job offer from Acme Nuclear Fuels, a company
innovating in energy alternatives to fossil fuels. Each choice leads to
vastly different life trajectories --- in daily routines, income, social
interactions, romantic interests, and possibly your overall sense of
purpose.

Formally, let \(A\) signify the decision to attend graduate school
(\(A = 1\)) or work in industry (\(A = 0\)). The potential outcomes are
\(Y_{\text{you}}(1)\) and \(Y_{\text{you}}(0)\), representing the
hypothetical life scenarios under each choice. To assess the causal
effect of your decision, we consider the difference
\(Y_{\text{you}}(1) - Y_{\text{you}}(0)\), although this difference is
inherently unobservable. Once a decision is made, it is impossible to
know with certainty the life path not taken.

\[
(Y_{\text{you}}|A_{\text{you}} = 1) = Y_{\text{you}}(1) \quad \text{implies} \quad Y_{\text{you}}(0)|A_{\text{you}} = 1~ \text{is counterfactual}.
\]

and vice versa for the opposite treatment when you receive \(A = 0\).

This scenario exemplifies `the fundamental problem of causal inference'
as identified by Rubin and Holland (\citeproc{ref-holland1986}{Holland
1986}; \citeproc{ref-rubin2005}{Rubin 2005}): we cannot observe both
potential outcomes for the same individual.

\subsubsection{Understanding Relationships of Cause and Effect Through
Intervention
Outcomes}\label{understanding-relationships-of-cause-and-effect-through-intervention-outcomes}

Supposwe we want to examine the causal effect of easy access to urban
green spaces on psychological well-being
(\citeproc{ref-nguyen2021green}{Nguyen \emph{et al.} 2021};
\citeproc{ref-reyes2021linking}{Reyes-Riveros \emph{et al.} 2021}).
Let's focus on `subjective happiness,' hereafter referred to as
`happiness.'' Assume happiness exists and is measurable. We will
represent happiness as \(Y\), which denotes the outcome under
consideration.

For simplicity, we will consider the intervention or treatment, `ample
access to green space,' to be a binary variable. Our points will
generalise to continuous treatments, so this simplification does not
come at the loss of generality. Denote this treatment with the letter
\(A\). Define \(A = 1\) as `having ample access to greenspace' and
\(A = 0\) as `lacking ample access to greenspace.' Assume these
conditions are mutually exclusive. Our target population is New
Zealanders in the 2020s. A preliminary causal question might be:

`In New Zealand, does proximity to abundant green spaces increase
self-perceived happiness compared to environments lacking such spaces?''

Next, suppose that it were ethical to experimentally manipulate and
randomise individuals into different treatment conditions. We assume the
fundamental problem of causal inference abides. Once an individual is
assigned to one treatment condition, we cannot observe that individual's
outcome for the treatment condition that was not assigned. How might
experiments help us to identify causality?

The first point to notice is that in the context of causal inference,
even well-designed experiments face the challenge of missing values in
the potential outcomes. Indeed this observation formed the basis of
Jerzy Neyman's interest in how exactly agricultural experiments permit
causal inferences (\citeproc{ref-neyman1923}{Neyman 1923}). This issue
is inherent to the nature of causal inference where, for each
individual, we can only observe one of the potential outcomes. The
fundamental problem of causal inference applies to every individual who
assigned to one condition or another.

To elaborate, let us break down the Average Treatment Effect (ATE) in
the context of an experiment according to the potential outcomes that
are actually observed. By average treatment effect we mean a contrast
between the average effect (for the sampled population) in one treatment
condition with the average treatment effect (for the sampled population)
in a well-defined contrast condition. The ATE is the causal effect that
experiments hope to recover. Recall that we are interested in potential
outcomes, and we use the notiation \(Y(a)\) to denote the outcome under
a specific condition, for example \(Y(A=1)\) or \(Y(A=0)\). We assume
that when a person receives a treatment their potential outcome for the
treatment received is observed, that is,
\(Y_{\text{i}}|A_i = a) = Y_{\text{a}}(1)\) for all individuals
\(i\dots N\) in the study.

Breaking down the ATE into observed and unobserved outcomes gives us:

\[
\text{Average Treatment Effect} = \left(\underbrace{\underbrace{\mathbb{E}[Y(1)|A = 1]}_{\text{observed for } A = 1} + \underbrace{\mathbb{E}[Y(1)|A = 0]}_{\text{unobserved for } A = 0}}_{\text{effect among treated}}\right) - \left(\underbrace{\underbrace{\mathbb{E}[Y(0)|A = 0]}_{\text{observed for } A = 0} + \underbrace{\mathbb{E}[Y(0)|A = 1]}_{\text{unobserved for } A = 1}}_{\text{effect among untreated}}\right).
\]

In this expression, \(\mathbb{E}[Y(1)|A = 1]\) represents the average
outcome when the treatment is given, which is observable. Conversely,
\(\mathbb{E}[Y(1)|A = 0]\) represents the average outcome if the
treatment had been given to those who were actually untreated, which
remains unobservable. This duality is also true for the control
condition, creating a situation where half of the necessary values for
calculating the ATE are inherently missing. Another way of putting this
is that the fundamental problem of causal inference lurks in the
background of experiments. For each individual we cannot know how the
intervention would have turned out had they received the alternative
treatment to what they in fact received, any more than you can
quantiatively deterime how life would have turned out had you decided to
take the job at Acme Nuclear Fuels after deciding in fact to attend the
University of Canturbury.

To understand how experiments magically recover the missing observations
needed to compute average treatment effects we need to consider the
concept of a `confounder.'

\paragraph{Understanding Confounders in Causal
Inference}\label{understanding-confounders-in-causal-inference}

Causal inference encompasses various types of confounders, which we will
explore in Part 2. For now, let us focus on a specific type of
confounder critical for understanding how experiments achieve unbiased
average treatment effects. This type is a common cause of both the
treatment (\(A\)) and the outcome (\(Y\)), often referred to in
psychology as the `third variable.' It is the focal point of `control'
strategies.

For instance, when we `control for' income in an analysis, we aim to
address the possibility that income may simultaneously influence both
the intervention and the outcome. By doing so, we strive to isolate the
effect of the intervention from the potential confounding influence of
income.

Put simply, a confounder is a variable that simultaneously affects the
treatment assignment and the outcome. Its presence can lead to biased
estimates of causal effects, as it introduces variations in the outcome
not directly attributable to the treatment. Understanding and
controlling for confounders is therefore crucial for accurate causal
inference.

\paragraph{Understanding the balance of confounders in
experiments}\label{understanding-the-balance-of-confounders-in-experiments}

In experimental designs the random assignment of treatment is intended
to create balance in confounders across treatment groups. Balance means
that the distribution of all confounders is, on average, the same in
both the treatment and control groups. This balance allows us to assume
that the only systematic difference between these groups is the
treatment itself.

Let us denote the set of all possible confounders by the letter \(L\).
In causal inference, we can express the absence of confounding in one of
two equivalent ways. `Balance,'' in this context, means that the
distribution of all potential confounding variables, collectively
denoted as \(L\), is essentially the same across both the treatment and
control groups. This uniformity ensures that any differences observed
between these groups can be attributed solely to the treatment and not
to underlying differences in the participants.

\textbf{Expressing the absence of confounding}: In causal inference, we
can articulate the absence of confounding in two equivalent ways:

\begin{itemize}
\item
  Considering the potential outcomes given the treatment and
  confounders: \[
     Y(a) \coprod A \mid L
     \]
\item
  Or, looking at the treatment assignment given the potential outcomes
  and confounders: \[
     A \coprod Y(a) \mid L
     \]
\end{itemize}

Although this mathematical formalism my perhaps seem heavy handed, it
will be useful for better understand strategies in causal inference for
emulating the randomisation of experiments.

\textbf{What randomisation delivers}: in an ideal RCT, effective random
treatment assignment leads to balanced conditions across all variables
that might simultaneously influence both the receipt of treatment and
the outcome. The absence of confounding, achieved through this balance,
enables us to draw more reliable causal inferences.

Your points on the steps in establishing causal inference in a
Randomized Controlled Trial (RCT) are well-framed. Let's refine and
expand on them for enhanced clarity and completeness:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Exchangeability}: this principle asserts that within each
  treatment group, the potential outcomes are statistically independent
  of the treatment assignment, given the confounders. Simply put, this
  means that the treatment groups are comparable. Exchangeability is
  crucial because it ensures that any differences observed between
  groups are attributable to the treatment rather than pre-existing
  differences. In an RCT, randomisation facilitates this exchangeability
  by evenly distributing both observed and unobserved confounders across
  treatment groups.
\item
  \textbf{Consistency}: this principle asserts that there is an
  alignment between the observed outcome for an individual and the
  potential outcome under the treatment they actually received.
  Consistency is critical as it links the theoretical construct of
  potential outcomes to the actual observed data, enabling the practical
  application of causal inference methods.
\item
  \textbf{Positivity}: this principle states that there is a nonzero
  probability of receiving each treatment level within every subgroup
  defined by the covariates \(L\). This condition is crucial for
  estimating treatment effects across different levels of confounders
  and avoiding extrapolation beyond the observed data. This principle is
  often implicitly assumed a randomised experiment, however, as we shall
  see, positivity must be an explicit consideration in observational
  studies.
\end{enumerate}

These three principles are foundational in addressing the core challenge
of causal inference: making valid inferences about potential outcomes
that are not directly observable. In randomised experiments, provided
that randomisation is successful and the sample size is sufficiently
large, we can be confident that the average treatment effects observed
within the groups are representative of the population. It is important
to note that, while randomisation inherently supports exchangeability
and often positivity, these assumptions still need to be verified. We
will return to thse points in section 3. For now, we have seen that
``under the hood'', exchangeability consistency, and positivity, allow
experimental scientists to obtain valid \textbf{average treatment
effects} for interventions despite the fact that for every individual we
cannot directly compute individual treatment effects from data. Causal
inference at some level fundamentally involves reasoning about how
events would have turned out had the world been different to how it was
realised.

\paragraph{Causal Inference in Observational
Settings}\label{causal-inference-in-observational-settings}

In observational research, where the researcher does not have control
over the treatment assignment, the objective is to emulate a controlled
experimental environment as closely as possible. However, this emulation
presents unique challenges, particularly in defining and applying
treatments consistently across the study population.

\textbf{Variability in treatment definition}

Take, for example, the scenario of assessing the impact of living near
green spaces. The concept of `proximity to green spaces' itself varies
significantly, leading to a diverse range of experiences that can all be
classified under the same `treatment'. This variability can be broken
down into several factors:

\begin{itemize}
\item
  \textbf{Diversity of green spaces}: the biodiversity and aesthetic
  value of these spaces can vary widely. Some individuals might have
  access to well-maintained parks with a rich variety of flora and
  fauna, while others might only have access to basic recreational areas
  with limited natural appeal. This diversity means that what one person
  experiences as `access to green spaces' can be fundamentally different
  from another's experience.
\item
  \textbf{Availability of amenities}: the presence of amenities such as
  walking paths, benches, recreational facilities, and cafes can
  significantly enhance the experience of a green space. Such amenities
  can encourage more frequent and prolonged visits, potentially leading
  to greater benefits in terms of mental and physical well-being.
\item
  \textbf{Variation in proximity}: the distance to the nearest green
  space can also influence how often and how easily individuals can
  access these areas. For someone living right next to a park, the green
  space might be an integral part of their daily routine. In contrast,
  for someone living several kilometres away, visits might be infrequent
  and less impactful.
\item
  \textbf{Size and type of green space}: the type of green space (e.g.,
  urban park, community garden, or large forested area) and its size can
  also affect the nature of the interaction individuals have with it.
  Larger, more naturalistic spaces might offer a different set of
  psychological and physical benefits compared to smaller, urban parks.
\end{itemize}

\textbf{Challenges with exchangeability}

In observational studies, achieving exchangeability -- where the groups
being compared are similar in all respects except for the treatment --
is a significant challenge. Individuals who live near green spaces might
differ systematically from those who do not in several ways:

\begin{itemize}
\item
  \textbf{Socioeconomic status}: there might be a correlation between an
  individual's economic background and their proximity to green spaces.
  For instance, more affluent individuals might be able to afford
  housing in areas with better access to high-quality green spaces.
\item
  \textbf{Age demographics}: different age groups might naturally
  gravitate towards or away from green spaces. Younger individuals or
  families with children might place more value on having access to
  parks, whereas this might be less of a priority for other
  demographics.
\item
  \textbf{Mental health considerations}: people with existing mental
  health issues might either seek out green spaces for their therapeutic
  benefits or avoid them due to social anxiety or other factors.
\item
  \textbf{Lifestyle choices}: Those who prefer outdoor activities might
  choose to live near green spaces. In these cases, it is challenging to
  disentangle whether the proximity to green spaces is causing improved
  well-being or whether it is merely a characteristic of individuals who
  already lead healthier lifestyles.
\item
  \textbf{Personal values and social connections}: The decision to live
  near green spaces might also be influenced by personal values like
  environmentalism or the desire to be part of a community that values
  green spaces. Such values and social connections can affect how
  individuals interact with and benefit from these spaces.
\end{itemize}

In observational studies, these and other unmeasured factors can
introduce biases that complicate the interpretation of causal
relationships.

\textbf{Understanding and assessing positivity}

Recall that positivity is the requirement that every individual has a
chance of receiving each level of treatment to be compared. In
observational studies, this can be a major challenge. For instance, in
some urban areas, it might be practically impossible for certain
demographics to have access to green spaces due to factors like housing
prices or availability. This lack of variability in treatment exposure
within certain strata can make it difficult to draw valid causal
inferences. Meeting the challenges posed by observational settings
necessitates a thorough understanding of the context and a meticulous
application of statistical methods to closely mimic the conditions of a
randomised experiment. In essence, the more closely our data
approximates a randomised controlled experiment, the more confidence we
can place in our causal inferences. However, it is important to
acknowledge that often, the data may not provide a high level of
confidence. By consistently referencing the gold standard of a
randomised experiment, environmental psychologists can enhance their
understanding and effectively communicate both the strengths and
limitations of observational data in answering causal questions, many of
which cannot be addressed experimentally due to practical or ethical
constraints.

In Section 2, we will explore how causal diagrams can significantly
augment our understanding of these strengths and limitations. These
diagrams serve as powerful tools for visualising and analysing the
relationships and potential confounding factors within observational
data. They provide a framework for identifying and addressing the
assumptions necessary for causal inference, thereby offering a clearer
pathway for interpreting complex data sets. This approach is
instrumental in bridging the gap between the idealised conditions of a
randomised experiment and the realities of observational studies,
ultimately enriching the environmental psychologist's toolkit for
scientific understanding.

\subsection{Part 2. An Introduction to Causal
Diagrams}\label{part-2.-an-introduction-to-causal-diagrams}

Causal diagrams are powerful tools for evaluating causal inferences
(\citeproc{ref-greenland1999}{Greenland \emph{et al.} 1999};
\citeproc{ref-pearl1995}{Pearl 1995}, \citeproc{ref-pearl2009}{2009}).

\begin{table}

\caption{\label{tbl-01}Terminology for causal diagrams. This table is
adapted from (\citeproc{ref-bulbulia2023}{Bulbulia 2023})}

\centering{

\terminologylocalconventions

}

\end{table}%

Table~\ref{tbl-01} presents coventions we will use for describing
variables in causal diagrams

\begin{table}

\caption{\label{tbl-02}Basic conventions. This table is adapted from
(\citeproc{ref-bulbulia2023}{Bulbulia 2023})}

\centering{

\terminologygeneral

}

\end{table}%

Table~\ref{tbl-02} presents the coventions we will use for constructing
causal diagrams

\begin{table}

\caption{\label{tbl-03}This table is adapted from
(\citeproc{ref-bulbulia2023}{Bulbulia 2023})}

\centering{

\terminologydirectedgraph

}

\end{table}%

\begin{table}

\caption{\label{tbl-04}This table is adapted from
(\citeproc{ref-bulbulia2023}{Bulbulia 2023})}

\centering{

\terminologyelconfounders

}

\end{table}%

To use causal diagrams, we must understand the following terminology and
conventions:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Nodes and edges}: nodes represent variables or events within a
  causal system, while edges signify relationships or interactions
  between these variables.
\item
  \textbf{Ancestors and descendants}: we call a variable an ``ancestor''
  if it directly or indirectly influences another variable. Conversely,
  we call a variable a ``descendant'' if it is influenced, directly or
  indirectly, by another variable.
\item
  \textbf{D-separation}: we call a path ``blocked,'' or ``d-separated,''
  if a node along it prevents the transmission of influence. Two
  variables are considered d-separated if all paths between them are
  blocked; otherwise, they are
  d-connected(\citeproc{ref-pearl1995}{Pearl 1995}).
\item
  \textbf{Acyclic}: Causal diagrams must be acyclic -- they cannot
  contain feedback loops. More precisely: no variable can be an ancestor
  or descendant of itself. \emph{Therefore, in cases where repeated
  measurements are taken, nodes must be indexed by time.} As mentioned,
  repeated measures time series data are almost always required to
  estimate causal effects quantitatively. In Part 3 we consider how
  adding baseline measures of the outcome and exposure in a three-wave
  repeated measures design greatly enhances causal estimation
  (\citeproc{ref-pearl2009}{Pearl 2009}). To represent the nodes of this
  design on a graph we must index them by time because the nodes are
  repeated.
\end{enumerate}

Pearl showed that the principles of d-separation enable us to evaluate
relationships between nodes in a causal diagram
(\citeproc{ref-pearl1995}{Pearl 1995}).

The rules of d-separation:

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\item
  \textbf{Chain rule}: in a chain structure, where three variables are
  connected sequentially (represented as
  \(A \rightarrow B \rightarrow C\),) conditioning on \(B\) d-separates
  \(A\) and \(C\).
\item
  \textbf{Fork rule}: in a fork structure, where \(B\) is a common cause
  of both \(A\) and \(C\) (represented as
  \(A \leftarrow B \rightarrow C\),) conditioning on \(B\) d-separates
  \(A\) and \(C\).
\item
  \textbf{Collider rule}: in a collider structure, where \(B\) is a
  common effect of both \(A\) and \(C\) (represented as
  \(A \rightarrow B \leftarrow C\),) \(B\) d-separates \(A\) and \(C\)
  only if neither \(B\) nor any of \(B\)'s descendants are conditioned
  upon.
\end{enumerate}

In each case, if \(B\) does not d-separate \(A\) and \(C\), \(A\) and
\(C\) are considered to be d-connected given \(B\). This suggests an
open path between \(A\) and \(C\). If all paths between \(A\) and \(C\)
are blocked, or equivalently, if no path remains open, then \(A\) and
\(C\) are d-separated given a set of conditioning variables
(\citeproc{ref-pearl2009}{Pearl 2009}).

The rules of d-separation clarify which variables to adjust for when
estimating causal effects: we seek a set of variables that d-separates
the exposure from the outcome. By conditioning on such an adjustment
set, we block all confounding paths, leaving only the causal effect
(\citeproc{ref-pearl2009}{Pearl 2009}).

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{5}
\item
  \textbf{Adjustment set}: a collection of variables that we either
  condition upon or deliberately avoid conditioning upon to block all
  backdoor paths between the exposure and the outcome in the causal
  diagram (\citeproc{ref-pearl2009}{Pearl 2009}).
\item
  \textbf{Confounders}: a member of an adjustment set. Importantly,
  \emph{we call a variable as a ``confounder'' in relation to a specific
  adjustment set.}\footnote{VanderWeele's Modified Disjunctive Cause
    Criterion provides practical guidance for controlling for
    confounding (\citeproc{ref-vanderweele2019}{VanderWeele 2019}):
    According to this criterion, a member of any set of variables that
    can reduce or remove the bias caused by confounding is deemed a
    member of this confounder set. VanderWeele's strategy for defining a
    confounder set is as follows: a. Control for any variable that
    causes the exposure, the outcome, or both. b. Control for any proxy
    for an unmeasured variable that is a shared cause of both exposure
    and outcome. c. Define an instrumental variable as a variable
    associated with the exposure but does not influence the outcome
    independently, except through the exposure. Exclude any instrumental
    variable that is not a proxy for an unmeasured confounder from the
    confounder set.

    Note that the concept of a ``confounder set'' is broader than the
    concept of an ``adjustment set.'' Every adjustment set is a member
    of a confounder set. So the Modified Disjunctive Cause Criterion
    will eliminate confounding when the data permit. However a
    confounder set includes variables that will reduce confounding in
    cases where confounding cannot be eliminated. Confounding can almost
    never be elimiated with certainty. For this reason we perform
    sensitivity analyses. However confounding should be reduced wherever
    possible. Hence we opt for ``confounder sets.''}
\end{enumerate}

\subsubsection{Criteria for
conditioning}\label{criteria-for-conditioning}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{7}
\item
  \textbf{Backdoor criterion}: a set of conditions under which the
  effect of a treatment on an outcome can be obtained by controlling for
  a specific set of variables. The backdoor criterion guides the
  selection of \textbf{adjustment sets} (\citeproc{ref-pearl1995}{Pearl
  1995}).
\item
  \textbf{Identification problem}: the challenge of estimating the
  causal effect of a variable using observed data. Causal diagrams were
  developed to address the identification problem.
\end{enumerate}

Alice now turns to the description of the elemental confounds

\subsubsection{1. The problem of confounding by a common
cause}\label{the-problem-of-confounding-by-a-common-cause}

The problem of confounding by common cause arises when there is a
variable, denoted by \(L\), that influences both the exposure, denoted
by \(A\), and the outcome, denoted by \(Y.\) Because \(L\) is a common
cause of both \(A\) and \(Y\), \(L\) may create a statistical
association between \(A\) and \(Y\) that does not reflect a causal
association.

For instance, in the context of green spaces, consider people choosing
to live closer to green spaces (exposure \(A\)) and their experience of
improved mental health (outcome \(Y\)). A common cause could be
socioeconomic status (\(L\)). Individuals with higher socioeconomic
status may have the financial capacity to afford housing near green
spaces and simultaneously afford better healthcare and lifestyle
choices, contributing to improved mental health. Thus, while the data
may show a statistical association between living closer to green spaces
(\(A\)) and improved mental health (\(Y\)), this association may not
reflect a direct causal relationship due to the confounding by
socioeconomic status (\(L\)).

The figure referenced as Figure~\ref{fig-dag-common-cause} represents
such a scenario. The association of \(A\) and \(Y\) in the data is
confounded by the common cause \(L\). The dashed red arrow in the graph
signifies the bias introduced by the open backdoor path from \(A\) to
\(Y\) that arises due to their common cause \(L\).

\begin{figure}[htb]

\centering{

\includegraphics[width=0.8\textwidth,height=\textheight]{causal-tutorial-env_files/figure-pdf/fig-dag-common-cause-1.pdf}

}

\caption{\label{fig-dag-common-cause}Counfounding by a common cause. The
dashed path indicates bias arising from the open backdoor path from A to
Y.}

\end{figure}%

\subsubsection{Advice: attend to the temporal order of all measured
variables}\label{advice-attend-to-the-temporal-order-of-all-measured-variables}

Addressing confounding by a common cause involves its adjustment. This
adjustment effectively closes the backdoor path from the exposure to the
outcome. Equivalently, conditioning on \(L\) d-separates \(A\) and
\(Y\). Common adjustment methods include regression, matching, inverse
probability of treatment weighting, and G-methods (covered in
(\citeproc{ref-hernuxe1n2023}{Hernán and Monge 2023})).
Figure~\ref{fig-dag-common-cause-solution} clarifies that any confounder
that is a common cause of both \(A\) and \(Y\) must precede \(A\) (and
hence \(Y\)), since effects follow their causes chronologically.

After we have time-indexing the nodes on the graph it becomes evident
that \textbf{control of confounding generally necessitates time-series
data.}

\begin{figure}[htb]

\centering{

\includegraphics[width=0.8\textwidth,height=\textheight]{causal-tutorial-env_files/figure-pdf/fig-dag-common-cause-solution-1.pdf}

}

\caption{\label{fig-dag-common-cause-solution}Solution: adjust for
pre-exposure confounder. The implication: obtain time series data to
ensure the confounder occurs before the exposure.}

\end{figure}%

\subsubsection{2. Confounding by collider stratification (conditioning
on a common
effect)}\label{confounding-by-collider-stratification-conditioning-on-a-common-effect}

Conditioning on a common effect, also known as collider stratification,
occurs when a variable, denoted by \(L\), is influenced by both the
exposure, denoted by \(A\), and the outcome, denoted by \(Y\).

Imagine, in the context of green spaces, an individual's choice to live
closer to green spaces (exposure \(A\)) and their improved mental health
(outcome \(Y\)) are both influencing the individual's overall
satisfaction with life (common effect \(L\)). Initially, \(A\) and \(Y\)
could be independent, represented as \(A \coprod Y(a)\), suggesting that
the decision to live near green spaces is not directly causing improved
mental health.

However, when we condition on the life satisfaction \(L\) (the common
effect of \(A\) and \(Y\)), a backdoor path between \(A\) and \(Y\) is
opened. This could potentially induce a non-causal association between
living closer to green spaces and improved mental health. The reason
behind this is that the overall life satisfaction \(L\) can provide
information about both the proximity to green spaces \(A\) and the
mental health status \(Y\). Hence, it may appear that there is an
association between \(A\) and \(Y\) even when there may not be a direct
causal relationship.

\begin{figure}[htb]

\centering{

\includegraphics[width=0.8\textwidth,height=\textheight]{causal-tutorial-env_files/figure-pdf/fig-dag-common-effect-1.pdf}

}

\caption{\label{fig-dag-common-effect}Confounding by conditioning on a
collider. The dashed red path indicates bias from the open backdoor path
from A to Y.}

\end{figure}%

\subsubsection{Advice: attend to the temporal order of all measured
variables}\label{advice-attend-to-the-temporal-order-of-all-measured-variables-1}

To address the problem of conditioning on a common effect, we should
\emph{generally} ensure that:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  all confounders \(L\) that are common causes of the exposure \(A\) and
  the outcome \(Y\) are measured before \(A\) has occurred, and
\item
  \(A\) is measured before \(Y\) has occurred.
\end{enumerate}

If such temporal order is preserved, \(L\) cannot be an effect of \(A\),
and thus neither of \(Y\).\footnote{This rule is not absolute. As
  indicated in Figure~\ref{fig-dag-descendent-solution}, it may be
  helpful in certain circumstances to condition on a confounder that
  occurs after the outcome has occurred.}

\begin{figure}[htb]

\centering{

\includegraphics[width=0.8\textwidth,height=\textheight]{causal-tutorial-env_files/figure-pdf/fig-dag-common-effect-solution-1.pdf}

}

\caption{\label{fig-dag-common-effect-solution}Solution: time idexing of
confounders helps to avoid collider bias and maintain d-separation. The
graph makes the imperative clear: we must collect time series data with
confounders measured before the exposure, and that we must likewise
measure the exposure before the outcome, with data collected
repeatitively on the same units.}

\end{figure}%

\subsubsection{M-bias: conditioning on a collider that occurs before the
exposure may introduce
bias}\label{m-bias-conditioning-on-a-collider-that-occurs-before-the-exposure-may-introduce-bias}

Typically, indicators for confounders should be included only if they
are known to be measured before their exposures - with notable
exceptions described below in fig-dag-descendent-solution-2.

In the context of green spaces, consider the scenario where an
individual's level of physical activity (\(L\)) is influenced by an
unmeasured factor related to their propensity to live near green spaces
(\(A\)) and another unmeasured factor linked to their mental health
(\(Y\)). Here, physical activity \(L\) does not directly affect the
decision to live near green spaces \(A\) or mental health status \(Y\),
but is a descendent of unmeasured variables that do.

If we condition on physical activity \(L\) in this scenario, we evoke
what is known as ``M-bias''. If \(L\) is neither a common cause of \(A\)
and \(Y\) nor the effect of a shared common cause, then \(L\) should not
be included in a causal model. Figure~\ref{fig-m-bias} represents a case
where \(A \coprod Y(a)\) but \(A \cancel{\coprod} Y(a)| L\).

M-bias is another example of collider stratification bias, a phenomenon
where conditioning on a common effect or a descendent of a common effect
induces an association between variables that were previously
independent (\citeproc{ref-cole2010}{Cole \emph{et al.}
2010}).\footnote{Note, when we draw a chronologically ordered path from
  left to right the M shape for which ``M-bias'' takes its name changes
  to an E shape We shall avoid proliferating jargon and retain the term
  ``M bias.''}

\begin{figure}[htb]

\centering{

\includegraphics[width=0.8\textwidth,height=\textheight]{causal-tutorial-env_files/figure-pdf/fig-m-bias-1.pdf}

}

\caption{\label{fig-m-bias}M-bias: confounding control by including
previous outcome measures. The dashed red path indicates bias from the
open backdoor path from A to Y by conditioning on pre-exposure variable
L. The solution: do not condition on L. The graph makes it evident that
conditioning on variables measured before the exposure is not sufficient
to prevent confounding.}

\end{figure}%

\subsubsection{Advice: adopt the modified disjunctive cause criterion
for confounding
control}\label{advice-adopt-the-modified-disjunctive-cause-criterion-for-confounding-control}

Again, the modified disjunctive cause criterion will satisfy the
backdoor criterion in all cases and reduce bias where this criterion
cannot be fully satisfied:

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\tightlist
\item
  Control for any variable that causes the exposure, the outcome, or
  both.
\item
  Control for any proxy for an unmeasured variable that is a shared
  cause of both exposure and outcome.
\item
  Define an instrumental variable as a variable associated with the
  exposure but does not influence the outcome independently, except
  through the exposure. Exclude any instrumental variable that is not a
  proxy for an unmeasured confounder from the confounder set (see:
  VanderWeele \emph{et al.} (\citeproc{ref-vanderweele2020}{2020}) page
  441, (\citeproc{ref-vanderweele2019}{VanderWeele 2019}))
\end{enumerate}

Of course, the difficulty is in determining which variables belong to a
confounder set. Specialist knowledge can facilitate this task. However,
the data alone typically do not settle this question. (For exceptions
see: bulbulia2021).

\subsubsection{3. Mediator bias}\label{mediator-bias}

Applying this to our green spaces example, again we consider proximity
to green spaces as the exposure (\(A\)), mental health as the outcome
(\(Y\)), and physical activity as the mediator (\(L\)).

In this scenario, living close to green spaces (\(A\)) influences
physical activity (\(L\)), which subsequently impacts mental health
(\(Y\)). If we condition on physical activity (\(L\)), we may bias our
estimates of the total effect of proximity to green spaces (\(A\)) on
mental health (\(Y\)). This bias arises because conditioning on \(L\)
can obscure the direct effect of \(A\) on \(Y\), as it blocks the
indirect path through \(L\). This phenomenon, known as mediator bias, is
depicted in Figure~\ref{fig-dag-mediator}.

One might assume that conditioning on a mediator does not introduce bias
when there is no causal relationship between \(A\) and \(Y\). However,
this is not always the case. Consider a situation where \(L\) is a
common effect of the exposure \(A\) and an unmeasured variable \(U\)
linked to the outcome \(Y\). Here, including \(L\) may inflate the
association between \(A\) and \(Y\), even if \(A\) is not related to
\(Y\) and \(U\) does not cause \(A\). This case is depicted in
Figure~\ref{fig-dag-descendent}.

Hence, unless one is specifically investigating mediation analysis, it
is generally inadvisable to condition on a post-treatment variable.
Being aware of the timeline in the spatial organisation of the graph
underlines a critical principle for data collection: if we cannot
guarantee that \(L\) is measured before \(A\), and if \(A\) may affect
\(L\), including \(L\) in our model could lead to mediator bias. This
scenario is illustrated in Figure~\ref{fig-dag-descendent}.

\begin{figure}[htb]

\centering{

\includegraphics[width=0.8\textwidth,height=\textheight]{causal-tutorial-env_files/figure-pdf/fig-dag-mediator-1.pdf}

}

\caption{\label{fig-dag-mediator}Confounding by conditioning on a
mediator. The dashed black arrow indicates bias arising from partially
blocking the path between A and Y.}

\end{figure}%

\subsubsection{Advice: attend to the temporal order of all measured
variables}\label{advice-attend-to-the-temporal-order-of-all-measured-variables-2}

To mitigate the issue of mediator bias, particularly when focusing on
total effects, we should generally avoid conditioning on a mediator. We
avoid this problem by ensuring that \(L\) occurs before the treatment
\(A\) and the outcome \(Y\) (Note: a counter-example is presented in
Figure~\ref{fig-dag-descendent-solution-2}). Again, we discover the
importance of explicitly stating and measuring the temporal order of our
variables.\footnote{Note that if \(L\) were associated with \(Y\) and
  could not be caused by \(A\), conditioning on \(L\) would typically
  enhance the precision of the causal effect estimate of \(A \to Y\).
  This precision enhancement holds even if \(L\) occurs after \(A\).
  However, the onus is on the researcher to show that the post-treatment
  factor cannot be a consequence of the exposure.}

\begin{figure}[htb]

\centering{

\includegraphics[width=0.8\textwidth,height=\textheight]{causal-tutorial-env_files/figure-pdf/fig-dag-mediator-solution-1.pdf}

}

\caption{\label{fig-dag-mediator-solution}Solution: do not condition on
a mediator. The implication: by ensuring temporal order in data
collection we diminish the probabilty of mistaking an effect of an
exposure for its confounder.}

\end{figure}%

\subsubsection{4. Conditioning on a descendant may induce
confounding}\label{conditioning-on-a-descendant-may-induce-confounding}

Let us consider this principle in the context of our green spaces
example. Again denote ``proximity to green spaces'' by \(A\), ``mental
health'' by \(Y\). Denote ``physical activity'' by \(L\), and ``sun
exposure'' by \(L^\prime\).

In this scenario, assume \(L^\prime\), sun exposure, is caused by an
unobserved variable \(U\), and is influenced by \(A\), the proximity to
green spaces. Further, assume \(U\) affects the outcome \(Y\), mental
health.

Conditioning on \(L^\prime\), which is a descendant of \(A\) and \(U\),
can lead to a spurious association between \(A\) and \(Y\) through the
path \(A \to L^\prime \to U \to Y\). This situation, shown in
Figure~\ref{fig-dag-descendent}, illustrates how conditioning on a
descendant can introduce confounding, resulting in a distorted causal
estimation.

\begin{figure}[htb]

\centering{

\includegraphics[width=0.8\textwidth,height=\textheight]{causal-tutorial-env_files/figure-pdf/fig-dag-descendent-1.pdf}

}

\caption{\label{fig-dag-descendent}Confounding by descent: the red
dashed path illustrates the introduction of bias by conditioning on the
descendant of a confounder that is affected by the exposure, thus
opening of a backdoor path between the exposure, A, and the outcome, Y.}

\end{figure}%

Again, the advice is evident from the chronology of the graph: we should
measure the (\(L^\prime\)) before the exposure (\(A\)). This solution is
presented in Figure~\ref{fig-dag-descendent-solution}.

\begin{figure}[htb]

\centering{

\includegraphics[width=0.8\textwidth,height=\textheight]{causal-tutorial-env_files/figure-pdf/fig-dag-descendent-solution-1.pdf}

}

\caption{\label{fig-dag-descendent-solution}Solution: again the graph
makes it clear that our data must ensure temporal order of the
measurements. By ensuring that L occurs before A confounding is
controlled. The figure also makes it evident that L need not affect Y to
be a confounder (i.e.~a member of a confounder set).}

\end{figure}%

\subsubsection{5. Conditioning on a descendent may reduce
confounding}\label{conditioning-on-a-descendent-may-reduce-confounding}

In our greenspace example, consider an unmeasured confounder \(U\),
perhaps a genetic factor, that impacts both proximity to green spaces
\(A\), mental health \(Y\), and a variable \(L^\prime\), such as a
behavioural trait that only manifests later in life.

As shown in Figure~\ref{fig-dag-descendent-solution-2}, if we adjust for
\(L^\prime\), we might be able to reduce the confounding caused by the
unmeasured \(U\). Even though \(L^\prime\) may occur after the exposure
and even after the outcome, conditioning on it can help control for the
confounding because it acts as a proxy for an unmeasured common cause of
the exposure and the outcome.

This scenario reveals that adhering strictly to a rule that only allows
us to condition on pre-exposure and pre-outcome variables may not always
be optimal. It highlights the need for careful contemplation of data
collection strategies. We cannot resort solely to algorithmic rules for
confounding control. Every case necessitates its unique approach.

\begin{figure}[htb]

\centering{

\includegraphics[width=0.8\textwidth,height=\textheight]{causal-tutorial-env_files/figure-pdf/fig-dag-descendent-solution-2-1.pdf}

}

\caption{\label{fig-dag-descendent-solution-2}Solution: conditioning on
a confounder that occurs after the exposure and the outcome might
address a problem of unmeasured confounding if the confounder is a
descendent of a prior common cause of the exposure and outcome. The
dotted paths denote that the effect of U on A and Y is partially
adjusted by conditioning on L', even though L' occurs after the outcome.
The paths are dotted to represent a reduction of bias by conditioning on
the post-outcome descendent of an unmeasured common cause of the
exposure and outcome. How might this work? Consider a genetic factor
that affects the exposure and the outcome early in life might be
measured by an indicator late that is expressed (and may be measured)
later in life. Adjusting for such an indicator would constitute an
example of post-outcome confounding control.}

\end{figure}%

\subsubsection{6. Using causal diagrams to inform data collection: the
three-wave panel
design}\label{using-causal-diagrams-to-inform-data-collection-the-three-wave-panel-design}

In a three-wave panel design, we collect data across three intervals to
facilitate observational causal inference. The causal diagram in
Figure~\ref{fig-dag-6} clarifies the virtues of panel data collection

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Baseline Data Collection}:

  \begin{itemize}
  \item
    \textbf{Confounding Data}: At baseline, we collect data on
    confounders, along with data on the exposure and the outcome. This
    initial data collection phase is crucial for measuring common causes
    of the treatment and outcome, or the descendents of such common
    causes.
  \item
    \textbf{Exposure and Outcome Data}: Baseline measurements of
    exposure and outcome allow our data collection to more effectively
    mimic an experiment.

    \begin{itemize}
    \item
      \textbf{Incidence Effect Evaluation}: The baseline exposure allows
      us to interpret the post-baseline exposure effect as an incidence
      effect rather than a prevalence effect. This interpretation means
      we can assess the change due to a new occurrence (incidence) of
      the exposure rather than its overall presence (prevalence).
    \item
      \textbf{Sample Adequacy for Rare Exposures}: Especially when the
      exposure is uncommon, measuring the baseline exposure and outcome
      can help assess the adequacy of the sample size.
    \item
      \textbf{Temporal Ordering and Confounding Control}: Incorporating
      the outcome at baseline helps confirm the temporal order of the
      cause-effect relationship, thereby guarding against reverse
      causation. Moreover, when we also control for the exposure at
      baseline, an unmeasured confounder would have to negate the
      association between the exposure at one wave post-baseline and the
      outcome at two waves post-baseline, independent of the baseline
      effect.
    \end{itemize}
  \end{itemize}
\item
  \textbf{First Follow-Up Data Collection (Baseline +1)}:

  \begin{itemize}
  \tightlist
  \item
    At this stage, we measure the exposure. This follow-up allows us to
    capture the causal effect of changes in the exposure since baseline.
  \end{itemize}
\item
  \textbf{Second Follow-Up Data Collection (Baseline +2)}:

  \begin{itemize}
  \tightlist
  \item
    We measure the outcome at this stage. Similar to the first
    follow-up, this measurement allows us to capture a controlled effect
    for the outcome since the baseline measurement.
  \end{itemize}
\end{enumerate}

Through this design, any unmeasured confounder affecting both the
exposure and the outcome would need to do so independently of these
baseline measurements of exposure and outcome. Causal diagrams
effectively illustrate this methodology, as shown in
Figure~\ref{fig-dag-6}, clarifying the paths of causation, potential
sources of confounding, and the methods used for controlling these
confounders. As a result, these diagrams are powerful tools for
observational causal inference in a three-wave panel design.

\begin{figure}[htb]

\centering{

\includegraphics[width=0.8\textwidth,height=\textheight]{causal-tutorial-env_files/figure-pdf/fig-dag-6-1.pdf}

}

\caption{\label{fig-dag-6}Causal diagram adapted from Vanderweele et
al.'s three-wave panel design. The dotted line indicates a reduction in
bias arising from including baseline measures for the exposure and
outcome. For an unmeasured confounder U to bias the exposure-outcome
association, it would need to do so independently of these outcome and
exposure baseline measures. The graph clarifies that by measuring
confounders before the exposure and the exposure before the outcome, we
reduce the potential for reverse causation, collider stratification, and
mediator biases.}

\end{figure}%

\subsubsection{7. The Inevitability of Unmeasured
Confounding}\label{the-inevitability-of-unmeasured-confounding}

In observational research, there is an unavoidable issue known as
unmeasured confounding. These are confounders that are not included in
the data set, thus not accounted for during analysis, which can
introduce bias into the study. This may occur due to constraints such as
data availability, financial or ethical reasons. Despite all efforts, it
is virtually impossible to measure and control for every possible
confounder. Therefore, it is necessary to estimate the potential impact
of these unmeasured confounders and consider their influence on the
study's findings.

One of the methods to assess the potential impact of unmeasured
confounding is through sensitivity analysis, which quantifies how strong
an unmeasured confounder would need to be to fully explain away an
observed association. A commonly used metric for this purpose is the
E-Value.

The E-Value quantifies the minimum strength of association that an
unmeasured confounder would require with both the exposure and outcome,
over and above the measured confounders, to explain away an observed
exposure-outcome association. An E-Value close to 1 suggests that the
observed association is vulnerable to unmeasured confounding, whereas a
high E-Value suggests that a remarkably strong unmeasured confounder
would be necessary to explain the observed association, thus providing
evidence towards the robustness of the observed association.

E-Values can be computed through the \texttt{EValue} package in R
(\citeproc{ref-mathur2018}{Mathur \emph{et al.} 2018}). This package
provides an intuitive, user-friendly interface for researchers to
compute E-Values. Observational researchers have no excuses not to
conduct sensitivity analyses.

\subsubsection{Part 3. Summary, Pitfalls and
Tips.}\label{part-3.-summary-pitfalls-and-tips.}

\subsubsection{Summary}\label{summary}

We introduced the potential outcomes framework of causal inference,
focussing on three fundamental assumptions:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Causal Consistency}: This assumption posits that for every
  unit, the outcome under treatment is equal to the observed outcome if
  the unit was treated, and vice versa for non-treatment. In essence, it
  says that `the treatment' is consistently defined.
\item
  \textbf{Exchangeability}: The assignment of treatment is random and
  independent of the potential outcomes. This is a essential virtue of
  experimental designs The process of randomisation ensures that every
  participant has an equal chance of being assigned to the treatment or
  control group, creating balance in the factors that might affect the
  outcomes under the different treatments. Randomisation is powerful
  because it removes any systematic bias in the treatment assignment.
\item
  \textbf{Positivity}: Each unit under study has a non-zero probability
  of receiving the treatment. This ensures that comparisons between
  treated and untreated units are meaningful and well-defined.
\end{enumerate}

We observed that in the context of randomised experiments, these
assumptions are generally met, and in being met, experimentalists
address the problem of missing potential outcomes in the treatment
groups that researchers compare.

Having built core intuitions for how experiments recover average causal
effect estimates, we considered how the three fundamental assumptions
required for causal inferences may be easily violated in observational
studies. Where treatment assignment is not random and can be influenced
by observed or unobserved variables, correlation is not equivalent to
causation.

Next, we clarified the conditions where, assuming the three fundamental
assumptions of causal inference have been satisified, we may to recover
causal effect estimates from observational data.

To obtain such causal effect estimates we must:

\begin{itemize}
\tightlist
\item
  Precisely defining the interventions that need to be compared.
\item
  Conditioning on confounders, variables associated with both the
  treatment and the outcome, or that are descendents of such common
  causes.
\item
  Ensuring positivity, that is, each individual has some probability of
  receiving each level of treatment. This ensures the comparability of
  treatment groups.
\end{itemize}

The second part of the article discusses the use of causal diagrams for
dealing with identification problems in observational settings. The key
points take home messages were:

\begin{itemize}
\tightlist
\item
  For confounding control to work, researchers generally require
  time-series data that determine the temporal order of events.
\item
  Causal diagrams reveal that the three-wave panel design is a powerful
  tool for addressing causal questions with observational data.
\item
  Nevertheless, sensitivity analysis should always be performed, and can
  be performed relatively easily.
\end{itemize}

We conclude by summarising our advice through ``Tips'' and ``Pitfalls''

\subsubsection{Data Collection Tips}\label{data-collection-tips}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Use time series data.
\item
  Ensure significant change from baseline in treatment (positivity).
\item
  Clearly define measurements for treatment, outcome, and baseline
  confounders.
\item
  Include baseline treatment measures.
\item
  Include baseline outcome measures.
\item
  Strive for high sample retention.
\end{enumerate}

\subsubsection{Graph Drawing Tips}\label{graph-drawing-tips}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Define all nodes unambiguously.
\item
  Keep the graph simple and focused.
\item
  Explicitly state any novel conventions.
\item
  Maintain acyclicity in the graph.
\item
  Arrange nodes in chronological order.
\item
  Time-stamp nodes to reflect the temporal sequence of causation.
\item
  Apply a modified disjunctive cause criterion pragmatically.
\item
  Add nodes for unmeasured confounding where helpful.
\item
  Illustrate nodes for post-treatment selection.
\item
  Remember, causal diagrams are qualitative tools, not detailed maps.
\end{enumerate}

\subsubsection{Pitfalls to Avoid}\label{pitfalls-to-avoid}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Avoid using cross-sectional data.
\item
  Don't misuse causal diagrams without understanding counter-factual
  data science.
\item
  Don't create diagrams without time indices.
\item
  Avoid excessive nodes in the graph.
\item
  Don't draw arrows into the manipulation in experimental studies.
\item
  Don't inaccurately describe bias when exposure and outcome are
  d-separated.
\item
  Don't ignore causal diagrams during research design.
\item
  Avoid representing interactions and non-linear dynamics in causal
  diagrams.
\item
  Remember that structural equation models are not true structural
  models: do not mistake structural equation models for causal diagrams
  (NOTE Don, we haven't yet said this, but we)
\end{enumerate}

\newpage{}

\subsection{Funding}\label{funding}

This work is supported by a grant from the Templeton Religion Trust
(TRT0418). JB received support from the Max Planck Institute for the
Science of Human History. The funders had no role in preparing the
manuscript or the decision to publish it.

\subsection{References}\label{references}

\phantomsection\label{refs}
\begin{CSLReferences}{1}{0}
\bibitem[\citeproctext]{ref-bulbulia2022}
Bulbulia, JA (2022) A workflow for causal inference in cross-cultural
psychology. \emph{Religion, Brain \& Behavior}, \textbf{0}(0), 1--16.
doi:\href{https://doi.org/10.1080/2153599X.2022.2070245}{10.1080/2153599X.2022.2070245}.

\bibitem[\citeproctext]{ref-bulbulia2023}
Bulbulia, JA (2023) Causal diagrams (directed acyclic graphs): A
practical guide.

\bibitem[\citeproctext]{ref-cole2010}
Cole, SR, Platt, RW, Schisterman, EF, \ldots{} Poole, C (2010)
Illustrating bias due to conditioning on a collider. \emph{International
Journal of Epidemiology}, \textbf{39}(2), 417--420.
doi:\href{https://doi.org/10.1093/ije/dyp334}{10.1093/ije/dyp334}.

\bibitem[\citeproctext]{ref-cook2002experimental}
Cook, TD, Campbell, DT, and Shadish, W (2002) \emph{Experimental and
quasi-experimental designs for generalized causal inference}, Vol. 1195,
Houghton Mifflin Boston, MA.

\bibitem[\citeproctext]{ref-greenland1999}
Greenland, S, Pearl, J, and Robins, JM (1999) Causal diagrams for
epidemiologic research. \emph{Epidemiology (Cambridge, Mass.)},
\textbf{10}(1), 37--48.

\bibitem[\citeproctext]{ref-hernan2023}
Hernan, MA, and Robins, JM (2023) \emph{Causal inference}, Taylor \&
Francis. Retrieved from
\url{https://books.google.co.nz/books?id=/_KnHIAAACAAJ}

\bibitem[\citeproctext]{ref-hernuxe1n2023}
Hernán, MA, and Monge, S (2023) Selection bias due to conditioning on a
collider. \emph{BMJ}, \textbf{381}, p1135.
doi:\href{https://doi.org/10.1136/bmj.p1135}{10.1136/bmj.p1135}.

\bibitem[\citeproctext]{ref-holland1986}
Holland, PW (1986) Statistics and causal inference. \emph{Journal of the
American Statistical Association}, \textbf{81}(396), 945960.

\bibitem[\citeproctext]{ref-mancuso2018revolutionary}
Mancuso, S (2018) \emph{The revolutionary genius of plants: A new
understanding of plant intelligence and behavior}, Simon; Schuster.

\bibitem[\citeproctext]{ref-mathur2018}
Mathur, MB, Ding, P, Riddell, CA, and VanderWeele, TJ (2018) Website and
r package for computing {E}-values. \emph{Epidemiology (Cambridge,
Mass.)}, \textbf{29}(5), e45.

\bibitem[\citeproctext]{ref-mcelreath2020}
McElreath, R (2020) \emph{Statistical rethinking: A {B}ayesian course
with examples in r and stan}, CRC press.

\bibitem[\citeproctext]{ref-neyman1923}
Neyman, JS (1923) On the application of probability theory to
agricultural experiments. Essay on principles. Section 9.(tlanslated and
edited by dm dabrowska and tp speed, statistical science (1990), 5,
465-480). \emph{Annals of Agricultural Sciences}, \textbf{10}, 151.

\bibitem[\citeproctext]{ref-nguyen2021green}
Nguyen, P-Y, Astell-Burt, T, Rahimi-Ardabili, H, and Feng, X (2021)
Green space quality and health: A systematic review. \emph{International
Journal of Environmental Research and Public Health}, \textbf{18}(21),
11028.

\bibitem[\citeproctext]{ref-pearl1995}
Pearl, J (1995) Causal diagrams for empirical research.
\emph{Biometrika}, \textbf{82}(4), 669--688.

\bibitem[\citeproctext]{ref-pearl2009}
Pearl, J (2009) \emph{\href{https://doi.org/10.1214/09-SS057}{Causal
inference in statistics: An overview}}.

\bibitem[\citeproctext]{ref-reyes2021linking}
Reyes-Riveros, R, Altamirano, A, De La Barrera, F, Rozas-Vásquez, D,
Vieli, L, and Meli, P (2021) Linking public urban green spaces and human
well-being: A systematic review. \emph{Urban Forestry \& Urban
Greening}, \textbf{61}, 127105.

\bibitem[\citeproctext]{ref-robins1986}
Robins, J (1986) A new approach to causal inference in mortality studies
with a sustained exposure period---application to control of the healthy
worker survivor effect. \emph{Mathematical Modelling}, \textbf{7}(9-12),
1393--1512.

\bibitem[\citeproctext]{ref-rohrer2018}
Rohrer, JM (2018) Thinking clearly about correlations and causation:
Graphical causal models for observational data. \emph{Advances in
Methods and Practices in Psychological Science}, \textbf{1}(1), 2742.

\bibitem[\citeproctext]{ref-rubin1976}
Rubin, DB (1976) Inference and missing data. \emph{Biometrika},
\textbf{63}(3), 581--592.
doi:\href{https://doi.org/10.1093/biomet/63.3.581}{10.1093/biomet/63.3.581}.

\bibitem[\citeproctext]{ref-rubin2005}
Rubin, DB (2005) Causal inference using potential outcomes: Design,
modeling, decisions. \emph{Journal of the American Statistical
Association}, \textbf{100}(469), 322--331. Retrieved from
\url{https://www.jstor.org/stable/27590541}

\bibitem[\citeproctext]{ref-vanderweele2015}
VanderWeele, TJ (2015) \emph{Explanation in causal inference: Methods
for mediation and interaction}, Oxford University Press.

\bibitem[\citeproctext]{ref-vanderweele2019}
VanderWeele, TJ (2019) Principles of confounder selection.
\emph{European Journal of Epidemiology}, \textbf{34}(3), 211219.

\bibitem[\citeproctext]{ref-vanderweele2020}
VanderWeele, TJ, Mathur, MB, and Chen, Y (2020) Outcome-wide
longitudinal designs for causal inference: A new template for empirical
studies. \emph{Statistical Science}, \textbf{35}(3), 437466.

\end{CSLReferences}



\end{document}
