---
title: "Causal Inference in Environmental Psychology"
abstract: |
  Quantifying causal effects from observational data presents substantial challenges for environmental psychology, however, recent progress in methods for causal inference offers hope. In this chapter, we provide an accessible introduction Causal Diagrams, also known as Directed Acyclic Graphs (DAGs), which are powerful tools for evaluating evidence for causality in observational settings. We start with a conceptual introduction to the basic framework of causal inference within which causal diagrams find there utility. This non-mathematical introduction helps to build intuition for what is needed for obtaining unbiased causal effect estimates. We next offer a primer on how to construct causal diagrams, focussing on the four elemental causal diagrams from which more complicated causal diagrams are built. We conclude with practical guidelines for data collection and modelling, highlighting “tips” and “common pitfalls.” This chapter will interest environmental psychology researchers who wish to draw stronger causal inferences from data derived from non-experimental research designs.
authors: 
  - name: Donald W Hine
    orcid: 0000-0002-3905-7026
    email: donald.hine@canterbury.ac.nz
    affiliation: 
      name: University of Canterbury, School of Psychology, Speech and Hearing
      city: Wellington
      country: New Zealand
      url: www.wgtn.ac.nz/cacr
  - name: Joseph A. Bulbulia
    orcid: 0000-0002-5861-2056
    email: joseph.bulbulia@vuw.ac.nz
    affiliation: 
      name: Victoria University of Wellington, New Zealand, School of Psychology, Centre for Applied Cross-Cultural Research
      department: Psychology/Centre for Applied Cross-Cultural Research
      city: Wellington
      country: New Zealand
      url: www.wgtn.ac.nz/cacr
keywords:
  - DAGS
  - Causal Inference
  - Confounding
  - Environmental
  - Psychology
  - Panel
format:
  pdf:
    fig-pos: 'htb'
    sanitize: true
    keep-tex: true
    link-citations: true
    colorlinks: true
    documentclass: article
    classoption: [singlecolumn]
    lof: false
    lot: false
    geometry:
      - top=30mm
      - left=20mm
      - heightrounded
    header-includes:
      - \input{/Users/joseph/GIT/templates/latex/custom-commands.tex}
date: last-modified
execute:
  echo: false
  warning: false
  include: true
  eval: true
fontfamily: libertinus
bibliography: /Users/joseph/GIT/templates/bib/references.bib
csl: ./camb-a.csl
---

```{r}
#| label: load-libraries
#| echo: false
#| include: false
#| eval: false

#   html:
#    html-math-method: katex

# Include in YAML for Latex
# sanitize: true
# keep-tex: true
# include-in-header:
#       - text: |
#           \usepackage{cancel}


# for making graphs
library("tinytex")
library(extrafont)
loadfonts(device = "all")


# libraries for jb (when internet is not accessible)
# read libraries
source("/Users/joseph/GIT/templates/functions/libs2.R")

# read functions
source("/Users/joseph/GIT/templates/functions/funs.R")

# read data/ set to path in your computer
pull_path <-
  fs::path_expand(
    "/Users/joseph/v-project\ Dropbox/data/current/nzavs_13_arrow"
  )

# for saving models. # set path fo your computer
push_mods <-
  fs::path_expand(
    "/Users/joseph/v-project\ Dropbox/data/nzvs_mods/00drafts/23-causal-dags"
  )


# keywords: potential outcomes, DAGs, causal inference, evolution, religion, measurement, tutorial
# 10.31234/osf.io/b23k7

```

## Introduction

Causal inference involves using empirical observations to draw conclusions about the causal relationships between variables or events. It typically involves determining whether changes in one or more independent variables are linked to changes in one or more dependent variables, while controlling for potential sources of confounding. The aim of causal inference is to identify cause-effect relationships with a high level of confidence, understanding the mechanisms underlying these relationships, while ruling out plausible alternative explanations [@cook2002experimental]

Causal inference is sometimes characterised as a complex cognitive process unique to humans. However, it may be more appropriate to think of causal inference as existing throughout the plant and animal kingdoms on a spectrum, or potentially multiple spectra, that vary in terms conscious awareness and sophistication [@mancuso2018revolutionary].  For example, plants and microorganisms are genetically preprogrammed to recognise and respond to environmental conditions relevant to reproduction and survival.  Birds and rodents have well-established capacities to learn, but not to the level of complexity exhibited by larger mammals and primates.   During the past several thousand years, humans have developed a broad range for frameworks for causal inference such as the law of karma in Hindu, Jain, and Buddhist traditions, Aristotle’s four (material, formal, efficient, and final) causes, early work empiricism and experimentation by Francis Bacon and John Locke, Newtonian mechanics, as well as more contemporary conceptualisations based on counterfactual reasoning such as those described in this chapter.      


## Correlation in Psychological Research 

As taught in most introductory undergraduate psychology courses, correlations between two more variables does not imply the presence of a causal process. However, in most psychological research, quantifying a causal effect requires drawing inferences based on research design, statistical modelling, and correlations present in data.

In experimental settings, quantitative causal inferences from associational models are made possible through randomisation, controlled interventions, and chronologically ordered data collection.  For example, even in non-longitudinal experimental studies, manipulations always precede the measurement of outcomes. 
By contrast, in observational settings, the data researchers collect are not the products of randomisation and control. Moreover, observational researchers are often limited to cross-sectional data, which lack temporal order. In observational settings, then, credible causal inference from correlations can be, at best, indirect. The problems of inferring causation in observational settings are widely known.  For example, in cross-sectional studies, correlations between two variables may be due to the presence of a third variable that causes changes in one or both.  Furthermore, variables can be correlated, but the direction of causation can be ambiguous.  Variable A may cause variable B?  Or B may cause A?  In the absence of an appropriate research design is may be impossible to tell.   

Despite this, many observational researchers persist in using hedging causal language to reporting their results, offering cautious policy recommendations couched in words such as “might effect”, “suggestive of a causal link”, and “is consistent with our hypothesis that A causes B” [@bulbulia2022].  This is true of our own work, and also that of many of colleagues in environmental psychology.  However, because correlations can be caused by unmeasured variables and true causal effects, when they exist, can run in the opposite direction of the proposed causal model, researchers need to be extremely cautious when inferring causal conclusions.


Are there any settings in which we might we obtain valid causal inferences from observational data? If so, by which pathways might we pursue such inferences?

Environmental psychologists have strong motivations to ask these questions. Among the most compelling is an interest in understanding the possible effects of interventions on environmental attitudes and behaviours, which is essential for offering effective policy advice. Moreover, although randomised experiments can sometimes afford causal understanding, experimental studies are often expensive to perform at scale, and random assignment to treatment conditions can pose unacceptable ethical risks. Designing and implementing interventions and then tracking their effects over long periods are typically infeasible, and often decisions cannot wait. Experimental control often comes at the price of ecological validity, limiting direct applications in non-laboratory contexts.  [Although see MIT Poverty Action lab for examples for examples of large scale RCTs.] For these reasons, the bandwidth of psychologically interesting and practically relevant questions that experiments may address is limited. By contrast, observational data is abundant. If we could obtain causal insights from observational data, the state of knowledge in environmental psychology would be greatly accelerated and the field would be better equipped to offer advice.
Considerable progress in the methods for estimating causation from observational data during the past 20 years offer reasons for hope [@vanderweele2015]. Although most of this progress has occurred outside of psychology, interest amongst psychological scientists is growing [@mcelreath2020;@rohrer2018]. Efforts to unite methods for causal inference with observational environmental psychology research is both promising and timely.

We believe that rate of progress in the uptake of causal inference in environmental psychology (and in psychological science more broadly) will turn on how rapidly researchers understand the theoretical basis on which methods for causal inference rely. The theoretical basis of causal effect estimation is vital because to obtain causal effect estimates from observational data relies on assumptions. That is, before researchers apply statistical methods to data, they must devote considerable attention to planning and design than is their custom. Whether the assumptions required for causal inference are credibly met will typically require the input of experts familiar with the questions at hand. In some cases the data allow reasoned guesses about whether assumptions have been satisfied. However such cases are the exception are rare. That causal inference relies on untestable assumptions invites many risks. On the one hand, if we persist in demanding verification from data researchers obtained from conventional thresholds they risk missing causal inferences that are supported by reasonable assumptions. On the other hand, researchers might be tempted to admit implausible assumptions without warrant. This latter risk is particularly worrying in those social sciences where publication biases favour compelling narratives couched in causal language, often with a one or two sentence caveat about strong causal attributions at the end of the paper. Too few journals are interested in studies that report uncertainty, or that provide detailed accounts of methodological limitations. . We believe that to accelerate progress in environmental psychology, and in the human sciences more generally, the theoretical foundations of causal inference must be accessible and transparently communicated. The assumptions that underpin causal inference must be explained in a way that builds a clear understanding about the underlying problem each assumption addresses. Researchers must understand how these assumptions flow on to the tasks of data collection and data organisation. For causal inference to bring about the scientific transformation, each practical step from assumption to data collection must also be grounded in scientifically supported assumptions. Because the theoretical foundations of causal inference remain unfamilar and opaque to most psychological scientists, including those with strong backgrounds in statistical modelling, we hope this introduction outlining the foundations of causal inference from association data will encourage more environmental psychologists to adopt these new conceptual frameworks and methods.

## Overview

**Part 1**  introduces the “potential outcomes” framework of causal inference [@hernan2023], and describes the three fundamental assumptions of causal inference. Here, we develop causal intuitions by anchoring them in the framework of randomised experiments, which will be familiar to most readers. We shall discover that the building intuitions for causal inference from an understanding of experiments help demystify the assumptions on which causal inference relies and offers clear directives for data collection. Surprisingly, perhaps, this discussion will bring new theoretical understanding of experiments that may be helpful to improving their designs.

**Part 2** introduces Directed Acyclic Graphs (DAGs) – or causal diagrams – as powerful tools for investigating the implications of causal assumptions. We note that the functionality of causal diagrams rests on three rules, and that these three rules, in turn, rest on a robust system of mathematical proofs. Although these proofs should inspire confidence, it will be come clear that causal inference typically relies on strong assumptions about how causation operates in the world. Indeed a causal diagram typically assumes ever causal relationship except the specific causal effect to be evaluated. Thus, not only does causal inference rely on the three general fundamental assumptions described in part 1, causal inference inevitably relies on a considerable scientific assumptions. For this reason, the assumptions encoded in a causal diagram are sometimes called 'structural assumptions.' Of course, not all scientists agree about the world. However, causal diagrams are tools by which scientists can make their disagreements explicit. Moreover, different causal diagrams can be constructed to reflect different assumptions. In some cases, points of disagreement may be irrelevant to the question at hand, either because the valid causal estimates may be obtained regardless of where one stands, or because confounding is unavoidable.  Of course the magnitude of the causal effect under consideration -- also called the 'treatment effect' or the 'exposure effect' -- cannot be assumed.  If it could, there would be no point to conducting one's study. Causal diagrams are qualitative tools that transform difficult mathematical and statistical problems into problems that can be readily understood by visual inspection of the graph.  Although we cannot hope to cover the full power of causal diagrams to assist researchers in investigating causal questions, we offer basic strategies for constructing and interpreting causal diagrams that will be suitable for many question that arise in environmental psychology. 

**Part 3**  experiments are the gold standard for quantifying the causal effects of interventions. To investigate causality, environmental psychologists should use experiments wherever doing so is possible. Unfortunately it is impractical or unethical to perform experiments for many fundamental questions in environmental psychology. For this reason, questions about whether environmental psychologists should use experiments are often framed as questions about internal and external validity. In Part 3, we use causal diagrams to examine how threats to internal validity arise for experimental designs irrespective of external validity. Widespread confusion about confounding in experiments is particularly worrying when experimentals assume that randomisation studies in large samples avoid problems of confounding. Here, we demonstrate how methods for causal inference in observational settings are very much relevant for experimentalists. We hope to provide readers of this volume with an understanding of why methods for causal should be routinely taught as part of experimental methodology courses.

## Part 1: Introducing the Potential Outcomes Framework of Causal Inference

The potential outcomes framework was introduced by Jerzy Neyman in his work on agricultural experiments and their effectiveness [@neyman1923]. Half a century later, Harvard statistician Donald Rubin expanded this framework for causal inference in non-experimental contexts [@rubin1976]. Later, Jamie Robins further developed it to address confounding in scenarios with multiple treatments and time-varying factors [@robins1986]. A key concept in this framework is 'counterfactual outcomes.''

To understand the role of counterfactual outcomes in causal inference, let's use a personal crossroads scenario. Suppose you are finishing university and deciding between graduate school and an industry job. You get accepted into your dream graduate program at the University of Canterbury and start planning your move to Christchurch, New Zealand. Then, you receive a job offer from Acme Nuclear Fuels, a company innovating in energy alternatives to fossil fuels. Each choice leads to vastly different life trajectories — in daily routines, income, social interactions, romantic interests, and possibly your overall sense of purpose.

Formally, let $A$ signify the decision to attend graduate school ($A = 1$) or work in industry ($A = 0$). The potential outcomes are $Y_{\text{you}}(1)$ and $Y_{\text{you}}(0)$, representing the hypothetical life scenarios under each choice. To assess the causal effect of your decision, we consider the difference $Y_{\text{you}}(1) - Y_{\text{you}}(0)$, although this difference is inherently unobservable. Once a decision is made, it is impossible to know with certainty the life path not taken.

$$
(Y_{\text{you}}|A_{\text{you}} = 1) = Y_{\text{you}}(1) \quad \text{implies} \quad Y_{\text{you}}(0)|A_{\text{you}} = 1~ \text{is counterfactual}.
$$

and vice versa for the opposite treatment when you receive $A = 0$.

This scenario exemplifies 'the fundamental problem of causal inference' as identified by Rubin and Holland [@rubin2005; @holland1986]: we cannot observe both potential outcomes for the same individual.


### Understanding Relationships of Cause and Effect Through Intervention Outcomes

Supposwe we want to examine the causal effect of easy access to urban green spaces on psychological well-being [@nguyen2021green; @reyes2021linking]. Let's focus on 'subjective happiness,' hereafter referred to as 'happiness.'' Assume happiness exists and is measurable.  We will represent happiness as $Y$, which denotes the outcome under consideration. 

For simplicity, we will consider the  intervention or treatment, 'ample access to green space,' to be a binary variable. Our points will generalise to continuous treatments, so this simplification does not come at the loss of generality. Denote this treatment with the letter $A$. Define $A = 1$ as 'having ample access to greenspace' and $A = 0$ as 'lacking ample access to greenspace.' Assume these conditions are mutually exclusive. Our target population is New Zealanders in the 2020s. A preliminary causal question might be:

'In New Zealand, does proximity to abundant green spaces increase self-perceived happiness compared to environments lacking such spaces?''


Next, suppose that it were ethical to experimentally manipulate and randomise individuals into different treatment conditions. We assume the fundamental problem of causal inference abides. Once an individual is assigned to one treatment condition, we cannot observe that individual's outcome for the treatment condition that was not assigned. How might experiments help us to identify causality?

The first point to notice is that in the context of causal inference, even well-designed experiments face the challenge of missing values in the potential outcomes. Indeed this observation formed the basis of  Jerzy Neyman's interest in how exactly agricultural experiments permit causal inferences [@neyman1923]. This issue is inherent to the nature of causal inference where, for each individual, we can only observe one of the potential outcomes. The fundamental problem of causal inference applies to every individual who assigned to one condition or another. 

To elaborate, let us break down the Average Treatment Effect (ATE) in the context of an experiment according to the potential outcomes that are actually observed. By average treatment effect we mean a contrast between the average effect (for the sampled population) in one treatment condition with the average treatment effect (for the sampled population) in a well-defined contrast condition. The ATE is the causal effect that experiments hope to recover. Recall that we are interested in potential outcomes, and we use the notiation $Y(a)$ to denote the outcome under a specific condition, for example $Y(A=1)$ or $Y(A=0)$.  We assume that when a person receives a treatment their potential outcome for the treatment received is observed, that is, $Y_{\text{i}}|A_i = a) = Y_{\text{a}}(1)$ for all individuals $i\dots N$ in the study.  

Breaking down the ATE into observed and unobserved outcomes gives us: 

$$
\text{Average Treatment Effect} = \left(\underbrace{\underbrace{\mathbb{E}[Y(1)|A = 1]}_{\text{observed for } A = 1} + \underbrace{\mathbb{E}[Y(1)|A = 0]}_{\text{unobserved for } A = 0}}_{\text{effect among treated}}\right) - \left(\underbrace{\underbrace{\mathbb{E}[Y(0)|A = 0]}_{\text{observed for } A = 0} + \underbrace{\mathbb{E}[Y(0)|A = 1]}_{\text{unobserved for } A = 1}}_{\text{effect among untreated}}\right).
$$

In this expression, $\mathbb{E}[Y(1)|A = 1]$ represents the average outcome when the treatment is given, which is observable. Conversely, $\mathbb{E}[Y(1)|A = 0]$ represents the average outcome if the treatment had been given to those who were actually untreated, which remains unobservable. This duality is also true for the control condition, creating a situation where half of the necessary values for calculating the ATE are inherently missing. Another way of putting this is that the fundamental problem of causal inference lurks in the background of experiments. For each individual we cannot know how the intervention would have turned out had they received the alternative treatment to what they in fact received, any more than you can quantiatively deterime how life would have turned out had you decided to take the job at Acme Nuclear Fuels after deciding in fact to attend the University of Canturbury. 

To understand how experiments magically recover the missing observations needed to compute average treatment effects we need to consider the concept of a 'confounder.'

#### Understanding Confounders in Causal Inference

Causal inference encompasses various types of confounders, which we will explore in Part 2. For now, let us focus on a specific type of confounder critical for understanding how experiments achieve unbiased average treatment effects. This type is a common cause of both the treatment ($A$) and the outcome ($Y$), often referred to in psychology as the 'third variable.' It is the focal point of 'control' strategies.

For instance, when we 'control for' income in an analysis, we aim to address the possibility that income may simultaneously influence both the intervention and the outcome. By doing so, we strive to isolate the effect of the intervention from the potential confounding influence of income.

Put simply, a confounder is a variable that simultaneously affects the treatment assignment and the outcome. Its presence can lead to biased estimates of causal effects, as it introduces variations in the outcome not directly attributable to the treatment. Understanding and controlling for confounders is therefore crucial for accurate causal inference.

#### Understanding the balance of confounders in experiments


In experimental designs the random assignment of treatment is intended to create balance in confounders across treatment groups. Balance means that the distribution of all confounders is, on average, the same in both the treatment and control groups. This balance allows us to assume that the only systematic difference between these groups is the treatment itself.

Let us denote the set of all possible confounders by the letter $L$.  In causal inference, we can express the absence of confounding in one of two equivalent ways. 'Balance,'' in this context, means that the distribution of all potential confounding variables, collectively denoted as $L$, is essentially the same across both the treatment and control groups. This uniformity ensures that any differences observed between these groups can be attributed solely to the treatment and not to underlying differences in the participants.

**Expressing the absence of confounding**: In causal inference, we can articulate the absence of confounding in two equivalent ways:
   
- Considering the potential outcomes given the treatment and confounders:
     $$
     Y(a) \coprod A \mid L
     $$
   
- Or, looking at the treatment assignment given the potential outcomes and confounders:
     $$
     A \coprod Y(a) \mid L
     $$
   
Although this mathematical formalism my perhaps seem heavy handed, it will be useful for better understand strategies in causal inference for emulating the randomisation of experiments. 

**What randomisation delivers**: in an ideal RCT, effective random treatment assignment leads to balanced conditions across all variables that might simultaneously influence both the receipt of treatment and the outcome. The absence of confounding, achieved through this balance, enables us to draw more reliable causal inferences.

Your points on the steps in establishing causal inference in a Randomized Controlled Trial (RCT) are well-framed. Let's refine and expand on them for enhanced clarity and completeness:

1. **Exchangeability**: this principle asserts that within each treatment group, the potential outcomes are statistically independent of the treatment assignment, given the confounders. Simply put, this means that the treatment groups are comparable. Exchangeability is crucial because it ensures that any differences observed between groups are attributable to the treatment rather than pre-existing differences. In an RCT, randomisation facilitates this exchangeability by evenly distributing both observed and unobserved confounders across treatment groups.

2. **Consistency**: this principle asserts that there is an alignment between the observed outcome for an individual and the potential outcome under the treatment they actually received. Consistency is critical as it links the theoretical construct of potential outcomes to the actual observed data, enabling the practical application of causal inference methods.

3. **Positivity**: this principle states that there is a nonzero probability of receiving each treatment level within every subgroup defined by the covariates $L$. This condition is crucial for estimating treatment effects across different levels of confounders and avoiding extrapolation beyond the observed data. This principle is often implicitly assumed a randomised experiment, however, as we shall see, positivity must be an explicit consideration in observational studies. 

These three principles are foundational in addressing the core challenge of causal inference: making valid inferences about potential outcomes that are not directly observable. In randomised experiments, provided that randomisation is successful and the sample size is sufficiently large, we can be confident that the average treatment effects observed within the groups are representative of the population. It is important to note that, while randomisation inherently supports exchangeability and often positivity, these assumptions still need to be verified.  We will return to thse points in section 3.  For now, we have seen that "under the hood", exchangeability consistency, and positivity, allow experimental scientists to obtain valid **average treatment effects** for interventions despite the fact that for every individual we cannot directly compute individual treatment effects from data. Causal inference at some level fundamentally involves reasoning about how events would have turned out had the world been different to how it was realised. 

#### Causal Inference in Observational Settings

In observational research, where the researcher does not have control over the treatment assignment, the objective is to emulate a controlled experimental environment as closely as possible. However, this emulation presents unique challenges, particularly in defining and applying treatments consistently across the study population.

**Variability in treatment definition**

Take, for example, the scenario of assessing the impact of living near green spaces. The concept of 'proximity to green spaces' itself varies significantly, leading to a diverse range of experiences that can all be classified under the same 'treatment'. This variability can be broken down into several factors:

- **Diversity of green spaces**: the biodiversity and aesthetic value of these spaces can vary widely. Some individuals might have access to well-maintained parks with a rich variety of flora and fauna, while others might only have access to basic recreational areas with limited natural appeal. This diversity means that what one person experiences as 'access to green spaces' can be fundamentally different from another's experience.

- **Availability of amenities**: the presence of amenities such as walking paths, benches, recreational facilities, and cafes can significantly enhance the experience of a green space. Such amenities can encourage more frequent and prolonged visits, potentially leading to greater benefits in terms of mental and physical well-being.

- **Variation in proximity**: the distance to the nearest green space can also influence how often and how easily individuals can access these areas. For someone living right next to a park, the green space might be an integral part of their daily routine. In contrast, for someone living several kilometres away, visits might be infrequent and less impactful.

- **Size and type of green space**: the type of green space (e.g., urban park, community garden, or large forested area) and its size can also affect the nature of the interaction individuals have with it. Larger, more naturalistic spaces might offer a different set of psychological and physical benefits compared to smaller, urban parks.

**Challenges with exchangeability**

In observational studies, achieving exchangeability – where the groups being compared are similar in all respects except for the treatment – is a significant challenge. Individuals who live near green spaces might differ systematically from those who do not in several ways:

- **Socioeconomic status**: there might be a correlation between an individual’s economic background and their proximity to green spaces. For instance, more affluent individuals might be able to afford housing in areas with better access to high-quality green spaces.

- **Age demographics**: different age groups might naturally gravitate towards or away from green spaces. Younger individuals or families with children might place more value on having access to parks, whereas this might be less of a priority for other demographics.

- **Mental health considerations**: people with existing mental health issues might either seek out green spaces for their therapeutic benefits or avoid them due to social anxiety or other factors.

- **Lifestyle choices**: Those who prefer outdoor activities might choose to live near green spaces. In these cases, it is challenging to disentangle whether the proximity to green spaces is causing improved well-being or whether it is merely a characteristic of individuals who already lead healthier lifestyles.

- **Personal values and social connections**: The decision to live near green spaces might also be influenced by personal values like environmentalism or the desire to be part of a community that values green spaces. Such values and social connections can affect how individuals interact with and benefit from these spaces.

In observational studies, these and other unmeasured factors can introduce biases that complicate the interpretation of causal relationships.

**Understanding and assessing positivity**

Recall that positivity is the requirement that every individual has a chance of receiving each level of treatment to be compared. In observational studies, this can be a major challenge. For instance, in some urban areas, it might be practically impossible for certain demographics to have access to green spaces due to factors like housing prices or availability. This lack of variability in treatment exposure within certain strata can make it difficult to draw valid causal inferences.
Meeting the challenges posed by observational settings necessitates a thorough understanding of the context and a meticulous application of statistical methods to closely mimic the conditions of a randomised experiment. In essence, the more closely our data approximates a randomised controlled experiment, the more confidence we can place in our causal inferences. However, it is important to acknowledge that often, the data may not provide a high level of confidence. By consistently referencing the gold standard of a randomised experiment, environmental psychologists can enhance their understanding and effectively communicate both the strengths and limitations of observational data in answering causal questions, many of which cannot be addressed experimentally due to practical or ethical constraints.

In Section 2, we will explore how causal diagrams can significantly augment our understanding of these strengths and limitations. These diagrams serve as powerful tools for visualising and analysing the relationships and potential confounding factors within observational data. They provide a framework for identifying and addressing the assumptions necessary for causal inference, thereby offering a clearer pathway for interpreting complex data sets. This approach is instrumental in bridging the gap between the idealised conditions of a randomised experiment and the realities of observational studies, ultimately enriching the environmental psychologist's toolkit for scientific understanding.

## Part 2. An Introduction to Causal Diagrams

Causal diagrams are powerful tools for evaluating causal inferences [@pearl1995; @pearl2009; @greenland1999].


::: {#tbl-01}

```{=latex}
\terminologylocalconventions
```
Terminology for causal diagrams. This table is adapted from [@bulbulia2023]

:::


@tbl-01 presents coventions we will use for describing variables in causal diagrams


::: {#tbl-02}

```{=latex}
\terminologygeneral
```
Basic conventions. This table is adapted from [@bulbulia2023]

:::


@tbl-02 presents the coventions we will use for constructing causal diagrams




::: {#tbl-03}

```{=latex}
\terminologydirectedgraph
```
This table is adapted from [@bulbulia2023]

:::




::: {#tbl-04}

```{=latex}
\terminologyelconfounders
```
This table is adapted from [@bulbulia2023]

:::





To use causal diagrams, we must understand the following terminology and conventions:

1.  **Nodes and edges**: nodes represent variables or events within a causal system, while edges signify relationships or interactions between these variables.

2.  **Ancestors and descendants**: we call a variable an "ancestor" if it directly or indirectly influences another variable.
    Conversely, we call a variable a "descendant" if it is influenced, directly or indirectly, by another variable.

3.  **D-separation**: we call a path "blocked," or "d-separated," if a node along it prevents the transmission of influence.
    Two variables are considered d-separated if all paths between them are blocked; otherwise, they are d-connected[@pearl1995].

4.  **Acyclic**: Causal diagrams must be acyclic -- they cannot contain feedback loops.
    More precisely: no variable can be an ancestor or descendant of itself.
    *Therefore, in cases where repeated measurements are taken, nodes must be indexed by time.* As mentioned, repeated measures time series data are almost always required to estimate causal effects quantitatively.
    In Part 3 we consider how adding baseline measures of the outcome and exposure in a three-wave repeated measures design greatly enhances causal estimation [@pearl2009].
    To represent the nodes of this design on a graph we must index them by time because the nodes are repeated.

Pearl showed that the principles of d-separation enable us to evaluate relationships between nodes in a causal diagram [@pearl1995].

The rules of d-separation:

a.  **Chain rule**: in a chain structure, where three variables are connected sequentially (represented as $A \rightarrow B \rightarrow C$,) conditioning on $B$ d-separates $A$ and $C$.

b.  **Fork rule**: in a fork structure, where $B$ is a common cause of both $A$ and $C$ (represented as $A \leftarrow B \rightarrow C$,) conditioning on $B$ d-separates $A$ and $C$.

c.  **Collider rule**: in a collider structure, where $B$ is a common effect of both $A$ and $C$ (represented as $A \rightarrow B \leftarrow C$,) $B$ d-separates $A$ and $C$ only if neither $B$ nor any of $B$'s descendants are conditioned upon.

In each case, if $B$ does not d-separate $A$ and $C$, $A$ and $C$ are considered to be d-connected given $B$.
This suggests an open path between $A$ and $C$.
If all paths between $A$ and $C$ are blocked, or equivalently, if no path remains open, then $A$ and $C$ are d-separated given a set of conditioning variables [@pearl2009].

The rules of d-separation clarify which variables to adjust for when estimating causal effects: we seek a set of variables that d-separates the exposure from the outcome.
By conditioning on such an adjustment set, we block all confounding paths, leaving only the causal effect [@pearl2009].

6.  **Adjustment set**: a collection of variables that we either condition upon or deliberately avoid conditioning upon to block all backdoor paths between the exposure and the outcome in the causal diagram [@pearl2009].

7.  **Confounders**: a member of an adjustment set.
    Importantly, *we call a variable as a "confounder" in relation to a specific adjustment set.*[^1]

### Criteria for conditioning 


[^1]: VanderWeele's Modified Disjunctive Cause Criterion provides practical guidance for controlling for confounding [@vanderweele2019]: According to this criterion, a member of any set of variables that can reduce or remove the bias caused by confounding is deemed a member of this confounder set.
    VanderWeele's strategy for defining a confounder set is as follows: a.
    Control for any variable that causes the exposure, the outcome, or both.
    b.
    Control for any proxy for an unmeasured variable that is a shared cause of both exposure and outcome.
    c.
    Define an instrumental variable as a variable associated with the exposure but does not influence the outcome independently, except through the exposure.
    Exclude any instrumental variable that is not a proxy for an unmeasured confounder from the confounder set.

    Note that the concept of a "confounder set" is broader than the concept of an "adjustment set." Every adjustment set is a member of a confounder set.
    So the Modified Disjunctive Cause Criterion will eliminate confounding when the data permit.
    However a confounder set includes variables that will reduce confounding in cases where confounding cannot be eliminated.
    Confounding can almost never be elimiated with certainty.
    For this reason we perform sensitivity analyses.
    However confounding should be reduced wherever possible.
    Hence we opt for "confounder sets."

8.  **Backdoor criterion**: a set of conditions under which the effect of a treatment on an outcome can be obtained by controlling for a specific set of variables.
    The backdoor criterion guides the selection of **adjustment sets** [@pearl1995].

9.  **Identification problem**: the challenge of estimating the causal effect of a variable using observed data.
    Causal diagrams were developed to address the identification problem.

Alice now turns to the description of the elemental confounds

### 1. The problem of confounding by a common cause

The problem of confounding by common cause arises when there is a variable, denoted by $L$, that influences both the exposure, denoted by $A$, and the outcome, denoted by $Y.$ Because $L$ is a common cause of both $A$ and $Y$, $L$ may create a statistical association between $A$ and $Y$ that does not reflect a causal association.

For instance, in the context of green spaces, consider people choosing to live closer to green spaces (exposure $A$) and their experience of improved mental health (outcome $Y$).
A common cause could be socioeconomic status ($L$).
Individuals with higher socioeconomic status may have the financial capacity to afford housing near green spaces and simultaneously afford better healthcare and lifestyle choices, contributing to improved mental health.
Thus, while the data may show a statistical association between living closer to green spaces ($A$) and improved mental health ($Y$), this association may not reflect a direct causal relationship due to the confounding by socioeconomic status ($L$).

The figure referenced as @fig-dag-common-cause represents such a scenario.
The association of $A$ and $Y$ in the data is confounded by the common cause $L$.
The dashed red arrow in the graph signifies the bias introduced by the open backdoor path from $A$ to $Y$ that arises due to their common cause $L$.

```{tikz}
#| label: fig-dag-common-cause
#| fig-cap: "Counfounding by a common cause. The dashed path indicates bias arising from the open backdoor path from A to Y."
#| out-width: 80%
#| echo: false

\usetikzlibrary{positioning}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations}
\tikzstyle{Arrow} = [->, thin, preaction = {decorate}]
\tikzset{>=latex}
% Define a simple decoration
\tikzstyle{cor} = [-, dashed, preaction = {decorate}]


\begin{tikzpicture}[{every node/.append style}=draw]
\node [ellipse, draw=white] (L) at (0, 0) {L$_{t0}$};
\node [rectangle, draw=white] (A) at (4, 0) {A$_{t1}$};
\node [rectangle, draw=white] (Y) at (8, 0) {Y$_{t2}$};
\draw [cor, draw=red] (A) to (Y);
\draw [-latex, bend left, draw =black] (L) to (Y);
\draw [-latex, black] (L) to (A);
\end{tikzpicture}
```

### Advice: attend to the temporal order of all measured variables

Addressing confounding by a common cause involves its adjustment.
This adjustment effectively closes the backdoor path from the exposure to the outcome.
Equivalently, conditioning on $L$ d-separates $A$ and $Y$.
Common adjustment methods include regression, matching, inverse probability of treatment weighting, and G-methods (covered in [@hernán2023]).
@fig-dag-common-cause-solution clarifies that any confounder that is a common cause of both $A$ and $Y$ must precede $A$ (and hence $Y$), since effects follow their causes chronologically.

After we have time-indexing the nodes on the graph it becomes evident that **control of confounding generally necessitates time-series data.**

```{tikz}
#| label: fig-dag-common-cause-solution
#| fig-cap: "Solution: adjust for pre-exposure confounder. The implication: obtain time series data to ensure the confounder occurs before the exposure."
#| out-width: 80%
#| echo: false

\usetikzlibrary{positioning}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations}
\tikzstyle{Arrow} = [->, thin, preaction = {decorate}]
\tikzset{>=latex}

% Define a simple decoration
\tikzstyle{cor} = [-, dashed, preaction = {decorate}]

\begin{tikzpicture}[{every node/.append style}=draw]
\node [rectangle, draw=black] (L) at (0, 0) {L$_{t0}$};
\node [ellipse, draw=white] (A) at (4, 0) {A$_{t1}$};
\node [rectangle, draw=white] (Y) at (8, 0) {Y$_{t2}$};
\draw [-latex, draw=white] (A) to (Y);
\draw [-latex, bend left, draw =black] (L) to (Y);
\draw [-latex, black] (L) to (A);
\end{tikzpicture}

```

### 2. Confounding by collider stratification (conditioning on a common effect)

Conditioning on a common effect, also known as collider stratification, occurs when a variable, denoted by $L$, is influenced by both the exposure, denoted by $A$, and the outcome, denoted by $Y$.

Imagine, in the context of green spaces, an individual's choice to live closer to green spaces (exposure $A$) and their improved mental health (outcome $Y$) are both influencing the individual's overall satisfaction with life (common effect $L$).
Initially, $A$ and $Y$ could be independent, represented as $A \coprod Y(a)$, suggesting that the decision to live near green spaces is not directly causing improved mental health.

However, when we condition on the life satisfaction $L$ (the common effect of $A$ and $Y$), a backdoor path between $A$ and $Y$ is opened.
This could potentially induce a non-causal association between living closer to green spaces and improved mental health.
The reason behind this is that the overall life satisfaction $L$ can provide information about both the proximity to green spaces $A$ and the mental health status $Y$.
Hence, it may appear that there is an association between $A$ and $Y$ even when there may not be a direct causal relationship.

```{tikz}
#| label: fig-dag-common-effect
#| fig-cap: "Confounding by conditioning on a collider. The dashed red path indicates bias from the open backdoor path from A to Y."
#| out-width: 80%
#| echo: false

\usetikzlibrary{positioning}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations}
\tikzstyle{Arrow} = [->, thin, preaction = {decorate}]
\tikzset{>=latex}

% Define a simple decoration
\tikzstyle{cor} = [-, dashed, preaction = {decorate}]

\begin{tikzpicture}[{every node/.append style}=draw]
\node [rectangle, draw=white] (A) at (0, 0) {A$_{t0}$};
\node [ellipse, draw=white] (Y) at (4, 0) {Y$_{t1}$};
\node [rectangle, draw=black] (L) at (8, 0) {L$_{t2}$};
\draw [-latex, draw=black, bend right] (A) to (L);
\draw [-latex, draw=black] (Y) to (L);
\draw [cor, draw=red] (A) to (Y);

\end{tikzpicture}

```

### Advice: attend to the temporal order of all measured variables

To address the problem of conditioning on a common effect, we should *generally* ensure that:

1.  all confounders $L$ that are common causes of the exposure $A$ and the outcome $Y$ are measured before $A$ has occurred, and
2.  $A$ is measured before $Y$ has occurred.

If such temporal order is preserved, $L$ cannot be an effect of $A$, and thus neither of $Y$.[^2]

[^2]: This rule is not absolute.
    As indicated in @fig-dag-descendent-solution, it may be helpful in certain circumstances to condition on a confounder that occurs after the outcome has occurred.

```{tikz}
#| label: fig-dag-common-effect-solution
#| fig-cap: "Solution: time idexing of confounders helps to avoid collider bias and maintain d-separation. The graph makes the imperative clear: we must collect time series data with confounders measured before the exposure, and that we must likewise measure the exposure before the outcome, with data collected repeatitively on the same units."
#| out-width: 80%
#| echo: false

\usetikzlibrary{positioning}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations}
\tikzstyle{Arrow} = [->, thin, preaction = {decorate}]
\tikzset{>=latex}
% Define a simple decoration
\tikzstyle{cor} = [-, dashed, preaction = {decorate}]


\begin{tikzpicture}[{every node/.append style}=draw]

\node [rectangle, draw=black] (L) at (0, 0) {L$_{t0}$};
\node [ellipse, draw=white] (A) at (4, 0) {A$_{t1}$};
\node [rectangle, draw=white] (Y) at (8, 0) {Y$_{t2}$};
\draw [-latex, draw=white] (A) to (Y);
\draw [-latex, bend left, draw =black] (L) to (Y);
\draw [-latex, black] (L) to (A);
\end{tikzpicture}

```

### M-bias: conditioning on a collider that occurs before the exposure may introduce bias

Typically, indicators for confounders should be included only if they are known to be measured before their exposures - with notable exceptions described below in fig-dag-descendent-solution-2.

In the context of green spaces, consider the scenario where an individual's level of physical activity ($L$) is influenced by an unmeasured factor related to their propensity to live near green spaces ($A$) and another unmeasured factor linked to their mental health ($Y$).
Here, physical activity $L$ does not directly affect the decision to live near green spaces $A$ or mental health status $Y$, but is a descendent of unmeasured variables that do.

If we condition on physical activity $L$ in this scenario, we evoke what is known as "M-bias".
If $L$ is neither a common cause of $A$ and $Y$ nor the effect of a shared common cause, then $L$ should not be included in a causal model.
@fig-m-bias represents a case where $A \coprod Y(a)$ but $A \cancel{\coprod} Y(a)| L$.

M-bias is another example of collider stratification bias, a phenomenon where conditioning on a common effect or a descendent of a common effect induces an association between variables that were previously independent [@cole2010].[^3]

[^3]: Note, when we draw a chronologically ordered path from left to right the M shape for which "M-bias" takes its name changes to an E shape We shall avoid proliferating jargon and retain the term "M bias."

```{tikz}
#| label: fig-m-bias
#| fig-cap: "M-bias: confounding control by including previous outcome measures. The dashed red path indicates bias from the open backdoor path from A to Y by conditioning on pre-exposure variable L. The solution: do not condition on L.  The graph makes it evident that conditioning on variables measured before the exposure is not sufficient to prevent confounding."
#| out-width: 80%
#| echo: false

\usetikzlibrary{positioning}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations}
\tikzstyle{Arrow} = [->, thin, preaction = {decorate}]
\tikzstyle{DoubleArrow} = [-, preaction = {decorate}]
\tikzset{>=latex}

% Define a simple decoration
\tikzstyle{cor} = [-, dashed, preaction = {decorate}]


\begin{tikzpicture}[{every node/.append style}=draw]

\node [rectangle, draw=white] (U1) at (0, 2) {U1};
\node [rectangle, draw=white] (U2) at (0, -2) {U2};
\node [rectangle, draw=black, align=left] (L) at (4, 0) {L$_{t0}$};
\node [rectangle, draw=white] (A) at (8, 0) {A$_{t1}$};
\node [rectangle, draw=white] (Y) at (12, 0) {Y$_{t2}$};

\draw [-latex, draw=black] (U1) to (L);
\draw [-latex, draw =black] (U2) to (L);
\draw [-latex, draw=black, bend left] (U1) to (Y);
\draw [-latex, draw =black, bend right] (U2) to (A);
\draw [cor,  draw=red] (A) to (Y);


\end{tikzpicture}
```

### Advice: adopt the modified disjunctive cause criterion for confounding control

Again, the modified disjunctive cause criterion will satisfy the backdoor criterion in all cases and reduce bias where this criterion cannot be fully satisfied:

a.  Control for any variable that causes the exposure, the outcome, or both.
b.  Control for any proxy for an unmeasured variable that is a shared cause of both exposure and outcome.
c.  Define an instrumental variable as a variable associated with the exposure but does not influence the outcome independently, except through the exposure. Exclude any instrumental variable that is not a proxy for an unmeasured confounder from the confounder set (see: @vanderweele2020 page 441, [@vanderweele2019])

Of course, the difficulty is in determining which variables belong to a confounder set.
Specialist knowledge can facilitate this task.
However, the data alone typically do not settle this question.
(For exceptions see: bulbulia2021).

### 3. Mediator bias

Applying this to our green spaces example, again we consider proximity to green spaces as the exposure ($A$), mental health as the outcome ($Y$), and physical activity as the mediator ($L$).

In this scenario, living close to green spaces ($A$) influences physical activity ($L$), which subsequently impacts mental health ($Y$).
If we condition on physical activity ($L$), we may bias our estimates of the total effect of proximity to green spaces ($A$) on mental health ($Y$).
This bias arises because conditioning on $L$ can obscure the direct effect of $A$ on $Y$, as it blocks the indirect path through $L$.
This phenomenon, known as mediator bias, is depicted in @fig-dag-mediator.

One might assume that conditioning on a mediator does not introduce bias when there is no causal relationship between $A$ and $Y$.
However, this is not always the case.
Consider a situation where $L$ is a common effect of the exposure $A$ and an unmeasured variable $U$ linked to the outcome $Y$.
Here, including $L$ may inflate the association between $A$ and $Y$, even if $A$ is not related to $Y$ and $U$ does not cause $A$.
This case is depicted in @fig-dag-descendent.

Hence, unless one is specifically investigating mediation analysis, it is generally inadvisable to condition on a post-treatment variable.
Being aware of the timeline in the spatial organisation of the graph underlines a critical principle for data collection: if we cannot guarantee that $L$ is measured before $A$, and if $A$ may affect $L$, including $L$ in our model could lead to mediator bias.
This scenario is illustrated in @fig-dag-descendent.

```{tikz}
#| label: fig-dag-mediator
#| fig-cap: "Confounding by conditioning on a mediator. The dashed black arrow indicates bias arising from partially blocking the path between A and Y."
#| out-width: 80%
#| echo: false

\usetikzlibrary{positioning}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations}
\tikzstyle{Arrow} = [->, thin, preaction = {decorate}]
\tikzset{>=latex}

% Define a simple decoration
\tikzstyle{cor} = [-, dashed, preaction = {decorate}]

\begin{tikzpicture}[{every node/.append style}=draw]
\node [ellipse, draw=white] (A) at (0, 0) {A$_{t0}$};
\node [rectangle, draw=black] (L) at (4, 0) {L$_{t1}$};
\node [rectangle, draw=white] (Y) at (8, 0) {Y$_{t2}$};
\draw [-latex, bend left, draw=black, dotted] (A) to (Y);
\draw [-latex, draw =black] (L) to (Y);
\draw [-latex, black] (A) to (L);
\end{tikzpicture}
```

### Advice: attend to the temporal order of all measured variables

To mitigate the issue of mediator bias, particularly when focusing on total effects, we should generally avoid conditioning on a mediator.
We avoid this problem by ensuring that $L$ occurs before the treatment $A$ and the outcome $Y$ (Note: a counter-example is presented in @fig-dag-descendent-solution-2).
Again, we discover the importance of explicitly stating and measuring the temporal order of our variables.[^4]

[^4]: Note that if $L$ were associated with $Y$ and could not be caused by $A$, conditioning on $L$ would typically enhance the precision of the causal effect estimate of $A \to Y$.
    This precision enhancement holds even if $L$ occurs after $A$.
    However, the onus is on the researcher to show that the post-treatment factor cannot be a consequence of the exposure.

```{tikz}
#| label: fig-dag-mediator-solution
#| fig-cap: "Solution: do not condition on a mediator. The implication: by ensuring temporal order in data collection we diminish the probabilty of  mistaking an effect of an exposure for its confounder."
#| out-width: 80%
#| echo: false

\usetikzlibrary{positioning}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations}
\tikzstyle{Arrow} = [->, thin, preaction = {decorate}]
\tikzset{>=latex}
% Define a simple decoration
\tikzstyle{cor} = [-, dashed, preaction = {decorate}]

\begin{tikzpicture}[{every node/.append style}=draw]
\node [rectangle, draw=black] (L) at (0, 0) {L$_{t0}$};
\node [ellipse, draw=white] (A) at (4, 0) {A$_{t1}$};
\node [rectangle, draw=white] (Y) at (8, 0) {Y$_{t2}$};
\draw [-latex, draw=blue] (A) to (Y);
\draw [-latex, bend left, draw =black] (L) to (Y);
\draw [-latex, black] (L) to (A);
\end{tikzpicture}
```

### 4. Conditioning on a descendant may induce confounding

Let us consider this principle in the context of our green spaces example.
Again denote "proximity to green spaces" by $A$, "mental health" by $Y$.
Denote "physical activity" by $L$, and "sun exposure" by $L^\prime$.

In this scenario, assume $L^\prime$, sun exposure, is caused by an unobserved variable $U$, and is influenced by $A$, the proximity to green spaces.
Further, assume $U$ affects the outcome $Y$, mental health.

Conditioning on $L^\prime$, which is a descendant of $A$ and $U$, can lead to a spurious association between $A$ and $Y$ through the path $A \to L^\prime \to U \to Y$.
This situation, shown in @fig-dag-descendent, illustrates how conditioning on a descendant can introduce confounding, resulting in a distorted causal estimation.

```{tikz}
#| label: fig-dag-descendent
#| fig-cap: "Confounding by descent: the red dashed path illustrates the introduction of bias by conditioning on the descendant of a confounder that is affected by the exposure, thus opening of a backdoor path between the exposure, A, and the outcome, Y."
#| out-width: 80%
#| echo: false

\usetikzlibrary{positioning}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations}
\tikzstyle{Arrow} = [->, thin, preaction = {decorate}]
\tikzset{>=latex}
% Define a simple decoration
\tikzstyle{cor} = [-, dashed, preaction = {decorate}]


\begin{tikzpicture}[{every node/.append style}=draw]

\node [rectangle, draw=white] (U) at (0, 0) {U};
\node [ellipse, draw=white] (A) at (2, 0) {$A_{t0}$};
\node [rectangle, draw=black](L) at (4, 0) {$L^\prime_{t1}$};
\node [ellipse, draw=white] (Y) at (6, 0) {$Y_{t2}$};

\draw [-latex, bend right=50] (U) to (L);
\draw [-latex, bend left, draw=red] (U) to (Y);
\draw [-latex,draw=black] (A) to (L);
\draw [cor, draw=red] (A) to (U);

\end{tikzpicture}
```

Again, the advice is evident from the chronology of the graph: we should measure the ($L^\prime$) before the exposure ($A$).
This solution is presented in @fig-dag-descendent-solution.

```{tikz}
#| label: fig-dag-descendent-solution
#| fig-cap: "Solution: again the graph makes it clear that our data must ensure temporal order of the measurements. By ensuring that L occurs before A confounding is controlled. The figure also makes it evident that L need not affect Y to be a confounder (i.e. a member of a confounder set)."
#| out-width: 80%
#| echo: false

\usetikzlibrary{positioning}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations}
\tikzstyle{Arrow} = [->, thin, preaction = {decorate}]
\tikzset{>=latex}

\begin{tikzpicture}[{every node/.append style}=draw]

\node [rectangle, draw=white] (U) at (0, 0) {U};
\node [rectangle, draw=black] (L) at (2, 0) {$L^{\prime}_{t0}$};
\node [rectangle, draw=white](A) at (4, 0) {$A_{t1}$};
\node [ellipse, draw=white] (Y) at (6, 0) {$Y_{t2}$};

\draw [-latex, draw=black] (U) to (L);
\draw [-latex, draw=black] (L) to (A);
\draw [-latex, bend right=50, draw =black] (U) to (Y);


\end{tikzpicture}

```

### 5. Conditioning on a descendent may reduce confounding

In our greenspace example, consider an unmeasured confounder $U$, perhaps a genetic factor, that impacts both proximity to green spaces $A$, mental health $Y$, and a variable $L^\prime$, such as a behavioural trait that only manifests later in life.

As shown in @fig-dag-descendent-solution-2, if we adjust for $L^\prime$, we might be able to reduce the confounding caused by the unmeasured $U$.
Even though $L^\prime$ may occur after the exposure and even after the outcome, conditioning on it can help control for the confounding because it acts as a proxy for an unmeasured common cause of the exposure and the outcome.

This scenario reveals that adhering strictly to a rule that only allows us to condition on pre-exposure and pre-outcome variables may not always be optimal.
It highlights the need for careful contemplation of data collection strategies.
We cannot resort solely to algorithmic rules for confounding control.
Every case necessitates its unique approach.

```{tikz}
#| label: fig-dag-descendent-solution-2
#| fig-cap: "Solution: conditioning on a confounder that occurs after the exposure and the outcome might address a problem of unmeasured confounding if the confounder is a descendent of a prior common cause of the exposure and outcome. The dotted paths denote that the effect of U on A and Y is partially adjusted by conditioning on L', even though L' occurs after the outcome. The paths are dotted to represent a reduction of bias by conditioning on the post-outcome descendent of an unmeasured common cause of the exposure and outcome.  How might this work? Consider a genetic factor that affects the exposure and the outcome early in life might be measured by an indicator late that is expressed (and may be measured) later in life. Adjusting for such an indicator would constitute an example of post-outcome confounding control."
#| out-width: 80%
#| echo: false

\usetikzlibrary{positioning}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations}
\tikzstyle{Arrow} = [->, thin, preaction = {decorate}]
\tikzset{>=latex}


\begin{tikzpicture}[{every node/.append style}=draw]

\node [rectangle, draw=white] (U) at (0, 0) {U};
\node [rectangle, draw=black] (L) at (6, -1) {$L^{\prime}_{t3}$};
\node [rectangle, draw=white](A) at (2, 0) {A$_{t1}$};
\node [ellipse, draw=white] (Y) at (4, 0) {Y$_{t2}$};

\draw [-latex, bend right = 10, draw=black] (U) to (L);
\draw [-latex, draw=black, dotted] (A) to (Y);
\draw [-latex, bend right=20, draw =black, dotted] (U) to (Y);
\draw [-latex, draw =black, dotted] (U) to (A);

\end{tikzpicture}

```

### 6. Using causal diagrams to inform data collection: the three-wave panel design

In a three-wave panel design, we collect data across three intervals to facilitate observational causal inference.
The causal diagram in @fig-dag-6 clarifies the virtues of panel data collection

1.  **Baseline Data Collection**:

    -   **Confounding Data**: At baseline, we collect data on confounders, along with data on the exposure and the outcome.
        This initial data collection phase is crucial for measuring common causes of the treatment and outcome, or the descendents of such common causes.

    -   **Exposure and Outcome Data**: Baseline measurements of exposure and outcome allow our data collection to more effectively mimic an experiment.

        -   **Incidence Effect Evaluation**: The baseline exposure allows us to interpret the post-baseline exposure effect as an incidence effect rather than a prevalence effect.
            This interpretation means we can assess the change due to a new occurrence (incidence) of the exposure rather than its overall presence (prevalence).

        -   **Sample Adequacy for Rare Exposures**: Especially when the exposure is uncommon, measuring the baseline exposure and outcome can help assess the adequacy of the sample size.

        -   **Temporal Ordering and Confounding Control**: Incorporating the outcome at baseline helps confirm the temporal order of the cause-effect relationship, thereby guarding against reverse causation.
            Moreover, when we also control for the exposure at baseline, an unmeasured confounder would have to negate the association between the exposure at one wave post-baseline and the outcome at two waves post-baseline, independent of the baseline effect.

2.  **First Follow-Up Data Collection (Baseline +1)**:

    -   At this stage, we measure the exposure. This follow-up allows us to capture the causal effect of changes in the exposure since baseline.

3.  **Second Follow-Up Data Collection (Baseline +2)**:

    -   We measure the outcome at this stage. Similar to the first follow-up, this measurement allows us to capture a controlled effect for the outcome since the baseline measurement.

Through this design, any unmeasured confounder affecting both the exposure and the outcome would need to do so independently of these baseline measurements of exposure and outcome.
Causal diagrams effectively illustrate this methodology, as shown in @fig-dag-6, clarifying the paths of causation, potential sources of confounding, and the methods used for controlling these confounders.
As a result, these diagrams are powerful tools for observational causal inference in a three-wave panel design.

```{tikz}
#| label: fig-dag-6
#| fig-cap: "Causal diagram adapted from Vanderweele et al.'s three-wave panel design. The dotted line indicates a reduction in bias arising from including baseline measures for the exposure and outcome. For an unmeasured confounder U to bias the exposure-outcome association, it would need to do so independently of these outcome and exposure baseline measures. The graph clarifies that by measuring confounders before the exposure and the exposure before the outcome, we reduce the potential for reverse causation, collider stratification, and mediator biases."
#| out-width: 80%
#| echo: false

\usetikzlibrary{positioning}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations}
\tikzstyle{Arrow} = [->, thin, preaction = {decorate}]
\tikzset{>=latex}
\tikzstyle{cor} = [-, dashed, preaction = {decorate}]


\begin{tikzpicture}[{every node/.append style}=draw]
\node [rectangle, draw=white] (U) at (0, 0) {U};
\node [rectangle, draw=black, align=left] (L) at (2, 0) {L$_{t0}$ \\A$_{t0}$ \\Y$_{t0}$};
\node [rectangle, draw=white] (A) at (4, 0) {A$_{t1}$};
\node [ellipse, draw=white] (Y) at (6, 0) {Y$_{t2}$};
\draw [-latex, draw=black] (U) to (L);
\draw [-latex, draw=black] (L) to (A);
\draw [cor, draw = black, dotted] (A) to (Y);
\draw [-latex, bend left=50, draw =black] (L) to (Y);
\draw [-latex, bend right=50, draw =black, dotted] (U) to (Y);
\draw [-latex, bend left=50, draw =black, dotted] (U) to (A);




\end{tikzpicture}
```

### 7. The Inevitability of Unmeasured Confounding

In observational research, there is an unavoidable issue known as unmeasured confounding.
These are confounders that are not included in the data set, thus not accounted for during analysis, which can introduce bias into the study.
This may occur due to constraints such as data availability, financial or ethical reasons.
Despite all efforts, it is virtually impossible to measure and control for every possible confounder.
Therefore, it is necessary to estimate the potential impact of these unmeasured confounders and consider their influence on the study's findings.

One of the methods to assess the potential impact of unmeasured confounding is through sensitivity analysis, which quantifies how strong an unmeasured confounder would need to be to fully explain away an observed association.
A commonly used metric for this purpose is the E-Value.

The E-Value quantifies the minimum strength of association that an unmeasured confounder would require with both the exposure and outcome, over and above the measured confounders, to explain away an observed exposure-outcome association.
An E-Value close to 1 suggests that the observed association is vulnerable to unmeasured confounding, whereas a high E-Value suggests that a remarkably strong unmeasured confounder would be necessary to explain the observed association, thus providing evidence towards the robustness of the observed association.

E-Values can be computed through the `EValue` package in R [@mathur2018].
This package provides an intuitive, user-friendly interface for researchers to compute E-Values.
Observational researchers have no excuses not to conduct sensitivity analyses.

### Part 3. Summary, Pitfalls and Tips.

### Summary

We introduced the potential outcomes framework of causal inference, focussing on three fundamental assumptions:

1.  **Causal Consistency**: This assumption posits that for every unit, the outcome under treatment is equal to the observed outcome if the unit was treated, and vice versa for non-treatment.
    In essence, it says that 'the treatment' is consistently defined.

2.  **Exchangeability**: The assignment of treatment is random and independent of the potential outcomes.
    This is a essential virtue of experimental designs The process of randomisation ensures that every participant has an equal chance of being assigned to the treatment or control group, creating balance in the factors that might affect the outcomes under the different treatments.
    Randomisation is powerful because it removes any systematic bias in the treatment assignment.

3.  **Positivity**: Each unit under study has a non-zero probability of receiving the treatment.
    This ensures that comparisons between treated and untreated units are meaningful and well-defined.

We observed that in the context of randomised experiments, these assumptions are generally met, and in being met, experimentalists address the problem of missing potential outcomes in the treatment groups that researchers compare.


Having built core intuitions for how experiments recover average causal effect estimates, we considered how the three fundamental assumptions required for causal inferences may be easily violated in observational studies.
Where treatment assignment is not random and can be influenced by observed or unobserved variables, correlation is not equivalent to causation.

Next, we clarified the conditions where, assuming the three fundamental assumptions of causal inference have been satisified, we may to recover causal effect estimates from observational data.

To obtain such causal effect estimates we must:

-   Precisely defining the interventions that need to be compared.
-   Conditioning on confounders, variables associated with both the treatment and the outcome, or that are descendents of such common causes.
-   Ensuring positivity, that is, each individual has some probability of receiving each level of treatment. This ensures the comparability of treatment groups.

The second part of the article discusses the use of causal diagrams for dealing with identification problems in observational settings.
The key points take home messages were:

-   For confounding control to work, researchers generally require time-series data that determine the temporal order of events.
-   Causal diagrams reveal that the three-wave panel design is a powerful tool for addressing causal questions with observational data. 
-   Nevertheless, sensitivity analysis should always be performed, and can be performed relatively easily. 

We conclude by summarising our advice through "Tips" and "Pitfalls" 

### Data Collection Tips

1.  Use time series data.
2.  Ensure significant change from baseline in treatment (positivity).
3.  Clearly define measurements for treatment, outcome, and baseline confounders.
4.  Include baseline treatment measures.
5.  Include baseline outcome measures.
6.  Strive for high sample retention.

### Graph Drawing Tips

1.  Define all nodes unambiguously.
2.  Keep the graph simple and focused.
3.  Explicitly state any novel conventions.
4.  Maintain acyclicity in the graph.
5.  Arrange nodes in chronological order.
6.  Time-stamp nodes to reflect the temporal sequence of causation.
7.  Apply a modified disjunctive cause criterion pragmatically.
8.  Add nodes for unmeasured confounding where helpful.
9.  Illustrate nodes for post-treatment selection.
10. Remember, causal diagrams are qualitative tools, not detailed maps.

### Pitfalls to Avoid

1.  Avoid using cross-sectional data.
2.  Don't misuse causal diagrams without understanding counter-factual data science.
3.  Don't create diagrams without time indices.
4.  Avoid excessive nodes in the graph.
5.  Don't draw arrows into the manipulation in experimental studies.
6.  Don't inaccurately describe bias when exposure and outcome are d-separated.
7.  Don't ignore causal diagrams during research design.
8.  Avoid representing interactions and non-linear dynamics in causal diagrams.
9.  Remember that structural equation models are not true structural models: do not mistake structural equation models for causal diagrams (NOTE Don, we haven't yet said this, but we)

{{< pagebreak >}}

## Funding

This work is supported by a grant from the Templeton Religion Trust (TRT0418).
JB received support from the Max Planck Institute for the Science of Human History.
The funders had no role in preparing the manuscript or the decision to publish it.

## References

::: {#refs}
:::





<!-- #### **Step 6.1 Understand how we may use the theory of causal inference under multiple versions of treatment to address causal consistency in observational settings**.

In observational research, there are typically multiple versions of treatment.
The theory of causal inference under multiple versions of treatment proves we can consistently estimate causal effects where the different versions of treatment are conditionally independent of the outcomes [@vanderweele2009, @vanderweele2009; @vanderweele2013; @vanderweele2018] (See Appendix 1 for additional discussion of the proof and its implications.)

Let $\coprod$ denote independence.
Where there are $K$ different versions of treatment $A$ and no confounding for $K$'s effect on $Y$ given measured confounders $L$ such that

$$
Y(k) \coprod K | L
$$

Then it can be proved that causal consistency follows.According to the theory of causal inference under multiple versions of treatment, the measured variable $A$ functions as a "coarsened indicator" for estimating the causal effect of the multiple versions of treatment $K$ on $Y(k)$ [@vanderweele2009; @vanderweele2013; @vanderweele2018].

In the context of green spaces, $A$ might represent the general action of moving closer to any green space and $K$ represents the different versions of this treatment.
For instance, $K$ could denote moving closer to different types of green spaces such as parks, forests, community gardens, or green spaces with varying amenities and features.

Here, the conditional independence implies that, given measured confounders $L$ (e.g. socioeconomic status, age, personal values), the type of green space one moves closer to ($K$) is independent of the outcomes $Y(k)$ (e.g. mental well-being under the $K$ conditions).
In other words, the version of green space one chooses to live near does not affect the $K$ potential outcomes, provided the confounders $L$ are properly controlled for in our statistical models.

Here, the measured variable $A$ (moving closer to green spaces) functions as a simplified or "coarsened indicator" for the causal effect of multiple versions of the treatment $K$ (moving closer to different types of green spaces) on $Y(k)$ (health outcomes).
This suggests we can estimate the causal effect of moving closer to green spaces in general, even though this treatment consists of multiple versions, as long as the conditions of conditional independence are satisfied.

### **Step 6.2 Understand how we may condition on confounders to address the assumption of (conditional) exchangeability**

We say that conditional exchangeability is satisfied if, after controlling for observed covariates, the assignment of treatment is independent of the potential outcomes under treatment.
Conceptually, assuming both causal consistency (including no interference) and positivity are satisfied, satisfaction of the conditional exchangeability assumption implies that if units were swapped between treatment conditions, the distribution of potential outcomes under different exposures would remain unchanged.
This assumption is needed to ensure a balance between exposures in confounders that might affect the outcome.
With this assumption satisfied, the counterfactual observations derived from the consistency and positivity assumptions can be viewed as randomly assigned to the exposure conditions under which they were observed.
In effect (so to speak), satisfying the conditional exchangeability assumption is an attempt to simulate experimentally controlled randomisation with observational data.

Let $L$ to be the set of measured covariates required to ensure conditional independence.
Let $\coprod$ denote independence.
We express the exchangeability of counterfactual outcomes conditional on measured covariates $L$ as

$$
Y(a) \coprod  A|L
$$

Where the exchangeability assumption holds, along with the consistency and positivity assumptions, we may express the average treatment effect (ATE) on the difference scale

$$
\delta_{ATE}  = \mathbb{E}[Y(a^*)|L = l] - \mathbb{E}[Y(a)|L = l]
$$

Satisfaction of this assumption allows us to approximate a randomised experiment with observational data.

Considering once again causal inference for the effect on metnal well-being of moving closer to green spaces, $A$ represents this move (the treatment), while $Y(a)$ denotes the potential outcomes of move on well-being.

$L$ denotes the set of measured confounders that might influence both the decision to move closer to a green space and the potential outcomes (or that may be an effect of such factors).
Again, these confounders might include factors such as socioeconomic status, age, mental health status, lifestyle preferences, personal values, social connections, occupational demands, family circumstances, economic factors.

If the set of observed covariates is sufficient to measure all the factors that could explain away the association of treatment and outcome, we may say that the conditional exchangeability assumption is satisfied.
Typically this assumption cannot be tested by observational data.
For this reason, it is important to perform sensitivity analyses to evaluate how much unmeasured confounding would be required to explain away any observed associations. -->