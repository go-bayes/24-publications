---
title: "Causal Diagrams: A Practical Guide"
author: 
  name: Joseph A. Bulbulia
  orcid: 0000-0002-5861-2056
  email: joseph.bulbulia@vuw.ac.nz
  affiliation: 
    - name: Victoria University of Wellington, New Zealand, School of Psychology, Centre for Applied Cross-Cultural Research
      department: Psychology/Centre for Applied Cross-Cultural Research
      city: Wellington
      country: New Zealand
      url: www.wgtn.ac.nz/cacr
abstract: > 
  In causal inference, accurately quantifying a causal effect requires contrasting hypothetical counterfactual states simulated from data. This complex task relies on a framework of explicit assumptions and systematic, multi-step workflows. Causal diagrams (directed acyclic graphs, or DAGs) are powerful tools for evaluating assumptions and designing research. However, when misused, causal diagrams encourage false confidence. This guide offers practical advice for creating sequentially ordered causal diagrams that are effective and safe. Focussing on the time structure of causation reveals the benefits of sequential order in the spatial organisation of causal diagrams, both for data analysis and collection. After reviewing fundamentals, we employ *sequentially ordered causal diagrams* to elucidate often misunderstood concepts of causal interaction (moderation), mediation, and dynamic longitudinal feedback, as well as measurement-error bias and target validity. Overall, sequentially ordered causal diagrams underscore causal inference’s mission-critical demand for accuracy in the relative timing of confounders, exposures, and outcomes recorded in one’s data.
keywords:
  - Directed Acyclic Graph
  - Causal Inference
  - Confounding
  - Feedback
  - Interaction
  - Internal validity
  - External validity
  - Mediation
format:
  pdf:
    sanitize: true
    keep-tex: true
    link-citations: true
    colorlinks: true
    documentclass: article
    classoption: [singlecolumn]
    lof: false
    lot: false
    geometry:
      - top=30mm
      - left=20mm
      - heightrounded
    header-includes:
      - \input{/Users/joseph/GIT/templates/latex/custom-commands.tex}
date: last-modified
execute:
  echo: false
  warning: false
  include: true
  eval: true
fontfamily: libertinus
bibliography: /Users/joseph/GIT/templates/bib/references.bib
csl: /Users/joseph/GIT/templates/csl/camb-a.csl
editor: 
  markdown: 
    wrap: 72
---

```{r}
#| label: load-libraries
#| echo: false
#| include: false
#| eval: false

#   html:
#   html-math-method: katex

# Include in YAML for Latex
# sanitize: true
# keep-tex: true
# include-in-header:
#       - text: |
#           \usepackage{cancel}


# for making graphs
library("tinytex")
library(extrafont)
loadfonts(device = "all")

#quarto install tinytex --update-path

# libraries for jb (when internet is not accessible)
# read libraries
source("/Users/joseph/GIT/templates/functions/libs2.R")

# read functions
source("/Users/joseph/GIT/templates/functions/funs.R")

# read data/ set to path in your computer
pull_path <-
  fs::path_expand(
    "/Users/joseph/v-project\ Dropbox/data/current/nzavs_13_arrow"
  )

# for saving models. # set path fo your computer
push_mods <-
  fs::path_expand(
    "/Users/joseph/v-project\ Dropbox/data/nzvs_mods/00drafts/23-causal-dags"
  )


# keywords: potential outcomes, DAGs, causal inference, evolution, religion, measurement, tutorial
# 10.31234/osf.io/b23k7

#xxx words
# 75 refs
# 32 figs
```

## Introduction


<!-- Correlation does not imply causation. This adage is widely known.
Nevertheless, many human scientists report manifest correlations and use
hedging language that implies causation. However, such reporting
typically lacks justification. Making matters worse, widely adopted
analytic strategies for confounding control, such as indiscriminate
co-variate adjustment, are known to enhance biases [@mcelreath2020].
Across many human sciences, including the evolutionary human sciences,
persistent confusion in the analysis and reporting of correlations
continue to impede scientific progress, revealing a *causality crisis*
[@bulbulia2022].

We have reasons to hope for better. First, the open science movement has
demonstrated that attention to the problems of replication, analysis,
and reporting can bring considerable improvements to the reliability of
research within a short period. Although much remains to be
done, basic corrective practices for open science have become normative.
Second, several decades of active development in causal inference across
the health sciences, computer sciences, and economics, have yielded both
considerable conceptual clarifications and rigorous analytic tool-kits
for inference [@splawa1990application; @rubin1976; @robins1986;
@pearl1995; @pearl2009a; @vanderweele2015; @deffner2022; @hernan2023].
Although causal data science is still evolving [@vansteelandt2022;
@hoffman2023; @díaz2021], a substantial foundation exists. Much of this
foundation is written in a system of mathematical proofs that bring
confidence. Debates within the causal data sciences are peripheral to causal inference's theoretical and conceptual foundations. Given science's ability to self-correct, and causal inference's bedrock of theoretical and conceptual foundation, we can be
optimistic for a rapid uptake of causal inference methodology across the human sciences that presently lack it. The articles in this special issue of *Evolutionary Human Sciences* give
testimony to this hope.

Within the frameworks of causal data science, causal diagrams, also
known as 'directed acyclic graphs' or 'DAGs,' have been developed as
powerful inferential tools. Their applications are grounded in a robust
system of formal mathematical proofs that should instil confidence.
Nevertheless, causal diagrams do not require mathematical training and
are broadly accessible. This accessibility is a great advantage.

However, the accessibility of causal diagrams also invites risks. Causal
diagrams only acquire their significance when integrated within the
broader theoretical frameworks and workflows of causal data science.
These frameworks differ from those of traditional data science by
attempting to estimate pre-specified contrasts, or 'estimands,' among
counterfactual states of the world. Although we assume these
counterfactual states to be real, they must be simulated from data, under
explicit assumptions (really *assertions*) that must be justified [@vansteelandt2012;
@robins1986; @edwards2015]. These *structural assumptions (assertions)* differ from
the *statistical assumptions* familiar to traditionally trained data
scientists. Yet, because causal inference recovers counterfactuals by
applying statistical models to observed data, careful statistical
validations must also enter its workflows. We cannot assume that
traditionally trained human scientists, even those with excellent
statistical training, have familiarity with the demands of
counterfactual inference [@ogburn2021; @bulbulia2023a]. Using causal
diagrams without understanding causal inference risks inadvertently worsen
the causality crisis by fostering misguided confidence where none is
due. -->

Here, I offer readers of *Evolutionary Human Science* practical guidance
for creating causal diagrams.

**Part 1** introduces core concepts and theories in causal data science, emphasising the fundamental assumptions and the demands they impose on causal inferential workflows. We review the motivation in causal data science for adopting an 'estimands first' approach — pre-specifying counterfactual contrasts to address a well-defined causal question within a clearly defined target population. 
<!-- Although the overview is brief, it provides a necessary orientation to settings in
which causal diagrams possess their utility, outside of which their
application offers no guarantees. -->

<!-- Although we do not assume the presence or absence of a causal effect for the exposure on the outcome, structural features of the world that may compromise valid inference must be
explicitly stipulated in advance of observing data. This stipulation is
required because causality is inevitably under-determined by observational data. The need for structural assumptions sets a high bar for researcher integrity. *With great power comes great responsibility.*  -->

**Part 2** introduces essential terminology and explores elementary use cases, focussing on the benefits of sequentially order in one's graph for evaluating confounding biases.  

<!-- These diagrams highlight the critical role of timing, underscoring its significance not only in analysis but also in securing quality measurements for the relative timing of events within the dataset. -->
<!-- a. the units present in a study at baseline (before random assignment, or pseudo-random assignment in the case of observational data); 
b. the features or characteristics of these units that hold scientific interest; 
c. the (mis)measurement of these features at baseline and throughout the study; 
d. the target population for which the study aims to clarify the effects of interventions
 -->

**Part 3** uses sequentially ordered causal diagrams to clarify concepts of interaction (moderation) and effect modification.


**Part 4** clarifies biases arising from measurement error bias. 


**Part 5** clarifies restriction biases arising from (a) attrition/ censoring; (b) mismatch between a study sample population and a target sample population. We also consider heterogeniety biases arising from failures to sufficiently restrict one's sample.

**Part 6** uses causal diagrams to clarify causal estimation in which there are multiple exposures, focussing on causal mediation, and time-varying treatments.

There are many good resources available for learning causal diagrams
[@rohrer2018; @hernan2023; @cinelli2022; @barrett2021; @mcelreath2020;
@greenland1999; @suzuki2020; @pearl2009]. This work hopes to contribute to
these resources, first by providing additional conceptual orientation to
the frameworks and workflows of causal data science, outside of which
the application of causal diagrams is risky; second, by underscoring the
benefits of sequential order in one's causal diagrams to clarify demands
for data collection and analysis; third by using causal diagrams to clarify threats to target validity arising from sample/target population mismatch, fourth, by using causal diagrams to clarify
questions of causal interaction, mediation, and longitudinal feedback, about
which there remains considerable confusions among many human scientists.


## Part 1. Overview of Causal Data Science

The first step in answering a causal question is to ask it
[@hernán2016]. Causal diagrams come later, when we consider which forms
of data might enable us to address our pre-specified causal questions.
<!-- This section introduces key concepts and broader workflows within which
causal diagrams find their purposes and utilities. It begins by
considering what is at stake when we ask a causal question. -->

#### 1.1.1 The fundamental problem of causal inference

To ask a causal question, we must consider the concept of causality
itself. Consider an intervention, $A$, and its effect, $Y$. We say that
$A$ causes $Y$ if altering $A$ would lead to a change in $Y$ [@hume1902;
@lewis1973]. If altering $A$ would not change $Y$, we say that $A$ has
no causal effect on $Y$.

In causal inference, we aim to quantitatively contrast the potential
outcomes of $Y$ in response to different levels of a well-defined
intervention. Commonly, we refer to such interventions as 'exposures' or
'treatments;' we refer to the possible effects of interventions as
'potential outcomes.'

Consider a binary treatment variable $A \in \{0,1\}$. For each unit $i$
in the set $\{1, 2, \ldots, n\}$, when $A_i$ is set to 0, the potential
outcome under this condition is denoted, $Y_i(0)$. Conversely, when
$A_i$ is set to 1, the potential outcome is denoted, $Y_i(1)$. We
refer to the terms $Y_i(1)$ and $Y_i(0)$ as 'potential outcomes' because
until realised, the effects of interventions describe counterfactual states.

Suppose that each unit $i$ receives either $A_i = 1$ or $A_i = 0$. The
corresponding outcomes are realised as $Y_i|A_i = 1$ or $Y_i|A_i = 0$.
For now, let us assume that each realised outcome under that
intervention is equivalent to one of the potential outcomes required for a quantitative causal contrast, such that $[(Y_i(a)|A_i = a)] = (Y_i|A_i = a)$. Thus when $A_i = 1$,  $Y_i(1)|A_i = 1$ is observed. However, if $A_i = 1$, it follows that $Y_i(0)|A_i = 1$ is not observed:

$$
Y_i|A_i = 1 \implies Y_i(0)|A_i = 1~ \text{is counterfactual}
$$

Conversely, if $A_i = 0$, we may assume the potential outcome $Y_i(0)|A_i = 0$) is observed as $Y_i|A_i = 0$. However, the potential outcome $Y_i(1)|A_i = 0$ is never realised and so not observed:

$$
Y_i|A_i = 0 \implies Y_i(1)|A_i = 0~ \text{is counterfactual}
$$

We define $\tau_i$ as the individual causal effect for unit $i$ and
express the individual causal effect:

$$
\tau_i = Y_i(1) - Y_i(0)
$$

Notice that each unit can only be exposed to only one level of the exposure
$A_i = a$ at a time. This implies that $\tau_i$, is not merely unobserved but inherently *unobservable* (see discussion in Appendix 3). 

Although we cannot observe potential outcomes that do not occur, it is tempting to ask questions about them, 'What if Isaac Newton had not witnessed the falling apple?’ What if Leonardo da Vinci had never pursued art?' or 'What if Archduke Ferdinand had not been assassinated?' There are abundant examples from literature. Robert Frost contemplates, 'Two roads diverged in a yellow wood, and sorry I could not travel both, and be one traveller, long I stood...' (see: Robert Frost, 'The Road Not Taken':
https://www.poetryfoundation.org/poems/44272/the-road-not-taken). We have counterfactual questions for our personal experiences: 'What if I had had not interviewed for that job?' 'What if I had stayed in that relationship?' We may speculate, with reasons, but we cannot directly observe the potential outcomes we would need to verify our speculations. The physics of middle-sized dry goods prevents the joint realisation of the facts required for quantitative comparisons. That individual causal effects cannot be identified from observations is known as '*the fundamental problem of causal inference*' [@rubin1976; @holland1986].


#### 1.1.2 Causal effects from randomised experiments

It is not feasible to compute individual causal effects. However, under
certain assumptions, we can estimate *average* treatment effects -- also called 'marginal effects,' -- by comparing groups that have received different levels of treatment. The average treatment effect, $ATE$, is defined as the difference between the expected outcomes under treatment and contrast conditions. Suppose the treatment, $A$, is a binary
variable $A \in \{0,1\}$:

$$
\text{Average Treatment Effect}  = \mathbb{E}[Y(1)] - \mathbb{E}[Y(0)].
$$
A challenge remains in computing these treatment-group averages when individual causal effects are unobservable. Consider the problem framed in terms of *full data* that would be required to compute these averages — that is in terms of the complete counterfactual dataset where the missing potential outcomes, inherent in observational data, were somehow available:

$$
\text{Average Treatment Effect} = \left(\underbrace{\underbrace{\mathbb{E}[Y(1)|A = 1]}_{\text{observed for } A = 1} + \underbrace{\mathbb{E}[Y(1)|A = 0]}_{\text{unobserved for } A = 0}}_{\text{effect among treated, by the law of total expectation}}\right) - \left(\underbrace{\underbrace{\mathbb{E}[Y(0)|A = 0]}_{\text{observed for } A = 0} + \underbrace{\mathbb{E}[Y(0)|A = 1]}_{\text{unobserved for } A = 1}}_{\text{effect among untreated, by the law of total expectation}}\right).
$$

Consider that for each treatment condition, half the observations needed to compute a treatment-group average are (inherently) unobserved. 

Randomisation allows investigators to recover the treatment group averages even though treatment groups contain inherently missing observations. When investigators effectively randomise units into treatment conditions, and there is full adherence, the distributions of confounding factors that could explain differences in the potential outcomes are balanced across the conditions. Randomisation (in a perfect experiment) ensures that nothing can explain a difference in treatment group average except the treatment. This implies (by the law of iterated expectations):

$$
\widehat{\mathbb{E}}[Y(0) | A = 1] = \widehat{\mathbb{E}}[Y(0) | A = 0]
$$

and

$$
\widehat{\mathbb{E}}[Y(1) | A = 1] = \widehat{\mathbb{E}}[Y(1) | A = 0]
$$

We assume, (by causal consistency, see: $\S 1.2.1$):

$$\widehat{\mathbb{E}}[Y(1) | A = 1] = \widehat{\mathbb{E}}[Y| A = 1]$$

and

$$\widehat{\mathbb{E}}[Y(0) | A = 0] = \widehat{\mathbb{E}}[Y| A = 0]$$

It follows that the average treatment effect of the randomised experiment can be computed (by the law of iterated expectations):

$$
\text{The Estimated Average Treatment Effect} = \widehat{\mathbb{E}}[Y | A = 1] - \widehat{\mathbb{E}}[Y | A = 0].
$$

<!-- There are four critical aspects for how ideally randomised experiments enable the estimation of average treatment effects worth highlighting.

First, the investigators should specify a population for whom they seek to
generalise their results. We refer to this population as the *target population*. If the study population differs from the target population in the distribution of covariates that interact with the treatment, the investigators will have no guarantees their results will generalise (see the discussion below: $\S 3.1.6$). 'Target validity' (or 'external validity') remains a challenge in every causal inferential workflow. In **Part 5** we will consider how modified sequential causal diagrams may clarify the demands of target validity in settings where there is sample/target population mismatch; also refer to: @imai2008misunderstandings; @westreich2019target; @westreich2017; @pearl2022; @bareinboim2013general; @stuart2018generalizability;
@webster2021directed. 

Second, because the units in the study sample may differ over time,
investigators must be careful to avoid biases that arise from mismatch
between the study sample at baseline, before random treatment
assignment, and the study sample at each point thereafter -- including administration of the treatment. Importantly, a randomised experiment recovers the causal effect of random treatment assignment, not of the treatment itself, which may differ if some participants do not adhere to their treatment. The effect randomised assignment is called the *Intent-to-treat effect*. The effect of perfect adherence is called the *per protocol effect* [@hernan2017per; @lash2020]. To obtain the per protocol effects for randomised experiments requires the application of methods for causal inference in observational settings (see: $\S 2.3$)

Third, I have presented the average treatment effect on the difference
scale, that is, as a difference in average potential outcomes for the
target population under two distinct levels of treatment. However,
depending on the scientific question at hand, investigators may wish to
estimate causal effects on the risk-ratio scale, the rate-ratio scale,
the hazard-ratio scale, or another scale. Where there are interactions
such that treatment effects vary across different strata of the
population, an estimate of the causal effect on the risk difference
scale will differ in at least one stratum to be compared from the
estimate on the risk ratio scale [@greenland2003quantifying].  -->

<!-- We will
return to this point in **Part 3** when we consider effect-modification.
For now, it should be clear that when we speak of an 'average treatment
effect' we must describe not just the target population but also the
causal effect-scale on which our contrast will be made. -->

<!-- Fourth, in observational studies, investigators might wish to describe
the target population of interest as a restriction of the study sample
population. For example, investigators might wish to estimate the
average treatment effect only in the population that received the
treatment. This treatment effect is sometimes called the average treatment effect in the treated ($ATT$), and may be expressed:

$$\text{Average Treatment Effect in the Treated} = \mathbb{E}[Y(1) - Y(0) | A = 1]$$

Consider that if investigators are interested in the average treatment
effect in the treated, counterfactual comparisons are deliberately *restricted* to the sample population that was treated. That is, the investigators will seek to obtain the average of the missing counterfactual outcomes for *the treated population were they not treated*, without necessarily obtaining the counterfactual outcomes for the untreated population were they treated. This difference in focus may imply different assumptions and analytic workflows. **Appendix 1** describes an example for which the assumptions required to estimate the average treatment effect may be preferred. In what follows, we will use the term $ATE$ as a placeholder to mean the average treatment effect, or equivalently the 'marginal effect', for a target population on a pre-specified scale of causal contrast.

Setting aside the important detail that the 'average treatment effect' requires considerable care in its specification, it is worth pausing to marvel at how an ideally conducted randomised controlled experiment provides a means for identifying inherently unobservable counterfactuals. It does so by using a Sherlock-Holmes-method of inference by elimination of confounders, which randomisation balances across treatments. -->
<!-- 
 When experimenters observe a difference in average treatment effects, and all else goes right, they may infer that the distribution of potential outcomes differs by treatment because randomisation exhausts every other explanation except that of the treatment. They are entitled to this inference because randomisation balances the distribution of potential confounders across the treatment groups to be compared. -->

<!-- Outside of randomised experiments, however, we lack guarantees of balance in the confounders. For this reason, investigators should prefer developing sound randomised experiments for
addressing every causal question that experiments can address.
Unfortunately, randomised experiments cannot address many scientifically
important questions. This bitter constraint is familiar to evolutionary
human scientists. We typically confront 'What if?' questions that are
rooted in the unidirectional nature of human history. -->

Understanding how randomisation obtains the missing
counterfactual outcomes that we require to consistently estimate average
treatment effects clarifies the tasks of causal inference in
non-experimental settings [@hernán2008a; @hernán2006; @hernán2022]. 

<!-- We next examine these identification assumptions in greater detail because using causal diagrams without understanding these assumptions is unsafe. -->

### 1.2 Fundamental Identification Assumptions

There are three fundamental identification assumptions that must be
satisfied to consistently estimate causal effects with data.

#### 1.2.1 Assumption 1: Causal Consistency

We satisfy the causal consistency assumption if, for each unit $i$ in
the set $\{1, 2, \ldots, n\}$, the observed outcome corresponds to one
of the specific counterfactual outcomes to be compared such that:

$$
Y_i^{observed}|A_i = 
\begin{cases} 
Y_i(a^*) & \text{if } A_i = a^* \\
Y_i(a) & \text{if } A_i = a
\end{cases}
$$

The causal consistency assumption implies that the observed outcome at a
specific exposure level equates to the counterfactual outcome for that
individual at the observed exposure level. Although it seems
straightforward to equate an individual's observed outcome with their
counterfactual outcome, treatment conditions vary, and treatment
heterogeneity poses considerable challenges for satisfying this
assumption. See: **Appendix A**


#### 1.2.2 Assumption 2: Conditional Exchangeability (no unmeasured confounding)

We satisfy the conditional exchangeability assumption if the treatment
groups are conditionally balanced in the variables that could affect the
potential outcomes. In experimental designs, random assignment
facilitates satisfaction of the conditional exchangeability assumption.
In observational studies more effort is required. We must control for
any covariate that could account for observed correlations between $A$
and $Y$ in the absence of a causal effect of $A$ on $Y$.

Let $\coprod$ again denote independence. Let $L$ denote the set of
covariates necessary to ensure this conditional independence. We satisfy
conditional exchangeability when:

$$
Y(a) \coprod A | L \quad \text{or equivalently} \quad A \coprod Y(a) | L
$$

Assuming conditional exchangeability is satisfied and the other
assumptions required for consistent causal inference also hold, we may
compute the average treatment effect (ATE) on the difference scale:

$$
ATE = \mathbb{E}[Y(1) | L] - \mathbb{E}[Y(0) | L]
$$

In the disciplines of cultural evolution, where experimental control is
impractical, causal inferences hinge on the plausibility of satisfying
this 'no unmeasured confounding' assumption (see: **Appendix 1**)

Importantly, causal diagrams primarily function to identify sources of bias that may influence the association between an exposure and outcome. They highlight those aspects of the assumed causal order relevant to the assessment of 'no-unmeasured confounding.' Although causal diagrams can also be useful for examining structural features of measurement-error bias and threats to target validity from sample/population mismatch, certain of these threats are not amenable to Markov factorisation (see $\S 2.3$). For instance, in $\S 2.8$, we consider the limitations of causal diagrams in addressing uncorrelated measurement error bias, which does not manifest on causal diagrams. In  $\S 3.1.6$, we employ causal diagrams to clarify threats to external validity from sample restriction (effect-modifier-restriction bias). In these instances, we must introduce colouring conventions that repurpose causal diagrams for somewhat different problems than those they were originally designed to address, namely, problems of confounding bias.

<!-- Note that a common mistake when -->

<!-- creating a causal diagram is to provide too much detail, obscuring -->

<!-- rather than clarifying structural sources of bias. We return to this -->

<!-- point below. -->

Finally, it is important to underscore that without randomisation,
we cannot fully ensure the no-unmeasured confounding assumption that
enables us to recover the missing counterfactuals we require to
consistently estimate causal effects from data [@stuart2015;
@greifer2023]. Because we must nearly always assume unmeasured confounding, the workflows of causal data science must ultimately rely on sensitivity analyses to clarify how much unmeasured confounding would be required to compromise a study's findings [@vanderweele2019].

#### 1.2.3 Assumption 3: Positivity

We satisfy the positivity assumption if there is a non-zero probability
of receiving each treatment level for every combination of covariates
that occurs in the population. Where $A$ is the exposure and $L$ is a
vector of covariates, we say positivity is achieved if:

$$
0 < Pr(A = a | L = l) < 1, \quad \text{for all } a, l \text{ with } Pr(L = l) > 0
$$

There are two types of positivity violation:

1.  **Random non-positivity** occurs when an exposure is theoretically
    possible, but specific exposure levels are not represented in the
    data. Notably, random non-positivity is the only identifiability
    assumption verifiable with data.

2.  **Deterministic non-positivity** occurs when the exposure is
    implausible by nature. For instance, a hysterectomy in biological
    males would appear biologically implausible.

Satisfying the positivity assumption can present considerable data
challenges [@westreich2010]. Suppose we had access to extensive panel
data that has tracked 20,000 individuals randomly sampled from the
target population over three years. Suppose further that we wanted to
estimate a one-year causal effect of weekly religious service attendance
on charitable donations. We control for baseline attendance to recover
an incident exposure effect estimate (see: $\S 2.3$ and **Appendix 2**).
Assume that the natural transition rate from no religious service
attendance to weekly service attendance is low, say one in a thousand
annually. In that case, the effective sample for the treatment condition
dwindles to 20. This example clarifies the problem. For rare exposures,
the data required for valid causal contrasts may be sparse, even in
large datasets. Where the positivity assumption is violated, causal
diagrams will be of limited utility because observations in the data do
not support valid causal inferences. (**Appendix 1** develops a worked
example that illustrates the difficulty of satisfying this assumption in
a setting of a cultural evolutionary question.)

### 1.3 Conceptual, Data, and Modelling Assumptions

We have reviewed the three fundamental assumptions of causal inference.
However, we must also consider further conceptual, data, and modelling
assumptions that, in addition to the foundational assumptions we just
reviewed, must also be satisfied to obtain valid causal inferences.
We next consider a subset of these assumptions. **Appendix B** describes these further assumptions

### Summary of Part 1

Causal data science is not ordinary data science. In causal data science, the initial step involves formulating a precise causal question
that clearly identifies the exposure, outcome, and population of interest. 
We must then satisfy the three fundamental assumptions required for causal inference, which are implicit in the ideal of a randomised experiment: causal consistency: ensuring outcomes at a specific exposure level align with their counterfactual counterparts; conditional exchangeability: the  absence of unmeasured confounding; positivity: the existence of a
non-zero probability for each exposure level across all covariate


## Part 2. Applications of Sequentially Ordered Causal Diagrams for Understanding Structural Sources of Bias

Having outlined basic features of the causal inference framework, we are
now in a position to use causal diagrams to elucidate elementary structural sources of bias [@pearl1995; @pearl2009; @greenland1999]. We begin by defining our terminology.

### 2.1 Variable naming conventions



@#tbl-conventionsnaming presents our variable naming conventions.

::: {#tbl-conventionsnaming}


```{=latex}
\terminologylocalconventions 
```

Variable naming conventions in this article.

:::


### 2.2 Graphical conventions


::: {#tbl-conventionsgraph}


```{=latex}
\terminologylocalconventions 
```

Graphical conventions.

:::

### 2.3 Terminology

#### 2.3.1 Terminology pertaining to units, samples and populations

**Unit**: an entity, such as an object or person or culture, that may be considered an individual member of a population of similar such entities.

**Sample**: The collection of units observed and recorded in a study. The concept of causation, which unfolds over time, necessitates further distinctions:

- **Baseline sample**: the specific units present in a study at random-treatment assignment or pseudo-randomisation (in observational studies.) 

- **Censored sample**: the set of units remaining in a study after (pseudo)random-treatment assignment. This material collection of entities may over time as units drop out, and possibly re-enter the study. Although there can be multiple censored samples reflecting different points or conditions in the study. For example, if attrition is censoring.

**Population**:  an abstract concept denoting a collection of units characterised by certain features and events. Because causation unfolds over time, it is helpful to distinguish between different types of populations. 

- **Restricted (selected) population**: a population is considered 'restricted' relative to another population if its units share some but not all features of the larger group.

- **Target population**: the population for which a study aims to generalise its findings, defined by a set of features and events, specified in advance of the study. For example, if a study is investigating the effects of religion on charity, its target population might be all people who identify as Christian in New Zealand (see; **Appendix 1**).

- **Source population**: the actual population from which the study's sample is drawn. It could be a subset of the target population. For example, all individuals living in New Zealand who may be sampled from the New Zealand electoral roll. Whether the source population is a restriction of the target population depends on the details of a study. For example, if a study pertains to religious service, the source population might be broader if it included people who did not attend religious services. The source and target populations may be equivalent.

- **Baseline sample population**: the population from which the units assigned to treatment are drawn. For example, those New Zealanders (from the source population) who, after being invited randomly from the New Zealand Electoral Roll to participate, enrol in the study are sampled from this population. Depending on the scientific question and context, investigators may use eligibility criteria to restrict the baseline sample population so that it better aligns with the target population. Investigators may also apply sampling weights to the baseline population to better align it with the target population [@cole2010generalizing].  Because weights carry modelling assumptions, weighting must be done with care.

- **Censored sample population**: over time, the units that comprise the baseline sample may change, for example, from attrition (right censoring). Importantly, population from which the censored units are drawn may no longer resemble the study population at baseline. Censoring is *uninformative* if the change in units does not affect the outcome, or if there is no effect of treatment (the sharp causal null hypothesis). Censoring is *informative*  if the baseline population differs in the distribution of those features that modify the effect of the treatment, and no correction is applied. Unbiased effect estimates for the baseline population will nevertheless be biased for the target population in at least one measure of effect [@greenland2009commentary; @lash2020]. This is why it is important for investigators to state a causal effect of interest with respect to *the full data* that includes the counterfactual quantities for the treatments to be compared in a clearly defined target population and with a specific causal contrast [@westreich2017].

#### 2.3.2 General terminology pertaining to causal inference

**Randomised controlled experiment**: a study in which the investigators assign units to treatments by chance. Random assignment ensures an even distribution of both known and unknown confounders across treatment groups, thus addressing *the identification problem.*

**Observational study**: a study in which treatment assignment is not controlled by the investigators. In such studies, treatments (equivalently 'exposures') occur naturally without investigator intervention. Workflows in causal inference aim to ensure balance in the distribution of confounders across treatments. We cannot assume that such methods can fully account for all confounding (*the assumption of unmeasured confounding*). No statistical test can verify the assumption of no unmeasured confounding (*the underdetermination of causality by data*).

**Natural experiment**: an observational study in which treatment assignment, although not controlled by the investigators, occurs in a manner that approximates randomness, allow for stronger confidence of unbiased causal effect estimates than is typical observational studies. However, because nature might not flip coins as we think, assumptions of random treatment assignment must be scrutinised.

**Average treatment effect (ATE)**: quantifies the contrast in the average of the potential outcomes for the *entire* population under two treatment levels, at some scale of contrast, such as the difference scale.

**Marginal effect**: synonym for the average treatment effect.

**Intent-to-treat effect**: the causal effect of random treatment assignment. Ideally randomised experiments consistently estimate the effect of randomisation to a condition, not adherence. If experimenters simply select the sample that adheres to treatment (removing other cases as irrelevant) they may introduce confounding. It is a mistake to select participants into a study after randomisation on the basis of adherence (@hernan2017per).

**Per-protocol effect**: the effect of adherence under randomisation [@hernan2017per]. A safe assumption is that:
$\widehat{ATE}_{\text{target}}^{\text{per-protocol}} \ne \widehat{ATE}_{\text{target}}^{\text{intent-to-treat}}$ [@hernán2004; @tripepi2007]. 

**Confounding bias**: the association between the exposure and the outcome does not reflect a true causal association for the study population because the potential outcomes are not independent of the exposure, conditional on measure covariates: $Y(a)\cancel\coprod A|L$.

**Measurement error bias**: a discrepancy between a variable's true value and its observed or recorded value.

**Selection bias**: this term has different meanings in different areas of causal inference. Our interest is in settings where the association between the exposure and the outcome in a study population does not reflect the causal association in the target population [@hernán2017].

<!-- Such bias can manifest as *confounding bias* in when restriction of the baseline sample is caused by both by the treatment and an unmeasured common cause of censoring and the outcome (see $\S 3.1.6.1$). Selection bias can also manifest in *effect-modifier-restriction bias*, in which censoring alters the distribution of effect-modifiers in the censored sample. As mentioned, some use the term 'selection bias' more broadly to include 'collider-stratification bias', which is an over-adjustment bias. We examine this bias in $\S 2.7.3$. An advantage of causal diagrams is that they allow us to state the threat to causal inference without the need for terminology. Throughout we will rely on our graphs, not terminology, to clarify the threats and opportunities to hand. -->

**Informative censoring**: occurs when the units in one's study differ from the target population in ways that are related to the underlying causal question, potentially introducing bias into causal inferences (see: $\S 3.1.5$).

**Non-informative censoring:** the censoring mechanism is independent of the outcome in the uncensored population and does not itself introduce bias. 

**Target validity**: we say that a study achieves target validity if its results generalise to the target population. This concept is also known as 'external validity.' 

<!-- Study findings can be applied beyond the sample population to the target population. An unbiased causal effect estimate is a necessary but not sufficient condition for target validity. For example if the sample population differs in target population, target validity will fail. -->


#### 2.3.2 Terminology pertaining to causal diagrams 


#### 2.3.2.1 Elements of causal diagrams 

**Node**: characteristic or features of units in a population ('variable') represented on a causal diagram. In a causal diagram, nodes are drawn with reference to variables defomed for the target population.

**Arrow**: denotes a causal relationship linking nodes. Unless the pathway pertains to the treatment/outcome path, all arrows are *assumed*. For example, $L\to A$ denotes an assumed causal relationship between $A$ and $Y$. A causal diagram evaluates whether the causal relationship between exposure and outcome can be identified according to the assumptions that are *asserted* by a causal diagram. Again, we do not *assert* the $A\to Y$ path, nor do we *assert* its absence. All other paths on the diagram must be asserted. This should be done without regard to data collection. The diagram should refer to causal pathways in the target population -- not the sample population, unless the sample population is the target population. Because these causal paths are asserted, causal diagrams should be informed by expert judgement. *With great power comes great responsibility*. 

Note that when estimating total effects, we should generally regard nodes other than the exposure and outcome as nuisance parameters that are of no intrinsic interest. Again when our interest is in estimating average treatment effects for a target population, it is advisable to report coefficients in statistical without interpretation because we have no assurance that these estimates accurately reflect causation [@westreich2013]. Given the present state of the causal crisis, unless one has built a clearly defined causal model for investigating effect-modification [@athey2021; @athey2019], it is arguably better not to report these nuisance regression coefficients at all.


**Ancestor (parent)**: a node with a direct or indirect influence on
others, positioned upstream in the causal chain.

**Descendant (child)**: a node influenced, directly or indirectly, by
upstream nodes (parents).

**Acyclic**: a causal diagram cannot contain feedback loops. More
precisely, no variable can be an ancestor or descendant of itself. If
variables are repeatedly measured here, it is especially important to
index nodes by the relative timing of the nodes.

**The identification problem**: the challenge of estimating the causal
effect of a variable using by adjusting for measured variables on units
in a study. Causal diagrams were developed to address the identification
problem by application of the rules of d-separation to a causal diagram.
<!-- Although this is their primary function, as we shall consider in **Part
3**, causal diagrams may also be useful for clarifying features of
effect-modification that may invalidate inferences for the target
population. -->

**Conditioning**: the process of explicitly accounting for a variable in
our statistical analysis to address the identification problem. In
causal diagrams, we usually represent conditioning by drawing a box
around a node of the conditioned variable, for example,
$\boxed{L_{0}}\to A_{1} \to L_{2}$. We do not box exposures and
outcomes, because we assume they are included in a model by default.
Depending on the setting, we may condition by regression stratification,
inverse-probability of treatment weighting, g-methods, doubly robust
machine learning algorithms, or other methods.

**Adjustment set**: a collection of variables we must either condition
upon or deliberately avoid conditioning upon to obtain a consistent
causal estimate for the effect of interest [@pearl2009].

**Confounder**: a member of an adjustment set. Notice a variable is a
'confounder' in relation to a specific adjustment set. 'Confounder' is a
relative concept [@lash2020].

**Collider**: a variable in a causal diagram at which two incoming
paths meet head-to-head. For example if
$A \rightarrowred \boxed{L} \leftarrowred Y$, then $L$ is a collider. If
we do not condition on a collider (or its descendants), the path between
$A$ and $Y$ remains closed. Conditioning on a collider (or its
descendants) will induce an association between $A$ and $Y$

**Effect-modifier**: a variable is an effect-modifier, or 'effect-measure modifier' if its presence changes the magnitude or direction of the effect of an exposure or treatment on an outcome across the levels or values of this variable. In other words, the effect of the exposure is different at different levels of the effect-modifier. 

<!-- Although effect-modification can be evaluated through interaction terms in regression models or by causal forests, the magnitude or even presence of effect-modification will typically be a function of other variables included in the model. Unless the exposure has no effect, where there is effect-modification of the exposure on the outcome, and the target population and analytic sample population differ in the
distribution of effect-modifiers, the treatment effect inferred in the sample will differ from the target population in at least one measure of effect [@hernán2017]. As such effect-modification has strong potential to introduce biased inference for the target population, which remains even if a causal effect is consistently estimated in the censored sample. -->

**Instrumental variable**: an ancestor of the exposure but not of the outcome. An instrumental variable affects the outcome only through its effect on the exposure and not otherwise. Whereas conditioning on a variable causally associated with the outcome but not with the exposure will generally increase modelling precision, we should avoid conditioning on instrumental variables [@cinelli2022]. There are two exceptions to this rule. First, we may be interested in instrumental variable analysis (see **Lonati et al,** this issue). Second, when an instrumental variable is the descendant of an unmeasured confounder, we should generally condition the instrumental variable to provide a partial adjustment for a confounder.


#### 2.3.2.2 Markov factorisation and related concepts


**Markov factorisation**  represents the joint probability distribution of all variables in a causal diagram. It asserts that this joint distribution can be decomposed into a product of conditional distributions, each corresponding to a variable conditioned on its parents in the DAG. Formally, if we have variables $X_1, X_2, \dots, X_n$ and a causal diagram that encodes causal assumptions about these variables, then the joint probability distribution of these variables can be written as:

$$
P(X_1, X_2, \dots, X_n) = \prod_{i=1}^{n} P(X_i | \text{Parents}(X_i))
$$


Again, the structure of a causal diagram encodes assumptions about causal relationships. Markov factorisation uses this structure to decompose the joint probability distribution of the relationships defined by a causal daiagram.

It does this by factoring the conditional independencies implied by the causal diagram. If there is no direct path between two variables in the causal diagram, and they do not share a common ancestor, the variables are conditionally independent given each variables' parents. This is a fundamental feature because it allows for the simplification of complex joint distributions into more manageable parts. By breaking down the joint probability into parts, Markov factorisation greatly simplifies understanding of complex dependencies so they can be read from graphs [@lauritzen1990; @pearl1988; @pearl2009a].


**Causal Markov assumption:** states that when conditioned on its direct antecedents, any given variable is rendered independent from all other variables that it does not cause [@hernan2023]. Thus, once we account for a variable's immediate causes, it ceases to provide additional causal information about any other variables in the system, except for those it directly causes. This assumption allows for inferring the causal effects of interventions from causal diagrams [@pearl2009a].

**Compatibility**: the joint distribution of the variables is said to be compatible with the graph if it upholds the conditional independencies the graph implies [@pearl2009a].

**Faithfulness**: a causal diagram is considered faithful to a given set of data if all the conditional independencies present in the data are accurately depicted in the graph. Conversely, the graph is faithful if every dependency implied by the graph's structure can be observed in the data. This concept ensures that the graphical representation of relationships between variables aligns with the empirical evidence [@pearl1995].

Although the assumption of faithfulness or 'weak faithfulness' allows for the possibility that some of the independencies in the data might occur by coincidence (i.e., because of a cancellation of different effects), the assumption of strong faithfulness does not. The strong faithfulness condition assumes that the observed data's statistical relationships directly reflect the underlying causal structure, with no independence relationships arising purely by coincidental cancellations. This is a stronger assumption than (weak) faithfulness and is often more practical in real-world applications of causal inference. Note that the faithfulness assumption (whether weak or strong) is not testable by observed data -- it is an assumption about the relationship between the observed data and the underlying causal structure.



### 2.4 Rules of d-separation

**D-separation**: in a causal diagram, a path is 'blocked' or
'd-separated' if a node along it interrupts causation. Two variables are
d-separated if all paths connecting them are blocked, making them
conditionally independent. Conversely, unblocked paths result in
'd-connected' variables, implying potential dependence [@pearl1995].

The rules of d-separation are as follows:

a.  **Fork rule** ($A \leftarrowred \boxed{L} \rightarrowred Y$): $A$
    and $Y$ are independent when conditioning on $L$ ($A \coprod Y | L$)
    [@pearl1995].

b.  **Chain rule** ($A \to \boxed{L} \rightarrowdotted Y$): Conditioning
    on $L$ blocks the path between $A$ and $Y$ ($A \coprod Y | L$)
    [@pearl1995]. (This will introduce bias if in reality, $A \to Y$)

c.  **Collider rule** ($A \rightarrowred \boxed{L} \leftarrowred Y$):
    $A$ and $Y$ are independent until conditioning on $L$, which
    introduces dependence ($A \cancel{\coprod} Y | L$) [@pearl1995].

According to the rules of d-separation:

1.  An open path (no variables conditioned on) is blocked only if two
    arrows point to the same node: $A \rightarrowred L \leftarrowred Y$
    is blocked. The node of common effect is called a *collider*.
2.  Conditioning on a collider does not block a path, such that
    $A \rightarrowred \boxed{L} \leftarrowred Y$ may suggest an
    association of A with Y in the absence of causation.
3.  Conditioning on a descendant of a collider does not block a path,
    such that if $L \to \boxed{L'}$, then
    $A \rightarrowred \boxed{L'} \leftarrowred Y$ is open. (Note: Unless
    our interest is causal mediation it is important to avoid
    conditioning on any mediator $L$ in which
    ($A \rightarrow {L} \rightarrowdotted Y$). This may bias a true
    effect of $A \to Y$.)
4.  If a path does not contain a collider, any variable conditioned
    along the path is blocked, such that
    $A \rightarrow \boxed{L} \rightarrowdotted Y$ blocks the path from
    $A\to Y$; see @hernan2023, p 78.

To obtain an unbiased estimate for the causal effect of $A$ on $Y$, we
must block all backdoor paths. We do this by conditioning on a set of
covariates $L$ that is sufficient to close all backdoor paths linking
$A$ and $Y$. A path is effectively blocked by $L$ if it includes at
least one non-collider that is a member of $L$, or if it does not
contain any collider or descendants of a collider. The 'backdoor
criterion' uses the rules of d-separation to evaluate structural sources
of bias, and close all paths in a causal diagram that connect a
treatment (or exposure), $A$, and an outcome, $Y$, in the absence of
causation [@pearl2009a].

**Modified Disjunctive Cause Criterion**: @vanderweele2019 recommends
obtaining a maximally efficient adjustment which he calls a 'confounder
set' A member of this set is any set of variables that can reduce or
remove a structural sources of bias. The strategy is as follows:

a.  Control for any variable that causes the exposure, the outcome, or
    both.
b.  Control for any proxy for an unmeasured variable that is a shared
    cause of both the exposure and outcome.
c.  Define an instrumental variable as a variable associated with the
    exposure but does not influence the outcome independently, except
    through the exposure. Exclude any instrumental variable that is not
    a proxy for an unmeasured confounder from the confounder set
    [@vanderweele2019].

Note that the concept of a 'confounder set' is broader than that of an
'adjustment set.' Every adjustment set is a member of a confounder set.
Hence, the Modified Disjunctive Cause Criterion will eliminate bias when
the data permit. However, a confounder set includes variables that will
reduce bias in cases where confounding cannot be eliminated.

This is a useful strategy because confounding can rarely be eliminated
with certainty (recall our *the assumption of universal unmeasured confounding*.) The *Modified Disjunctive Cause Criterion* allows us to
mitigate bias as best we can in advance of sensitivity analyses that evaluate the robustness of our results to unmeasured confounding.

Importantly, software tools such as the present versions of `Dagitty` and `ggdag` [@textor2011; @barrett2021], although beneficial for identifying adjustment sets conditional on a supplied model, may nevertheless overlook pragmatically optimal strategies for confounding control. The software will not select the best confounder set where unmeasured confounding persists. Therefore, reliance on these tools should be balanced with independent causal diagram interpretation skills. For this reason, I recommend learning to visually inspect graphs to identify sources of bias and strategies for bias reduction, even when bias cannot be fully eliminated. We shall discover that sequentially ordered graphs greatly benefit the demands of such inspection.

**Prevalent exposure effect**: evaluates the association between the
exposure or treatment status at time $t1$ and the outcome observed at a
later time $t2$. It is defined by the pathway $A_{1} \to Y_{2}$. The
prevalent exposure effect does not consider the initial status of the
exposure. It is expressed:

$$
\text{Prevalent exposure effect:} \quad A_{1} \to Y_{2}
$$

The prevalence exposure effect describes the effect of current or
ongoing exposures on outcomes. However, such and effect estimate is
rarely of interest, and risks pointing to erroneous conclusions. For
example where the effects of an exposure are deadly, the exposure will
select out those units who are susceptible from a population, leaving
only robust units. It may appear that $A_{1} \to Y_{2}$ is helpful when,
in fact, the treatment is harmful; see: @hernán2016; @danaei2012;
@vanderweele2020; @bulbulia2022.

**Incident exposure effect**: evaluates the association between the
exposure or treatment status at time $t1$ and the outcome observed at a
later time $t2$ conditional on the baseline exposure:
$A_{0} \to A_{1} \to Y_{2}$. This model more closely emulates an
experiment because it considers the transition in treatment or exposure
status from $A_0$ to $A_1$. The initiation of a treatment provides a
clearer intervention from which to estimate a causal effect at $Y_2$. It
is expressed:

$$
\text{Incident exposure effect:} \quad \boxed{A_{0}} \to A_{1} \to Y_{2}
$$

Further control is obtained by including the baseline outcome $Y_0$ as
well as the baseline exposure $A_0$ such that:


$$
\boxed{
\begin{aligned}
L_{0} \\
A_{0} \\
Y_{0}
\end{aligned}
}
\to A_{1} \to Y_{2}
$$

The incident exposure effect better emulates a 'target trial' or a the
organisation of observational data into a hypothetical experiment in
which there is a 'time-zero' initiation of treatment in the data; see:
@hernán2016; @danaei2012; @vanderweele2020; @bulbulia2022. *To obtain
the incident exposure effect, we generally require that events in the
data can be accurately classified into at least three relative time
intervals.*

**Time-varying confounding:** occurs when a confounder that changes over
time also acts as a mediator or collider in the causal pathway between
exposure and outcome. Controlling for such a confounder can introduce
bias. Not controlling for it can retain bias. We discuss time-varying
confounders in **Part 3**.

**Statistical model:** a mathematical representation of the
relationships between variables in which we quantify covariances and
their corresponding uncertainties in the data. Statistical models
typically correspond to multiple causal structures [@pearl2018;
@vanderweele2022b; @hernan2023]. That is, the causes of such covariances
cannot be identified without assumptions.

**Structural model:** defines assumptions about causal relationships.
Causal diagrams graphically encode these assumptions [@hernan2023],
leaving out the assumption about whether the exposure and outcome are
causally associated. Outside of randomised experiments, we cannot
compute causal effects in the absence of structural models. A structural
model is needed to interpret the statistical findings in causal terms.
Structural assumptions should be developed in consultation with experts.
The role of structural assumptions when interpreting statistical results
remains poorly understood across many human sciences and forms the
motivation for my work here.

### 2.5 Further terminology (my conventions)

**Causal incoherence**: the relative temporal order of events necessary
to evaluate causation is not maintained. For example, if investigators
stratify on post-treatment mediators or colliders, or if they attempt to
estimate $A_2 \rightarrowred Y_0$, their study will be causally incoherent.
Sequentially ordered causal diagrams assist investigators in avoiding
*causal incoherence*. Note that the 'relative timing of events' includes
the restriction (or 'selection') of units in one's study. For example,
if the study at baseline is restricted, yet the restriction is not
considered, the relative temporal order of the events necessary to
evaluate causation is not maintained.

**Estimands first**: before answering a causal question, ask it by stating a clearly defined causal contrast and scale of contrast, for a well-defined target population. That is, put estimands first. 

**Under-determination of causality by the data**: when constructing
causal diagrams, investigators must assume all causal paths depicted in
the diagram, except the direct path from exposure to outcome, which must
remain open in the presence of a true treatment effect and closed in its
absence. Additionally, investigators must rigorously consider the
implications of measurement error, interactions between exposure and
outcome, and mismatches between relevant characteristics of the units in
the sample and those of the target population. The data do not determine most assumptions required for causal inference. A causal diagram qualitatively encodes these investigator-determined assumptions.

**The assumption of ubiquitous effect-modifiers**: assume the causal effect of the exposure on the outcome is modified by other variables. This 'assumption' cannot generally be verified with data. It is motivated by caution.

**The assumption of informative censoring**: assume effect-modifier-restriction bias from an unit attrition in one's sample (censoring). Note, this assumption applies whether attrition induces confounding bias, linking exposure to outcome via a back-door path. It also applies to potential mismatch between the baseline population and the target population in the distributions of treatment effect modifiers. This assumption invokes *the assumption of ubiquitous effect-modifiers*. 

**The assumption of universal measurement error:** nearly every instrument measures with error. Assess the structural role that measurement error plays in biasing effect estimates. This 'assumption' cannot generally be verified with data. It is motivated by caution.

**Selection bias Rorschach test:** the term 'selection bias' is like a Rorschach test everyone sees something different. Causal diagrams function as interpreters that clarifying the exact problem an investigator has in mind.

**The assumption of universal unmeasured confounding**: even if the if treatment-assignment
is not random, if you seek inference for a per-protocol effect, assume unmeasured confounding. 

**Sequentially ordered causal diagram**: A causal diagram is deemed 'sequential' when its nodes and arrows are arranged to consistently reflect the progression of time. In such diagrams, the spatial representation follows the temporal sequence. As such, the graphs are 'ordered'. Conversely, if the arrangement does not adhere to the temporal sequence, the diagram is described as 'disordered'. For instance, $L_0 \to A_1 \to Y_2$ presents an ordered structure, while $A_1 \leftarrow L_0 \to Y_2$ presents a disordered structure. Despite their mathematical equivalence, sequentially structured causal diagrams provide clearer insights into the data collection requirements due to their temporal coherence.

**Your DAG is always wrong (advice)**:  your causal diagram (DAG) should only include as much information as required to address your causal question. By design your causal diagram (DAG) is 'wrong' representation of causality because it is only meant to show those features of a causal system that must be understood to obtain valid causal effect estimates. If your causal diagram is not wrong, you should consider making it so.

**Make your DAG good (responsibility)**: *the assumption of universal unmeasured confounding* implies that your causal diagrams should present unmeasured sources of confounding.

**With great power comes great responsibility:** follows from the *under-determination of causality by the data.* The data do not
inherently reveal causal relationships; instead, causal relationships are discerned through the frameworks of theory and assumptions of science. With the power to shape interpretations and draw conclusions from *assumptions* comes the significant responsibility to act
openly, judiciously, and thoroughly.

**With great responsibility comes great baggage:** causal data science is hard. It demands rigour, both in data analysis and collection.

### 2.6 Advice for drawing a sequentially ordered causal diagram

A causal diagram is intended to succinctly depict structural sources of
bias, rather than to represent data statistically. This distinction is
fundamental because the structure suggested by a causal diagram is often
not verifiable by data, making it 'structural' in nature, as distinct
from the graphs used in structural equation modelling [@pearl2009a;
@greenland1999c; @hernan2023; @bulbulia2022]. Misunderstanding this
difference between structural and statistical models has led to
considerable confusion across the human sciences [@vanderweele2015;
@vanderweele2022; @vanderweele2022b].

Although a sequentially ordered causal diagram is mathematically
identical to one without such order, the following examples reveal that
'sequential hygiene' in a diagram's layout can considerably enhance the
understanding of causal relationships. A sequentially hygienic graph
aligns the arrangement of nodes and arrows to reflect the assumed
temporal sequence of events. The conventions I adopt for maintaining
sequential hygiene are:

**Clearly define all nodes on the graph**: ambiguity leads to confusion.

**Simplify the graph by combining nodes where this is possible:** Keep
only those nodes and edges essential for clarifying the identification problem at hand. Make your DAG wrong (i.e. simplify.)

**Present unmeasured confounders**: they cannot be ruled out. Make your DAG good.

**Apply a multi-step graphing strategy**: we must address confounding bias, measurement-error bias, and threats to target validity. Addressing all-threats in one graph stands in tension with the demand for simplicity. @hernan2023 suggests that we initially, isolate confounding bias and 'selection bias' (threats to target validity, which may include confounding bias), then contemplate measurement bias using a secondary graph. Their argument is that we may require multiple graphs to retain focus when addressing structural sources of bias, see
@hernan2023 p.125.

 In short, to *make your DAG both wrong and good* may require making multiple causal diagrams. 


**Define the graph for the target population**: As we shall see in
$\S 3.1.6$ causal effects may be unbiased in a restricted population yet
fail to generalise to the target population. For this reason, it is
important to represent graph causal assumptions for the target
population. We denote censoring events using the convention
$Y^{\text{C}}\leftarrowblue U_{C=1}\rightarrowblue $ which
denotes the distribution of the covariates $L$ may differ in the
restricted popoulation $C=1$ compared with the unrestricted population $C = 1$,
where the counterfactual outcomes are stated for the uncensored sample
population $Y^{\text{C}}$, the potential outcomes for the uncensored (target) population.

**Maintain sequential order in the spatial organisation of the graph:**
Generally arrange nodes in *relative* temporal sequence, usually from
left to right or top to bottom. Although drawing the sequence to scale
is unnecessary, the order of events should be clear from the layout.
This provides an intuitive visual representation of how one event is
assumed to precede another in time.

**Time-index nodes**: in addition to spatially
ordering one's graph to match the flow of time, it is often helpful to
time-index nodes by the relative appearance of the event in time. This
explicit indexing helps in demarcating the temporal relationship between
variables, adding precision to the diagram with the organisation:

$$\boxed{L_{0}} \to A_{1} \to Y_{2}$$

This arrangement clearly illustrates the temporal sequence of these
variables. When measures are repeated, it is essential to time-index
nodes. For example, a powerful confounding control strategy is to
condition on baseline exposure and outcome. Because causal diagrams must
be acyclic, indexing the nodes allows us to keep track of the repeated
measurements.

$$
\boxed{
\begin{aligned}
A_{0} \\
Y_{0}
\end{aligned}
}
\to A_{1} \to Y_{2}
$$

**Define any novel convention in a causal diagram explicitly**: do not
assume familiarity.

**Ensure acyclicity in the graph**: This guarantees that a node cannot
be its own ancestor, thereby eliminating circular paths.

**Draw nodes for unmeasured confounders**: assume unmeasured confounding
always exists, whether depicted on the graph or not. This assumption
reveals the importance of sensitivity analyses when estimating causal
effects.

**Illustrate nodes for informative censoring.** This facilitates
understanding of potential sources of selection bias (see: $\S 3.1.6$).

**Causal diagrams are not designed to present non-linear effects** 
Do not draw arrows into arrows. Causal diagrams are qualitative tools that encode assumptions about causal relationships and dependencies. When we extend these diagrams to clarify structural threats to validity from measurement error bias and sample-restriction bias, some of our
conventions will be 'off-label.' Strictly speaking, causal diagrams
facilitate a spatial Markov factorisation of causal networks to identify
structural sources of bias within these networks. 
<!-- 
The networks are
described in terms of features (or characteristics) -- what we call
'variables' -- measured on units (individuals, cultures, languages)
sampled from populations. The intervention (exposure or treatment) is
considered an event that may lead to a change in the distribution of
features within units from these populations. We evaluate causality by
using data to simulate potential outcomes for units receiving
well-defined treatments; we quantitatively contrast these simulations on
a specified scale, such as the difference scale. The potential outcomes
that we simulate from data are defined in terms of the features these
units would have had, had they been subject to different levels of
intervention. There is much about causation of scientific interest not
included in a causal diagram, for example, non-linear effects.
Generally, evaluating non-linear effects is not necessary when focusing
on confounding. However, as we shall demonstrate, causal diagrams can
clarify other forms of bias, with both measurement error bias and
selection arising from structural confounding. Therefore, it would be
unwise to rigidly dictate the use of causal diagrams, except that
investigators should be clear in stating their purposes and conventions.
Overall, causal diagrams should be considered compasses, not
comprehensive atlases. We will revisit this theme in Part 3 when
considering interaction. -->

### 2.7 The four elemental structural sources of confounding bias

We have reviewed key terminology, conventions, and rules. It is time to
put causal diagrams into action, focusing on what Richard McElreath
calls the 'four fundamental confounders' see: @mcelreath2020 p.185. Note
that because we distinguish between the concepts of 'confounders' and
'confounding', we will examine the four elemental structural sources of
confounding bias.

#### 2.7.1. The elemental structural source of confounding bias from an unadjusted common cause

The first elemental structural source of bias arises when there is a
common cause, $L$, of the exposure, $A$, and outcome, $Y$. In this
setting, $L$ may create a statistical association between $A$ and $Y$,
implying causation in its absence.

Consider an example where smoking, $L$, is a common cause of both yellow
fingers, $A$, and cancer, $Y$. Here, $A$ and $Y$ may show an association
without causation. If we were to intervene to scrub the hands of
smokers, this would not affect their cancer rates. @fig-dag-common-cause
represents this elemental bias, where the red arrow signifies the bias
from the open path connecting $A$ and $Y$, caused by their common cause
$L$.

```{tikz}
#| label: fig-dag-common-cause
#| fig-cap: "Confounding by a common cause. The red path indicates bias from the open backdoor path from A to Y."
#| out-width: 80%
#| echo: false

\usetikzlibrary{positioning}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations}
\tikzstyle{Arrow} = [->, thin, preaction = {decorate}]
\tikzset{>=latex}
% Define a simple decoration
\tikzstyle{cor} = [-, dashed, preaction = {decorate}]


\begin{tikzpicture}[{every node/.append style}=draw]
\node [ellipse, draw=white] (L) at (0, 0) {L$_{0}$};
\node [rectangle, draw=white] (A) at (4, 0) {A$_{1}$};
\node [rectangle, draw=white] (Y) at (8, 0) {Y$_{2}$};
\draw [-latex, bend left, draw =red] (L) to (Y);
\draw [-latex, red] (L) to (A);
\end{tikzpicture}
```

#### 2.7.1.2 Advice: avoid causal incoherence by ensuring $L$ occurs before $A$

To address confounding by a common cause, we should adjust for it by
blocking the backdoor path from the exposure to the outcome. Such
adjustment will restore balance across the levels of $A$ in the
distribution of confounders that might affect the potential outcomes to
be contrasted, $Y(a*), Y(a)$. Again, standard methods for adjustment
include regression, matching, inverse probability of treatment
weighting, classical G-methods [@hernan2023], and more recently,
targeted learning frameworks [@hoffman2023].

@fig-dag-common-cause-solution quickly reveals what is needed:

1.  Ensure $A_1$ occurs before $Y_2$.
2.  Only condition on $L_0$ if $L_0$ occurs before $A_1$.


```{tikz}
#| label: fig-dag-common-cause-solution
#| fig-cap: "Solution: adjust for pre-exposure confounder. The implication: obtain time series data to ensure that confounders occur before the exposure."
#| out-width: 80%
#| echo: false

\usetikzlibrary{positioning}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations}
\tikzstyle{Arrow} = [->, thin, preaction = {decorate}]
\tikzset{>=latex}

% Define a simple decoration
\tikzstyle{cor} = [-, dashed, preaction = {decorate}]

\begin{tikzpicture}[{every node/.append style}=draw]
\node [rectangle, draw=black] (L) at (0, 0) {L$_{0}$};
\node [ellipse, draw=white] (A) at (4, 0) {A$_{1}$};
\node [rectangle, draw=white] (Y) at (8, 0) {Y$_{2}$};
\draw [-latex, draw=white] (A) to (Y);
\draw [-latex, bend left, draw =black] (L) to (Y);
\draw [-latex, black] (L) to (A);
\end{tikzpicture}

```

After time-indexing the nodes on the graph, it becomes evident that
*control of confounding generally requires being able to accurately
identify the relative timing of events in our data.* Timing is
everything, as they say. For instance, if the data contain measurements
of $A$ and $Y$, we must ensure that the data support $A_1\to Y_2$ and
not $A_2 \to Y_1$. Consider if an experiment reported that they were
unsure whether outcomes were recorded prior to or following the
experiment, or if it was deemed sufficient to report correlations
without precise timing of the experiment. 'Might have been before
treatment, might have been after.' We would be confused. Yet, this is
the setting we often face with many, albeit not all, cross-sectional
data sets.

To clarify, cross-sectional data can be relevant for causal inference,
provided that the data align with the assumptions outlined in a causal
diagram. For example, in cases where exposures are infrequent, we may
opt for retrospective data collection, mindful of how measurement error
in recollected data might pave pathways for bias, as illustrated in
examples from @barrett2021. Importantly, a causal diagram should ideally
be conceptualised prior to data collection, ensuring that causal demands
are not merely shaped by the available data. Once established, a
sequentially ordered causal diagram becomes instrumental in illuminating
the risks associated with relying on data where the sequence of events
encoded in the graph cannot be justifiably assumed for the measured
variables. Notably, concerns persist even with precise time-series data,
particularly in scenarios where clear baseline values for treatment
initiation are undefined or undetermined [@hernán2016]. In numerous
cross-sectional datasets, we risk modelling relationships incorrectly,
such as $Y_2 \to A_1$ or $Y_1 \rightarrowred \boxed{L_3} \leftarrowred A_1$. These challenges, elucidated by sequentially ordered causal diagrams, highlight the
importance of meticulous planning in data collection or comprehensive
validating assumptions for existing data if investigators are to
circumvent *causally incoherent* models.

**Appendix 2** clarifies the advantages of time-series models that
include baseline values for both the outcome and the exposure in units
tracked over time. Such models facilitate determining the relative
timing of not merely of relevant events but of their initiation, such
that:

$$
U \to
\boxed{
\begin{aligned}
L_{0} \\
A_{0} \\
Y_{0}
\end{aligned}
}
\to A_{1} \to Y_{2}
$$

When a sequence of events that includes baseline values of the exposure
and outcome can be discerned from the data, and provided that other
critical conditions are met --- including those related to measurement
error [@hernán2009; @vanderweele2012a; @blackwell2017; @bulbulia2023e],
selection and sampling biases [@hernán2004a; @hernán2017;
@westreich2012berkson; @lu2022], and the correct
specification of all models --- our confidence in causal inferences is
bolstered. Although sequential order avoids *causal incoherence*, few observational
datasets can guarantee complete protection against bias even with
appropriately sequenced variables. Consequently, no causal inference
workflow is complete without the inclusion of sensitivity analyses
[@vanderweele2017], a recurring mantra in this guide

#### 2.7.2. The elemental structural bias from conditioning on a mediator

If we condition on $L$ and it forms part of the causal pathway linking
the treatment and the outcome, conditioning on $L$ may bias the effect
of $A$ on $Y$. Here, we focus on *mediator bias*.

Take 'beliefs in big Gods' to be the treatment $A_{0}$, 'social
complexity' to be the outcome $Y_{2}$, and 'economic trade' to be the
stratified mediator $L_{1}$.

In this example, beliefs in big Gods $A_{0}$ directly influence economic
trade $L_{1}$, which then affects social complexity $Y_{2}$.
Conditioning on economic trade, $L_{1}$, will downwardly bias estimates
of the total effect of beliefs in Big Gods $A$ on social complexity
$Y_{2}$. @fig-dag-mediator presents this problem.

```{tikz}
#| label: fig-dag-mediator
#| fig-cap: "Bias from conditioning on a mediator. The dashed red arrow indicates bias arising from partially blocking the path between A and Y. Here, a true effect of A on Y is attenuated."
#| out-width: 80%
#| echo: false

\usetikzlibrary{positioning}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations}
\tikzstyle{Arrow} = [->, thin, preaction = {decorate}]
\tikzset{>=latex}

% Define a simple decoration
\tikzstyle{cor} = [-, dashed, preaction = {decorate}]

\begin{tikzpicture}[{every node/.append style}=draw]
\node [ellipse, draw=white] (A) at (0, 0) {A$_{0}$};
\node [rectangle, draw=black] (L) at (4, 0) {L$_{1}$};
\node [rectangle, draw=white] (Y) at (8, 0) {Y$_{2}$};
\draw [-latex, bend left, draw=red, dotted] (A) to (Y);
\draw [-latex, draw =black] (L) to (Y);
\draw [-latex, black] (A) to (L);
\end{tikzpicture}
```

#### 2.7.2.2 Advice: avoid causal incoherence by ensuring $L$ occurs before $A$

@fig-dag-common-effect-solution-2 presents the solution. We have
encountered the solution before. To avoid mediator bias:

1.  Ensure $A_1$ occurs before $Y_2$.
2.  Only condition on $L_0$ if $L_0$ occurs before $A_1$.


Our sequentially ordered causal diagram shows demands on data collection
and for data integrity. If we are interested in estimating the total
effect of $A\to Y$, we must ensure we have measured the relative timing
in the occurrences of $L$, $A$, and $Y$.

Note that mediator bias can and does occur in randomised experiments,
wherever the measured variables, such as demographic variables, are
taken *after* random assignment. Do not 'control' for post-treatment
variables in an experiment, and collect demographic information prior to
administering the treatment! See: @montgomery2018.

```{tikz}
#| label: fig-dag-common-effect-solution-2
#| fig-cap: "Solution: we avoid mediator bias by ensuring the correct temporal measurement of the confounder. Here, we assume confounding control conditional on measured co-variates L. We draw the black path between A and Y because this path will not be biased when there is a true causal effect of A on Y."
#| out-width: 80%
#| echo: false

\usetikzlibrary{positioning}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations}
\tikzstyle{Arrow} = [->, thin, preaction = {decorate}]
\tikzset{>=latex}
% Define a simple decoration
\tikzstyle{cor} = [-, dashed, preaction = {decorate}]


\begin{tikzpicture}[{every node/.append style}=draw]

\node [rectangle, draw=black] (L) at (0, 0) {L$_{0}$};
\node [ellipse, draw=white] (A) at (4, 0) {A$_{1}$};
\node [rectangle, draw=white] (Y) at (8, 0) {Y$_{2}$};
\draw [-latex, draw=black] (A) to (Y);
\draw [-latex, bend left, draw =black] (L) to (Y);
\draw [-latex, black] (L) to (A);
\end{tikzpicture}

```

#### 2.7.3 The elemental structural bias from conditioning on a common effect.

#### 2.7.3.1.1 Case when the collider is a common effect of the exposure and outcome

Consider a scenario in which a variable $L$ is affected by the treatment
$A$ and outcome $Y$ [@cole2010]. According to the rules of d-separation,
conditioning on a common effect, $L$, will open a non-causal association
between $A$ and $Y$. In mathematical terms, when $A$ and $Y$ are
independent, their joint probability should equal the product of their
individual probabilities: $P(A, Y) = P(A)P(Y)$. However, conditioning on
$L$ alters this relationship. The joint probability of $A$ and $Y$ given
$L$, $P(A, Y | L)$, does not equal the product of $P(A | L)$ and
$P(Y | L)$. Thus, the common effect $L$ creates an association between
$A$ and $Y$ that is not causal.

Imagine a randomised experiment investigating the effects of different
settings on individuals' self-rated health. In this study, participants
are assigned to either civic settings (e.g., community centres) or
religious settings (e.g., places of worship). The exposure of interest,
$A$, is the type of setting, and the outcome, $Y$, is self-rated health.
Suppose there is no effect of setting on self-rated health. However,
suppose both setting and rated health independently influence a third
variable: cooperativeness. Specifically, imagine religious settings
encourage cooperative behaviour, and at the same time, individuals with
better self-rated health are more likely to engage cooperatively.

Suppose the investigators decide to condition on cooperativeness, which
is the common effect of an $A$, and the outcome $Y$. Their rational
might be to study the effects of setting on health among those who are
more cooperative, or perhaps to 'control for' cooperation in the health
effects of religious setting. By introducing such 'control' the
investigators would inadvertently introduce collider bias, because the
control variable is a common effect of the exposure and the outcome.

```{tikz}
#| label: fig-dag-common-effect
#| fig-cap: "Bias from conditioning on a collider. The red path indicates bias from the open backdoor path from A to Y."
#| out-width: 80%
#| echo: false

\usetikzlibrary{positioning}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations}
\tikzstyle{Arrow} = [->, thin, preaction = {decorate}]
\tikzset{>=latex}

% Define a simple decoration
\tikzstyle{cor} = [-, dashed, preaction = {decorate}]

\begin{tikzpicture}[{every node/.append style}=draw]
\node [rectangle, draw=white] (A) at (0, 0) {A$_{0}$};
\node [ellipse, draw=white] (Y) at (4, 0) {Y$_{1}$};
\node [rectangle, draw=black] (L) at (8, 0) {L$_{2}$};
\draw [-latex, draw=red, bend right] (A) to (L);
\draw [-latex, draw=red] (Y) to (L);


\end{tikzpicture}

```

#### 2.7.3.1.2 Advice: avoid causal incoherence by ensuring $L$ occurs before $A$

We have encountered the solution to this problem before. To avoid
collider bias:

1.  Ensure $A_1$ occurs before $Y_2$.
2.  Only condition on $L_0$ if $L_0$ occurs before $A_1$.


@fig-dag-common-effect-solution-3 repeats the previous solutions given
in @fig-dag-common-cause-solution, @fig-dag-common-effect, and
@fig-dag-common-effect-solution-2. We are again directed to demands for
ensuring the relative timing in the occurrence of the variables we need
to model. To quantitatively model causality, we must accurately locate
the relative occurrence of $L$, $A$, and $Y$ in time. This strategy
helps to avoid the self-inflicted injury of conditioning on a collider
of the exposure and outcome. Note that with much cross-sectional data,
we cannot ensure that $L$ occurs before $A$ and $Y$. Unless the relative
timing of events can be extracted, cross-sectional data tempts *causally
incoherent* confounding control strategies.

```{tikz}
#| label: fig-dag-common-effect-solution-3
#| fig-cap: "Solution: we ensure that A and Y are d-separated by ensuring L occurs before A occurs."
#| out-width: 80%
#| echo: false

\usetikzlibrary{positioning}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations}
\tikzstyle{Arrow} = [->, thin, preaction = {decorate}]
\tikzset{>=latex}
% Define a simple decoration
\tikzstyle{cor} = [-, dashed, preaction = {decorate}]


\begin{tikzpicture}[{every node/.append style}=draw]

\node [rectangle, draw=black] (L) at (0, 0) {L$_{0}$};
\node [ellipse, draw=white] (A) at (4, 0) {A$_{1}$};
\node [rectangle, draw=white] (Y) at (8, 0) {Y$_{2}$};
\draw [-latex, draw=white] (A) to (Y);
\draw [-latex, bend left, draw =black] (L) to (Y);
\draw [-latex, black] (L) to (A);
\end{tikzpicture}

```

#### 2.7.3.2.1 Case when the collider is the effect of exposure but not of the outcome

We have considered how mediator bias may attenuate the total effect
estimate of $A$ on $Y$. However, we should not imagine that conditioning
on the effect of an exposure will always bias effect estimates downward.
Consider a scenario in which $L$ is affected by both the exposure $A$
and an unmeasured variable $U$ related to the outcome $Y$ but not to
$A$. Assume that there is no causal effect of $A$ on $Y$. In this
scenario, conditioning on $L$ introduces bias by opening a backdoor path
between $A$ and $Y$.

For example, consider a randomised experiment designed to assess the
effect of setting on self-rated health, comparing civic settings with
religious settings. Assume that the setting, $A$, does not actually
affect health, $Y$. However, suppose that religious settings
inadvertently increase people's cooperativeness, $L$. Also, imagine
there are prior health conditions, an unmeasured variable $U$, that
influence both cooperativeness and self-rated health. If the researchers
condition their analysis on cooperativeness, a side effect of the
religious setting, they might inadvertently create a spurious link from
the setting to self-rated health. This could suggest a causal effect
where none exists. In contrast, had the researchers not conditioned on
this post-treatment variable, cooperativeness, the influence of the
unmeasured confounder, $U$, on health would have remained balanced
across settings, avoiding the introduction of collider bias.
@fig-dag-descendant presents these biasing paths in red.

```{tikz}
#| label: fig-dag-descendant
#| fig-cap: "Confounding by descent: the red path illustrates the biasing path introduced by conditioning on a variable L that is affected by the treatment in a setting where an unmeasured confounder U affects both L and the outcome. Although L is not a mediator, conditioning on post-treatment variable L opens a backdoor path between A and Y."
#| out-width: 80%
#| echo: false

\usetikzlibrary{positioning}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations}
\tikzstyle{Arrow} = [->, thin, preaction = {decorate}]
\tikzset{>=latex}
% Define a simple decoration
\tikzstyle{cor} = [-, dashed, preaction = {decorate}]


\begin{tikzpicture}[{every node/.append style}=draw]

\node [rectangle, draw=white] (U) at (0, 0) {U};
\node [ellipse, draw=white] (A) at (2, 0) {$A_{0}$};
\node [rectangle, draw=black](L) at (4, 0) {$L_{1}$};
\node [ellipse, draw=white] (Y) at (6, 0) {$Y_{2}$};
\draw [-latex, bend right=50, draw = red] (U) to (L);
\draw [-latex, bend left, draw=red] (U) to (Y);
\draw [-latex,draw=red] (A) to (L);

\end{tikzpicture}
```

@fig-dag-descendant shows the setting of post-exposure *collider bias*.
Conditioning on the collider $L_{1}$ in the analysis induces a
non-causal association between $A_{0}$ and $Y_{2}$.

#### 2.7.3.2.2 Advice: avoid causal incoherence by ensuring $L$ occurs before $A$

The strategy builds on the strategy presented in
@fig-dag-common-cause-solution, @fig-dag-common-effect, and
@fig-dag-common-effect-solution-2 and @fig-dag-common-effect-solution-3.

1.  Ensure $A_1$ occurs before $Y_2$.
2.  Only condition on $L_0$ if $L_0$ occurs before $A_1$.


```{tikz}
#| label: fig-dag-descendant-solution
#| fig-cap: "This problem of confounding by descent is solved by conditioning on the pre-exposure confounder L, that is, by ensuring it is not measured post-exposure."
#| out-width: 80%
#| echo: false

\usetikzlibrary{positioning}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations}
\tikzstyle{Arrow} = [->, thin, preaction = {decorate}]
\tikzset{>=latex}
% Define a simple decoration
\tikzstyle{cor} = [-, dashed, preaction = {decorate}]


\begin{tikzpicture}[{every node/.append style}=draw]

\node [rectangle, draw=white] (U) at (0, 0) {U};
\node [rectangle, draw=black] (L) at (2, 0) {$L_{0}$};
\node [rectangle, draw=white](A) at (4, 0) {$A_{1}$};
\node [ellipse, draw=white] (Y) at (6, 0) {$Y_{2}$};
\draw [-latex, draw = black] (U) to (L);
\draw [-latex, bend left, draw=black] (U) to (Y);
\draw [-latex,draw=black] (L) to (A);

\end{tikzpicture}
```

#### 2.7.3.3.1 Case of conditioning on a pre-exposure collider (M-bias)

```{tikz}
#| label: fig-m-bias
#| fig-cap: "M-bias: Control by including previous outcome measures. The problem arises because L is a collider of unmeasured confounders U1 and U2, and conditioning on L opens a path from U2 to U1 to A and Y, d-connecting A and Y. This path is shown in red."
#| out-width: 80%
#| echo: false

\usetikzlibrary{positioning}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations}
\tikzstyle{Arrow} = [->, thin, preaction = {decorate}]
\tikzstyle{DoubleArrow} = [-, preaction = {decorate}]
\tikzset{>=latex}

% Define a simple decoration
\tikzstyle{cor} = [-, dashed, preaction = {decorate}]


\begin{tikzpicture}[{every node/.append style}=draw]

\node [rectangle, draw=white] (U1) at (0, 2) {U1};
\node [rectangle, draw=white] (U2) at (0, -2) {U2};
\node [rectangle, draw=black, thick] (L) at (4, 0) {L$_{0}$};
\node [rectangle, draw=white] (A) at (8, 0) {A$_{1}$};
\node [rectangle, draw=white] (Y) at (12, 0) {Y$_{2}$};

\draw [-latex, draw=red] (U1) to (L);
\draw [-latex, draw =red] (U2) to (L);
\draw [-latex, draw=red, bend left] (U1) to (Y);
\draw [-latex, draw =red, bend right] (U2) to (A);

\end{tikzpicture}
```

One must be cautious not to over-condition on pre-exposure variables. In
settings where we condition on a variable that is itself not associated
with the exposure or outcome but is the descendant of an unmeasured
instrumental variable as well as of an unmeasured cause of the outcome,
we may inadvertently induce biasing path known as 'M-bias', illustrated
in @fig-m-bias,

M-bias can arise even though a variable $L$ that induces it occurs
before the treatment $A$. Conditioning on $L$ creates a spurious
association between $A$ and $Y$ by opening the path between the
unmeasured confounders. We assume that $A$ and $Y$ d-separated, and as
such, $A \coprod Y(a)$. However, when stratified by $L$, this
independence is violated: $A \cancel{\coprod} Y(a)| L$. This form of
bias is another manifestation of collider stratification bias.

Note: when the path is ordered sequentially from left to right, the 'M'
shape, giving M-bias its name, changes to an 'E' shape. However, we
retain the term 'M-bias' to spare terminological confusion.

#### 2.7.3.3.2 Advice: causal coherence is not sufficient: do not condition *indiscriminately* on pre-exposure variables

```{tikz}
#| label: fig-m-bias-solution
#| fig-cap: "M-bias is avoided by not conditioning on pre-exposure variable L. Advice: be weary of McElreath’s 'Causal Salad', and instead condition indiscriminately on pre-exposure variables in L."
#| out-width: 80%
#| echo: false

\usetikzlibrary{positioning}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations}
\tikzstyle{Arrow} = [->, thin, preaction = {decorate}]
\tikzstyle{DoubleArrow} = [-, preaction = {decorate}]
\tikzset{>=latex}

% Define a simple decoration
\tikzstyle{cor} = [-, dashed, preaction = {decorate}]


\begin{tikzpicture}[{every node/.append style}=draw]

\node [rectangle, draw=white] (U1) at (0, 2) {U1};
\node [rectangle, draw=white] (U2) at (0, -2) {U2};
\node [rectangle, draw= white, align=left] (L) at (4, 0) {L$_{0}$};
\node [rectangle, draw=white] (A) at (8, 0) {A$_{1}$};
\node [rectangle, draw=white] (Y) at (12, 0) {Y$_{2}$};

\draw [-latex, draw=black] (U1) to (L);
\draw [-latex, draw = black] (U2) to (L);
\draw [-latex, draw= black, bend left] (U1) to (Y);
\draw [-latex, draw = black, bend right] (U2) to (A);
\end{tikzpicture}
```

@fig-m-bias-solution illustrates the solution to the issue of
conditioning on a pre-exposure collider, which can lead to M-bias. The
mere occurrence of a confounder prior to exposure does not justify
indiscriminate conditioning.

In theory, the guidance is clear: steer clear of an arbitrary approach
to confounding control. Richard McElreath aptly calls the arbitrary
method, 'the causal salad' [@mcelreath2020].

In practice, however, making decisions about conditioning demands
careful consideration of the specific causal question being addressed.
It is often challenging to discern whether a variable is a collider, a
confounder, or a proxy for either. It is crucial to remember that a
causal diagram *stipulates* all relevant causal paths, except for the
one between exposure and outcome. These stipulations are a great power
that carries a correspondingly great responsibility. Across causal data
science, there is a consensus that the insights of subject-matter
experts are vital in developing a causal diagram. These assumed
structures, once defined, guide the investigators' decisions in data
modelling. The example of M-bias highlights a critical point: accurate
temporal measurement of variables does not negate the potential for bias
when conditioning on a pre-exposure variable.

1.  Ensure $A_1$ occurs before $Y_2$.
2.  Ensure $L_0$ occurs before $A_1$.
3.  If $L_0$ is not a common cause of $A_1$ and $Y_2$, ensure it is the descendant (proxy) of an unmeasured common cause.



#### 2.7.4 The elemental structural bias of conditioning on a descendant of a collider

#### 2.7.4.1.1 Case when conditioning on a descendant of a collider creates bias

Consider a randomised experiment designed to assess the effect of
setting on self-rated health, comparing civic settings with religious
settings. As discussed in previous examples, conditioning on
cooperativeness opens a non-causal path between the exposure (setting)
and the outcome (self-rated health). Now, suppose the investigators do
not directly condition on cooperativeness but instead on a proxy for
cooperativeness, such as 'community involvement,' defined as the extent
of individuals' engagement in community activities or initiatives. This
proxy is likely influenced by the type of setting, as religious settings
might encourage more community engagement, and at the same time,
individuals with better self-rated health might be more active in their
communities. By conditioning on 'community involvement,' a proxy for
cooperation, the investigators might inadvertently introduce bias by
inducing a spurious link between the setting and self-rated health,
similar to the bias introduced by directly conditioning on
cooperativeness. The variable $L'$ in @fig-dag-descendant-proxy denotes
the proxy; the red paths reveal the open back-door path that arises from
conditioning on a proxy of a collider.

```{tikz}
#| label: fig-dag-descendant-proxy
#| fig-cap: "Confounding by descent: the red path illustrates the biasing path introduced by conditioning on a variable L'. Here L' is a descendant of a variable, L, that is affected both by the treatment and an unmeasured confounder U. Because U affects the outcome, conditioning on the descendant of L, L' induces collider bias."
#| out-width: 80%
#| echo: false

\usetikzlibrary{positioning}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations}
\tikzstyle{Arrow} = [->, thin, preaction = {decorate}]
\tikzset{>=latex}
% Define a simple decoration
\tikzstyle{cor} = [-, dashed, preaction = {decorate}]


\begin{tikzpicture}[{every node/.append style}=draw]

\node [rectangle, draw=white] (U) at (0, 0) {U};
\node [ellipse, draw=white] (A) at (2, 0) {$A_{0}$};
\node [rectangle, draw=white](L) at (4, 0) {$L_{1}$};
\node [rectangle, draw=black](L2) at (6, 0) {$L^{\prime}_{1+\epsilon}$};
\node [ellipse, draw=white] (Y) at (8, 0) {$Y_{2}$};
\draw [-latex, bend right=50, draw = red] (U) to (L);
\draw [-latex, bend left, draw=red] (U) to (Y);
\draw [-latex,draw=red] (A) to (L);
\draw [-latex,draw=red] (L) to (L2);

\end{tikzpicture}
```

#### 2.7.4.1.2 Advice: avoid causal incoherence by ensuring $L_0$ occurs before $A$ and it's proxy is not a mediator our common effect of the $A_1$ and $Y_2$

1.  Ensure $A_1$ occurs before $Y_2$.
2.  Ensure $L_0$ occurs before $A_1$.
3.  If $L_0$ is not measured, ensure $L'$, a proxy of $L_0$, is not a common effect of $A_1$ or $Y_2$ or a mediator along their path. 

#### 2.7.4.2.1. Case when conditioning on a descendant of a confounder reduces bias

```{tikz}
#| label: fig-dag-descendant-solution-2
#| fig-cap: "Conditioning on a proxy of a confounder that occurs after both the exposure and the outcome can reduce unmeasured confounding. The red dotted paths underscore that the confounding influence of U on A and Y is diminished by conditioning on L'. Even though L' occurs after the outcome. The paths are dotted to represent the bias reduction achieved by conditioning on the post-outcome descendant of an unmeasured common cause of the exposure and outcome. An example is a genetic factor affecting the exposure and outcome early in life, which can be measured later in life. Adjusting for such an indicator is an example of post-outcome confounding control."
#| out-width: 80%
#| echo: false

\usetikzlibrary{positioning}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations}
\tikzstyle{Arrow} = [->, thin, preaction = {decorate}]
\tikzset{>=latex}

\begin{tikzpicture}[{every node/.append style}=draw]

\node [rectangle, draw=white] (U) at (0, 0) {U};
\node [rectangle, draw=black] (L) at (6, -1) {$L^{\prime}_{3}$};
\node [rectangle, draw=white](A) at (2, 0) {A$_{1}$};
\node [ellipse, draw=white] (Y) at (4, 0) {Y$_{2}$};

\draw [-latex, bend right = 10, draw=black] (U) to (L);
\draw [-latex, bend right=20, draw =red, dotted] (U) to (Y);
\draw [-latex, draw =red, dotted] (U) to (A);

\end{tikzpicture}
```

Consider a scenario in which adjusting for a post-treatment descendant
variable, $L^\prime$, can make strategic sense. Imagine an unmeasured
confounder $U$ that influences $A$, $Y$, and $L^\prime$, with the effect
on $L^\prime$ occurring after both the exposure, $A$, and the outcome,
$Y$ have occurred. Because $L^\prime$ is a descendant of $U$, adjusting
for $L^\prime$ will reduce the confounding caused by the unmeasured
confounder $U$.

Is this scenario plausible? Consider an unmeasured genetic factor $U$
that influences both the exposure $A$ and the outcome $Y$ early in life,
but is expressed later in life through a developmental marker
$L^\prime$. Assume that $L^\prime$ is not affected by $A$ or $Y$. Even
though $L'$ occurs after the outcome, conditioning on $L'$ is useful for
confounding control because $L'$ provides information about $U$. This
example is presented in @fig-dag-descendant-solution, and illustrates
the prospect for post-outcome conditioning for confounding control.

#### 2.7.4.2.2 Advice: it need not be causally incoherent to condition on a post-exposure proxy of a pre-outcome confounder

The strategy for confounding control given in
@fig-dag-descendant-solution follows the modified disjunctive cause
criterion, which suggests including as a covariate any proxy for an
unmeasured variable that is a common cause of both the exposure and the
outcome [@vanderweele2019]. The prospect that we may use descendants for
confounding control is however consistent with general advice given
throughout this section:

1.  Ensure $A_1$ occurs before $Y_2$.
2.  Ensure $L_0$ occurs before $A_1$.
3.  If $L_0$ is not measured, ensure $L'$, a proxy of $L_0$, is not a common effect of $A_1$ or $Y_2$ or a mediator along their path.

Practically speaking, determining which variables belong in the
confounder set can be challenging. We take instruction from the best
lights of experts. However, science is the practice of revising expert
opinion. We assume experts may be wrong. For this reason, we should
perform sensitivity analyses. It should go without saying that we will
not let experts tell us whether the $A\to Y$ path exists or does not
exist. Estimating the magnitude of the association between the exposure
and outcome is the scientific task at hand.

In **Appendix 2**, I consider how causal data scientists might apply a
sequential causal diagram to data collection in a three-wave panel
design.

### 2.8 Application of causal diagrams to measurement bias

```{tikz}
#| label: fig-measure-error
#| fig-cap: "Graph of uncorrelated non-differential measurement error. A true effect may be attenuated if either the exposure or outcome (or both) are measured with error."
#| out-width: 50%
#| echo: false
\usetikzlibrary{positioning}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations}
\tikzstyle{Arrow} = [->, thin, preaction = {decorate}]
\tikzset{>=latex}

% Define a simple decoration
\tikzstyle{cor} = [-, dashed, preaction = {decorate}]


\begin{tikzpicture}[{every node/.append style}=draw]

\node [rectangle, draw=white] (UA) at (0, 1) {U$_A$};
\node [rectangle, draw=white] (UY) at (4, 1) {U$_Y$};

\node [rectangle, draw=white] (A1) at (0, 0) {$A^{\prime}_{1}$};
\node [rectangle, draw=white] (Y2) at (4, 0) {$Y^{\prime}_{2}$};
\node [rectangle, draw=white] (Aeta1) at (0, -1) {$A_1$};
\node [rectangle, draw=white] (Yeta2) at (4, -1) {$Y_2$};


\draw [-latex, draw=black] (UA) to (A1);
\draw [-latex, draw=black] (UY) to (Y2);


\draw [-latex, draw=black, dotted] (A1) to (Y2);

\draw [-latex, draw=black] (Aeta1) to (A1);
\draw [-latex, draw=black] (Yeta2) to (Y2);

\end{tikzpicture}
```

#### 2.8.1 Structural approaches to measurement error bias

The primary purpose of a causal diagram is to evaluate *identification
problem* that may lead to biased estimation of the $A \to Y$ path
[@pearl2009a; @mcelreath2020; @hernan2023; @rohrer2018; @deffner2022;
@suzuki2020; @greenland1999].

However, recall, *the assumption of universal measurement error*: measurement error is ubiquitous. Certain problems of measurement error bias can be represented as structural problems of confounding [@hernán2009; @vanderweele2012a]. 
Researchers do this by decompose a variable $X$ into its measurement
$X'$ and the unmeasured source of error in its measurement, such that
$X \to X' \leftarrow U_{\text{X}}$ For example, if the true exposure can
effect the error in which the true outcome is measured, an open path can
link the true exposure the measured outcome in the absence of causation,
$X\rightarrowred U_{\text{Y}} \rightarrowred Y'$ [@bulbulia2023e].

#### 2.8.2 Causal diagrams are poorly suited for diagnosing non-structural measurement error bias

Not all measurement error bias can be fully encapsulated in a causal
diagram. For example, when there is a true causal effect of the exposure
on the outcome, but when the errors of the measured exposure and of the
measured outcome are uncorrelated, measurement error will *attenuate* a
true effect. @fig-measure-error presents the attenuation of a true
effect using a dotted arrow, however this convention does not reflect a
confounding bias. According to the rules of d-separation the outcome and
exposure remain d-connected in the presence of such errors. A standard
causal diagram would draw a line connecting $A$ and $Y$. However, the
bias here does not arise from confounding but rather from the obscuring
of true measurement states. 

Although not all measurement biases can be conveyed by by causal diagrams, @vanderweele2012a show that under certain conditions, we can infer the direction of a causal effect from observed associations. Specifically, if (1) the association between the measured variables $A^{\prime}_{1}$ and $Y^{\prime}_{2}$ is positive, (2) the measurement errors for these variables are not correlated, and (3) we assume distributional monotonicity for the effect of $A$ on $Y$ (applicable, for example, when both are binary), then a positive observed association implies a positive causal effect from $A$ to $Y$. Conversely, a negative observed association implies a negative causal effect, especially if the error terms are correlated positively or independently of the true exposure and outcome. This conclusion relies on the assumption of distributional monotonicity for the effect of $A$ on $Y$. Moreover, if the error terms are positively correlated, than a negative association in the measure measured variables indicates a negative effectt of $A$ on $Y$. (See @bulbulia2023e for more discussion.)



### 2.9 Sequential causal diagrams reveal give the lie to causally incoherence science

In *Part 2* we learned that we may avoid *causal incoherence* by ensuring accuracy in the timing of all measured variables. We also learned that such accuracy is not sufficient. We must also opporate on the correct representation of reality. Yet in our model of reality, all but the exposure/outcome path must be assumed. *With great power comes great responsibility.* 

1.  Ensure $A_1$ occurs before $Y_2$.
2.  Ensure $L_0$ occurs before $A_1$.
3.  If $L_0$ is not a common cause of $A_1$ and $Y_2$, ensure it is the descendant (proxy) of an unmeasured common cause.
4.  If $L_0$ is not measured, ensure $L'$, a proxy of $L_0$, is not a common effect of $A_1$ or $Y_2$ or a mediator along their path.

## Part 3. Interaction (Moderation), Mediation, and Longitudinal Feedback

### 3.1.1 Causation in interaction and effect-modification?

In causal data science, we may think of interaction or moderation in one
of two ways, either as (1) joint intervention or (2)
effect-modification. Before considering these distinct concepts,
a word of warning.

#### 3.1.2 Warning: a causal diagram is a non-parametric encoding of causal assumptions 

Causal diagrams were developed to investigate confounding, not interactions. It is generally ill-advised to draw arrows into arrows. In $\S 3.1.6$ we modify standard conventions to represent effect modification. However, these graphs are not, strictly speaking, causal diagrams. For this reason, we must clearly state our conventions and purposes there.

#### 3.1.3 Interaction as a joint intervention

This form of interaction occurs when the combined effect of two interventions (or treatments) is different from what would be expected based on their individual effects. 

Consider two treatments, denoted as $A$ and $B$, and their outcome as $Y$. A joint intervention causal interaction implies that the effect of $A$ and $B$ together on $Y$ (denoted as $Y(A,B)$) is not merely the sum of their individual effects. 

For instance, consider the effect of beliefs in Big Gods (exposure $A$)
on social complexity (outcome $Y$), potentially influenced by a
culture's monumental architecture (exposure $B$). To assess the
individual and combined effects of $A$ and $B$, we look for evidence of
causal interaction on the difference scale. Evidence for interaction
would be present if the following inequality were to hold:

$$\bigg(\underbrace{\mathbb{E}[Y(1,1)]}_{\text{joint exposure}} - \underbrace{\mathbb{E}[Y(0,0)]}_{\text{neither exposed}}\bigg) - \bigg[ \bigg(\underbrace{\mathbb{E}[Y(1,0)]}_{\text{only A exposed}} - \underbrace{\mathbb{E}[Y(0,0)]}_{\text{neither exposed}}\bigg) + \bigg(\underbrace{\mathbb{E}[Y(0,1)]}_{\text{only B exposed}} - \underbrace{\mathbb{E}[Y(0,0)]}_{\text{neither exposed}} \bigg)\bigg] \neq 0 $$

This equation simplifies to

$$ \underbrace{\mathbb{E}[Y(1,1)]}_{\text{joint exposure}} - \underbrace{\mathbb{E}[Y(1,0)]}_{\text{only A exposed}} - \underbrace{\mathbb{E}[Y(0,1)]}_{\text{only B exposed}} + \underbrace{\mathbb{E}[Y(0,0)]}_{\text{neither exposed}} \neq 0 $$

A positive value would indicate evidence for additive interaction. A
negative value would indicate evidence for sub-additive interaction. A
value near zero would imply no reliable evidence for interaction.

```{tikz}
#| label: fig-dag-interaction
#| fig-cap: "This diagram presents the individual and joint effects of two exposures, A and B, on outcome Y. We assume that A and B are causally independent. If either exposure affects the other, then we may conduct effect-modification analysis or mediation analysis, but we should avoid causal interaction analysis. The diagram includes confounders L and W. Control for these confounders is necessary to close the backdoor paths that relate each exposure, A and B, to the outcome. Each exposure has equal status in our model: Y(a,b) = Y(b,a). The red path denotes paths of confounding."
#| out-width: 80%
#| echo: false

\usetikzlibrary{positioning}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations}
\tikzstyle{Arrow} = [->, thin, preaction = {decorate}]
\tikzset{>=latex}

\begin{tikzpicture}[{every node/.append style}=draw]

\node [rectangle, draw=white] (LA) at (0, .5) {L$_{0}$};
\node [rectangle, draw=white] (LB) at (0, -.5) {W$_{0}$};
\node [rectangle, draw=white] (A) at (2, .5) {A$_{1}$};
\node [rectangle, draw=white] (B) at (2, -.5) {B$_{1}$};
\node [rectangle, draw=white] (Y) at (5, 0) {Y$_{2}$};


\draw [-latex, draw=red] (LA) to (A);
\draw [-latex, draw=red] (LB) to (B);
\draw [-latex, draw=red, bend left] (LA) to (Y);
\draw [-latex, draw=red, bend right] (LB) to (Y);

%\draw [-latex, draw=black] (A) to (Y);
%\draw [-latex, draw=black] (B) to (Y);


\end{tikzpicture}

```

@fig-dag-interaction clarifies the need to evaluate two sources of counfounding, one for each intervention ($A$ and $B$). The graph resembles others we have considered. By adjusting for $L_{0}$ we obtain an unbiased estimate of the $A\to Y$ path. By adjusting for $W_{0}$ we obtain an unbiased estimate of the $B\to Y$ path. As indicated in @fig-dag-interaction-solved, we  must condition on both $L_{0}$ and $W_{0}$ to identify causal interaction conceived as a joint interaction. 

An important complication is that evidence for causal interaction may differ depending on the measurement scale one chooses to assess it [@vanderweele2014,@vanderweele2012]. Evidence for the strength of a causal effect estimate for interaction in the presence of effect-modification will differ depending on whether the effect is measured on the ratio scale as opposed to the difference scale (see: @vanderweele2014, who recommends using the causal difference scale for most policy settings.)

```{tikz}
#| label: fig-dag-interaction-solved
#| fig-cap: "We adjust for confounding in causal interaction analysis by adjusting for all confounders of the A to Y path as well as all for the B to Y path. The box over the confounders indicates the biasing paths are closed."
#| out-width: 80%
#| echo: false

\usetikzlibrary{positioning}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations}
\tikzstyle{Arrow} = [->, thin, preaction = {decorate}]
\tikzset{>=latex}

\begin{tikzpicture}[{every node/.append style}=draw]

\node [rectangle, draw=black] (LA) at (0, .5) {L$_{0}$};
\node [rectangle, draw=black] (LB) at (0, -.5) {W$_{0}$};
\node [rectangle, draw=white] (A) at (2, .5) {A$_{1}$};
\node [rectangle, draw=white] (B) at (2, -.5) {B$_{1}$};
\node [rectangle, draw=white] (Y) at (5, 0) {Y$_{2}$};


\draw [-latex, draw=black] (LA) to (A);
\draw [-latex, draw=black] (LB) to (B);
\draw [-latex, draw=black, bend left] (LA) to (Y);
\draw [-latex, draw=black, bend right] (LB) to (Y);

%\draw [-latex, draw=black] (A) to (Y);
%\draw [-latex, draw=black] (B) to (Y);


\end{tikzpicture}

```

Again we arrange @fig-dag-interaction-solved to follow the assumed sequence of
causation. Doing so better clarifies demands for data quality -- the timing of the events must be ensured. Data collection should also draw on expert knowledge about how $A$ and $B$ may be related over time; measurements of $A$ and $B$ should be taken within intervals in which $A$ and $B$ are unlikely to affect each other.

#### 3.1.4 Interaction as effect-modification of a single intervention

```{tikz}
#| label: fig-dag-effect-modification
#| fig-cap: "A simple sequentially ordered graph depicting effect-modification. We are interested in G as an effect-modifier of A on Y. A must not affect G (although G may affect A, we do not suppose such an effect). The blue path indicates effect-modification of A by G. The red path indicates an open backdoor path. We draw a box around G to indicate that we are conditioning on G in our model. We are interested in estimating the effect of A on Y. We must close all backdoor paths between A and Y, in this case, by conditioning on L. We are not interested in the effect of double intervention of G and A. Hence, we do not need to condition on Z to close the backdoor path from G to Y."
#| out-width: 80%
#| echo: false

\usetikzlibrary{positioning}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations}
\tikzstyle{Arrow} = [->, thin, preaction = {decorate}]
\tikzset{>=latex}

\begin{tikzpicture}[{every node/.append style}=draw, emod/.style={rectangle, fill=blue!10},
emoddot/.style={circle,fill=blue!10,dotted}]

\node [rectangle, draw=white] (LA) at (0, .5) {L$_{0}$};
\node [emoddot, draw=white] (LG) at (-2, -.5) {Z};
\node [rectangle, draw=white] (A) at (2, .5) {A$_{1}$};
\node [emod, draw=blue] (G) at (0, -.5) {G};
\node [rectangle, draw=white] (Y) at (4, 0) {Y$_{2}$};


\draw [-latex, draw=red] (LA) to (A);
\draw [-latex, draw=blue] (LG) to (G);
\draw [-latex, draw=blue] (G) to (Y);
\draw [-latex, draw=red, bend left] (LA) to (Y);
\draw [-latex, draw=blue, bend right] (LG) to (Y);
\end{tikzpicture}
```

It is often scientifically interesting to consider whether treatment effects vary over levels of other variable without imagining a double intervention. We call a variable over which the treatment effect varies, an 'effect-modifier' or an 'effect-measure modifier.' We call the phenomenon of variation in the effect of the exposure over levels of a covariate, 'effect-modification,' or 'effect-measure modification' or 'treatment effect heterogeneity. The counterfactual contrasts required to estimate effect-modification differ from those of a joint intervention. Suppose $A$ is the treatment, $G$ is the modifier, and $Y$ is the outcome. effect-modification assesses whether the effect of $A$ on $Y$ is different across levels of $G$ (i.e., whether the effect of $A$ on $Y$ is different when $G = g1$ compared to when $G = g2$).

@fig-dag-effect-modification consider whether effect-modification of $A$ on $Y$ across levels of $G$. Because we are not interested in the causal effect of $G$ as such, but rather, how the effect of $A$ varies across $G$, we would not need to adjust $G$ by $Z$. However, as we shall consider in the next section, the presence and absence of effect-modification may depend on other variables in a causal network, as well as on which other variables investigators condition on in their models. To foreshadow, we suppose that $Z$, a parent of $G$, is an effect-modifier of $A$ on $Y$. Were we to include $Z$ in the model, the effect estimate for $G$ on $Y$ may be attenuated or erased. There is here no clear fact of the matter about whether and how much $G$ is an effect-modifier outside of researcher modelling decisions. I remind readers: *with absolute power comes absolute responsibility.*

To better understand the interest of effect-modification, again consider a study investigating whether beliefs in big Gods affect social complexity. Suppose we compare two distinct geographical groups: North American societies ($G=1$) and Continental societies ($G=2$). Suppose we want to examine the causal effect of changing the exposure from $A = 0$ to $A = 1$ within each group and then compare these effects across the groups. The relevant causal contrasts are given as follows:

1.  **Causal effect within North American societies (**$G=1$):
    $$\hat{\tau}_{g1} = \hat{\mathbb{E}}[Y(1)|G=1] - \hat{\mathbb{E}}[Y(0)|G=1]$$ 
    
Here, $\hat{\tau}_{g1}$ represents the estimated causal effect of changing the exposure from $A = 0$ to $A = 1$ within the North American societies.

2.  **Causal effect within Continental societies (**$G=2$):

    $$\hat{\tau}_{g2} = \hat{\mathbb{E}}[Y(1)|G=2] - \hat{\mathbb{E}}[Y(0)|G=2]$$

    Similarly, $\hat{\tau}_{g2}$ denotes the estimated causal effect for the Continental societies.

3.  **Comparing causal effects across groups**:

    $$\hat{\gamma} = \hat{\tau}_{g1} - \hat{\tau}_{g2}$$ 
    
The estimated quantity $\hat{\gamma}$ computes the difference in the causal estimands between the two groups. A nonzero $\hat{\gamma}$ indicates effect-modification, suggesting that the effect of changing the exposure differs between the two groups. If we were to observe that $\hat{\gamma} \neq 0$, this would provide evidence for variability in the effect of the exposure on the outcome in different groups. Note that the causal effect for one group might be indistinguishable from zero, and yet we might nevertheless find evidence for effect-modification if the comparison group exhibits reliably different responses from the contrast group that is indistinguishable from zero.

#### 3.1.5 Evidence for effect-modification is relative to inclusion of other variables in the model

The 'sharp-null hypothesis' states there is no effect of the exposure on
the outcome for any unit in the target population. Unless the 'sharp-null hypothesis' is false, there may be effect-modification. For any study worth conducting, we cannot evaluate whether the sharp-null hypothesis is false. If we could the experiment would be otiose. Therefore, we must assume the possibility of effect-modification. Notably, whether a variable is an effect-modifier also depends on which other variables are included in the model. That is, just as for the concept of a 'confounder', where a variable is an 'effect-modifier' cannot be stated without reference to an assumed causal order and an explicit statement about which other variables will be included in the model [@vanderweele2012].

@fig-eff-mod-rel presents a scenario in which the marginal association
between $A$ and $Y$ is unbiased. The exposure $A$ is unconditionally
associated with $Y$. Recall our convention $\boxedblue{G}$ denotes
effect-modification with conditioning and $\circledotted{Z}$ indicates
effect-modification without conditioning. This graph states that the
conditional association of $A$ on $Y$ varies within levels of
$\boxedblue{G}$ (which is conditioned on), and furthermore that $G$ is
an *effect-modifier by proxy* [@vanderweele2009distinction]. Here,
$\circledotted{Z}$ fully mediates the association of $G$ and $Y$. That is
$\circledotted{Z}$ causes both $G$ and $Y$, and $G$ has no causal effect
on $Y$.

```{tikz}
#| label: fig-eff-mod-rel
#| fig-cap: "Consider a randomised experiment. There is no confounding. Here, the marginal association between A and Y provides an unbiased estimate for the causal effect of A on Y. Does the conditional association of A on Y vary within levels of G? The causal diagram allows for a classification of G as an effect modifier of A on Y by proxy. G modifies A's effect on Y in virtue of G's relationship to Z, which, according to this graph, is a direct effect modifier for the effect of A on Y."
#| out-width: 80%
#| echo: false


\begin{tikzpicture}[{every node/.append style}=draw, emod/.style={rectangle, fill=blue!10,draw=blue, thick},
emoddot/.style={circle, fill=blue!10, draw=blue, dotted}]


\node [rectangle, draw=white] (A) at (0, 0) {A$_{1}$};
\node [rectangle, draw=white] (Y) at (2, 0) {Y$_{2}$};
\node [emoddot] (Z) at (-3,0) {Z};
\node [emod] (G) at (-1, 0) {G};

\draw [-latex, draw=black] (A) to (Y);
\draw [-latex, bend left, draw=blue] (Z) to (Y);
\draw [-latex, draw = blue] (Z) to (G);

\end{tikzpicture} 


```

@fig-dag-effect-modification-4 presents the same a randomised experiment
as in the previous causal diagram. We again assume that there is no
confounding of the marginal association between the exposure, $A$, and
the outcome, $Y$. However, suppose we were to adjust for $Z$ and ask,
does the conditional association of $A$ on $Y$ vary within levels of
$G$, after adjusting for $Z$? That is, does $G$ remain an
effect-modifier of the exposure on the outcome? @vanderweele2007 proved
that for effect-modification to occur, at least one other arrow besides
the treatment must enter into the outcome. According to
@fig-dag-effect-modification-4, the only arrow into $Y$ other than $A$
arrives from $Z$. Because $Y$ is independent of $G$ conditional on $Z$
we may infer that $G$ is no longer an effect modifier for the effect of
$A$ on $Y$. Viewed another way, $G$ no longer co-varies with $Y$
conditional on $Z$ and so cannot act as an effect-modifier.

```{tikz}
#| label: fig-dag-effect-modification-4
#| fig-cap: "Conditioning on Z renders G independent of Y. G is no longer an effect modifier after conditioning on Z because G is independent of Y. Although Z is an unconditional effect modifier, G is not."
#| out-width: 80%
#| echo: false


\usetikzlibrary{positioning}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations}
\tikzstyle{Arrow} = [->, thin, preaction = {decorate}]
\tikzset{>=latex}

\begin{tikzpicture}[{every node/.append style}=draw, emod/.style={rectangle, fill=blue!10,draw=blue, thick},
squarednode/.style={rectangle, draw=blue!60, fill=red!20, very thick, minimum size=5mm}]
\node [rectangle, draw=white] (A) at (0, 0) {A$_{1}$};
\node [rectangle, draw=white] (Y) at (2, 0) {Y$_{2}$};
\node [emod] (Z) at (-3,0) {Z};
\node [rectangle, draw=white] (G) at (-1, 0) {G};

\draw [-latex, draw=black] (A) to (Y);
\draw [-latex, bend left, draw=blue] (Z) to (Y);
\draw [-latex, draw = black] (Z) to (G);

\end{tikzpicture} 


```

@fig-dag-effect-modification-5 presents the same a randomised experiment
as in the previous graph. If we do not condition on $B$, then $G$ will
not modify the effect of $A$ on $Y$ because $G$ will not be associated
with $Y$. However, if we were to condition on $B$, then both $B$ (an
effect modifier by proxy) and $G$ may become effect-modifiers for the
causal effect of $A$ on $Y$. In this setting, both $B$ and $G$ are
conditional effect-modifiers.

Note that casual graphs help us to evaluate classifications of
conditional and unconditional effect modifiers. They may also help to
clarify conditions in which conditioning on unconditional
effect-modifiers may remove conditional effect-modification. However we
cannot not tell from a causal diagram whether the ancestors of an
unconditional effect-modifier will be conditional effect-modifiers for
the effect of the exposure on the outcome; see: @vanderweele2007, also
@suzuki2013counterfactual. Causal diagrams express non-parametric
relations. I have adopted an off-label colouring convention to denote
instances of effect-modification to highlight possible pathways for
effect-modification, which may be relative to other variables in a
model.

```{tikz}
#| label: fig-dag-effect-modification-5
#| fig-cap: "Blue path denotes effect-modification for G by conditioning on B. Both B and G are conditional effect modifiers."
#| out-width: 80%
#| echo: false

\usetikzlibrary{positioning}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations}
\tikzstyle{Arrow} = [->, thin, preaction = {decorate}]
\tikzset{>=latex}

\begin{tikzpicture}[{every node/.append style}=draw, emod/.style={rectangle, fill=blue!10,draw=blue, thick},
emoddot/.style={circle, fill=blue!10, draw=blue, dotted}]

\node [rectangle, draw=white] (A) at (1, 0) {A$_{1}$};
\node [rectangle, draw=white] (Y) at (3, 0) {Y$_{2}$};
\node [emoddot, dotted, draw=blue] (Z) at (-2, 1) {Z};
\node [emod] (G) at (-2, -1) {G};
\node [emod] (B) at (-1,0) {B};

\draw [-latex, draw=black] (A) to (Y);
\draw [-latex, bend left, draw=blue] (Z) to (Y);
\draw [-latex, draw=blue] (Z) to (B);
\draw [-latex, draw=blue] (G) to (B);
\end{tikzpicture} 

```

@fig-dag-effect-modification-5b reveals the relativity of
effect-modification. If investigators do not condition on $B$, then $G$
cannot be a conditional effect-modifier because $G$ would then be
independent of $Z$ because $B$ is a collider. However, as we observed in
@fig-dag-effect-modification-5, conditioning on $B$, a collider, may
open a path for effect-modification of $G$ by $Z$. Both $B$ and $G$ are
conditional effect modifiers.

```{tikz}
#| label: fig-dag-effect-modification-5b
#| fig-cap: "Blue path denotes effect-modification. Here G is not an effect modifier because B, a common effect (collider) of G and Z, is not conditioned on. Any conditional effect modification for G would require conditioning on B, and not-conditioning on G. Otherwise G will be d-separated from Y."
#| out-width: 80%
#| echo: false

\usetikzlibrary{positioning}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations}
\tikzstyle{Arrow} = [->, thin, preaction = {decorate}]
\tikzset{>=latex}

\begin{tikzpicture}[{every node/.append style}=draw, emod/.style={rectangle, fill=blue!10,draw=blue},
emoddot/.style={circle, fill=blue!10, draw=blue, dotted}]

\node [rectangle, draw=white] (A) at (1, 0) {A$_{1}$};
\node [rectangle, draw=white] (Y) at (3, 0) {Y$_{2}$};
\node [emoddot, dotted, draw=blue] (Z) at (-2, 1) {Z};
\node [rectangle, draw = black] (G) at (-2, -1) {G};
\node [emoddot] (B) at (-1,0) {B};

\draw [-latex, draw=black] (A) to (Y);
\draw [-latex, bend left, draw=blue] (Z) to (Y);
\draw [-latex, draw=blue] (Z) to (B);
\draw [-latex, draw=black] (G) to (B);
\end{tikzpicture} 

```

```{tikz}
#| label: fig-dag-effect-modification-5c
#| fig-cap: "Blue path denotes effect-modification. Neither, G nor B are unconditional effect-modifiers for the effect of A on Y after Z is conditioned upon. If investigators condition on Z, the causal diagram implies they will not find evidence for effect-modification by B or G, which are conditionally independent of Y once Z is conditioned upon."
#| out-width: 80%
#| echo: false

\usetikzlibrary{positioning}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations}
\tikzstyle{Arrow} = [->, thin, preaction = {decorate}]
\tikzset{>=latex}

\begin{tikzpicture}[{every node/.append style}=draw, emod/.style={rectangle, fill=blue!10,draw=blue, thick},
emoddot/.style={circle, fill=blue!10, draw=blue, dotted}]

\node [rectangle, draw=white] (A) at (1, 0) {A$_{1}$};
\node [rectangle, draw=white] (Y) at (3, 0) {Y$_{2}$};
\node [emod, draw=blue] (Z) at (-2, 1) {Z};
\node [rectangle, draw = black] (G) at (-2, -1) {G};
\node [rectangle] (B) at (-1,0) {B};

\draw [-latex, draw=black] (A) to (Y);
\draw [-latex, bend left, draw=blue] (Z) to (Y);
\draw [-latex, draw=black] (Z) to (B);
\draw [-latex, draw=black] (G) to (B);
\end{tikzpicture} 

```

@fig-dag-effect-modification-5c considers the implications of
conditioning on $Z$, which is the only unconditional effect-modifier on
the graph. If $Z$ is measured, conditioning on $Z$ will remove
effect-modification for $B$ and $G$ because $B,G\coprod Y |Z$. This
examples again reveals the context dependency of effect-modification.
Here, causal diagrams are useful for clarifying features of dependent
and independent effect modification. For further discussion, see:
@suzuki2013counterfactual; @vanderweele2009distinction.

#### 3.1.6 Effect-modification and selection-restriction bias

#### 3.1.6.1 Selection-restriction arising from confounding

Suppose it is the 1950s. The investigators wish to examine whether a hand-grip-release-response varies with the voltage of electricity running through a grip. They devise an experiment consistent with ethics of that era.

-   $A$: the intervention: electric shock at 250v or electric shock at
    30v administered with following responses to a 1950s statistics quiz
-   $Y$: the outcome: speed of release reflex.
-   $Z$: potential effect modifiers: features on which the effect of voltage, $A$, on grip release speed, $Y$, may vary, such as shock sensitivity.
-   $\Pr(\text{strong shock})$: the probability of administering the shock, set at $\Pr(\text{strong shock})= 0.33$ for each condition, no matter how a subject responds
-   Participants are told that if they achieve a score of 70% on their test, and remain in the experiment to completion, they will be rewarded with a zoot suit (male) or high-waisted trousers (females).


```{tikz}
#| label: fig-experiment-selection-collider-adjustment
#| fig-cap: "In some settings, bias from loss-to-follow up may arise from confounding. Estimates are biased because the exposure affects attrition, as does another variable, U, that also affects the outcome. Note that the causal diagram presents the outcome Y_2 for the uncensored population (C=1). This is the population for which investigators seek to compute causal contrasts."
#| out-width: 80%
#| echo: false

\usetikzlibrary{positioning}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations}
\tikzstyle{Arrow} = [->, thin, preaction = {decorate}]
\tikzset{>=latex}


\begin{tikzpicture}[{every node/.append style}=draw]


\node [rectangle, draw=white] (A) at (0, 0) {A$_1$};
\node [ellipse, draw=white] (U) at (-2, 0) {U};
\node [rectangle, draw=black, thick](C) at (2, 0) {C$=1$};
\node [ellipse, draw=white] (Y) at (4, 0) {Y$_2^{\text{C}}$};


\draw [-latex, bend left=50, draw=red] (U) to (Y);
\draw [-latex, draw=red] (A) to (C);
\draw [-latex, draw=red, bend right=50,] (U) to (C);


\end{tikzpicture}


```

The randomisation of the treatment assignment (low voltage vs hi voltage) ensures that the potential outcomes under treatment assignment are
independent of all confounders. However, the treatment involves
administering a sequence of strong shocks with 1/3 probability. Suppose
participants are allowed to drop out. Suppose there is attrition. The
investigators surmise their intervention has something to do with
attrition. In this setting, the effects estimated for the censored sample population (the sample after attrition) may differ from those that would have been estimated for the baseline population had everyone remained in the study. A bias emerges because attrition, which is affected by the treatment, is also the effect of an unmeasured shock sensitivity that affects both attrition and the outcome (reflex speed). The resulting bias from the confounding bias from the censoring is presented in @fig-experiment-selection-collider-adjustment.

#### 3.1.6.2 Selection-restriction in the absence of confounding (effect-modifier distribution bias)

@fig-experiment-selection-restriction-adjustment-2 presents a general
problem arising from restriction of the target sample at every phase of
the study, even if the exposure does not cause the restriction, and
indeed even if restriction occurs before treatment assignment. Assume
the baseline population aligns with the target population in all
features relevant for causal effect estimation. Suppose further that
censoring is unrelated to the exposure. Imagine Australians are fonder of zoot suits. They are more inclined to remain in the study. In that case, the marginal effect of the exposure on the outcome would differ for the target population (Australians and New Zealanders) because the distribution of effect-modifiers would change *on the units* measured after censoring. Notice there is no backdoor path linking the exposure to the outcome. As such, there is no confounding bias. Here, the threat of *effect-modifier distribution bias* applies to generalisation or *target validity*, not to confounding. This threat arises wherever there is a difference in the baseline population and the censored population in the distribution of variables $L$ that modify the effects of the exposure on the outcome. Effect-modifiers should be assumed to be commonplace. As such, wherever there is restriction such that the units in a study differ from the target population, the results investigators obtain at the end of data collection may not be those that they seek. (A derivation is presented in **Appendix 4.**). 

@fig-experiment-selection-restriction-adjustment-2 presents the scenario just described. Note  $Y_2$ defines the outcome for the target population. Effect modification is denoted by $\boxedblue{Z^{C=1}}$. An unmeasured cause of censoring event $U_{C=1}$ relates the censoring event to the potential outcome, indicated by the blue path. The relationship presented here is not one of confounding. Nevertheless, the marginal effect estimated for the censored population will not necessarily reflect the marginal effect in the uncensored population, which cannot be directly observed from the censored data. Note that such *informative censoring* requires a model for recovering the missing outcomes that would have been observed had censoring not occurred. This typically requires the assumption that pre-censoring data together with a correctly specified missingness model are sufficient to recover marginal effects for the uncensored; see: @malinsky2022semiparametric; @li2023non. 

```{tikz}
#| label: fig-experiment-selection-restriction-adjustment-2
#| fig-cap: "In the presence of effect-modification for any variables in Z, the effect estimated in the end-of-study (censored) population will not necessarily be the same as that estimated in the uncensored (baseline) population, except under the sharp null hypothesis or in exceptional circumstances (see Appendix 4). The sharp null hypothesis cannot be assumed, nor can the exceptional circumstances be assessed from data because the treatment effects for the missing censored population are not observed. Again, note the causal diagram presents the outcome Y_2 for the uncensored population (C=1)."
#| out-width: 80%
#| echo: false

\usetikzlibrary{positioning}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations}
\tikzstyle{Arrow} = [->, thin, preaction = {decorate}]
\tikzset{>=latex}
\usetikzlibrary{positioning}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations}
\tikzstyle{Arrow} = [->, thin, preaction = {decorate}]
\tikzset{>=latex}


\begin{tikzpicture}[{every node/.append style}=draw, emod/.style={rectangle, draw=blue, thick, fill=blue!20},
squarednode/.style={rectangle, draw=blue!60, fill=red!20, very thick, minimum size=5mm}]


\node [rectangle, draw=white] (A) at (0, 0) {A$_{1}$};
\node [emod, draw=blue] (Z) at (-2, 0) {$Z^{C=\{0,1\}}$};
\node [font=\tiny, emod](C) at (2, 0) {$Z^{C=1}$};
\node [ellipse, draw=white] (Y) at (4, 0) {Y$_2^{C=\{0,1\}}$};
\node [rectangle, draw=white] (U) at (-2, -1){U$_{\text{C=1}}$};

\draw [-latex, draw=blue] (U) to (C);
\draw [-latex, bend right, draw=blue, bend right = 30] (U) to (Y);


\draw [-latex, bend left=50, draw=blue] (Z) to (Y);
\draw [-latex, draw=black, bend left =50] (A) to (Y);


\end{tikzpicture}



```


Consider further that the baseline population is a *restriction* of
the target population. As such, the same considerations that apply to restriction of the sample after treatment assignment apply to the sample before treatment assignment with respect to the target population. That is, we may think of the sample population as a censoring of the target population. Consistently estimated effects for the sample population will need not generalise if there are effect modifiers for the treatment effect.  Therefore, where population distributions can be obtained for known effect-modifiers, investigators should weight individuals in the study to these target population distributions. There is a long tradition in survey research for computing such weights; see: @horvitz1952generalization; @cole2010generalizing; @lumley2004analysis; @imai2008misunderstandings, however any model brings model mis-specification risks; see: @dahabreh2019; @dahabreh2021study.

```{tikz}
#| label: fig-experiment-selection-restriction-adjustment-4
#| fig-cap: "If conditioning on Q removes a conditional effect-modification of A on Y by Z, we may seek to condition on Q, an unconditional effect modifier. However, where censoring is informative, adjustment by Q will not ensure that the distribution of Q remains constant in the censored population. Our graph brings into focus the problem of informative censoring such that Pr(Q=q|C=1) ≠ Pr(Q=q|C). Again, note the causal diagram presents the outcome Y_2 for the uncensored population (C=1)."
#| out-width: 80%
#| echo: false

\usetikzlibrary{positioning}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations}
\tikzstyle{Arrow} = [->, thin, preaction = {decorate}]
\tikzset{>=latex}
\usetikzlibrary{positioning}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations}
\tikzstyle{Arrow} = [->, thin, preaction = {decorate}]
\tikzset{>=latex}



\begin{tikzpicture}[{every node/.append style}=draw, emoddot/.style={circle, fill=blue!20, dotted},
emod/.style={rectangle, fill=blue!20, thick, draw=blue, minimum size=5mm}]


\node [emoddot] (U) at (-6, 2) {U};
\node [rectangle, draw = white] (UC) at (-6, -4) {U$_{\text{C=1}}$};

\node [emod] (B) at (-6, -2) {B};
\node [rectangle, draw=white] (A) at (0, 0) {A$_{1}$};
\node [emod] (Q) at (-4, 0) {$Q^{C=\{0,1\}}$};
\node [rectangle, draw=black] (Z) at (-2, 0) {Z};
\node [font=\tiny, emod](C) at (2, 0) {$Q^{C=1}$};
\node [ellipse, draw=white] (Y) at (4, 0) {$Y_2^{C=\{0,1\}}$};
\draw [-latex, draw=blue] (UC) to (C);
\draw [-latex, bend right=10, draw=blue] (UC) to (Y);


\draw [-latex, draw=black] (Q) to (Z);
\draw [-latex, draw=blue] (U) to (Q);
\draw [-latex, draw=blue] (B) to (Q);
\draw [-latex, draw=blue, bend left] (U) to (Y);
\draw [-latex, draw=blue, bend left] (Q) to (Y);
%\draw [-latex, bend left=50, draw=black] (Z) to (Y);
\draw [-latex, draw=black,bend left] (A) to (Y);


\end{tikzpicture}


```

It might be tempting to try to address the problem of censoring bias using
regression adjustment. For example @lu2022 correctly note that selection
raises challenges for causal inference not from over-adjustment bias,
but rather from selection of a stratum of the population. The authors further correctly note that the problem for inference in after stratum specific selection
is a problem of *target validity* such that one may consistently compute
causal effects within the censored population without bias yet still
fail to recover the desired causal effect estimates for the target
population. @lu2022 propose a solution in which a conditioning on the
single common cause of selection and the exposure restores target
validity see: @lu2022, p704, Figure 4. However, as noted above, the
variables in $Z$ over which the exposure $A$ modifies $Y$ do not
necessarily (and perhaps not generally) cause censoring. To
conditionally remove the effect-modification of $Z$ by conditioning on
$Q$ would require that:

1.  $Q$ is an unconditional effect-modifier of $A$ on $Y$ and all variables in $Z$ are conditional-effect modifiers.
2.  $Q$ captures all effect-modification of $A$ on $Y$.
3.  Conditioning on $Q$ does not induce new conditional effect-modification.
4.  The distribution of $Q$ is known to be the same in the restricted sample as in the unrestricted sample.

These conditions impose strong, unverifiable demands on $Q$. 

Finally, return to the example of censoring with confounding bias discussed in $\S 3.1.6.1$, depicted in @fig-experiment-selection-collider-adjustment. Suppose the investigators believe that they can measure the confounder denoted $V$. By conditioning on the measured confounder $V$, there is no longer a backdoor path linking the exposure to the outcome. Conditioning on $V$ removes confounding.
However, if $V$ were an unconditional effect-modifier, even a consistently estimated effect of $A$ on $Y$ would produce a marginal effect estimate for the censored sample that may differ from the marginal effect estimate of the
target population. Additionally, there may be other unconditional effect modifiers other than $V$ for whom censoring is informative. @fig-experiment-selection-collider-adjustment-measured presents the inadequacy of conditioning on $V$.

```{tikz}
#| label: fig-experiment-selection-collider-adjustment-measured
#| fig-cap: "Target-validity may be compromised even if causal effect estimates in the censored sample are unbiased for the censored sample population. The red path linking exposure to outcome is blocked by conditioning on V. However, if censoring is informative, even if the confounder of the censoring event is blocked, sample/population mismatch in the distribution of effect-modifiers will threaten target (or external) validity."
#| out-width: 80%
#| echo: false

\usetikzlibrary{positioning}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations}
\tikzstyle{Arrow} = [->, thin, preaction = {decorate}]
\tikzset{>=latex}

\begin{tikzpicture}[{every node/.append style}=draw, emoddot/.style={circle, fill=blue!20, dotted},
emod/.style={rectangle, fill=blue!20, thick, draw=blue, minimum size=5mm}]

\node [rectangle, draw=white] (A) at (0, 0) {A$_1$};
\node [emod] (V) at (-2, 0) {V};
\node [font=\tiny, emod](C) at (2, 0) {$\Pr(Z=z|C=1)$};
\node [ellipse, draw=white] (Y) at (4, 0) {$Y_2^{\text{C}}$};
\draw [-latex, bend left=50, draw=blue] (V) to (Y);
\draw [-latex, bend left=50, draw=blue] (A) to (Y);
\draw [-latex, draw=red] (A) to (C);
\draw [-latex, draw=blue, bend right=50] (V) to (C);
\draw [-latex, bend right=48, draw=red] (V) to (C);

\end{tikzpicture}

```
#### 3.1.7 Causal diagrams and effect-modification

Causal diagrams primarily address confounding but can also clarify structural biases, particularly when sample units differ from the target population in ways that affect treatment outcomes. 

Even without confounding, *effect-modifier distribution bias* arising from sample restriction can persist. Adjustment is needed at two stages: before treatment assignment, using weights for any known effect-modifiers in the target population, and after treatment, adjusting for variables that modify the direct effect of treatment. Failure to correct for skewed distributions of effect modifiers can compromise the generalisability of findings. 

Recovering missing counterfactuals after restriction requires combining sampling weights with models accounting for censoring events. While multiple imputation can estimate missing outcomes, it must be applied cautiously to avoid model mis-specification and preserve causal order. Incorrect borrowing of information post-treatment can introduce confounding bias [@westreich2015; @bulbulia2023a].

Despite the best formal implementation, model mis-specification is an inherent risk, especially in censoring and survey models. Developing models for consistent, valid causal contrasts in the target population is a current challenge at the intersection of machine learning and causal inference [@künzel2019; @bareinboim2022; @cui2020]. Causal diagrams are part of complex workflows, and their use comes with significant responsibility and considerations [@cole2010generalizing; @bareinboim2013general; @kern2016assessing; @westreich2017transportability; @dahabreh2021study; @hernán2023; @suzuki2016; @bulbulia2023a].

<!-- To address censored/uncensored population mismatch in the distribution of effect-modifiers, techniques such as re-weighting the censored sample to reflect the  distribution of unconditional effect-modifiers are required. Another method is multiple imputation, in which the missing outcomes are imputed with uncertainty for the baseline sample were it not censored, (on the assumption that the baseline sample is the target population). Any such method relies on a correctly specified model.  For further discussions, see @cole2010generalizing; @bareinboim2013general; @kern2016assessing; @westreich2017transportability; @dahabreh2021study; @hernán2023; @suzuki2016; @bulbulia2023a. -->

### 3.2. Causation in mediation analysis

#### 3.2.1 Structural equation models lack structure

In the human sciences, mediation analysis is often mired in confusion, a situation exacerbated by the complex nature of causal relationships it aims to reveal. However, confusion dissipates when we define our causal question in relation to the counterfactuals we hope to estimate. Beyond
the intrinsic challenges of mediation analysis, much of the prevailing
confusion stems from the prevalent use of structural equation models
(SEMs). These models generally lack a systematic way of modelling the
complex counterfactual contrasts that are relevant to evaluating
causality. The widespread disconnect between the dominant modelling
traditions and the demands of causal data science is a particularly
worrying feature of the causal crisis that pervades many human sciences
presently. We have no guarantees they that such models are
interpretable. However, we can do better by clearly defining our
estimands with respect to a clearly defined target population. Causal
diagrams are powerful compasses by which to clarify the conditions under
which these estimands may be identified from data.

#### 3.2.2 Structure comes from clearly defining estimands

To gain a clearer understanding of what causal mediation entails, it is
helpful to deconstruct the total effect into the natural direct and
indirect effects.

The total effect of treatment $A$ on outcome $Y$ is defined as the
aggregate difference between the potential outcomes when the treatment
is applied versus when it is not. The estimand for the total effect (TE)
can be expressed as follows:

$$
TE = \mathbb{E}[Y(1)] - \mathbb{E}[Y(0)]
$$

The total effect can be further decomposed into direct and indirect
effects, which allow us to address questions of mediation. The potential
outcome $Y(1)$ taking into account the mediator can be expanded:

$$ 
\mathbb{E}[Y(1)] = \mathbb{E}[Y(1, M(1))]
$$

Here, the effect of the exposure, set to $A = 1$, is considered along
with the effect of the mediator at its natural value when $A = 1$.

Similarly, the potential outcome $\mathbb{E}[Y(0)]$ taking into account the mediator
can be expanded:

$$ 
\mathbb{E}[Y(0)] = \mathbb{E}[Y(0, M(0))]
$$

Here, we focus on the effect of the exposure, set to $A = 0$, along
with the effect of the mediator at its natural value when $A = 0$.

Next consider these quantities of interest as they relate to causal
mediation analysis. We can clarify our estimand by decomposing the Total
Effect (TE, which is equivalent to the average treatment effect, or marginal effect) into the Natural Direct Effect (NDE) and the Natural
Indirect Effect (NIE).

**Natural Direct Effect (NDE)** is the effect of the treatment on the
outcome while maintaining the mediator at the level it would have been
if the treatment had *not* been applied. The Natural Direct Effect (NDE)
is given:

$$
 NDE = \textcolor{blue}{\mathbb{E}[Y(1, M(0))]} - \mathbb{E}[Y(0, M(0))]
 $$

Here, the counterfactual quantities that are not directly realised in
the data are highlighted in blue: $\textcolor{blue}{\mathbb{E}[Y(1, M(0))]}$. Noticethat we add this term to the potential outcomes when $A=0$, namely,
$\mathbb{E}[Y(0)]$, recalling: $\mathbb{E}[Y(0, M(0))] = Y(0)$

**Natural Indirect Effect (NIE):** is the effect of the exposure on the
outcome that is mediated. To obtain these quantities we must compare the
potential outcome $Y$ under treatment, where the mediator assumes its
natural level under treatment with the potential outcome when the
mediator assumes its natural value under no treatment is given:

$$
 NIE = \mathbb{E}[Y(1, M(1))] - \textcolor{blue}{\mathbb{E}[Y(1, M(0))]}
$$

Here, the counterfactual quantities that are not directly realised in
the data are again highlighted in blue: $\textcolor{blue}{\mathbb{E}[Y(1, M(0))]}$.
Notice that we subtract the term from the potential outcomes when $A=1$,
namely, $\mathbb{E}[Y(1)]$, recalling: $\mathbb{E}[Y(1, M(1))] = \mathbb{E}[Y(1)]$.

Then, by rearranging this decomposition, we can demonstrate that the
total effect (TE) is the sum of the NDE and NIE. We do this by adding
and subtracting the term $\textcolor{blue}{\mathbb{E}[Y(1, M(0))]}$, highlighted in blue to our equation is given:

$$
\text{Total Effect (TE)} = \underbrace{\bigg\{\mathbb{E}[Y(1, M(1))] - \textcolor{blue}{\mathbb{E}[Y(1, M(0))]}\bigg\}}_{\text{Natural Indirect Effect (NIE)}} + \underbrace{\bigg\{\textcolor{blue}{\mathbb{E}[Y(1, M(0))]} - \mathbb{E}[Y(0, M(0))]\bigg\}}_{\text{Natural Direct Effect (NDE)}}
$$

The decomposition of the total effect into natural direct and indirect
effects greatly clarifies the targets of interest in causal mediation
analysis where the interest is in recovering natural indirect and direct
effects, see @vanderweele2015. These are the quantities that causal
mediation analysis often seeks [@vansteelandt2012; @valeri2014;
@vanderweele2014a; @shi2021; @steen2017]. However, to express these
quantities requires conceptualising them in relation to counterfactuals.
Lacking a counterfactual framework, it is unclear what our statistical
analysis would be estimating. Note that @vanderweele2015 provides a full
decomposition that includes causal interaction in settings of causal
mediation.

We next use sequentially ordered causal diagrams assess the stringent
demands for satisfying the assumptions of 'no unmeasured confounding' in
causal mediation analysis.

#### 3.2.3 Sequential causal diagrams in causal mediation analysis

```{tikz}
#| label: fig-dag-mediation-assumptions
#| fig-cap: "This causal diagram illustrates the four fundamental assumptions needed for causal mediation analysis. The first assumption pertains to the brown paths. It requires the absence of an unmeasured exposure-outcome confounder, and assumes that conditioning on L is sufficient for such confounding control. The second assumption pertains to the blue paths. It requires the absence of an unmeasured mediator-outcome confounder and assumes that conditioning on V is sufficient for such confounding control. The third assumption pertains to the green paths. It requires the absence of an unmeasured exposure-mediator confounder and assumes that conditioning on Q is sufficient for such confounding control. The fourth and final assumption pertains to the red paths. It requires the absence of a mediator-outcome confounder that is affected by the exposure and assumes that there is no path from the exposure to V to M. If the exposure were to affect V, then conditioning on V would block the exposure's effect on the mediator, as indicated by dashed red path."
#| out-width: 80%
#| echo: false

\usetikzlibrary{positioning}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations}
\tikzstyle{Arrow} = [->, thin, preaction = {decorate}]
\tikzset{>=latex}

\begin{tikzpicture}[{every node/.append style}=draw]


\node [rectangle, draw=black] (L1) at (0, 2) {L$_{0}$};
\node [rectangle, draw=black] (L3) at (0, -3) {Q$_{0}$};
\node [ellipse, draw=white] (A) at (3, 0) {A$_{1}$};
\node [rectangle, draw=black](L2) at (6, -3) {V$_{2}$};
\node [rectangle, draw=white](M) at (9, 0) {M$_{3}$};
\node [rectangle, draw=white](Y) at (12, 0) {Y$_{4}$};


\draw [-latex, draw=brown] (L1) to (A);
\draw [-latex, draw=brown, bend left] (L1) to (Y);
\draw [-latex, draw=green] (L3) to (A);
\draw [-latex, draw=green] (L3) to (M);
\draw [-latex, draw= black, dotted, thick] (A) to (M);
\draw [-latex, draw= black, bend left] (A) to (Y);
\draw [-latex, draw=red] (A) to (L2);
\draw [-latex, draw=blue] (L2) to (M);
\draw [-latex, draw=blue] (L2) to (Y);
\draw [-latex, draw= black] (M) to (Y);

\end{tikzpicture}

```

Recall the advice for confounding control that we encountered repeatedly
in Part 2:

1.  Ensure $A_1$ occurs before $Y_2$.
2.  Ensure $L_0$ occurs before $A_1$.


Although this advice is useful when estimating the causal effects of
single exposures, matters are more complicated when estimating the
causal effects of multiple exposures.

Consider again the hypothesis that cultural beliefs in 'big Gods'
influence social complexity, with political authority serving as a
mediator. We assume for present purposes we have well-defined
interventions and outcomes. What requirements are necessary to answer
our causal mediation question?

1.  **No unmeasured exposure-outcome confounder**

This requirement is expressed: $Y(a,m) \coprod A | L$. After accounting
for the covariates in set $L$, there must be no unmeasured confounders
influencing cultural beliefs in Big Gods, $A$, and social complexity
$Y$. For example, if our study examines the causal effect of cultural
beliefs in Big Gods (the exposure) on social complexity (the outcome),
and the covariates in $L$ include factors such as geographic location
and historical context, we need to ensure that these covariates
effectively block any confounding paths between $A$ and $Y$.
@fig-dag-mediation-assumptions shows this confounding path in brown.

2.  **No unmeasured mediator-outcome confounder**

This requirement is expressed: $Y(a,m) \coprod M | V$. After controlling
for the covariate set $V$, we must ensure that no other unmeasured
confounders affect the political authority $M$ and social complexity
$Y$. For instance, if trade networks affect political authority and
social complexity, to obstruct the unblocked path linking our mediator
and outcome we must account for trade networks. Furthermore, we must be
entitled to assume the absence of any other confounders for the
mediator-outcome path. @fig-dag-mediation-assumptions shows this
confounding path in blue.

3.  **No unmeasured exposure-mediator confounder**

This requirement is expressed: $M(a) \coprod A | Q$. After controlling
for the covariate set $Q$, we must ensure that no additional unmeasured
confounders affect cultural beliefs in big Gods $A$ and political
authority $M$. For example, the capability to construct large ritual
theatres may influence the belief in big Gods and the level of political
authority. If we have indicators for this technology measured prior to
the emergence of big Gods (these indicators being $Q$), we must assume
that accounting for $Q$ closes the backdoor path between the exposure
and the mediator. @fig-dag-mediation-assumptions shows this confounding
path in green.

4.  **No mediator-outcome confounder affected by the exposure**

This requirement is expressed: $Y(a,m) \coprod M(a^*) | V$. We must
ensure that no variables confounding the relationship between political
authority and social complexity in $V$ are themselves influenced by the
cultural beliefs in big Gods ($A$). For example, when studying the
effect of cultural beliefs in big Gods ($A$, the exposure) on social
complexity ($Y$, the outcome) as mediated by political authority
(mediator), there can be no un-modelled factors, such as trade networks
($V$), that influence both political authority and social complexity and
are themselves affected by the belief in big Gods.
@fig-dag-mediation-assumptions shows this confounding path,
$A\to \boxed{V}\rightarrowdotted M$.

Assumption 4, that there is no exposure-induced confounding in the
mediator-outcome relationship, is often a considerable obstacle for
causal mediation analysis. Where the exposure influences a confounder of
the mediator and outcome, we face a dilemma. Without adjusting for this
confounder, a backdoor path between the mediator and the outcome would
remain open. However, by adjusting for it, we partially obstruct the
path between the exposure and the mediator, leading to bias. In this
setting, we cannot recover the natural direct and indirect effects
directly from any observational data and may need to settle for
investigating controlled direct effects, which stipulate fixed values
for the mediator; see: @vanderweele2015; @robins1992.

Notice again that the requirements for counterfactual data analysis are
considerably stricter than has been appreciated in the structural
equation modelling traditions. Natural direct effect estimates and
natural indirect effects estimates require conceptualising a
counterfactual that is never directly observed from the data, namely:
$\textcolor{blue}{Y(1, M(0))}$ see: @vanderweele2015.

Unfortunately, a generation of researchers must unlearn the habit of
leaping from a description of a statistical process as embodied in a
structural equation diagram to the analysis of the data. It has been
over three decades since Robins and Greenland demonstrated that we
cannot understand the quantities we are estimating in mediation analysis
without first specifying the estimands of interest in terms of the
targeted counterfactuals of interest [@robins1992].

#### 3.2.4 Controlled direct effects (and other estimands for mediation)

In the previous section, we focused on the assumptions necessary for decomposing natural direct and indirect effects in causal mediation analysis. It is crucial to note that consistent estimates for natural direct and indirect effects are compromised if there exists a confounder affected by the exposure, which also influences the mediator-outcome relationship. Nonetheless, if all other assumptions hold, we can fix this mediator at a specific level to estimate a 'controlled direct effect' of the exposure at different mediator levels.

Consider a scenario where estimating a controlled direct effect is of interest. Suppose we aim to understand the effect of a stringent pandemic lockdown, $A$, on psychological distress, $Y$, focusing on trust in government, $M$, as a mediator. Further, suppose that pandemic lockdowns may plausibly influence attitudes towards the government through pathways that also affect psychological distress. For instance, people might trust the government more when it provides income relief payments, which may also reduce psychological distress. Under the rules of d-separation, conditioning on income relief payments, denoted as $V$, would attenuate the natural value of the mediator, trust in the government, under exposure to the lockdowns. This blocking of the exposure's effect is represented by the causal path $A \to \boxed{V} \rightarrowdotted Y$. Additionally, the exposure's effect on the mediator is partially blocked by the causal path $A \to \boxed{V} \rightarrowdotted M$. However, if we do not condition on $V$, the path from trust in government, $M$, to psychological distress, $Y$, would be confounded by the common cause $V$, hence: $Y \leftarrowred V \rightarrowred M$.

In such a scenario, it would not be feasible to consistently decompose the total effect of the exposure (pandemic lockdowns) on the outcome (psychological distress) into natural indirect and direct effects. Nevertheless, if all other assumptions hold, we could ascertain from data the controlled direct effect of pandemic lockdowns on psychological distress under fixed levels of trust in government. 

For example, we could examine the effect of the pandemic lockdown if we were able to intervene and set everyone's trust in government to, say, one standard deviation above the baseline, compared with fixing trust in government to the average level at baseline. We might use 'shift functions' that specify interventions as functions of the data. For instance, we might investigate interventions that 'shift only those whose mistrust of government was below the mean level of trust at baseline and compare these potential outcomes with those observed.' Asking and answering precisely formulated causal questions such as this might lead to clearer policy advice, especially in situations where policymakers can influence public attitudes towards the government; see: @williams2021; @díaz2021; @hoffman2022; @hoffman2023. 

In any case, I hope this brief discussion of causal mediation analysis clarifies that it would be unwise to simply examine the coefficients obtained from structural equation models and interpret them as meaningful as in statistical mediation analysis. We have no guarantees that these coefficients are interpretable. Rather, to answer any causal question, we must first state it, with respect to clearly defined counterfactual contrasts and a target population.

For those interested in estimands for causal mediation analysis, I recommend visiting the CMAverse website ([https://bs1125.github.io/CMAverse/articles/overview.html](https://bs1125.github.io/CMAverse/articles/overview.html), accessed 12 December 2023). This excellent resource provides comprehensive documentation, software, and practical examples, including sensitivity analyses. Next, we will consider more complex scenarios that involve feedback between treatments and confounders across multiple time points, settings in which traditional statistical methods also fail provide valid causal inferences.

### 3.3 Causation in repeated measures time-series data

#### 3.3.1 Multiple sequential exposures generate counterfactuals for each exposure that is modelled in time

Our discussion of causal mediation analysis focused on how effects from
two sequential exposures may combine to influence an outcome. This
concept can be expanded to investigate the causal effects of multiple
sequential exposures. In such cases, researchers often gravitate to
longitudinal growth models and multi-level models. Where do
counterfactuals fit within these modelling traditions? Without
incorporating counterfactuals, the conclusions we derive lack clear
causal interpretations. Sequentially ordered causal diagrams help us to
understand the challenges, and opportunities, when attempting to address
causal questions in these scenarios using repeated measures data.

Consider a study in which we hope to estimate the effects of multiple
sequential exposures on multiple sequential outcomes over time. Imagine
there are two treatment intervals and two outcomes at each interval,
each corresponding to one of the following fixed treatment regimens:

1.  **Always Treat (Y(1,1))**
2.  **Never Treat (Y(0,0))**
3.  **Treat Once First (Y(1,0))**
4.  **Treat Once Second (Y(0,1))**

| Type     | Description                            | Counterfactual Outcome |
|----------|----------------------------------------|------------------------|
| Regime   | Always treat                           | Y(1,1)                 |
| Regime   | Never treat                            | Y(0,0)                 |
| Regime   | Treat once first                       | Y(1,0)                 |
| Regime   | Treat once second                      | Y(0,1)                 |
| Contrast | Always treat vs. Never treat           | E\[Y(1,1) - Y(0,0)\]   |
| Contrast | Always treat vs. Treat once first      | E\[Y(1,1) - Y(1,0)\]   |
| Contrast | Always treat vs. Treat once second     | E\[Y(1,1) - Y(0,1)\]   |
| Contrast | Never treat vs. Treat once first       | E\[Y(0,0) - Y(1,0)\]   |
| Contrast | Never treat vs. Treat once second      | E\[Y(0,0) - Y(0,1)\]   |
| Contrast | Treat once first vs. Treat once second | E\[Y(1,0) - Y(0,1)\]   |

: Table outlines four fixed treatment regimens and six causal contrasts
in time-series data where exposure varies. {#tbl-regimens}

We can compute six causal contrasts for these four fixed regimens, as
shown in @tbl-regimens.

<!-- Note, the number of possible contrast combinations can be calculated: -->

<!--     $C(n, r) = \frac{n!}{(n-r)! \cdot r!}$ -->

Treatment assignments may be modelled as conditional shift functions of
previous outcomes, a concept known as 'time-varying treatment regimens'
[@robins2008estimation; @díaz2021] Comparisons between relevant
counterfactual quantities are necessary to estimate the causal effects
of time-varying treatment regimens. In causal mediation analysis
time-varying confounding was a concern (recall causal mediation
Condition 4: the exposure must not affect the confounders of the
mediator/outcome path). The same principle of confounding control
applies to sequential time-varying treatments. Unlike traditional causal
mediation analysis, however, we might be interested in treatment
sequences extended to more than two intervention intervals. Which
estimand we wish to recover must be explicitly stated, for only then can
we understand what it is that our statistical model aims to recover; see
@richardson2013.

#### 3.3.2 Sequential causal diagrams clarify treatment confounder feedback

Sequential causal diagrams help us to reveal limitations with
traditional multi-level regression analysis and structural equation
modelling analysis. Consider again a study that aims to assess certain
causal effects of beliefs in Big Gods on social complexity. We start by
estimating a fixed treatment regime. Assume we have well-defined
concepts of big Gods and social complexity, and assume we have accurate
measurements over time. Suppose we assess the effects of beliefs in big
Gods two centuries after a shift to big-Gods has occurred.

Fixed treatment strategies require comparing 'always believe in big
Gods' versus 'never believe in big Gods' and their effects on social
complexity conceived as a counterfactual contrast across conditionally
exchangeable groups of 'treated' and 'untreated' societies. Refer to
@fig-dag-9. Here, $A_{t}$ represents the belief in big Gods at time $t$,
and $Y_{t}$ denotes the outcome, social complexity, at time $t$. Imagine
economic trade, represented as $L_{t}$, is a time-varying confounder
that changes in response to the exposure, and influences economic trade.
An unmeasured confounder, $U$, such as oral traditions, might also
influence belief in big Gods and social complexity.

In a scenario where we can reasonably infer that the level of economic
trade at time $0$ ($L_{0}$) affects beliefs in big Gods at time $1$
($A_{1}$), we draw an arrow from $L_{0}$ to $A_{1}$. Conversely, if
belief in big Gods at time $1$ ($A_{1}$) affects future levels of
economic trade ($L_{2}$), an arrow from $A_{1}$ to $L_{2}$ is warranted.
This causal diagram demonstrates a feedback process between the
time-varying exposure $A$ and the time-varying confounder $L$.
@fig-dag-9 dispLs this exposure-confounder feedback dynamic. In
practical scenarios, the diagram might include more arrows, but our goal
here is to illustrate the issue of exposure-confounder feedback with the
minimal necessary arrows.

What would happen if we were to condition on the time-varying confounder
$L_{2}$? We would block all backdoor paths between the exposure $A_{1}$
and the outcome $Y_{4}$, which is crucial for eliminating confounding.
This result of our confounding strategy is positive: we exert
confounding control. However, this conditioning also closes previously
open paths, introducing structural sources of bias. For example, the
path $A_{1} \rightarrowred \boxed{ L_{2}} \leftarrowred U \rightarrowred Y_{4}$,
previously closed, would now be activated as the time-varying confounder
$\boxed{ L_{2}}$ is a common effect of $A_{1}$ and $U$. This result of
our confounding strategy is negative: we lose confounding control.
Conditioning on a time-varying confounder is a double-edged sword: it is
essential for blocking backdoor paths but it potentially opens other
problematic pathways of bias. This conundrum when conditioning on a
confounder affected by prior exposure -- being damned if we do, damned
if we don't -- is a critical consideration in longitudinal analysis.
Knowing nothing else, we may presume it to be the rule, rather than the
exception.

```{tikz}
#| label: fig-dag-9
#| fig-cap: "Exposure confounder feedback is a problem for time-series models. If we do not condition on L$_2$, a backdoor path is open from A$_3$ to Y$_4$ (we remove this path to simplify the graph). However, if conditioning on L$_2$ introduces collider bias, opening a path, coloured in red, between A$_1$ and Y$_4$. Here, we cannot use conventional methods to estimate the effects of multiple exposures. Instead we must use special methods, such G-methods, TMLE, SDR and others. Structural equation models (SEM) or multi-level models will not eliminate bias here."
#| out-width: 100%
#| echo: false

\usetikzlibrary{positioning}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations}
\tikzstyle{Arrow} = [->, thin, preaction = {decorate}]
\tikzset{>=latex}


\begin{tikzpicture}[{every node/.append style}=draw]
\node [rectangle, draw=white] (U) at (0, 0) {U};
\node [rectangle, draw=black] (L0) at (2, 0) {L$_{0}$};
\node [rectangle, draw=white] (A1) at (4, 0) {A$_{1}$};
\node [rectangle, draw=white] (Y2) at (6, 0) {Y$_{2}$};
\node [rectangle, draw=black] (L2) at (8, 0) {L$_{2}$};
\node [rectangle, draw=white] (A3) at (10, 0) {A$_{3}$};
\node [rectangle, draw=white] (Y4) at (12, 0) {Y$_{4}$};

\draw [-latex, draw=black] (U) to (L0);
\draw [-latex, draw=black] (L0) to (A1);
\draw [-latex, draw=black] (L2) to (A3);

\draw [-latex, bend left, draw=red] (A1) to (L2);
%\draw [-latex, bend left, draw=black] (L2) to (Y4);

\draw [-latex, bend right, draw=black] (U) to (Y2);
\draw [-latex, bend right, draw=red] (U) to (L2);
\draw [-latex, bend right, draw=red] (U) to (Y4);



%\draw [-latex, draw=black] (A1) to (Y2);
%\draw [-latex,  draw=black] (A3) to (Y4);

\end{tikzpicture}
```

#### 3.3.3 Sequential causal diagrams clarify time-varying confounding in the absence of treatment-confounder feedback

We find scope for time-varying confounding, even in the absence of
treatment-confounder feedback [@hernan2023]. When a time-varying
exposure and a time-varying confounder share a common cause, even in
cases where the exposure does not directly influence the confounder, a
backdoor path is opened because the time-varying confounder is a common
effect of two unmeasured confounders.
@fig-dag-time-vary-common-cause-A1-l1 presents this scenario, with the
red paths revealing the bias. Again, standard methods such as regression
and structural equation modelling (SEM) cannot recover unbiased causal
effect estimates.

```{tikz}
#| label: fig-dag-time-vary-common-cause-A1-l1
#| fig-cap: "Exposure confounder feedback is a problem for time-series models. Here, we do not assume that A_1 affects L_2. However, if an unmeasured variable, U2, affects both the exposure A_1 and the confounder L_2. Conditioning on L_2 opens a path, outlined in red, linking the exposure to outcome. We cannot use standard methods such as structural equation modelling (SEM) or multi-level models to recover our desired causal effect estimates."
#| out-width: 100%
#| echo: false

\usetikzlibrary{positioning}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations}
\tikzstyle{Arrow} = [->, thin, preaction = {decorate}]
\tikzset{>=latex}

\begin{tikzpicture}[{every node/.append style}=draw]
\node [rectangle, draw=white] (U1) at (0, 0) {U1};
\node [rectangle, draw=white] (U2) at (0, -2) {U2};
\node [rectangle, draw=black] (L0) at (2, 0) {L$_{0}$};
\node [rectangle, draw=white] (A1) at (4, 0) {A$_{1}$};
\node [rectangle, draw=white] (Y2) at (6, 0) {Y$_{2}$};
\node [rectangle, draw=black] (L2) at (8, 0) {L$_{2}$};
\node [rectangle, draw=white] (A3) at (10, 0) {A$_{3}$};
\node [rectangle, draw=white] (Y4) at (12, 0) {Y$_{4}$};

\draw [-latex, draw=black] (U1) to (L0);
\draw [-latex, draw=black] (L0) to (A1);
\draw [-latex, draw=black] (L2) to (A3);

%\draw [-latex, draw=black] (A1) to (Y2);
%\draw [-latex, draw=black] (A3) to (Y4);


\draw [-latex, bend right, draw=black] (U1) to (Y2);
\draw [-latex, bend right, draw=red] (U1) to (L2);
\draw [-latex, bend right, draw=red] (U1) to (Y4);

\draw [-latex, draw=red] (U2) to (L2);
\draw [-latex, draw=red] (U2) to (A1);

\end{tikzpicture}
```

#### 3.3.4 Standard methods fail to address time-varying confounding

Time varying confounding poses significant challenges in many human
sciences, and the problems are pronounced in the evolutionary human
science. Causal diagrams reveal these problem are not adequately
addressed by conventional regression-based methods, including
multi-level models and SEM because we cannot simultaneously control
confounding, avoid mediator bias, and avoid collider stratification bias
[@hernán2006; @robins1999; @robins1986]. The first wave of solutions
involved methods in which inverse treatment probability weighting
(marginal structural models) and regression simulation-prediction
methods (g-computation) and combinations thereof enabled robust
estimation of the desired quantities. Recent advances combining targeted
learning and semi-parametric estimation methods using machine learning
further reduce the demands on investigators to correctly specify the
functional forms of their models [@díaz2021; @vanderlaan2018; @díaz2021; @wager2018; @hoffman2022; @hoffman2023]. These methods have yet gained widespread acceptance across the human sciences, including the evolutionary
sciences. I hope this article encourages the readers of this special
issue of *Evolutionary Human Sciences* to learn more about these causal
inference methods. Useful overviews include: @hernan2023; @díaz2021;
@vanderweele2015; @hoffman2022; @hoffman2023; @chatton2020; @shiba2021;
@sjölander2016; @breskin2020; @vanderweele2009a; @vansteelandt2012;
@shi2021.

Finally, 'Single World Intervention Graphs' (SWIGs) explicitly represent
counterfactual outcomes on a graph. These graphs are powerful tools for
developing analytic strategies when there is time-varying confounding.
As their developers emphasises, SWIGs should be viewed as templates
rather than causal diagrams because they do not adhere to the property
of Markov factorisation. The use of SWIGs goes beyond the scope of this
guide, however, for those interested, more information can be found in
@richardson2013 and @hernan2023 ,pp 95-95.

## 4 Conclusions 

### 4.1 Summary 

Causation inherently occures in time, with causes preceding effects. However, quantifying causal effects requires contrasting counterfactual states -- events that could have occurred under different conditions but which did not occur. To estimate average causal effects, which are summaries of contrasts between these counterfactual states, causal data science relies on a framework of explicit assumptions and clear, multi-stepped workflows.

**Part 1** provided an overview of the basic assumptions for causal inference. We began by specifying an explicit causal question pertaining to clearly stated outcomes for a well-defined intervention on a target population. From there, we must ensure that the exposures of interest correspond to well-defined interventions (causal consistency), that measured covariates control for confounding (exchangeability), and that exposures to be compared have a non-zero probability of occurring at all levels of these measured covariates (positivity) in the exposures we seek to compare. We noted that there are further assumptions pertaining to measurement and the retention of alignment of one's sample with the target population, both at baseline and thereafter. There are, additionally, ever-present threats to valid inference from model misspecification. We learned that causal diagrams function primarily to assist researchers in evaluating the exchangeability assumption. This material is important because causal diagrams serve their purposes only within the framework of assumptions and workflows of causal data science. Outside this framework, causal diagrams can tempt false confidence.

**Part 2** outlined key concepts and fundamental techniques for utilising causal diagrams to address basic challenges in controlling confounding. We highlighted the importance of preserving a chronological sequence in causal diagrams to accurately reflect the presumed order of causation. For example, by assessing confounders before exposures and exposures before outcomes, we can prevent typical hazards to causal inference, like mediator bias and post-stratification bias, as well as the **causal incoherence** that arises from estimating outcomes based on exposures. Causal diagrams are not just useful for assessing control of confounding; they also clarify the essential requirements for data collection.

**Part 3:** combined attention to the counterfactual framework with
sequentially ordered causal diagrams to clarify the concepts of causal
interaction (moderation), causal mediation, and dynamic longitudinal
feedback. We discovered that the concept of causal interaction can
either mean a combined effect of two joint interventions or the
modification of a single effect across sub-populations. To properly
evaluate 'interaction', we must specify our causal questions in
advance. We found that causal mediation involves a dual exposure and
that the identification problems of causal mediation are subject to
stringent assumptions. We described special use cases for causal
diagrams in helping clarify features of treatment effect
heterogeneity (effect-modifier distribution bias), which may arise without confounding-bias. We considered confounder-treatment
feedback, in settings where we are interested in the causal effects of
multiple interventions over time, and again encountered considerable
identification problems that elude standard methods for complex time
series data. Throughout, sequential causal diagrams have enriched our understanding of the problems and opportunities for deriving valid causal inferences from events encoded in data.

<!-- Presently, causal inference remains scare across many human sciences. Misunderstandings and misinterpretations of correlations persist. There is an urgent need for a fundamental shift in education and practice to propel human scientific research past the mere presentation of ambiguous correlations. The substantial groundwork already laid in causal data science suggests that the widespread adoption of causal inference could revolutionise the human sciences. The contributions in this special edition of *Evolutionary Human Sciences* offer encouragement for hope that time has come. -->

### 4.2 The Causal Revolution Ahead

Although the causal revolution is progressing and gaining momentum, supporting institutions must evolve to facilitate it. The necessity for researchers to acquire new skills, coupled with the intensive requirement for precise time-series data collection, has significant implications for research design, funding, and the accepted speed of scientific advancement. To foster essential changes in causal inference education and practice, the human sciences need a shift from being predominantly output-focused to creating supportive environments that promote retraining and accurate time-series data collection. Such investments are worthwhile. They will transition the human sciences from butterfly collections of correlations to a deeper causal understanding of the complex evolutionary, cultural, and psychological dynamics that drive our curiosities.
{{< pagebreak >}}

## Funding

This work is supported by a grant from the Templeton Religion Trust (TRT0418) and RSNZ Marsden 3721245, 20-UOA-123; RSNZ 19-UOO-090. I also received support from the Max Planck Institute for the Science of Human History. The Funders had no role in preparing the manuscript or the decision to publish it.

## Acknowledgements

I am grateful to Dr Inkuk Kim for checking my manuscript and offering
feedback. 

I am also grateful to two anonymous reviewers and the editor, Charles Efferson, of *Evolutionary Human Sciences* for their constructive feedback that improved this manuscript. 

Any remaining errors are my own.

## References

::: {#refs}
:::

{{< pagebreak >}}

### Appendix A: Causal Consistency Under Multiple Versions of Treatment

To better understand how the causal consistency assumption might fail, consider a question that has been discussed in the evolutionary human science literature about whether a society's beliefs in big Gods affects its development of social complexity [@whitehouse2023; @slingerland2020coding; @beheim2021; @watts2015; @sheehan2022; @johnson2015; @norenzayan2016]. Historians and anthropologists report that such beliefs vary over time and across cultures in intensity, interpretations, institutional management, and rituals [@decoulanges1903; @wheatley1971; @bulbuliaj.2013;
@geertz2013]. Knowing nothing else, we might expect that variation in
content and settings could influence social complexity. Moreover, the
treatments realised in one society might affect the treatments realised
in other societies, that is, there might be *spill-over* effects in the exposures ('treatments') to be compared [@murray2021a; @shiba2023uncovering].

The theory of causal inference under multiple versions of treatment,
developed by VanderWeele and Hernán, formally addresses this challenge of
treatment-effect heterogeneity [@vanderweele2009; @vanderweele2013;
@vanderweele2018]. The authors proved that if the treatment variations,
$K$, are conditionally independent of the potential outcomes, $Y(k)$,
given covariates $L$, then conditioning on $L$ allows us to consistently
estimate causal effects over the heterogeneous treatments [@vanderweele2009].

Where $\coprod$ denotes independence, we may assume causal consistency
where the interventions to be compared are independent of their potential outcomes, conditional on covariates, $L$:

$$
K \coprod Y(k) | L
$$

That is, according to the theory of causal inference under multiple versions of treatment, we may think of $K$ as a 'coarsened indicator' for $A$.

Notice that the theory allows us to clarify what we require to estimate
causal effects in the presence of interference or 'spill-over', which we may consider to be a special case of treatment-effect heterogeneity. To handle
interference, we say the potential outcome of each unit $i \neq j$ must
be independent of its own treatment received as well as of the treatment
that all other units received on $j \neq i$, conditional on measured covariates $L$:

$$
    Y_i(k) \coprod K_i, K_j | L, \quad \forall i, \forall j \neq i
$$

Although the theory of causal inference under multiple versions of
treatment provides a formal solution to the problem of treatment-effect
heterogeneity, computing and interpreting causal effect estimates under
this theory can be challenging.

Consider the question of whether a reduction in Body Mass Index (BMI)
affects health [@hernán2008]. Weight loss can occur through various
methods, each with different health implications. Specific methods, such
as regular exercise or a calorie-reduced diet, benefit health. However,
weight loss might result from adverse conditions such as infectious
diseases, cancers, depression, famine, or accidental amputations, which
we may suppose are generally not beneficial to health, at least not in
the same way as, say, reducing weight by increasing physical activity.
Hence, even if causal effects of 'weight loss' could be consistently
estimated when adjusting for covariates $L$ in these settings, we might
be uncertain about how to interpret the effect we are consistently
estimating. This uncertainty highlights the need for precise and
well-defined causal questions. For example, rather than stating the
intervention vaguely as 'weight loss', we could state the intervention
clearly and specifically, say, 'weight loss achieved through aerobic
exercise over at least five years, compared with no weight loss.' This
specificity in the definition of the exposure, along with comparable
specificity in the statement of the outcomes helps to ensure that the
causal estimates we obtain are not merely unbiased but also
interpretable; for discussion see: @hernán2022; @murray2021a;
@hernán2008.

Beyond uncertainties for the interpretation of heterogeneous treatment effect estimates, there is, as just mentioned, the additional consideration that we cannot fully verify from data whether the measured covariates $L$ suffice to render the multiple versions of treatment independent of the counterfactual outcomes. This problem is acute when there is *interference*, which occurs when treatment effects are relative to the density and distribution of treatment effects in a population. Scope for interference will often make it difficult to warrant the assumption that the potential outcomes are independent of the many versions of treatment that have been realised, dependently, on the administration of previous versions of treatments across the population [@bulbulia2023a; @ogburn2022; @vanderweele2013].

In short, although the theory of causal inference under multiple
versions of treatment provides a formal solution for consistent causal
effect estimation in observational settings, *treatment heterogeneity*
remains a practical threat. Generally, we should assume that causal
consistency is unrealistic unless proven innocent.

For now, we note that the causal consistency assumption provides a
theoretical starting point for recovering the missing counterfactuals
required for computing causal contrasts. It identifies half of these
missing counterfactuals directly from observed data. The concept of
conditional exchangeability, which we explore next, offers a means for
recovering the remaining half.

{{< pagebreak >}}

## Appendix B: Additional Assumptions Beyond the Three Fundamental Assumptions of Causal Inference


#### 1.3.1 Overly ambitious estimands

In causal inference, the Average Treatment Effect (ATE) conceived as comparison between population-wide simulations at two levels of
exposure, $E[Y(1)] - E[Y(0)]$, is often artificial. Artificiality is
evident for continuous exposures, where such comparisons simplify the complexity of real-world phenomena into a low dimensional summary, such as a contrast of a one-standard-deviation difference in the mean, or a comparison of one quartile of exposure to another quartile of exposure. In practice, the requirements for targeting such contrasts impose a strong reliance on statistical models, which introduce further opportunities for bias. Such comparisons might also strain the positivity assumption because the relevant events occur infrequently or are absent within the strata of covariates required to satisfy conditional exchangeability. Moreover, because treatment effects arebrarely linear and may not be monotonic. For this reason, comparingbarbitrary points on a continuous scale, while relying on correctbmodelling specifications, risks drawing erroneous conclusionsb[@calonico2022; @ogburn2021]. In short, the simplifications and modelsbrequired for obtaining standard causal estimands often lack realism. The
practical inferences that we draw from them may be misleading
[@vansteelandt2022].

Furthermore, the 'average treatment effect' itself might not be our
primary scientific interest. In many setting we may want to understand
heterogeneity in treatment effects without a clear understanding in
advance of modelling where such heterogeneity may be found [@wager2018].
Presently, methods for valid causal inference in settings of
heterogeneous treatment effects remain inchoate see: @tchetgen2012;
@wager2018; @cui2020; @foster2023; @foster2023; @kennedy2023; @nie2021.

Recently, causal data scientists have explored new classes of estimands
and estimators, such as modified treatment policies or 'shift
interventions' [@hoffman2023; @díaz2021; @vanderweele2018;
@williams2021] and optimal treatment policies [@athey2021;
@kitagawa2018]. Such estimands allow researchers to specify and examine
a broader range of causal contrasts, such as treating those as treating
only those likely to respond, or those who meet certain ethical criteria
not determined by statisticians, or those who optimise a pre-specified
[@wager2018; @cui2020; @díaz2021]. A review of these promising
developments would take us beyond the scope of this discussion, however,
readers should be aware that causal inference is not bound to standard
$E[Y(1)] - E[Y(0)]$ estimands that require simulating often implausible or
even unhelpful counterfactual outcomes for the entire population at two
levels of a pre-specified intervention.

#### 1.3.2 Target validity

The question of whether one's results generalise as intended is at the
heart of science. Often the term 'selection bias' is used to describe threats from sample/target population mismatch. However, we shall limit our use of the term 'selection bias' because it is interpreted differently across disciplines. In economics, it often corresponds to what epidemiologists term 'confounding' or 'confounding bias'[@angrist2009mostly]. In epidemiology, confounding bias typically refers to a non-causal link between the exposure, $A$, and the outcome, $Y$, leading to a scenario where the potential outcomes are not independent of the exposure: $Y(a)\cancel\coprod A|L$. This concept incompletely overlaps with threats to valid inference sample/target population mismatch.

Additionally, some epidemiologists use 'selection bias' to include
*collider stratification bias*. This bias occurs when conditioning on a
collider, $C$, disrupts the conditional independence between $Y(a)$ and
$A$ given a set of covariates $L$: $Y(a) \coprod A \mid L$, and this
independence is violated with the inclusion of $C$, as in
$Y(a) \cancel\coprod A \mid L, C$ [@hernan2023; @hernán2004;
@greenland2003quantifying]. However, other epidemiologists use  'selection bias' to refer to a mismatch between the study population and the target population, leading to effect estimates that do not necessarily deliver what investigators hope. Such restriction can manifest as *confounding bias*, where the restriction event opens a backdoor path linking the exposure and outcome. However, selection-restriction can also present as *effect-modifier-restriction bias*, where the distributions of effect-modifiers differs between the restricted sample and the target population. Both problems affect randomised experiments where there is attrition [@lu2021revisiting; @lu2022toward; @lu2023selection]. For this reason, the term 'selection bias' is ambiguous.

In this guide, we examine *collider stratification bias* in $\S 2.7.3$.  When describing this bias we avoid using the term 'selection bias.' We discuss the implications of sample restriction bias in $\S 3.1.6$.  We describe how describe how this threat may arise either from *collider stratication bias* or *effect-modifier restriction bias* (or both). Thus, we avoid the term 'selection bias' and focus on the problem of interest. Causal diagrams help to avoid terminological confusions by plaining stating the source of bias in causal-effect estimation without relying on ambiguous terminology.

Setting terminology aside, it is crucial for investigators to recognise that a mismatch between the sample and target population can invalidate causal effect estimates. This is so, even if investigators consistently estimate the average treatment effect for the sample population. If the mismatch affects effect-modifiers of the treatment-effects there will be no guarantee that effect-estimates for the sample will generalise to the target population -- that is, no guarantee that the effect estimates will achieve 'target validity,' or equivalently 'external validity.' Worringly such threats cannot be fully evaluated from responses in the restricted or censored sample. Special workflows are required for addressing disparities in the distribution of effect modifiers influencing the treatment's effect on an outcome in the sample, as compared with the target population. Mismatch between the target and sample populations in the distributions of effect modifiers may occur at baseline, and mismatch may subsequently evolve over subsequent intervals during which study units are observed. The problem of aligning target and sample populations in these distributions is therefore a problem where measures are repeated over time (see: $\S 3.1.6$ and **Appendix 4**).

<!-- **effect-modifier-restriction bias**, this is a secondary application. We extend the use of causal diagrams to clarify relations they were not designed to handle. This is a 'off label' use of the tool. To mitigate hazards, we clearly state that our intent in this setting is not to evaluate confounding bias. -->

<!-- Assume that our analytic sample population at the end of the study is a subset of the target population. Again, we assume that we consistently estimate causal effects. We can only guarantee consistent causal effect estimates for the target population under the following conditions: -->

<!-- 1. **Sharp null hypothesis**: there is no causal effect in the super-population, from which both the analytic sample population at the end of the study and the target population are drawn [@hernán2017]. The validity of the sharp null hypothesis cannot be known a priori; if it were known, the study would be redundant. -->

<!-- 2. **Equivalence of ATEs**: The average treatment effects at different stages should be equivalent, as follows: -->

<!-- $$ -->

<!--    \widehat{ATE}_{\text{analytic sample population end of study}} = \\ -->

<!--    \widehat{ATE}_{\text{analytic sample population beginning of study}} = \\ -->

<!--    \widehat{ATE}_{\text{source population}} = \\ -->

<!--    \widehat{ATE}_{\text{target population}} -->

<!-- $$ -->

<!-- However, if any of these equivalences do not hold, selection (restriction) can compromise the generalisability to the target population. -->

<!-- More precisely, if the distribution of effect-modifiers differs between the target and end-of-study sample populations, an average treatment effect consistently estimated for the sample population will diverge from that for the target population. This difference persists unless a variable downstream of the effect-modifier blocks its path to the outcome. It is important to note that if the $\widehat{ATE}_{\text{analytic sample population end of study}}$ is not consistently estimated, the study's results will not generalise. Internal validity is a necessary but not sufficient condition for consistently computing causal contrasts for the target population. -->

<!-- In Part 3, we will explore how sequential causal diagrams can clarify strategies for addressing generalisation bias resulting from mismatches between sample and target populations. By generalisation bias I mean a threat to *external validity*. These strategies often involve blocking backdoor paths between effect-modifiers and the outcome, even in the absence of confounding. We will see that although sample restriction is not an problem of over-adjustment bias, attempting to address it might inadvertently induce such bias. Therefore, I will not only refrain from equating collider stratification bias with 'selection bias' (as it relates to over-adjustment rather than selection) but also encourage readers to move beyond the term 'collider restriction bias' and focus simply on the concept of restriction.  Restriction is a property of *samples*, or more specifically, of sampling populations as they relate to target populations.  
Thus to obtain valid inference for the target population, we must consider two selection event bias. 
1. **Stratum-specific selection bias at baseline**: we say there is stratum-specific selection bias at baseline if the analytic sample selected into the study is different from the pre-specified target population in the distributions
 of variables that modify the effect of the exposure on the pre-specified measure of contrast for which the pre-specified counterfactuals for the pre-specified target population are to be compared. Strategies for obtaining the $ATE_{target}$ might involve, for example, the use of post-treatment stratification survey weights to obtain the causal quantities of interest. The application of these survey weights do not typically relate to the task of ensuring conditional independence of the potential outcomes from the treatment.  More generally, assuming the $ATE_{\text{analytic sample}}$ is consistently estimated, researchers must ask whether they may obtain and apply a function $f_T(\cdot,T)$ such that if the $ATE_{\text{analytic sample population}}$ is consistently estimated:
$$
ATE_{\text{target population}} = f_T(ATE_{\text{analytic sample population}},T)
$$ 
Furthermore it can be demonstrated that where the distribution of effect-modifiers differs between the target population and the sample population, then the $ATE$ that is estimated for the sample population will *always* differ from the that of the target population in at least one scale of effect-measure, whether the difference scale or the ratio scale. Although it is possible to remove this bias by conditioning on an ancestor of the effect-modifier such that selection is conditionally independent of the outcome within all levels outcome, we will show in Part 3 that such conditioning 
<!-- Furthermore it can be demonstrated that the absence of interaction on the additive scale implies the presence of multiplicative interaction for relative risks and likewise, the absence of multiplicative interaction for relative risks implies the presence of additive interaction. In other words, if both of the two exposures have an effect on the outcome, then there must be interaction on some scale [@greenland2009commentary; @lash2020]. -->
<!-- 2. **Stratum-specific selection bias after baseline**: this bias arises when the sample that is selected after baseline d differs from that in the target population ($ATE_{\text{target}}$). Representing this threat to external validity through an M-bias graph, as is traditionally done, can be helpful. It allows us to explore conditioning strategies to mitigate the threat to generalisation see: @mathur2023common. However, this approach does not capture general features of the problem. Failures to generalisation from sample features can arise even in experimental settings without confounding. Therefore, it is advisable to describe source-selection bias in its most general form, conceptualised as a challenge to generalisation, stemming from effect-modification in the target sample. This enables source-selection bias, rooted in *pre-treatment* sample selection, to be represented as a problem of effect-modification (interaction) in which the source population responds differently from the target population in at least one effect-measure scale [@hernán2017]. M-bias treatments of this generalisation problem can be restated without loss of generality [@bulbulia2023c]. As we shall discover in Part 3, effect-modification can be depicted using sequential causal diagrams, which in the presence of source-selection bias, brings focus on the structural aspects of the bias, which may occur even without confounding, and whose features are relative variables included in the model [@hernán2017; @bulbulia2023c]. -->
<!-- **Stratum-specific selection bias after baseline (attrition-bias)**: this bias arises from attrition or non-response that occurs after treatment, leading to a discrepancy between the ATE in the analytic population at baseline and at follow-up. It primarily concerns 'internal validity'. Response-selection bias often arises from conditioning on a collider, or a proxy for a collider, creating an open backdoor path linking the exposure and the outcome. If the baseline ATE equals the source ATE, response-selection bias will compromise the validity of the $ATE_{\text{source}}$. This issue, a matter of 'internal validity', is well elucidated in sequential causal diagrams (see Appendix 2, @hernán2004a; @bulbulia2023c) and in literature addressing this topic [@hernán2004a; @imai2008misunderstandings; @cole2010generalizing; @bareinboim2013general; @kern2016assessing; @westreich2017transportability; @dahabreh2021study; @hernán2023]. Unfortunately, post-treatment selection bias cannot be fully addressed by conditioning on a set of baseline covariates $L$ [@lu2022]. -->
<!-- #### 4. Source-population /Target-population mismatch in the presence of effect-modification** -->
<!-- Where the a source population differs from the target population in the distribution of variables that modify causal effect, there is no guarantee that the treatment effects that researchers obtain will not be those they seek. Before analysing the data, causal effect estimates must be clearly articulated in terms of an effect measure, such as risk ratio, $ATE_{A/A'} = \frac{E[Y(a)]}{E[Y(a')]}$ or risk difference, $ATE_{A,A'} = E[Y(a)] - E[Y(a')]$, and they must be referenced to a specific target population. When the study sample is drawn from a population (source population) that is not representative of the target population, the treatment effects will not generally apply to the broader target population (TATE), or might obtain at one scale but not another (e.g. the causal risk difference, but not the causal risk ratio), (see: @cole2010, @hernán2017). Mismatch between the source and target populations arise in the context of randomised studies (see: @imai2008misunderstandings). If there is no function that allows for the transportability of results, such that $ATE_{\text{target}} \approx f_S(ATE_{\text{source}}, S)$, where $S$ represents factors specific to the target population that enable translation of the ATE from the source to the target, then causal inference will not deliver its pre-specified effect estimates. This will be so even if there are no structural sources of bias, such as a randomised experiment. (See also: @cole2010generalizing; @bareinboim2013general; @kern2016assessing; @westreich2017transportability; @dahabreh2021study; @bulbulia2023c). -->

#### 1.3.3 Measurement error bias

<!-- Restriction bias (selection bias), emerges from the mismatch between the sample population from which units in the study are drawn and the target population of interest.  -->

Measurement error bias originates from a discrepancy between a
variable's true value and its observed or recorded value. These errors
can arise from instrument calibration problems, respondent misreporting,
coding errors, and other factors. Measurement error is almost
inevitable, and measurement error bias can significantly distort causal
inferences [@blackwell2017; @hernan2023; @vanderweele2012; @bulbulia2023e].

**Random measurement error** arises from fluctuations in the measurement process and does not consistently bias effect estimates in any one direction. Although random measurement errors can increase data variability and reduce statistical power, they typically do not introduce bias in estimates of causal effects when there are no true effects. However, they can lead to attenuated estimates of causal effects, systematically bias the estimate of true causal effects towards
the null [@hernán2009; @hernan2023; @vanderweele2012b].

**Systematic measurement error** error occurs when either the errors of the exposure and outcome are correlated, or when the exposure affect the errors of the outcome (or both). Such structural sources of error can lead to biased causal effect estimates by consistently overestimating or underestimating the true causal magnitudes [@vanderweele2012b; @hernán2009].

Sequential causal diagrams can be useful in assessing certain structural
sources of bias arising from measurement error [@hernán2009; @vanderweele2012b; @hernan2023; @bulbulia2023e]. This is partly because the flow of information from error terms in causal diagrams mirrors the flow of confounding as it needs to be assessed to validate causal inferences. However, it is important to note that measurement error bias cannot be entirely encapsulated by confounding bias. Thus, employing causal diagrams to clarify measurement error bias represents another non-standard application. Although we do not have space here to cover measurement error bias in detail, $\S 2.8$ describes a common scenario where causal diagrams do not adequately elucidate structural sources of bias.

<!-- It is important point is this: causal diagrams are tools with which to evaluate confounding in the causal pathways that link events in time. Measurement error perturbs the measurement of these events.  Through we must distinguish between the units in our study, the features and events that are measured on these units, and the errors of these measurements, all of which may change over time, some of which indeed must change for causality b. -->

<!-- Measurement error bias is best addressed by improving data quality. When -->

<!-- this is not feasible, sensitivity analyses are necessary to gauge the -->

<!-- effect of measurement error biases on causal inferences [@hernan2023; -->

<!-- @vanderweele2020b]. -->

<!-- Note that there is an inherent tension in addressing structural sources of -->

<!--     bias. Simple causal diagrams are needed, but these do not encompass -->

<!--     the complexities associated with measurement errors, necessitating -->

<!--     more intricate diagrams. Hernán and Robins recommend a two-step -->

<!--     approach where separate diagrams are used to address different -->

<!--     threats to valid causal inference [@hernan2023, p 120]. -->

#### 1.3.4 Model mis-specification bias

Human scientists predominantly employ parametric models for statistical
analysis. These models are characterised by user-defined functional
forms and distributional assumptions. However, our reliance on
parametric models carries the risk of biased inferences due to model
mis-specification. The negative effects of model mis-specification are
evident in three key areas:

1.  **Limitations in establishing causation:**  Even if a parametric fits the data well, 
    it may not accurately represent causal relationships, which require simulating counterfactual data.

2.  **Regularisation bias:** even when causal effects are formally identifiable, the actual relationships between variables may be more intricate or differ from those assumed in one's statistical model. Parametric models, in particular, are susceptible to bias when capturing the inherent complexity of real-world phenomena [@wager2018; @vansteelandt2022].

3.  **Overstated precision:** a mis-specified model may erroneously imply a higher degree of precision in its estimates than warranted. This can occur through inaccurate estimation of parameter values or their standard errors, leading to misguided confidence in the results. [@díaz2021; @vansteelandt2022].

Recent developments in cross-validated non-parametric inference that
rely on machine learning offer promise for addressing threats to valid
inference from model mis-specification [@vanderlaan2011; @athey2019;
@díaz2021; @vanderlaan2018; @hahn2020; @künzel2019; @wager2018;
@williams2021]. Doubly robust versions of these estimation methods,
which model both the exposure and the outcome, are particularly
attractive because they can provide valid causal effect estimates even
if only one of the two models is correctly specified [@vanderlaan2011].
Presently however state-of-the-art machine learning techniques only
offer convergence guarantees for large samples [@vansteelandt2022].
These promising methods remain under active development. For the present
purposes, it is important that readers understand the risks for drawing invalid
conclusions because our statistical models are mis-specification
[@hoffman2022; @vansteelandt2022; @muñoz2012; @díaz2021; @williams2021;
@wager2018; @cui2020]. Causal diagrams serve as model-free, qualitative
tools for identifying structural biases but fall short in rectifying
model mis-specification and other biases. Employing them without a
rigorous, multi-stepped workflow for assumption-verification risks
drawing erroneous inferences, see discussion in @major2023exploring and
@auspurg2021has.


## Appendix 1: Causal Inference in History: The Difficulty In Satisfying the Three Fundamental Assumptions

Consider the Protestant Reformation of the 16$^{th}$ century, which
initiated religious change throughout much of Europe. Historians have
argued that Protestantism caused social, cultural, and economic changes
in those societies where it took hold; see: @weber1905; @weber1993;
@swanson1967; @swanson1971; @basten2013, and for an overview see:
@becker2016.

Suppose we were interested in estimating the 'Average Treatment Effect'
of the Protestant Reformation. Let $A = a^*$ denote the adoption of
Protestantism. We compare this effect with that of remaining Catholic,
represented as $A = a$. We assume that both the concepts of 'adopting
Protestantism' and of 'economic development' are well-defined (e.g. GDP
+1 century after a country has a Protestant majority contrasted with
remaining Catholic). The causal effect for any individual country is
$Y_i(a^*) - Y_i(a)$. Although we cannot identify this effect, if the
basic assumptions of causal inference are met, we can estimate the
average or marginal effect conditioning the confounding effects of $L$
gives us,

$$ATE_{\textnormal{economic~development}} = \mathbb{E}[Y(\textnormal{Became~Protestant}|L) - Y(\textnormal{Remained~Catholic}|L)]$$

When asking causal questions about the economic effect of adopting
Protestantism versus remaining Catholic, there are several challenges
that arise in relation to the three fundamental assumptions required for
causal inference.

**Causal Consistency**: requires the outcome under each level of
exposure is well-defined. In this context, defining what 'adopting
Protestantism' and 'remaining Catholic' mean may present challenges. The
practices and beliefs associated with each religion might vary
significantly across countries and time periods, and it may be difficult
to create a consistent, well-defined exposure. Furthermore, the
outcome - economic development - may also be challenging to measure
consistently across different countries and time periods.

There is undoubtedly considerable heterogeneity in the 'Protestant
exposure.' In England, Protestantism was closely tied to the monarchy
[@collinson2003]. In Germany, Martin Luther's teachings emphasised
individual faith in scripture, which, it has been claimed, supported
economic development by promoting literacy [@gawthrop1984]. In England,
King Henry VIII abolished Catholicism [@collinson2003]. The Reformation,
then, occurred differently in different places. The exposure needs to be
better-defined.

There is also ample scope for interference: 16th century societies were
interconnected through trade, diplomacy, and warfare. Thus, the
religious decisions of one society were unlikely to have been
independent from those of other societies.

**Exchangeability**: requires that given the confounders, the potential
outcomes are independent of the treatment assignment. It might be
difficult to account for all possible confounders in this context. For
example, historical, political, social, and geographical factors could
influence both a country's religious affiliations and its economic
development.

**Positivity**: requires that there is a non-zero probability of every
level of exposure for every strata of confounders. If we consider
various confounding factors such as geographical location, historical
events, or political circumstances, some countries might only ever have
the possibility of either remaining Catholic or becoming Protestant, but
not both. For example, it is unclear under which conditions 16th century
Spain could have been randomly assigned to Protestantism [@nalle1987].

Perhaps a more credible measure of effect in the region of our interests
is the Average Treatment Effect in the Treated (ATT) expressed:

$$ATT_{\textnormal{economic~development}} = \mathbb{E}[(Y(a*)- Y(a)|A = a*,L]$$

Where $Y(a*)$ represents the potential outcome if treated. $Y(a)$
represents the potential outcome if not treated. The expectation is
taken over the distribution of the treated units (i.e., those for whom
$A = a*$). $L$ is a set of covariates on which we condition to ensure
that the potential outcomes $Y(a*)$ and $Y(a)$ are independent of the
treatment assignment $A$, given $L$. This accounts for any confounding
factors that might bias the estimate of the treatment effect.

Here, the ATT defines the expected difference in economic success for
cultures that became Protestant compared with the expected economic
success if those cultures had not become Protestant, conditional on
measured confounders $L$, among the exposed ($A = a^*$). To estimate
this contrast, our models would need to match Protestant cultures with
comparable Catholic cultures effectively. By estimating the ATT, we
would avoid the assumption of non-deterministic positivity for the
untreated. However, whether matching is conceptually plausible remains
debatable. Ostensibly, it would seem that assigning a religion to a
culture is not as easy as administering a pill [@watts2018].

{{< pagebreak >}}

## Appendix 2: Sketch of Three-Wave Panel Design for Obtaining a Marginal Incident-Exposure Effect

Causal diagrams point the need for obtaining clearly defined time-series
data. How might we do it? Here, I sketch the outlines of a design for a
three-wave panel study that intends to estimate an *incident exposure
effect*. I do not intend this advice to be more than a sketch. However,
I believe it is important to give readers a concrete example for how
data collection for causal inference might occur.


### Step 1. Ask a causal question

In a three-wave panel design, ensuring the relative timing of events
essential for valid causal inference [@vanderweele2020].

Here is a causal question:

What is the causal effect of attending weekly religious services
compared to not attending services on charitable giving in the
population of New Zealanders who identify as Christian?

To answer this question we must assess how changes in religious service
attendance, measured from the beginning of the year (baseline) to
mid-year (wave 1), affect levels of charitable giving at the end of the
year (wave 2) In this design, the change in religious service attendance
is captured between the first and second waves, while the outcome,
charitable giving, is measured in the third wave. This establishes a
sequential order that mirrors the cause-and-effect relationship.
Ensuring such temporal ordering is crucial in any causal analysis. Note
additionally that we must obtain comparisons from continuous data for a
binary data. Depending on the data, such a contrast might not be well
supported. For example, change between these levels might occur only
rarely, in which case our inference might rely too heavily on parametric
model specifications. Focusing on the estimand:

#### Exposure:

-   A = 0: Attends less than once per month
-   A = 1: Attends weekly

#### Outcome:

-   Focus: One-year effect of shifting from A = 0 to A = 1.
-   Charitable giving as measured by self-reported giving

#### Scale of contrast:

-   ATE on the causal difference scale (per protocol).

#### Target population:

-   Individuals in New Zealand who might attend religious service and
    identify as Christian.

#### Source population:

-   National probability sample of New Zealanders (N = 34,000).

#### Baseline population:

-   Defined by eligibility criteria (including religious affiliation). If the baseline population differs from the target population, if sample weights for the distribution of covariates are available for the *target population*, these should be applied to the baseline population [although with caution, given potential for model mis-specification, see @stuart2015.]

Let $\widehat{ATE}_{target}$ denote the population average treatment
effect for the target population. Let $\widehat{ATE}_{\text{restricted}}$ denote the average treatment effect at the end of treatment. Let $W$ denote a set of variables upon which the restricted and target populations structurally differ. We say that
results *generalise* if we can guarantee that:

$$
\widehat{ATE}_{target} =  \widehat{ATE}_{restricted} 
$$

or if there is a known function such that:

$$
ATE_{target}\approx  f_W(ATE_{\text{restricted}}, W)
$$

In most cases, $f_W$ will be unknown, as it must account for potential
heterogeneity of effects and unobserved sources of bias. For further
discussion on this topic, see: @imai2008misunderstandings;
@cole2010generalizing; @stuart2018generalizability; @bulbulia2023c, and
$\S 3.1.6$

### 2. Ensure that the exposure is measured at wave 0 (baseline) and wave 1 (the exposure interval)

Measuring the exposure at both baseline (wave 0) and the exposure
interval (wave 1) has the following benefits:

1.  **Enables estimation of incident exposure effect**: by
    including baseline observations, we can distinguish between
    incidence (new occurrences) and prevalence (existing states)
    exposure effects. For instance, in a study on religious service
    attendance, assessing the incident exposure effect allows us to
    differentiate the effect of starting to attend services regularly
    from the effect of ongoing attendance.

2.  **Confounding control**: measuring the exposure at baseline helps
    control for time-invariant confounders. These are factors that do
    not change over time and might affect both the exposure and outcome.
    In the context of religious service attendance, personal attributes
    like inherent religiosity could influence both attendance and
    related outcomes.

3.  **Sample adequacy**: for rare exposures, baseline measurements can
    assess sample size adequacy. If a change in exposure is infrequent
    (e.g., infrequent to weekly religious service attendance), a larger
    sample may be needed to satisfy the positivity assumption and detect
    causal effects. By measuring the exposure at baseline, we can better
    evaluate whether our sample is representative and large enough to
    detect such rare changes.

### 3. Ensure that the outcome is measured at wave 0 (baseline) and wave 2 (post-exposure wave 1)

Measuring the outcome at both wave 0 (baseline) and the post-exposure
outcome wave (wave 2) offers the following advantages:

1.  **Temporal ordering**: causes precede effects. We need this to avoid
    *causal incoherence*. For example, ensuring order protects us from
    inadvertently estimating $Y\rightarrowred A$.

2.  **Confounding control**: including the baseline measure of both the
    exposure and outcome allows for better control of confounding. This
    approach helps to isolate the effect of the exposure on the outcome
    from the exposure wave (wave 1) to the outcome wave (wave 2),
    independent of their baseline levels. It reduces the risk of
    confounding, where unmeasured factors might influence both the
    exposure and the outcome, as shown in @fig-dag-1.

### 4. Measure observable common causes of the exposure and outcome

Next, we must identify and record at wave 0 (baseline) all potential
confounders that could influence both the exposure (e.g., frequency of
attending religious services) and the outcome (e.g., charitable giving).
Proper identification and adjustment for these confounders are crucial
for accurate causal inference. By obtaining measures of the confounders
at baseline we:

a.  **Minimise mediation bias**: by measuring confounders at baseline,
    it will be difficult to produce the *causally incoherent* model:
    $A_1\to \boxed{L_2} \rightarrowdotted Y_3$

b.  **Minimise collider bias**: by measuring confounders at baseline, it
    will be difficult to produce the *causally incoherent* model:
    $A_1\rightarrowred L_3 \leftarrowred Y_2$.

The topic of measurement construction is vast. For now, it is worth
noting that measures should be obtained in consultation with locals and
domain experts [@vanderweele2022].

### 5. Gather data for proxy variables of unmeasured common causes at the baseline wave

If any unmeasured confounders influence both the exposure and outcome,
but we lack direct measurements, we should make efforts to include
proxies for them at baseline. Even if this strategy cannot eliminate all
bias from unmeasured confounding, it will generally reduce bias.

###  6. Retain sample

Censoring leads to bias. Strategies for sample retention are essential.

a.  **Developing tracking protocols**: establish robust systems for
    tracking participants over the study period. This involves keeping
    updated records of contact information such as addresses, emails,
    phone numbers, and names, and accounting for changes in name over
    time.

b.  **Motivate retention**: implement strategies to encourage ongoing
    participation. These incentives should ideally not lead to bias in
    the distribution of effect-modifiers that might affect the outcome
    of interest. For example, retention should not appeal to trust in
    science if trust in science is the outcome of interest.

c.  **Investigators should avoid acting in ways that lead to
    differential retention**: for example, stay out of the news.

```{tikz}
#| label: fig-dag-1
#| fig-cap: "Three-wave panel design with selection-restriction bias. Where the exposure affects attrition, and an unmeasured confounder (U_C=1) may affect both attrition and the outcome, there is scope for restriction bias from censoring. Even if the exposure does not affect censoring, if U_C=1 leads to informative censoring, the marginal effect in the censored may differ from the marginal effect in the uncensored (Section 3.1.6). Note we seek inference on the distribution of Y over C=0,1 (the target population.) Post-treatment selection bias cannot be corrected by conditioning on baseline co-variates. The best strategy is to minimise attrition and non-response. However, because attrition is nearly inevitable, we apply correction methods such as censoring weighting or multiple imputation. These methods introduce scope for bias from model mis-specification."
#| out-width: 100%
#| echo: false
#| include: true
#| eval: true

\usetikzlibrary{positioning}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations}
\tikzstyle{Arrow} = [->, thin, preaction = {decorate}]
\tikzset{>=latex}
% Define a simple decoration
\tikzstyle{cor} = [-, dotted, preaction = {decorate}]

\begin{tikzpicture}[{every node/.append style}=draw]

\node [rectangle, draw=white] (U) at (0, 0) {U};
\node [rectangle, draw=black, align=left] (L) at (2, 0) {L$_{0}$ \\A$_{0}$ \\Y$_{0}$};
\node [rectangle, draw=white] (A) at (4, 0) {A$_{1}$};
\node [ellipse, draw=white] (US) at (4, -1) {U$_{\text{C=1}}$};
\node [font=\tiny, rectangle, draw=blue, thick](C) at (6, 0) {$\Pr(Z=z|C=1)$};
\node [ellipse, draw=white] (Y) at (8, 0) {Y$_{2}^{\text{C}}$};

\draw [-latex, draw=black] (U) to (L);
\draw [-latex, draw=black] (L) to (A);
\draw [-latex, bend left=50, draw=black] (L) to (Y);
\draw [-latex, bend right=50, draw=red, dotted] (U) to (Y);
\draw [-latex, bend left=50, draw=red, dotted] (U) to (A);
\draw [-latex, draw=red] (A) to (C);
\draw [-latex, draw=red,bend right =02] (US) to (C);
\draw [-latex, draw=blue,bend right =03] (US) to (C);
\draw [-latex, draw=red,bend right =02] (US) to (Y);
\draw [-latex, draw=blue, bend right =03] (US) to (Y);




\end{tikzpicture}


```

Although this article does not discuss estimation, it is worth
mentioning that we do not require a structural equation model or a
multi-level model to handle the repeated measures. We may estimate this
model using ordinary least squares. For example, in R we could write the
three-wave model that controls for baseline confounders, exposure, and
outcome as follows:

```{r}
#| out-width: 100%
#| echo: true
#| include: true
#| eval: false

# load libraries
library(tidyverse)

# set a seed for reproducibility
set.seed(1234)
# number of observations
n <- 100
# simulating data
# baseline religious service attendance (binary)
a0 <- rbinom(n, 1, 0.5)
# wave 1 religious service attendance (binary)
a1 <- rbinom(n, 1, 0.5)
# confounder (continuous)
l0 <- rnorm(n, 0, 1)
# baseline donations (continuous)
y0 <- 2 * a0 + l0 + rnorm(n, 0, 1)
# wave 2 donations (continuous)
y2 <- 3 * a1 + 2 * a0 + l0 + y0 + rnorm(n, 0, 1)

# create a dataframe
data <- tibble(a0 = a0, 
               a1 = a1, 
               l0 = l0, 
               y0 = y0, 
               y2 = y2)

# step 1 fit a linear model to data
#  model the outcome variable y at wave 2 (y2) as a function of:
# - religious service attendance at wave 1 (a1) and baseline (a0),
# - outcome at baseline (y0), and
# - a confounder measured at baseline (l0).

model <- lm( 
  y2 ~ a1 + a0 + y0 + l0, 
  data = data
  )

# step 2: predict the outcome (y2) assuming no religious service attendance at wave 1 (a1 = 0).
# keeping other vars at their measured levels

pred_a0 <- predict(
  model, newdata = data |> mutate(a1 = 0), se.fit = TRUE
  )

# step 3: predict the outcome (y2) assuming full religious service attendance at wave 1 (a1 = 1).

pred_a1 <- predict(
  model, newdata = data |> mutate(a1 = 1), se.fit = TRUE
  )

# step 4. calculate the causal contrast (the average treatment effect).
# this is the difference in predicted outcomes between attending and not attending
causal_contrast <- mean(pred_a1$fit) - mean(pred_a0$fit)

# print causal contrast
causal_contrast

# step 5: compute variance and standard error of the causal contrast.
# a. determine the sample size (N).
n <- length(pred_a1$fit)

# b. calculate the variance of the causal contrast using Delta method.
var_contrast <- (var(pred_a1$fit) + var(pred_a0$fit)) / n


# c. calculate the stand error for the mean difference.
se_contrast <- sqrt(var_contrast)

# d. output standard error.
print(se_contrast)
```

The method just described is called 'parametric g-computation' or (confusingly) 'regression standardisation.' Note: 'standardisation' has a completely different meaning in many human sciences. Here, we did not z-transform variables in our data. We simulated average causal effects by setting coefficients to one and another level of treatment and predicting outcomes for each condition across the entire population. There are other methods for simulating potential outcomes,
such as inverse probability treatment weighting, and more sophisticated
doubly robust estimators, including those that rely on machine learning
described in our discussion of model mis-specification $\S 1.3.4$. Here,
we simulate the counterfactual contrast using ordinary least squares. In
our simple three-wave model we recover an incident-exposure effect, see:
@hernán2016b; @hoffman2022; @hernán2008a; @bulbulia2022 @vanderweele2020.
Note the `clarify` package compute treatment effects with standard
errors, passing `model` to its functions [@greifer2023].

{{< pagebreak >}}

## Appendix 3: Counterfactual Reality

A distinctive feature of causal data science is the premise that
potential outcomes, $Y_i(1)$ and $Y_i(0)$, although never jointly
realised, may be assumed to be real and to exist independently of data
collection [@hernan2023, p.6]. In causal data science, we encounter a
unique missing data problem: the 'full data' necessary to compute any
causal contrast are missing at least half of their values [@ogburn2021;
@westreich2015; @edwards2015]. This challenge differs from typical
missing data scenarios, where the data could have been recorded but were
not. The missing information crucial for computing causal contrasts is
inherently linked to the irreversible nature of time. Sequentially
ordered causal diagrams encode assumptions about this structure.

There are implications for notation. As Hernán and Robins point out:
"Sometimes we abbreviate the expression 'individual $i$ has outcome
$Y^a = 1'$ by writing $Y^a_i = 1$. Technically, when $i$ refers to a
specific individual, such as Zeus, $Y^a_i$ is not a random variable
because we are assuming that individual counterfactual outcomes are
deterministic. Causal effect for individual $i$:
$Y^{a=1}_i \neq Y^{a=0}_i$" [@hernan2023, p.6].

As the Hernán and Robins quotation illustrates, there are different
conventions for denoting potential (counterfactual) outcomes. The following notations are
equivalent: $Y^a$, $Y_a$, and $Y(a)$.

{{< pagebreak >}}



## Appendix 4: Explanation for the Difference in Marginal Effects between Censored and Uncensored Populations

#### Definitions:

- **$A$**: Exposure variable
- **$Y$**: Outcome variable
- **$Z$**: Effect modifier
- **$C$**: Represents the uncensored population
- **$C=1$**: Represents the censored population

#### Average treatment effects for the uncensored and censored populations defined as:
$$
\Delta_{\text{uncensored}} = \mathbb{E}[Y(a^*) - Y(a) | C]
$$
$$
\Delta_{\text{censored}} = \mathbb{E}[Y(a^*) - Y(a) | C=1]
$$

#### By Causal Consistency, potential outcomes from observations given as:
$$
\Delta_{\text{uncensored}} = \mathbb{E}[Y|A=a^*,C] - \mathbb{E}[Y|A=a,C]
$$
$$
\Delta_{\text{censored}} = \mathbb{E}[Y|A=a^*,C=1] - \mathbb{E}[Y|A=a,C=1]
$$

#### By the Law of Total Probability, Average Treatment Effects weighted by $\Pr(Z|C)$ given as:

$$
\Delta_{\text{uncensored}} = \sum_{z} \bigg\{\mathbb{E}[Y|A=a^*,Z=z,C] - \mathbb{E}[Y|A=a,Z=z,C]\bigg\}\times \Pr(Z=z|C)
$$
$$
\Delta_{\text{censored}} = \sum_{z} \bigg\{\mathbb{E}[Y|A=a^*,Z=z,C=1] - \mathbb{E}[Y|A=a,Z=z,C=1]\bigg\} \times \Pr(Z=z|C=1)
$$

#### Assume informative censoring 

We assume the effect modifier $Z$ has a different distribution in the censored and uncensored populations:

$$\Pr(Z=z|C) \neq \Pr(Z=z|C=1)$$

Under this assumption, the probability weights used to calculate the marginal effects for the uncensored and censored populations differ.

#### Effect estimates for censored and uncensored will not be the same on at least one effect measurement scale

Given that $\Pr(Z=z|C) \neq \Pr(Z=z|C=1)$, we cannot guarantee that:

$$\Delta_{\text{uncensored}} = \Delta_{\text{censored}}$$

Indeed the equality of the marginal effects between the two populations will only hold if there is a universal null effect across all units, by chance, or under specific conditions discussed by @vanderweele2007 and further elucidated by @suzuki2013counterfactual, otherwise: 


$$\Delta_{\text{uncensored}} \ne \Delta_{\text{censored}}$$


Futhermore, @vanderweele2012 proved that if there is effect modification of $A$ by $Z$, there will be a difference in at least one scale of causal contrast, such that 

$$\Delta^{\text{risk ratio}}_{\text{uncensored }}  \ne \Delta^{\text{risk ratio}}_{\text{censored}}$$ 

or

$$\Delta^{\text{difference}}_{\text{uncensored }}  \ne \Delta^{\text{difference}}_{\text{censored}}$$ 


For comprehensive discussions on sampling and inference, refer to @dahabreh2019 and @dahabreh2021study.