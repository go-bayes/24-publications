% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames,x11names}{xcolor}
%
\documentclass[
  single column]{article}

\usepackage{amsmath,amssymb}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage[]{libertinus}
\ifPDFTeX\else  
    % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\usepackage[top=30mm,left=25mm,heightrounded,headsep=22pt,headheight=11pt,footskip=33pt,ignorehead,ignorefoot]{geometry}
\setlength{\emergencystretch}{3em} % prevent overfull lines
\setcounter{secnumdepth}{-\maxdimen} % remove section numbering
% Make \paragraph and \subparagraph free-standing
\makeatletter
\ifx\paragraph\undefined\else
  \let\oldparagraph\paragraph
  \renewcommand{\paragraph}{
    \@ifstar
      \xxxParagraphStar
      \xxxParagraphNoStar
  }
  \newcommand{\xxxParagraphStar}[1]{\oldparagraph*{#1}\mbox{}}
  \newcommand{\xxxParagraphNoStar}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
  \let\oldsubparagraph\subparagraph
  \renewcommand{\subparagraph}{
    \@ifstar
      \xxxSubParagraphStar
      \xxxSubParagraphNoStar
  }
  \newcommand{\xxxSubParagraphStar}[1]{\oldsubparagraph*{#1}\mbox{}}
  \newcommand{\xxxSubParagraphNoStar}[1]{\oldsubparagraph{#1}\mbox{}}
\fi
\makeatother


\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
% definitions for citeproc citations
\NewDocumentCommand\citeproctext{}{}
\NewDocumentCommand\citeproc{mm}{%
  \begingroup\def\citeproctext{#2}\cite{#1}\endgroup}
\makeatletter
 % allow citations to break across lines
 \let\@cite@ofmt\@firstofone
 % avoid brackets around text for \cite:
 \def\@biblabel#1{}
 \def\@cite#1#2{{#1\if@tempswa , #2\fi}}
\makeatother
\newlength{\cslhangindent}
\setlength{\cslhangindent}{1.5em}
\newlength{\csllabelwidth}
\setlength{\csllabelwidth}{3em}
\newenvironment{CSLReferences}[2] % #1 hanging-indent, #2 entry-spacing
 {\begin{list}{}{%
  \setlength{\itemindent}{0pt}
  \setlength{\leftmargin}{0pt}
  \setlength{\parsep}{0pt}
  % turn on hanging indent if param 1 is 1
  \ifodd #1
   \setlength{\leftmargin}{\cslhangindent}
   \setlength{\itemindent}{-1\cslhangindent}
  \fi
  % set entry spacing
  \setlength{\itemsep}{#2\baselineskip}}}
 {\end{list}}
\usepackage{calc}
\newcommand{\CSLBlock}[1]{\hfill\break\parbox[t]{\linewidth}{\strut\ignorespaces#1\strut}}
\newcommand{\CSLLeftMargin}[1]{\parbox[t]{\csllabelwidth}{\strut#1\strut}}
\newcommand{\CSLRightInline}[1]{\parbox[t]{\linewidth - \csllabelwidth}{\strut#1\strut}}
\newcommand{\CSLIndent}[1]{\hspace{\cslhangindent}#1}

\usepackage{booktabs}
\usepackage{longtable}
\usepackage{array}
\usepackage{multirow}
\usepackage{wrapfig}
\usepackage{float}
\usepackage{colortbl}
\usepackage{pdflscape}
\usepackage{tabu}
\usepackage{threeparttable}
\usepackage{threeparttablex}
\usepackage[normalem]{ulem}
\usepackage{makecell}
\usepackage{xcolor}
\input{/Users/joseph/GIT/latex/latex-for-quarto.tex}
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\AtBeginDocument{%
\ifdefined\contentsname
  \renewcommand*\contentsname{Table of contents}
\else
  \newcommand\contentsname{Table of contents}
\fi
\ifdefined\listfigurename
  \renewcommand*\listfigurename{List of Figures}
\else
  \newcommand\listfigurename{List of Figures}
\fi
\ifdefined\listtablename
  \renewcommand*\listtablename{List of Tables}
\else
  \newcommand\listtablename{List of Tables}
\fi
\ifdefined\figurename
  \renewcommand*\figurename{Figure}
\else
  \newcommand\figurename{Figure}
\fi
\ifdefined\tablename
  \renewcommand*\tablename{Table}
\else
  \newcommand\tablename{Table}
\fi
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Listing}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}
\makeatother
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\@ifpackageloaded{subcaption}{}{\usepackage{subcaption}}
\makeatother
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\usepackage{bookmark}

\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same} % disable monospaced font for URLs
\hypersetup{
  pdftitle={Causal Directed Acyclic Graphs (DAGs): A Practical Guide},
  pdfauthor={Joseph A. Bulbulia},
  colorlinks=true,
  linkcolor={blue},
  filecolor={Maroon},
  citecolor={Blue},
  urlcolor={Blue},
  pdfcreator={LaTeX via pandoc}}

\title{Causal Directed Acyclic Graphs (DAGs): A Practical Guide}

\usepackage{academicons}
\usepackage{xcolor}

  \author{Joseph A. Bulbulia}
            \affil{%
             \small{     Victoria University of Wellington, New Zealand
          ORCID \textcolor[HTML]{A6CE39}{\aiOrcid} ~0000-0002-5861-2056 }
              }
      


\date{2024-05-15}
\begin{document}
\maketitle
\begin{abstract}
Causal inference requires contrasting counterfactual states of the world
under pre-specified interventions. Obtaining counterfactual contrasts
from data, in turn, relies on a framework of explicit assumptions and
careful, multi-step workflows. Causal diagrams are powerful tools for
clarifying whether and how the counterfactual contrasts we seek may be
identified from data. Here, I explain how to use causal directed acyclic
graphs (causal DAGs) to clarify whether and how causal effects may be
identified from observational data. Along the way, I offer practical
tips for reporting and suggestions for avoiding common pitfalls.

\textbf{KEYWORDS}: \emph{Causal Inference}; \emph{Culture};
\emph{DAGs};* \emph{Evolution}; \emph{Human Sciences};
\emph{Longitudinal}.
\end{abstract}

\subsection{Introduction}\label{introduction}

Human research begins with two fundamental questions:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  What do I want to know?
\item
  For which population does this knowledge generalise?
\end{enumerate}

In the human sciences, our questions are typically causal. We aim to
understand the effects of interventions on certain variables. However,
many researchers report non-causal associations, collecting data,
applying complex regressions, and reporting coefficients. We often speak
of covariates as ``predicting'' outcomes. Yet, even when our models
predict well, it remains unclear how these predictions relate to the
scientific questions that sparked our interest. We fail to recognise
that these ``oracles'' lack coherent interpretation.

Some say that association cannot imply causation. However, our
experimental traditions reveal that when interventions are controlled
and randomised, the coefficients we recover from statistical models can
permit causal interpretations.

Despite familiarity with experiments, many researchers struggle to
emulate randomisation and control with non-experimental or
``real-world'' data. Though many use terms like ``control'' and employ
sophisticated adjustment strategies---such as multilevel modelling and
structural equation models---these practices are not systematic. We
often overlook that what we take as control can undermine our ability to
consistently estimate causal effects. Although the term ``crisis'' is
overused, the state of causal inference across many human sciences,
including experimental sciences, has much headroom for improvement.
Moreover, poor experimental designs unintentially weaken causal claims;
causal inferences in experiments is also deserving of greater attention.
Fortuantely, recent decades have seen progress in the the health
sciences, economics, and computer science have markedly improved our
ability to obtain causal inferences in scientific research. This work
demonstrates that obtaining causal inferences from data is feasible but
requires careful, systematic workflows.

Within the workflows of causal inference, causal diagrams---or causal
graphs---are powerful tools for evaluating whether and how causal
effects can be identified from data. My purpose here is to explain where
these tools fit within causal inference workflows and to illustrate
their practical applications. I focus on causal directed acyclic graphs
(causal DAGs) because they are relatively easy to use and optimally
clear for most applications. However, causal directed acyclic graphs can
also be misused. I will also explain common pitfalls and how to avoid
them.

In Part 1, we review the conceptual foundations of causal inference. The
basis of all causal inference lies in counterfactual contrasts. Although
there are different philosophical approaches to counterfactual
reasoning, they are largely similar in practice. The overview here
builds on the Neyman-Rubin potential outcomes framework, extended for
longitudinal data by epidemiologist James Robins. Although this is not
the framework within which causal directed acyclic graphs were
developed, the potential outcomes framework is easier to interpret. (I
discuss Pearl's non-parametric structural equation approach in Appendix
E).

In Part 2, we describe how causal directed acyclic graphs help identify
causal effects. We outline five elementary graphical structures that
encode all causal relations, forming the building blocks of all causal
directed acyclic graphs. We then examine five rules that clarify whether
and how investigators can identify causal effects from data.

In Part 3, we apply causal directed acyclic graphs to practical
problems, showing how repeated measures data collection can solve seven
common identification issues. Timing is critical, but not sufficient
alone. We also use causal diagrams to highlight the limitations of
repeated-measures data collection for identifying causal effects,
tempering enthusiasm for easy solutions. Indeed we will review how many
statistical structural equation models and sophisticated multi-level
models are not well-calibrated for identifying causal effects.

In Part 4, I offer practical suggestions for creating and reporting
causal directed acyclic graphs in scientific research. These graphs
represent investigator assumptions about causal (or structural)
relationships in nature. These relationships cannot typically be derived
from data alone and must be developed with scientific specialists. Where
ambiguity or debate exists, investigators should report multiple causal
diagrams and conduct distinct analyses. I explain how to do this and
walk through the steps.

By understanding and applying causal directed acyclic graphs,
researchers can more effectively identify and communicate causal
relationships, in the service of the causal questions that animate our
scientific interets.

\subsection{Part 1: Causal Inference as Counterfactual Data
Science.}\label{part-1-causal-inference-as-counterfactual-data-science.}

The first step in answering a causal question is to ask it
(\citeproc{ref-hernuxe1n2016}{Hernán \emph{et al.} 2016}).

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  What do causal quantity to I want to consistently estimate?
\item
  For which population does this knowledge generalise?
\end{enumerate}

Causal diagrams come after we have stated a causal question and the
population of interest, the `target population'. We begin by considering
what is required to state these questions precisely.

\paragraph{1.1.1 The fundamental problem of causal
inference}\label{the-fundamental-problem-of-causal-inference}

To ask a causal question, we must consider the concept of causality
itself. Consider an intervention, \(A\), and its effect, \(Y\). We say
that \(A\) causes \(Y\) if altering \(A\) would lead to a change in
\(Y\) (\citeproc{ref-hume1902}{Hume 1902};
\citeproc{ref-lewis1973}{Lewis 1973}). If altering \(A\) would not
change \(Y\), we say that \(A\) has no causal effect on \(Y\).

In causal inference, we aim to quantitatively contrast the potential
outcomes of \(Y\) in response to different levels of a well-defined
intervention. Commonly, we refer to such interventions as `exposures' or
`treatments;' we refer to the possible effects of interventions as
`potential outcomes.'

Consider a binary treatment variable \(A \in \{0,1\}\). For each unit
\(i\) in the set \(\{1, 2, \ldots, n\}\), when \(A_i\) is set to 0, the
potential outcome under this condition is denoted, \(Y_i(0)\).
Conversely, when \(A_i\) is set to 1, the potential outcome is denoted,
\(Y_i(1)\). We refer to the terms \(Y_i(1)\) and \(Y_i(0)\) as
`potential outcomes' because until realised, the effects of
interventions describe counterfactual states.

Suppose that each unit \(i\) receives either \(A_i = 1\) or \(A_i = 0\).
The corresponding outcomes are realised as \(Y_i|A_i = 1\) or
\(Y_i|A_i = 0\). For now, let us assume that each realised outcome under
that intervention is equivalent to one of the potential outcomes
required for a quantitative causal contrast, such that
\([(Y_i(a)|A_i = a)] = (Y_i|A_i = a)\). Thus when \(A_i = 1\),
\(Y_i(1)|A_i = 1\) is observed. However, when \(A_i = 1\), it follows
that \(Y_i(0)|A_i = 1\) is not observed:

\[
Y_i|A_i = 1 \implies Y_i(0)|A_i = 1~ \text{is counterfactual}
\]

Conversely, if \(A_i = 0\), we may assume the potential outcome
\(Y_i(0)|A_i = 0\)) is observed as \(Y_i|A_i = 0\). However, the
potential outcome \(Y_i(1)|A_i = 0\) is never realised and so not
observed:

\[
Y_i|A_i = 0 \implies Y_i(1)|A_i = 0~ \text{is counterfactual}
\]

We define \(\tau_i\) as the individual causal effect for unit \(i\) and
express the individual causal effect:

\[
\tau_i = Y_i(1) - Y_i(0)
\]

Notice that each unit can only be exposed to only one level of the
exposure \(A_i = a\) at a time. This implies that \(\tau_i\), is not
merely unobserved but inherently \emph{unobservable}.

That individual causal effects cannot be identified from observations is
known as `\emph{the fundamental problem of causal inference}'
(\citeproc{ref-holland1986}{Holland 1986};
\citeproc{ref-rubin1976}{Rubin 1976}).

\paragraph{1.1.2 Causal effects from randomised
experiments}\label{causal-effects-from-randomised-experiments}

Although it is not typically feasible to compute individual causal
effects. under certain assumptions, it may be possible to estimate
\emph{average} treatment effects -- also called `marginal effects.' We
define an average treatment effect (\(ATE\)) as the difference between
the expected or average outcomes under treatment and contrast conditions
for a pre-specified population, typically the population from which an
observed sample is drawn.

Consider a binary treatment, \(A\) \in \{0,1\}\$:

\[
\text{Average Treatment Effect}  = \mathbb{E}[Y(1)] - \mathbb{E}[Y(0)].
\]

This is our prespecified estimand for our target population. A challenge
remains for computing these treatment-group averages, given that
individual causal effects are unobservable. We can frame the problem
framed by referring to the \emph{full data} required to compute this
estimand --- that is in terms of the complete counterfactual dataset
where the missing potential outcomes, inherent in observational data,
were somehow available. The text highlighted in red denotes inherently
missing responses over the joint distribution of the full counterfactual
dataset (also stated by underbraces). We find that for each treatment
condition, half the observations over the joint distribution of the
counterfactual data are inherently unobserved.

\[
\text{Average Treatment Effect} = \left(\underbrace{\underbrace{\mathbb{E}[Y(1)|A = 1]}_{\text{observed for } A = 1} + \underbrace{\textcolor{red}{\mathbb{E}[Y(1)|A = 0]}}_{\textcolor{red}{\text{unobserved for } A = 0}}}_{\text{effect among treated}}\right) - \left(\underbrace{\underbrace{\mathbb{E}[Y(0)|A = 0]}_{\text{observed for } A = 0} + \underbrace{\textcolor{red}{\mathbb{E}[Y(0)|A = 1]}}_{\textcolor{red}{\text{unobserved for } A = 1}}}_{\text{effect among untreated}}\right).
\]

Randomisation allows investigators to recover the treatment group
averages even though treatment groups contain inherently missing
observations. We do not require the joint distribution over the
full-data (i.e.~the counterfactual data) to obtain average treatment
groups. This is because when investigators randomise units into
treatment conditions, there is full adherence, and the sample is
sufficiently large to rule out chance differences in the composition of
the treatment groups to be compared, the distributions of confounders
that could explain differences in the potential outcomes will be
balanced across the conditions. Randomisation under such conditions
rules out explanations for difference in treatment group average except
the treatment. Put differently randomisation implies:

\[
\widehat{\mathbb{E}}[Y(0) | A = 1] = \widehat{\mathbb{E}}[Y(0) | A = 0]
\]

and

\[
\widehat{\mathbb{E}}[Y(1) | A = 1] = \widehat{\mathbb{E}}[Y(1) | A = 0]
\]

We assume, (by causal consistency, see: \(\S 1.2.1\)):

\[\widehat{\mathbb{E}}[Y(1) | A = 1] = \widehat{\mathbb{E}}[Y| A = 1]\]

and

\[\widehat{\mathbb{E}}[Y(0) | A = 0] = {\mathbb{E}}[Y| A = 0]\]

It follows that the average treatment effect of the randomised
experiment can be computed:

\[
\text{The Average Treatment Effect} = {\mathbb{E}}[Y | A = 1] - \widehat{\mathbb{E}}[Y | A = 0].
\]

There are four critical aspects for how ideally randomised experiments
enable the estimation of average treatment effects worth highlighting.

First, the investigators must specify a population for whom they seek to
generalise their results. We refer to this population as the
\emph{target population}. If the study population differs from the
target population in the distribution of covariates that interact with
the treatment, the investigators will have no guarantees their results
will generalise (for discussions of sample/target population mismatch
refer to: Imai \emph{et al.}
(\citeproc{ref-imai2008misunderstandings}{2008}); Westreich \emph{et
al.} (\citeproc{ref-westreich2019target}{2019}); Westreich \emph{et al.}
(\citeproc{ref-westreich2017}{2017}); Pearl and Bareinboim
(\citeproc{ref-pearl2022}{2022}); Bareinboim and Pearl
(\citeproc{ref-bareinboim2013general}{2013}); Stuart \emph{et al.}
(\citeproc{ref-stuart2018generalizability}{2018}); Webster-Clark and
Breskin (\citeproc{ref-webster2021directed}{2021}).)

Second, because the units in the study sample at randomisation may
differ may differ from the units in the study after randomisation,
investigators must be careful to avoid biases that arise from
sample/population mismatch over time. If there is sample attrition or
non-response, the treatment effect investigators obtain for the sample
may differ from the treatment effect in the target population.

Third, a randomised experiment recovers the causal effect of random
treatment assignment, not of the treatment itself, which may differ if
some participants do not adhere to their treatment (even if they remain
in the study). The effect randomised assignment is called the
\emph{intent-to-treat effect}. The effect of perfect adherence is called
the \emph{per protocol effect} (\citeproc{ref-hernan2017per}{Hernán
\emph{et al.} 2017}; \citeproc{ref-lash2020}{Lash \emph{et al.} 2020}).
To obtain the per protocol effect for randomised experiments requires
the application of methods for causal inference in observational
settings.

Fourth, I have presented the average treatment effect on the difference
scale, that is, as a difference in average potential outcomes for the
target population under two distinct levels of treatment. However,
depending on the scientific question at hand, investigators may wish to
estimate causal effects on the risk-ratio scale, the rate-ratio scale
the hazard-ratio scale, or another scale. Where there are interactions
such that treatment effects vary across different strata of the
population, an estimate of the causal effect on the risk difference
scale will differ in at least one stratum to be compared from the
estimate on the risk ratio scale
(\citeproc{ref-greenland2003quantifying}{Greenland 2003}). The
sensitivity of treatment effects in the presence of interactions to the
scale of contrast underscores the importance of pre-specifying a scale
for the causal contrast investigators hope to obtain.

Fifth, investigators may uninentionally spoil randomisation by adjusting
for indicators that might be affected by the treatment, outcome, or
both, by excluding participants using attention checks, by collecting
covariate data that might be affected by the experimental conditions, by
failing to account for non-response and loss-to-follow up, and by
committing any number of other self-inflicted injuries. Unfortunately,
such practices are widespread (\citeproc{ref-montgomery2018}{Montgomery
\emph{et al.} 2018}). Notably causal graphical methods are useful for
describing causal identification in experiments (refer to Hernán
\emph{et al.} (\citeproc{ref-hernan2017per}{2017})), a topic we consider
elsewhere (\citeproc{ref-cite}{\textbf{cite?}})

Setting these considerations aside, understanding how randomisation
obtains the missing counterfactual outcomes that we require to
consistently estimate average treatment effects clarifies the tasks of
causal inference in non-experimental settings
(\citeproc{ref-hernuxe1n2008a}{Hernán \emph{et al.} 2008};
\citeproc{ref-hernuxe1n2022}{Hernán \emph{et al.} 2022};
\citeproc{ref-hernuxe1n2006}{Hernán and Robins 2006}).

\subsubsection{1.2 Fundamental Identification
Assumptions}\label{fundamental-identification-assumptions}

There are three fundamental identification assumptions that must be
satisfied to consistently estimate causal effects with data. These
assumptions are typically satisfied in randomised controlled
trials/experiments but not in real-world studies in which randomised
treatment assignment is absent.

\paragraph{1.2.1 Assumption 1: Causal
Consistency}\label{assumption-1-causal-consistency}

We satisfy the causal consistency assumption when, for each unit \(i\)
in the set \(\{1, 2, \ldots, n\}\), the observed outcome corresponds to
one of the specific counterfactual outcomes to be compared such that:

\[
Y_i^{observed}|A_i = 
\begin{cases} 
Y_i(a^*) & \text{if } A_i = a^* \\
Y_i(a) & \text{if } A_i = a
\end{cases}
\]

The causal consistency assumption implies that the observed outcome at a
specific exposure level equates to the counterfactual outcome for that
individual at the observed exposure level. Although it seems
straightforward to equate an individual's observed outcome with their
counterfactual outcome, treatment conditions vary, and treatment
heterogeneity poses considerable challenges for satisfying this
assumption. See: \textbf{Appendix A}

\paragraph{1.2.2 Assumption 2:
Positivity}\label{assumption-2-positivity}

We satisfy the positivity assumption if there is a non-zero probability
of receiving each treatment level for every combination of covariates
that occurs in the population. Where \(A\) is the exposure and \(L\) is
a vector of covariates, we say positivity is achieved if:

\[
0 < Pr(A = a | L = l) < 1, \quad \text{for all } a, l \text{ with } Pr(L = l) > 0
\]

There are two types of positivity violation:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Random non-positivity} occurs when an exposure is
  theoretically possible, but specific exposure levels are not
  represented in the data. Notably, random non-positivity is the only
  identifiability assumption verifiable with data.
\item
  \textbf{Deterministic non-positivity} occurs when the exposure is
  implausible by nature. For instance, a hysterectomy in biological
  males would appear biologically implausible.
\end{enumerate}

Satisfying the positivity assumption can present considerable data
challenges (\citeproc{ref-westreich2010}{Westreich and Cole 2010}).
Suppose we had access to extensive panel data that has tracked 20,000
individuals randomly sampled from the target population over three
years. Suppose further that we wanted to estimate a one-year causal
effect of weekly religious service attendance on charitable donations.
We control for baseline attendance to recover an incident exposure
effect estimate (see: \(\S 2.3\) and \textbf{Appendix 2}). Assume that
the natural transition rate from no religious service attendance to
weekly service attendance is low, say one in a thousand annually. In
that case, the effective sample for the treatment condition dwindles to
20. This example clarifies the problem. For rare exposures, the data
required for valid causal contrasts may be sparse, even in large
datasets. Where the positivity assumption is violated, causal diagrams
will be of limited utility because observations in the data do not
support valid causal inferences. (\textbf{Appendix 1} develops a worked
example that illustrates the difficulty of satisfying this assumption in
a setting of a cultural evolutionary question.)

\paragraph{1.2.3 Assumption 3: Conditional Exchangeability (no
unmeasured
confounding)}\label{assumption-3-conditional-exchangeability-no-unmeasured-confounding}

We satisfy the conditional exchangeability assumption if the treatment
groups are conditionally balanced in the variables that could affect the
potential outcomes. In experimental designs, random assignment
facilitates satisfaction of the conditional exchangeability assumption.
In observational studies more effort is required. We must control for
any covariate that could account for observed correlations between \(A\)
and \(Y\) in the absence of a causal effect of \(A\) on \(Y\).

Let \(\coprod\) again denote independence. Let \(L\) denote the set of
covariates necessary to ensure this conditional independence. We satisfy
conditional exchangeability when:

\[
Y(a) \coprod A | L \quad \text{or equivalently} \quad A \coprod Y(a) | L
\]

Assuming conditional exchangeability is satisfied and the other
assumptions required for consistent causal inference also hold, we may
compute the average treatment effect (ATE) on the difference scale:

\[
ATE = \mathbb{E}[Y(1) | L] - \mathbb{E}[Y(0) | L]
\]

Note that in randomised controlled experiments, exchangeability is
unconditional. Although adjustment by interacting the treatment with
pre-treatment variables may improve efficiency and diminish threats to
randomisation from chance inbalances in measured covariates, it is a
confusion to think of such adjustment as ``control.''

In the disciplines of cultural evolution, where experimental control is
impractical and we are left with ``real-world'' data, obtaining
consistent causal inferences hinge on the plausibility of satisfying
this `no unmeasured confounding' assumption (see: \textbf{Appendix A}).
In \textbf{Appendix B} I describe assumptions and wordlfows that occur
downstream of these three fundamental assumptions of causal inference.

However these assumptions occur down stream from the primary functions
of causal directed acyclic graphs, which we now ready to state:

The primary function of a causal directed acyclic graph is to identify
sources of bias that may lead to an association between an exposure and
outcome in the absence of causation. That is, causal directed acycic
graphs visually encode features of a causal order necessary to evaluate
the assumptions of conditional exchangeability -- also called
`no-unmeasured confounding', `ignorability', and -- in the idiom of
causal graphical models -- `d-separation'. Although directed acyclic
graphs can also be useful for addressing broader threats and
opportunities for causal inferences such as sample restriction,
transportability, and measurement error bias, they are designed to
evaluate the assumptions of conditional exchangeability, or equivalently
of non-unmeasured confounding, ignorability, or of d-separation between
the treatments and outcomes.

Perhaps the most frequent error in the deployment of causal graphical
models is using them to encode assumptions about the causal order
pertaining to a study. However, their function is merely to evaluate
those features of the causal order relevant to evaluating whether
balance in the variables that might affect both the treatment and the
outcome can be ensured from measured covariates.

\subsubsection{Summary of Part 1}\label{summary-of-part-1}

Causal data science is not ordinary data science. In causal data
science, the initial step involves formulating a precise causal question
that clearly identifies the exposure, outcome, and population of
interest. We must then satisfy the three fundamental assumptions
required for causal inference, which are implicit in the ideal of a
randomised experiment: causal consistency: ensuring outcomes at a
specific exposure level align with their counterfactual counterparts;
positivity: the existence of a non-zero probability for each exposure
level across all covariate; conditional exchangeability: the absence of
unmeasured confounding, or equivalently the `ignorability' of treatment
assignment, conditional on measured confounders.

\newpage{}

\subsection{Part 2 Causal Directed Acyclic
Graphs}\label{part-2-causal-directed-acyclic-graphs}

We introduce causal directed acyclic graphs by describing the meaning of
our symbols.

\subsubsection{2.1 Variable naming
conventions}\label{variable-naming-conventions}

\begin{table}

\caption{\label{tbl-terminology}Variable naming conventions}

\centering{

\terminologylocalconventionssimple

}

\end{table}%

\paragraph{\texorpdfstring{\(X\)}{X}}\label{x}

Denotes a random variable without reference to its role

\paragraph{\texorpdfstring{\(A\)}{A}}\label{a}

Denotes the ``treatment'' or ``exposure'' - a random variable.

This is the variable for which we seek to understand the effect of
intervening on it. It is the ``cause.''

\paragraph{\texorpdfstring{\(A=a\)}{A=a}}\label{aa}

Denotes a fixed ``treatment'' or ``exposure''

Random variable \(A\) is set to level \(A=a\).

\paragraph{\texorpdfstring{\(Y\)}{Y}}\label{y}

Denotes the outcome or response of an intervention.

It is the ``effect.''

\paragraph{\texorpdfstring{\(Y(a)\)}{Y(a)}}\label{ya}

Denotes the counterfactual or potential state of \(Y\) in response to
setting the level of the exposure to a specific level, \(A=a\)**

The outcome \(Y\) as it would be observed when, perhaps contrary to
fact, treatment \(A\) is set to level \(A=a\)

There are different conventions for expressing a potential or
counterfactual outcome, such as \(Y^a\), \(Y_a\).

\paragraph{\texorpdfstring{\(L\)}{L}}\label{l}

Denotes a measured confounder or set of confounders.

This set, if conditioned upon, insures that any differences between the
potential outcomes under different levels of the treatment are the
result of the treatment, and the not the result of a common cause of the
treatement and the outcome. Mathematically we write this independence:

\(Y(a)\coprod A \mid L\)

\paragraph{\texorpdfstring{\(U\)}{U}}\label{u}

Denotes an unmeasured confounder, or confounders.

\(U\) is variable or set of variables that may affect both the treatment
and the outcome, that, even after conditioning on measured covariates,
leads to association in the absence of causality:

\(Y(a) \cancel\coprod A \mid L \quad \text{[because of unmeasured } U]\)

\paragraph{\texorpdfstring{\(Z\)}{Z}}\label{z}

Denotes a modifier of the treatment effect

\(Z\) that alters the magnitude or direction of the effect of a
treatment \(A\) on an outcome \(Y\).

\paragraph{\texorpdfstring{\(M\)}{M}}\label{m}

Denotes a mediator, a variable that transmits the effect of an exposure
(or treatment) \(A\) on an outcome \(Y\).

\paragraph{\texorpdfstring{\(\bar{X}\)}{\textbackslash bar\{X\}}}\label{barx}

Denotes a sequence of variables, for example, a sequence of treatments.
Example:

Imagine we were interested in the causal effect of marriage and
remarriage on well-being. In this case, there are two treatments \(A_0\)
and \(A_1\) and four potential contrasts. For the scenario of marriage
and remarriage affecting well-being, we denote the potential outcomes as
\(Y(a_0, a_1)\), where \(a_0\) and \(a_1\) represent the specific values
taken by \(A_0\) and \(A_1\), respectively. Given two treatments,
\(A_0\) and \(A_1\), there are four primary contrasts of interest
correspond to the different combinations of these treatments. These
contrasts allow us to compare the causal effects of being married versus
not and of being remarried versus not on well-being. The potential
outcomes under these conditions can be specified as follows:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \(Y(0, 0)\): The potential outcome when there is no marriage.
\item
  \(Y(0, 1)\): The potential outcome when there is marriage.
\item
  \(Y(1, 0)\): The potential outcome when there is divorce.
\item
  \(Y(1, 1)\): The potential outcome from marriage prevalence.
\end{enumerate}

Each of these outcomes allows for a specific contrast. Which do we want
to contrast? We can see the question about `the causal effects of
marriage on happiness' is ambiguous. We must stated the causal contrast
we are interested in, given our substantive research interests. This
statment requires a sequences of exposures to be contrasted.

\paragraph{\texorpdfstring{\(\mathcal{R}\)}{\textbackslash mathcal\{R\}}}\label{mathcalr}

Denotes a randomisation or a chance event.

As when treatment assignment is random.

\paragraph{\texorpdfstring{\(\mathcal{G}\)}{\textbackslash mathcal\{G\}}}\label{mathcalg}

Denotes a graph, here, a causal directed acyclic graph.

Note that investigators use a variety of different symbols. As long as
meanings are clear, the symbols we use are arbitrary.

\subsubsection{2.2 Conventions for Causal Directed Acyclic
Graphs}\label{conventions-for-causal-directed-acyclic-graphs}

The conventions we use to describe components of our causal graphs are
given in Table~\ref{tbl-general}.

\begin{table}

\caption{\label{tbl-general}Nodes, Edges, Conditioning Conventions.}

\centering{

\terminologygeneraldags

}

\end{table}%

\paragraph{Node:}\label{node}

A node represents characteristics or features of units within a
population on a causal diagram, termed a ``variable.'' In causal graphs,
we draw nodes with respect to the \emph{target population}, which is the
population for whom investigators seek causal inferences.

\paragraph{Edge without an Arrow:}\label{edge-without-an-arrow}

An edge without an arrow indicates a path of association, without
implying causality.

\paragraph{Arrow}\label{arrow}

An edge with an arrow represents a causal relationship from the node at
the base of the arrow to the node at the tip of the arrow. For instance,
in the absence of causation, we denote it as \(A -- Y\), not
\(A \rightarrow Y\). We typically refrain from drawing an arrow from
treatment to outcome in a causal directed acyclic graph (DAG) to avoid
asserting a causal path from \(A\) to \(Y\). The main function of a
causal DAG is to clarify causal identification.

Note that we should only draw a causal DAG for the sample population if
it corresponds to the target population, as supported by Suzuki et
al.~(2020). As these causal paths are asserted, constructing causal
diagrams requires expert judgement---bearing in mind that ``with great
power comes great responsibility.''

\paragraph{Red Arrow}\label{red-arrow}

We denote a potential non-causal association between the treatment and
outcome by colouring this arrow red. When applying d-separation rules to
assess identification, we will disregard these arrows if confounding
arises because the treatment and the outcome share a common cause, as in
\(\mathcal{G}\) where the confounding path runs from
\(A_1 \association L_0 \association Y_2\). The use of colour in arrows
is specific to this review to highlight biases. Investigators may adopt
other conventions, but the chosen convention should be clearly defined.

\paragraph{Dashed Arrow:}\label{dashed-arrow}

A dashed arrow denotes a true association between the treatment and
outcome that becomes partially obscured when conditioning on a mediator.
This specific convention, adopted here, highlights the causal direct
effect, assuming its existence.

\paragraph{Dashed Red Arrow}\label{dashed-red-arrow}

A dashed red arrow represents over-conditioning bias from conditioning
on a mediator.

\paragraph{Open Blue Arrow}\label{open-blue-arrow}

We use an open blue arrow to indicate effect modification, which occurs
when the levels of the effect of treatment vary within levels of a
covariate. We do not assess the causal effect of the effect-modifier on
the outcome, recognising that it may be incoherent to consider
intervening on the effect-modifier.

\paragraph{Boxed Variable}\label{boxed-variable}

Enclosing a variable in a box denotes a decision to control for,
condition on, or adjust for that variable. This convention is widely
used in causal data science.

\paragraph{Red-Boxed Variable}\label{red-boxed-variable}

Colouring the box red when conditioning on a variable highlights the
source of confounding bias.

\paragraph{Dashed Circle}\label{dashed-circle}

A dashed circle explicitly denotes that no adjustment is made for a
variable.

\paragraph{\texorpdfstring{\(\mathcal{R} \rightarrow A\):}{\textbackslash mathcal\{R\} \textbackslash rightarrow A:}}\label{mathcalr-rightarrow-a}

This denotes randomisation into the treatment condition.

\paragraph{Sequential Order}\label{sequential-order}

Causal directed acyclic graphs are -- as the name implies -- acyclic. No
descendent node can cause an ancestor node. Directed edges or arrows
define ancestral relations. Hence causal diagrams are, by default,
sequentially ordered.

Nevertheless, to make our causal graphs more readible we adopt the
followign conventions:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  The layout of a causal diagram is structured from left to right to
  reflect the assumed sequence of causality as it unfolds.
\item
  We often index our nodes using \(X_t\) to indicate their relative
  timing and chronological order, where \(t\) represents the time point
  or sequence in the timeline of events.
\item
  Where temporal order is uncertain or unknown, we use the notation
  \(X_{\phi t}\) to propose a temporal order that is uncertain.
\end{enumerate}

Typically, the timing of unmeasured confounders is not known, except
that they occur before the treatments of interest; hence, we place
confounders to the left of the treatments and outcomes they are assumed
to affect, but without any time indexing.

Again, both spatial and temporal organisation in a causal directed
acyclic graph are optional. Temporal order is implied the relationship
of nodes and edges. However, explicitly representing order in the layout
of one's causal graph often makes it easier to evaluate, and the
convention representing uncertainty is useful, particularly when the
data do not ensure the relative timing of the occurance of the variable
in a causal graph.

\subsubsection{2.3 Terminology for Statistical and Causal
Independence}\label{terminology-for-statistical-and-causal-independence}

The bottom panel of Table~\ref{tbl-general} illustrates the mathematical
notationa we use to describe statistical and causal dependencies in the
distributions of variables within a population.

\paragraph{\texorpdfstring{Independence
\(\coprod\)}{Independence \textbackslash coprod}}\label{independence-coprod}

We denote independence using the symbol \(\coprod\). For example,
\(A \coprod Y(a)\) signifies that the treatment assignment \(A\) is
independent of the potential outcomes \(Y(a)\).

Note that it is the distributions of \textbf{counterfactual outcomes}
that must be independent of the treatments. The distributions of
observed outcomes will depend on treatments wherever the treatments are
causally efficacious.

\paragraph{\texorpdfstring{Dependence
\(\cancel{\coprod}\)}{Dependence \textbackslash cancel\{\textbackslash coprod\}}}\label{dependence-cancelcoprod}

We denote dependence using the symbol \(\cancel{\coprod}\). For
instance, \(A \cancel{\coprod} Y(a)\) suggests that the treatment
assignment \(A\) is related to the potential outcomes \(Y(a)\), which
could introduce bias into causal estimates.

\paragraph{\texorpdfstring{Conditioning
\(|\)}{Conditioning \textbar{}}}\label{conditioning}

The vertical line \(|\) represents conditioning on a variable. We can
express both conditional independence and conditional dependence:

\begin{itemize}
\tightlist
\item
  \textbf{Conditional Independence (\(A \coprod Y(a) \mid L\)):}
  indicates that once we account for a set of variables \(L\), the
  treatment \(A\) and the potential outcomes \(Y(a)\) are independent.
\item
  \textbf{Conditional Dependence (\(A \cancel{\coprod} Y(a) \mid L\)):}
  indicates that the treatment \(A\) and the potential outcomes \(Y(a)\)
  are not independent after conditioning on \(L\), or perhaps because we
  have conditioned on \(L\).
\end{itemize}

\subsubsection{2.4 How Causal Directed Acyclic Graphs Relate
Observations to
Counterfactuals}\label{how-causal-directed-acyclic-graphs-relate-observations-to-counterfactuals}

\paragraph{Ancestral Relations in Directed Acyclic
Graphs}\label{ancestral-relations-in-directed-acyclic-graphs}

We define the relation of ``parent'' and ``child'' on a directed acyclic
graph as follows:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Node \(A\) is a \textbf{parent} of node \(B\) if there is a directed
  edge from \(A\) to \(B\), denoted \(A \rightarrow B\).
\item
  Node \(B\) is a \textbf{child} of node \(A\) if there is a directed
  edge from \(A\) to \(B\), denoted \(A \rightarrow B\).
\end{enumerate}

It follows that a parent and child are \textbf{adjacent nodes} connected
by a directed edge.

We denote the set of all parents of a node \(B\) as \(\text{Pa}(B)\).

In a directed acyclic graph, the directed edge \(A \rightarrow B\)
indicates a statistical dependency where \(A\) may provide information
about \(B\). In a causal directed acyclic graph, the directed edge
\(A \rightarrow B\) is interpreted as a causal relationship such that
\(A\) is a direct cause of \(B\).

We further define the relations of \textbf{ancestor} and
\textbf{descendant} on a directed acyclic graph as follows:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Node \(A\) is an \textbf{ancestor} of node \(C\) if there exists a
  directed path from \(A\) to \(C\). Formally, \(A\) is an ancestor of
  \(C\) if there exists a sequence of adjacent nodes
  \((A, B_1, B_2, \ldots, B_k, C)\) such that
  \(A \rightarrow B_1 \rightarrow B_2 \rightarrow \cdots \rightarrow B_k \rightarrow C\).
\item
  Node \(C\) is a \textbf{descendant} of node \(A\) if there exists a
  directed path from \(A\) to \(C\). Formally, \(C\) is a descendant of
  \(A\) if there exists a sequence of adjacent nodes
  \((A, B_1, B_2, \ldots, B_k, C)\) such that
  \(A \rightarrow B_1 \rightarrow B_2 \rightarrow \cdots \rightarrow B_k \rightarrow C\).
\end{enumerate}

It follows that a node can have multiple ancestors and descendants.

\paragraph{Bayesian Network Factorisation and The Local Markov
Assumption}\label{bayesian-network-factorisation-and-the-local-markov-assumption}

Pearl (\citeproc{ref-pearl2009a}{2009}) p 52 asks us to imagine the
following. Suppose we have a distribution P defined on n discrete
variables, \(X_1, X_2,  \dots, X_n\). By the chain rule, the joint
distribution for variables \(X_1, X_2, \dots, X_n\) on graph can be
decomposed into the product of \(n\) conditional distributions such
that:

\[
P(x_1, \dots, x_n) = \prod_{j=1}^n P(x_j \mid x_1, \dots, x_{j-1})
\]

This factorisation of a graph is called Bayesian Network Factorisation.
We translate nodes and edges on a graph into a set of conditional
independences that a graph implies over the distributions represented in
the notes on a graph.

According to \textbf{the local Markov assumption,} given its parents in
a directed acyclic graph, a node is said to be independent of all its
non-descendants:

\[
P(x_j \mid x_1, \dots, x_{j-1}) = P(x_j \mid \text{pa}_j)
\]

This factorisation simplifies the calculation of the joint distributions
encoded in the directed acyclic graph (causal or non-causal) by reducing
complex factorisations of the conditional distributions in \(P\) to
simpler conditional distributions in the set \(\text{PA}_j\),
represented in the structural elements of a directed acyclic graph
(\citeproc{ref-lauritzen1990}{Lauritzen \emph{et al.} 1990};
\citeproc{ref-pearl1988}{Pearl 1988}, \citeproc{ref-pearl1995}{1995},
\citeproc{ref-pearl2009a}{2009}).

\paragraph{Minimality Assumption}\label{minimality-assumption}

The Minimality assumption combines (a) the local Markov assumption with
(b) the assumption that adjacent nodes on the graph are dependent. This
is needed for causal directed acyclic graphs because the local Markov
assumption permits that adjacent nodes may be independent
(\citeproc{ref-neal2020introduction}{Neal 2020}).

\paragraph{Causal Edges Assumption}\label{causal-edges-assumption}

The causal edges assumption states that every parent is a direct cause
of its children. Given minimalism, the causal edges assumption allows us
to read causal dependence in directed acyclic graphs. In Pearl's
formalism, we use non-parametric structural equations to evaluate causal
assumptions using statistical distributions (refer to Appendix E; Neal
(\citeproc{ref-neal2020introduction}{2020})).

\paragraph{Compatibility Assumption}\label{compatibility-assumption}

The \textbf{compatibility assumption} ensures that the joint
distribution of variables aligns with the conditional independencies
implied by the causal graph. This assumption requires that the
probabilistic model conforms to the graph's structural assumptions.
Demonstrating compatibility directly from data is challenging, as it
involves verifying that all conditional independencies specified by the
causal directed acyclic graph (DAG) are present in the data. Therefore,
we typically assume compatibility rather than empirically proving it
(\citeproc{ref-pearl2009a}{Pearl 2009}).

\paragraph{Faithfulness}\label{faithfulness}

\textbf{Faithfulness} complements the local Markov assumption and
Bayesian Network factorisation in causal diagrams. A causal diagram is
considered faithful to a given set of data if all the conditional
independencies present in the data are accurately depicted in the graph.
Conversely, the graph is faithful if every dependency implied by the
graph's structure can be observed in the data
(\citeproc{ref-hernan2024}{\textbf{hernan2024?}}). This concept ensures
that the graphical representation of relationships between variables
aligns with empirical evidence (\citeproc{ref-pearl2009a}{Pearl 2009}).

The distinction between \textbf{weak faithfulness} and \textbf{strong
faithfulness} addresses the nature of observed independencies:

\begin{itemize}
\tightlist
\item
  \textbf{Weak faithfulness} allows for the possibility that some
  observed independencies might occur by coincidence, such as through a
  cancellation of effects among multiple causal paths.
\item
  \textbf{Strong faithfulness}, on the other hand, assumes that all
  observed statistical relationships reflect the underlying causal
  structure directly, with no independencies arising purely by chance.
  This stronger assumption is often more pragmatic in real-world
  applications of causal inference, where the complexities of data can
  obscure underlying causal relationships.
\end{itemize}

The faithfulness assumption, whether weak or strong, is not directly
testable from observed data. It is fundamentally a theoretical
assumption about the relationship between the observed data and the
underlying causal structure (\citeproc{ref-pearl2009a}{Pearl 2009}).

\subsubsection{2.5 The d-Separation
Criterion}\label{the-d-separation-criterion}

\textbf{d-Separation}: In a causal diagram, a path is `blocked' or
`d-separated' if a node along it interrupts causation. Two variables are
d-separated if all paths connecting them are blocked, making them
conditionally independent. Conversely, unblocked paths result in
`d-connected' variables, implying potential dependence
(\citeproc{ref-pearl1995}{Pearl 1995}, \citeproc{ref-pearl2009a}{2009}).
(Note that ``d'' stands for ``directed''.)

The rules of d-separation are as follows:

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\item
  \textbf{Fork rule} (\(A \leftarrow \boxed{B} \rightarrow C\)): \(A\)
  and \(C\) are independent when conditioning on \(B\)
  (\(A \coprod C \mid B\)) (\citeproc{ref-pearl1995}{Pearl 1995}).
\item
  \textbf{Chain rule} (\(A \rightarrowNEW \boxed{B} \rightarrowNEW C\)):
  Conditioning on \(B\)blocks the path between \(A\)and \(C\)
  (\(A \coprod C \mid B\)) (\citeproc{ref-pearl1995}{Pearl 1995}).
\item
  \textbf{Collider rule}
  (\(A \rightarrowred \boxed{B} \leftarrowred C\)): \(A\) and \(C\) are
  independent until conditioning on \(B\), which introduces dependence
  (\(A \cancel{\coprod} C \mid B\)) (\citeproc{ref-pearl1995}{Pearl
  1995}).
\end{enumerate}

According to these rules:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  An open path (no variables conditioned on) is blocked only if two
  arrows point to the same node: \(A \rightarrowred B \leftarrowred C\).
  The node of common effect is called a \emph{collider}.
\item
  Conditioning on a collider does not block a path, such that
  \(A \rightarrowred \boxed{B} \leftarrowred C\)may suggest an
  association of \(A\) with \(C\) in the absence of causation.
\item
  Conditioning on a descendant of a collider does not block a path, such
  that if \(L \rightarrowred \boxed{B'}\), then
  \(A \rightarrowred \boxed{B'} \leftarrowred C\) is open.
\item
  If a path does not contain a collider, any variable conditioned along
  the path is blocked, such that
  \(A \rightarrowdotted \boxed{B} \rightarrowdotted C\)blocks the path
  from \(A \rightarrowdotted C\)(\citeproc{ref-hernan2023}{Hernan and
  Robins 2023 p. 78}).
\end{enumerate}

\paragraph{The Backdoor Path
Criterion}\label{the-backdoor-path-criterion}

To obtain an unbiased estimate for the causal effect of \(A\)on \(C\),
we need to block all backdoor paths. We do this by conditioning on a set
of covariates \(B\)that is sufficient to close all backdoor paths
linking \(A\)and \(C\). A path is effectively blocked by \(B\)if it
includes at least one non-collider that is a member of \(B\), or if it
does not contain any collider or descendants of a collider.

Pearl defines this criterion more generally as follows: a set of
variables \(Z\) satisfies the backdoor criterion relative to variables
\(X_i\)and \(X_j\) in a causal directed acyclic graph \(\mathcal{G}\)
if:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  No node in \(Z\) is a descendant of \(X_i\); and
\item
  \(Z\) blocks every path between \(X_i\)and \(X_j\)that contains an
  arrow into \(X_i\).
\end{enumerate}

Similarly, if \(X\)and \(Y\) are two disjoint subsets of nodes in
\(\mathcal{G}\), then \(Z\) satisfies the backdoor criterion relative to
\((X, Y)\) if it satisfies the criterion relative to any pair
\((X_i, X_j)\) such that \(X_i \in X\) and \(X_j \in Y\). The name
``backdoor'' refers to condition (2), which requires that only paths
with arrows pointing at \(X_i\) be blocked; these paths can be viewed as
entering \(X_i\) through the back door.

The backdoor criterion uses the rules of d-separation to identify and
block all paths in a causal diagram that could introduce bias between a
treatment (or exposure), \(A\), and an outcome, \(C\), in the absence of
causation (\citeproc{ref-pearl2009a}{Pearl 2009}).

\paragraph{Frontdoor Path Criterion}\label{frontdoor-path-criterion}

To obtain an unbiased estimate for the causal effect of \(A\) on \(C\)
using the frontdoor criterion, we need to identify a set of variables
\(M\) that mediates the effect of \(A\) on \(C\).

Pearl defines the frontdoor criterion more generally as follows: a set
of variables \(B\) satisfies the frontdoor criterion relative to
variables \(A\) and \(C\) in a causal directed acyclic graph
\(\mathcal{G}\) if:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \(B\) is affected by \(A\).
\item
  \(B\) affects \(C\).
\item
  There are no backdoor paths from \(A\) to \(B\).
\item
  All backdoor paths from \(B\) to \(C\) are blocked by conditioning on
  \(A\).
\end{enumerate}

In other words, \(B\) must be an intermediate variable that captures the
entire causal effect of \(A\) on \(C\), with no confounding paths
remaining between \(A\) and \(B\), and any confounding between \(B\) and
\(C\) must be blocked by \(A\).

The frontdoor criterion is less widely used compared to the backdoor
criterion because it requires the identification of an appropriate
mediator that fully captures the causal effect
(\citeproc{ref-pearl2009a}{Pearl 2009}). Here, we state the frontdoor
path criterion for completeness.

\subsubsection{2.6 Comment on Pearl's Do-Calculus versus the Potential
Outcomes
Framework}\label{comment-on-pearls-do-calculus-versus-the-potential-outcomes-framework}

Here, we have developed counterfactual contrasts using the potential
outcomes framework, Pearl develops counterfactual contrasts using
operations on structural functionals, referred to as ``do-calculus''
(Appendix E.) In practice, whether one uses the do-calculus (and the
non-parametric structural equation models it relies on) or the potential
outcomes framework to interpret causal inferences is typically
irrelevant to identification results. However, there are theoretically
interesting debates about edge cases.

In some cases, Pearl's non-parametric structural equation models permit
the identification of contrasts that cannot be falsified under any
experiment (\citeproc{ref-richardson2013}{Richardson and Robins 2013a}).
Because advocates of non-parametric structual equation models treats
causality as primitive, they are less concerned with the requirement for
falsification Dı́az \emph{et al.} (\citeproc{ref-Diaz2023}{2023}). On the
other hand, some advocates of the potential outcomes framework require
falsifiability Shpitser and Tchetgen
(\citeproc{ref-shpitser2016causal}{2016}). Conversely, there are edge
cases where the potential outcomes framework achieves identification
while the do-calculus does not (\citeproc{ref-richardson2013}{Richardson
and Robins 2013a}).

I have presented the potential outcomes framework because it is easier
to interpret. Moreover, one does not need to be a verificationist to
adopt it. For most practical purposes, the two frameworks are equivalent
in terms of their utility for causal inference. Furthermore, readers
should be aware that there are causal diagrams called ``Single World
Intervention Graphs'' which enable investigators to represent
conditional independences of potential outcomes on graphs
(\citeproc{ref-richardson2013swigsprimer}{Richardson and Robins 2013b}),
which can be useful.

\#\#\# 2.7 The Five Elementary Structures of Causality Encoded In Causal
Directed Acyclic Graphs

\begin{table}

\caption{\label{tbl-fiveelementary}Five elementary structures of
causality used in causal directed acyclic graphs.}

\centering{

\terminologydirectedgraph

}

\end{table}%

Table~\ref{tbl-fiveelementary} presents the five elementary structures
of causality as represented in causal directed acyclic graphs:

Next consider that there are are five elemental structures from which
all causal directed acyclic graphs may be composed.

\paragraph{Causal Relations With Two
Variables}\label{causal-relations-with-two-variables}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Causality Absent:} There is no causal effect between variables
  \(A\) and \(B\). They do not influence each other, denoted as
  \(A \coprod B\), indicating they are statistically independent.
\item
  \textbf{Causality:} Variable \(A\) causally affects variable \(B\).
  This relationship suggests an association between them, denoted as
  \(A \cancel{\coprod} B\), indicating they are statistically dependent.
\end{enumerate}

\paragraph{Causal Relations with Three
Variables}\label{causal-relations-with-three-variables}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  \textbf{Fork Relation:} Variable \(A\) causally affects both \(B\) and
  \(C\). Variables \(B\) and \(C\) are conditionally independent given
  \(A\), denoted as \(B \coprod C \mid A\). This structure implies that
  knowing \(A\) removes any association between \(B\) and \(C\) due to
  their common cause.
\item
  \textbf{Chain Relation:} A causal chain exists where \(C\) is affected
  by \(B\), which in turn is affected by \(A\). Variables \(A\) and
  \(C\) are conditionally independent given \(B\), denoted as
  \(A \coprod C \mid B\). This indicates that \(B\) mediates the effect
  of \(A\) on \(C\), and knowing \(B\) breaks the association between
  \(A\) and \(C\).
\item
  \textbf{Collider Relation:} Variable \(C\) is affected by both \(A\)
  and \(B\), which are independent. However, conditioning on \(C\)
  induces an association between \(A\) and \(B\), denoted as
  \(A \cancel{\coprod} B \mid C\). This structure is important because
  it suggests that \(A\) and \(B\), while initially independent, become
  associated when we account for their common effect \(C\).
\end{enumerate}

Understanding the basic relationships between two variables allows us to
build upon these to create more complex relationships. These
elementalary structures can be assembled in different combinations to
clarify the causal relationships that are are presented in a causal
directed acyclic graph. Such clarity is crucial for ensuring whether
confounders may be balanced across treatment groups to be compared,
conditional on measured co-variates, so that \(Y(a) \coprod A \mid L\).

\newpage{}

\subsubsection{2.8 The Five Fundamental Rules of Confounding
Control}\label{the-five-fundamental-rules-of-confounding-control}

Table~\ref{tbl-terminologyconfounders} describe five elementary rules of
confounding control:

\begin{table}

\caption{\label{tbl-terminologyconfounders}Five elementary rules for
confounding control.}

\centering{

\terminologyelconfounders

}

\end{table}%

There are no shortcuts to reasoning about causality. Each causal
question must be asked in the context of a specific scientific question,
and each causal graph must be build under the best lights of domain
expertise. However, the following five elementary rules for confounding
control are implied by the theorems that underpin casuald directed
acyclic graphs. They may useful start for evaluating the prospects for
causal identification across a broad range of settings.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Ensure That Treatments Precedes Outcomes}: this rule is a
  logical consequence of our assumption that causality follows the arrow
  of time, and that a causal directed acyclic graph is faithful to this
  ordering. However, the assumption that treatments precede outcomes may
  be easily violated where investigators cannot ensure the relative
  timing of events from there data.
\end{enumerate}

Note that this assumption does raise concerns in settings where past
outcomes may not affect future treatments. Indeed, often an effective
strategy for confounding control in such settings is to condition on
past outcomes, and where relevant, one past treatments as well. For
example, if we wish to identify the cuasal effect of \(A_1\) on \(Y_2\),
if repeated-measures time series data are available, it may be useful to
condition such that \(\boxed{A_{-1}} \to \boxed{Y_0} \to A_1 ~~ Y_2\).
Critically, the relations of variables must be arranged sequentially
without cycles.

Note further that to estimate a causal effect of \(Y\) on \(A\) we would
focus on: \(\boxed{Y_{-1}} \to \boxed{A_0} \to Y_1 ~~ A_2\). Departing
from convention, here \(Y\) denotes the treatment and \(A\) denotes the
outcome. Graphs must be acyclic. Most processes in nature include
feedback loops. There is no contradition as long as we represent these
loops as sequential events.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\item
  \textbf{Condition on Common Causes or Their Proxies}: This rule
  applies to settings in which the treatment \(A\) and the outcome \(Y\)
  share common causes. By conditioning on these common causes, we block
  the open backdoor paths that could introduce bias into our causal
  estimates. Controlling for these common causes (or their proxies)
  helps to isolate the specific effect of \(A\) on \(Y\). Note that we
  do not draw a path from \(A \to Y\) in this context because it
  represents an interventional distribution. In a causal directed
  acyclic graph, conditioning does not occur on interventional
  distributions. We do not box \(A\) and \(Y\).
\item
  \textbf{Do Not Condition on a Mediator}: this rule applies to settings
  in which the variable \(L\) is a mediator of \(A \to Y\). Recall
  Pearl's backdoor path criterion require that we do not condition on a
  decendant of the treatment. Here, conditioning on a \(L\)\$ violates
  the backdoor path criterion, risking bias for a total causal effect
  estimate. If we are interested in total effect estimates, we must not
  condition on a mediator. Note we draw the path from \(A \to Y\) to
  underscore that this specific overconditioning threat occurs in in the
  presence of a true treatment effect. As we consider below,
  over-conditioning bias can operate in the absence of a true-treatment
  effect. This is important because condiitoning on a mediator might
  create associations in the absence of causation. In many settings,
  ensuring accuracy in the relative timing of events in our data will
  prevent the self-inflicted injury of conditioning on a common effect
  of the treatment.
\item
  \textbf{Do Not Condition on a Collider}: this rule applies to settings
  in which we \(L\) is a common effect of \(A\) and \(Y\). Conditioning
  on a collider may invoke a spurious association. Again the backdoor
  path criterion require that we do not condition on a decendant of the
  treatment. We will not be tempted to condition on \(L\) if we knew
  that it was an effect of \(A\). In many settings, ensuring accuracy in
  the relative timing of events in our data will prevent the
  self-inflicted injury of conditioning on a common effect of the
  treatment and outcome.
\item
  \textbf{Proxy Rule: Conditioning on a Descendent Is Akin to
  Conditioning on Its Parent}: this rule applies to settings in which we
  \(L’\) is an effect from another variable \(L\). The graph considers
  when \(L’\) is downstream of a collider. Here again, in many settings,
  ensuring accuracy in the relative timing of events in our data will
  prevent the self-inflicted injury of conditioning on a common effect
  of the treatment and outcome.
\end{enumerate}

\newpage{}

\subsection{Part 3. Use Cases}\label{part-3.-use-cases}

\subsubsection{3.1 Direct Acyclic Graphs Reveal The Importance of Timing
for
Identification}\label{direct-acyclic-graphs-reveal-the-importance-of-timing-for-identification}

\begin{table}

\caption{\label{tbl-elementary-chronological-hyg}}

\centering{

\captionsetup{labelsep=none}

\terminologychronologicalhygeine

}

\end{table}%

As hinted at in the previous section, the five elementary rules of
confounding control reveal the importance of ensuring accurate timing in
the occurance of the variables whose structural features a causal
directed acyclic graph encodes. We begin by consider seven examples of
confounding problems resolved when such accuracy is ensured.

\subsection{3.2 Collecting Time Series Data is Insufficient for
Identification}\label{collecting-time-series-data-is-insufficient-for-identification}

\begin{table}

\caption{\label{tbl-chronology-notenough}Common confounding scenarios in
which chronology is not enough.}

\centering{

\terminologychronologicalhygeineNOTENOUGH

}

\end{table}%

\subsection{Part 4. Practical Advice}\label{part-4.-practical-advice}

\subsubsection{4.1 How to Create Causal Diagrams to Address Causal
Identification Problems
\{sec-how-to-create-causal-diagrams\}}\label{how-to-create-causal-diagrams-to-address-causal-identification-problems-sec-how-to-create-causal-diagrams}

The \textbf{identification problem} centres on whether we can derive the
true causal effect of a treatment (\(A\)) on an outcome (\(Y\)) from
observed data. Addressing the identification problem has two core
components:

\paragraph{First, evaluate bias in the absence of a treatment
effect}\label{first-evaluate-bias-in-the-absence-of-a-treatment-effect}

Before attributing any statistical association to causality, we must
eliminate non-causal sources of correlation. We do this by:

\begin{itemize}
\tightlist
\item
  Identifying factors that influence both treatment (\(A\)) and outcome
  (\(Y\)).
\item
  Developing adjustment strategies to control for confounders.
\item
  Blocking backdoor paths that create indirect, non-causal links between
  \(A\) and \(Y\). By adjusting for confounders, we aim to achieve
  d-separation between \(A\) and \(Y\).
\end{itemize}

\paragraph{Second, evaluate bias in the presence of a treatment
effect}\label{second-evaluate-bias-in-the-presence-of-a-treatment-effect}

After addressing potential confounders, we must ensure any remaining
association between \(A\) and \(Y\) reflects a true causal relationship.
We address \textbf{over-conditioning bias} by:

\begin{itemize}
\tightlist
\item
  Avoiding mediator bias
\item
  Avoiding collider bias
\item
  Verifying that any association between \(A\) and \(Y\) after in
  unbiased after all adjustments.
\end{itemize}

Thus, causal inference demands a delicate balance: identify and control
for confounders but avoid introducing new biases. Here is how
investigators should construct their causal diagrams.

\paragraph{Step 1. Clarify the research question evaluated by the
diagram}\label{step-1.-clarify-the-research-question-evaluated-by-the-diagram}

Before attempting to draw any causal diagram, state the problem your
diagram addresses and the population to whom the problem applies. Causal
identification strategies may vary by question. For example, the
confounding control strategy for evaluating the path \(L\to Y\) will
differ from that of assessing the path \(A\to Y\). For this reason,
reporting coefficients other than the association between \(A \to Y\) is
typically ill-advised; see Westreich and Greenland
(\citeproc{ref-westreich2013}{2013}); McElreath
(\citeproc{ref-mcelreath2020}{2020}); Bulbulia
(\citeproc{ref-bulbulia2023}{2023}).

\paragraph{Step 2. Include all common causes of the exposure and
outcome}\label{step-2.-include-all-common-causes-of-the-exposure-and-outcome}

Incorporate all common causes (confounders) of both the exposure and the
outcome into your diagram. This includes both measured and unmeasured
variables. Where possible, aggregate functionally similar common causes
into a single variable notation (e.g., \(L_0\) for demographic
variables).

\paragraph{Step 3. Include all ancestors of measured confounders linked
with the treatment, the outcome, or
both}\label{step-3.-include-all-ancestors-of-measured-confounders-linked-with-the-treatment-the-outcome-or-both}

Include any ancestors (precursors) of measured confounders that are
associated with either the treatment, the outcome, or both. This step is
crucial for addressing hidden biases arising from unmeasured
confounding. Simplify the diagram by grouping similar variables.

\paragraph{Step 4. Explicitly state assumptions about relative
timing}\label{step-4.-explicitly-state-assumptions-about-relative-timing}

Explicitly annotate the temporal sequence of events using subscripts
(e.g., \(L_0\), \(A_1\), \(Y_2\)). It is imperative that causal diagrams
are acyclic.

\paragraph{Step 5. Arrange temporal order of causality
visually}\label{step-5.-arrange-temporal-order-of-causality-visually}

Arrange your diagram to reflect the temporal progression of causality,
either left-to-right or top-to-bottom. This arrangement enhances the
comprehensibility of causal relations and is vital for dissecting
identification issues as discussed in \hyperref[sec-part3]{\textbf{Part
3}}, establishing temporal ordering is necessary for evaluating
identification problems.

\paragraph{Step 6. Box variables are those variables that we adjust for
to control
confounding}\label{step-6.-box-variables-are-those-variables-that-we-adjust-for-to-control-confounding}

Mark variables for adjustment (e.g., confounders) with boxes.

\paragraph{Step 7. Represent paths structurally, not
parametrically}\label{step-7.-represent-paths-structurally-not-parametrically}

Focus on whether paths exist, not their functional form (linear,
non-linear, etc.). Parametric descriptions are not relevant for bias
evaluation in a causal diagram. (For an explanation of causal
interaction and diagrams, see: Bulbulia
(\citeproc{ref-bulbulia2023}{2023}).)

\paragraph{Step 8. Minimise paths to those necessary for the
identification
problem}\label{step-8.-minimise-paths-to-those-necessary-for-the-identification-problem}

Reduce clutter; only include paths critical for a specific question
(e.g., backdoor paths, mediators).

\paragraph{Step 9. Consider Potential Unmeasured
Confounders}\label{step-9.-consider-potential-unmeasured-confounders}

Leverage domain expertise to clarify potential unmeasured confounders
and represent them in your diagram. This proactive step aids in
anticipating and addressing \emph{all} possible sources of confounding
bias.

\paragraph{\texorpdfstring{\textbf{Step 10. State Graphical
Conventions}}{Step 10. State Graphical Conventions}}\label{step-10.-state-graphical-conventions}

Establish and explain the graphical conventions used in your diagram
(e.g., using red to highlight open backdoor paths). Consistency in
symbol use enhances interpretability, while explicit descriptions
improve accessibility and understanding.

Practical Guide For Constructing Causal Diagrams and Reporting Results
When Causal Structure is Unclear \{\#section-part4\}

\subsubsection{2.2. Reporting Causal Directed Acyclic Graphs in
Cross-sectional
designs}\label{reporting-causal-directed-acyclic-graphs-in-cross-sectional-designs}

In environmental psychology, researchers often grapple with whether
causal inferences can be drawn from cross-sectional data, especially
when longitudinal data are unavailable. The challenge is common to
cross-sectional designs. However, it is important to appreciate that
even longitudinal studies require careful assumption management. We next
discuss how causal diagrams can guide inference in both data types, with
examples relevant to environmental psychologists.

\paragraph{1. Graphically encode causal
assumptions}\label{graphically-encode-causal-assumptions}

Causal inference turns on assumptions. Although cross-sectional analyses
typically demand much stronger assumptions owing to the snapshot nature
of data, these assumptions, when transparently articulated, do not
permanently bar causal analysis. By stating different assumptions and
modelling the data following these assumptions, we might find that
certain causal conclusions are robust to these differences. Where the
implications of different assumptions disagree, we can better determine
the forms of data collection that would be required to settle such
differences. Below we consider an example where assumptions point to
different conclusions, revealing the benefits of collecting time-series
data to assess whether a variable is a confounder or a mediator.

\paragraph{2. Time-invariant
confounders}\label{time-invariant-confounders}

In cross-sectional studies, some confounders are inherently stable over
time, such as ethnicity, year and place of birth, and biological gender.
For environmental psychologists examining the relationship between
access to natural environments and psychological well-being, these
stable confounders can be adjusted for without concern for introducing
bias from mediators or colliders. For example, conditioning on one's
year of birth can help isolate recent urban development's effect on
mental health, independent of generational differences in attitudes
toward green spaces.

\paragraph{3. Stable confounders}\label{stable-confounders}

While not immutable, other confounders are less likely to be influenced
by the treatment. Variables such as sexual orientation, educational
attainment, and often income level fall into this category. For
instance, the effect of exposure to polluted environments on cognitive
outcomes can be analysed by conditioning on education level, assuming
that recent exposure to pollution is unlikely to change someone's
educational history retroactively.

\paragraph{4. Timing and reverse
causation}\label{timing-and-reverse-causation}

The sequence of treatment and outcome is crucial. Sometimes, the
temporal order is clear, reducing concerns about reverse causation.
Mortality is a definitive outcome where the timing issue is unambiguous.
If researching the effects of air quality on mortality, the causal
direction (poor air quality leading to higher mortality rates) is
straightforward. However, consider the relationship between
socio-economic status and health outcomes; the direction of causality is
complex because socioeconomic factors can influence health (through
access to resources), and poor health can affect socio-economic status
(through reduced earning capacity).

\paragraph{5. Create causal diagrams}\label{create-causal-diagrams}

Given the complexity of environmental influences on psychological
outcomes, it's prudent to construct multiple causal diagrams to cover
various hypothetical scenarios. For example, when studying the effect of
community green space on stress reduction, one diagram might assume the
direct benefits of green space on stress. At the same time, another
might include potential mediators like physical activity. By analysing
and reporting findings based on multiple diagrams, researchers can
examine the robustness of their conclusions across different theoretical
frameworks and sets of assumptions.

Table~\ref{tbl-cs} describes ambiguous confounding control arising from
cross-sectional data. Suppose again we are interested in the causal
effect of access to greenspace, denoted by \(A\) on ``happiness,''
denoted by \(Y\). We are uncertain whether exercise, denoted by \(L\),
is a common cause of \(A\) and \(Y\) and thus a confounder or whether
exercise is a mediator along the path from \(A\) to \(Y\). That is: (1)
those who exercise might seek access to green space, and (2) exercise
might increase happiness. Alternatively, the availability of green space
might encourage physical activity, which could subsequently affect
happiness. Causal diagrams can disentangle these relationships by
explicitly representing potential paths, thereby guiding appropriate
strategies for confounding control selection. We recommend using
multiple causal diagrams to investigate the consequences of different
plausible structural assumptions.

\textbf{Assumption 1: Exercise is a common cause of \(A\) and \(Y\)},
this scenario is presented in Table~\ref{tbl-cs} row 1. Here, our
strategy for confounding control is to estimate the effect of \(A\) on
\(Y\) conditioning on \(L\).

\textbf{Assumption 2: Exercise is a mediator of \(A\) and \(Y\)}, this
scenario is presented in Table~\ref{tbl-cs} row 2. Here, our strategy
for confounding control is simply estimating the effect of \(A\) on
\(Y\) without including \(L\) (assuming there are no other common causes
of the treatment and outcome).

\begin{table}

\caption{\label{tbl-cs}Example of reporting multiple causal graphs in a
cross-sectional design}

\centering{

\examplecrosssection

}

\end{table}%

We can simulate data and run separate regressions to clarify how answers
may differ, reflecting the different conditioning strategies embedded in
the different assumptions. The following simulation generates data from
a process in which exercise is a mediator (Scenario 2). (See Appendix C)

\begin{table}
\caption{Code for a simulation of a data generating process in which the effect
of exercise (L) fully mediates the effect of greenspace (A) on happiness
(Y).}\tabularnewline

\centering
\begin{tabular}{lcccccc}
\toprule
\multicolumn{1}{c}{ } & \multicolumn{3}{c}{Model: Exercise assumed confounder} & \multicolumn{3}{c}{Model: Exercise assumed to be a mediator} \\
\cmidrule(l{3pt}r{3pt}){2-4} \cmidrule(l{3pt}r{3pt}){5-7}
\textbf{Characteristic} & \textbf{Beta} & \textbf{95\% CI} & \textbf{p-value} & \textbf{Beta} & \textbf{95\% CI} & \textbf{p-value}\\
\midrule
A & -0.27 & -0.53, -0.01 & 0.043 & 2.9 & 2.6, 3.2 & <0.001\\
L & 1.6 & 1.5, 1.7 & <0.001 &  &  & \\
\bottomrule
\multicolumn{7}{l}{\rule{0pt}{1em}\textsuperscript{1} CI = Confidence Interval}\\
\end{tabular}
\end{table}

This table presents the conditional treatment effect estimates. We
present code for obtaining marginal treatment effects in
\hyperref[appendix-c]{Appendix C}

On the assumptions outlined in Table~\ref{tbl-cs} row 1, in which we
\emph{assert} that exercise is a confounder, the average treatment
effect of access to green space on happiness is ATE = 2.92, CI =
{[}2.66, 3.21{]}.

On the assumptions outlined in Table~\ref{tbl-cs} row 2, in which we
\emph{assert} that exercise is a mediator, the average treatment effect
of access to green space on happiness is ATE = -0.27, CI = {[}-0.52,
-0.01{]}.

Note that although the mediator \(L\) is ``highly statistically
significant'', including it in the model is a mistake. We obtain a
negative effect estimate for the causal effect of green space access on
happiness.

With only cross-sectional data, we must infer the results are
inconclusive. Such understanding, although not the definitive answer we
sought, is progress. The result tells us we should not be overly
confident with our analysis (whatever p-values we recover!), and it
clarifies that longitudinal data are needed.

These findings illustrate the role that assumptions about the relative
timing of exercise as a confounder or as a mediator play.

\subsubsection{4.1 Recommendations for Conducting and Reporting Causal
Analyses with Cross-Sectional
Data}\label{recommendations-for-conducting-and-reporting-causal-analyses-with-cross-sectional-data}

When analysing and reporting analyses with cross-sectional data,
researchers face the challenge of making causal inferences without the
benefit of temporal information.

The following recommendations aim to guide researchers in navigating
these challenges effectively:

\textbf{Warning}: before proceeding with cross-sectional analysis,
examine whether panel data are available. Longitudinal data can provide
crucial temporal information that aids in establishing causality,
offering a more robust framework for causal inference. If longitudinal
data are unavailable, the recommendations above become even more
critical for using cross-sectional data best.

\paragraph{\texorpdfstring{1. \textbf{Draw multiple causal
diagrams}}{1. Draw multiple causal diagrams}}\label{draw-multiple-causal-diagrams}

Draw various causal diagrams to represent different theoretical
assumptions about the relationships and timing of variables relevant to
an identification problem. This approach comprehensively examines
possible causal pathways, clarifying variables' roles as confounders,
mediators, or colliders. For example, in studying the effect of urban
green spaces on mental health, consider diagrams that account for both
direct effects and pathways involving mediators like physical activity
or social interaction.

\paragraph{\texorpdfstring{2. \textbf{Perform and report analyses for
each
assumption}}{2. Perform and report analyses for each assumption}}\label{perform-and-report-analyses-for-each-assumption}

Conduct and transparently report separate analyses for each scenario
your causal diagrams depict. This practice ensures that your study is
theoretically grounded for each model. Presenting results from each
analytical approach and the underlying assumptions and statistical
methods promotes a balanced interpretation of findings. Although this
practice may be unfamiliar to some editors and reviewers, it is crucial
to address the inherent challenges of cross-sectional analysis by
expanding the scope of investigation beyond a single hypothesis.

\paragraph{\texorpdfstring{3. \textbf{Interpret findings with attention
to
ambiguities}}{3. Interpret findings with attention to ambiguities}}\label{interpret-findings-with-attention-to-ambiguities}

Interpret results carefully, highlighting any ambiguities or
inconsistencies across analyses. Discuss how varying assumptions about
structural relationships and the timing of events can lead to divergent
conclusions. For instance, exploring the theoretical and empirical
implications of access to green spaces appears to positively affect
mental health when considering exercise as a mediator but a negative
effect when considered a confounder.

\paragraph{\texorpdfstring{4. \textbf{Report divergent
findings}}{4. Report divergent findings}}\label{report-divergent-findings}

Approach conclusions with caution, especially when findings suggest
differing practical implications. Acknowledge the limitations of
cross-sectional data in establishing causality and the potential for
alternative explanations.

\paragraph{\texorpdfstring{5. \textbf{Identify avenues for future
research}}{5. Identify avenues for future research}}\label{identify-avenues-for-future-research}

Target future research that could clarify ambiguities. Consider the
design of longitudinal studies or experiments capable of clarifying
these ambiguities.

\paragraph{\texorpdfstring{6. \textbf{Supplement observational data with
simulated
data}}{6. Supplement observational data with simulated data}}\label{supplement-observational-data-with-simulated-data}

Leverage data simulation to understand the complexities of causal
inference. Simulating data based on various theoretical models allows
researchers to examine the effect of different assumptions on their
findings. This method tests analytical strategies under controlled
conditions, assessing the robustness of conclusions against assumption
violations or unobserved confounders.

\paragraph{\texorpdfstring{7. \textbf{Conduct sensitivity analyses to
assess
robustness}}{7. Conduct sensitivity analyses to assess robustness}}\label{conduct-sensitivity-analyses-to-assess-robustness}

implement sensitivity analyses to determine how dependent conclusions
are on specific assumptions or parameters within your causal model. Use
data simulation as a tool for these analyses, evaluating the sensitivity
of results to various theoretical and methodological choices.

Cross-sectional data are limiting; however, by appropriately bounding
uncertainties in your causal inferences, you may use them to advance
understanding. May your clarity and caution serve as an example for
others.

\subsubsection{4.2 Recommendations for Reporting Causal Directed Acyclic
Graphs in Longitudinal
Designs}\label{recommendations-for-reporting-causal-directed-acyclic-graphs-in-longitudinal-designs}

Causation occurs in time. Longitudinal designs offer a substantial
advantage over cross-sectional designs for causal inference because
sequential measurements allow us to capture causation and quantify its
magnitude. We typically do not need to assert timing as in
cross-sectional data settings. Because we know when variables have been
measured, we can reduce ambiguity about the directionality of causal
relationships. For instance, tracking changes in ``happiness'' following
changes in access to green spaces over time can more definitively
suggest causation than cross-sectional snapshots.

Despite this advantage, longitudinal researchers still face assumptions
regarding the absence of unmeasured confounders or the stability of
measured confounders over time. These assumptions must be explicitly
stated. As with cross-sectional designs, wherever assumptions differ,
researchers should draw different causal diagrams that reflect these
assumptions and subsequently conduct and report separate analyses.

In this section, we simulate a dataset to demonstrate the benefits of
incorporating both baseline exposure and baseline outcomes into
analysing the effect of access to open green spaces on happiness. This
approach allows us to control for initial levels of exposure and
outcomes, offering a clearer understanding of the causal relationship.
\hyperref[appendix-d-simulation-of-different-confounding-control-strategies]{Appendix
D} provides the code.
\hyperref[appendix-e-non-parametric-estimation-of-average-treatment-effects-using-causal-forests]{Appendix
E} provides an example of a non-parametric estimator for the causal
effect. As mentioned before, by conditioning on baseline levels of
access to green spaces and baseline mental health, researchers can more
accurately estimate the \emph{incident effect} of changes in green space
access on changes in mental health. Table~\ref{tbl-lg} offers an example
of how we may use multiple causal diagrams to clarify the problem and
our confounding control strategy.

\begin{table}

\caption{\label{tbl-lg}This table is adapted from
(\citeproc{ref-bulbulia2023}{Bulbulia 2023})}

\centering{

\examplelongitudinal

}

\end{table}%

Our analysis assessed the average treatment effect (ATE) of access to
green spaces on happiness across three distinct models: uncontrolled,
standard controlled, and interaction controlled. These models were
constructed using a hypothetical cohort of 10,000 individuals,
incorporating baseline exposure to green spaces (\(A_0\)), baseline
happiness (\(Y_0\)), baseline confounders (\(L_0\)), and an unmeasured
confounder (\(U\)). The detailed simulation process and model
construction are given in
\hyperref[appendix-simulate-longitudinal-ate]{Appendix D}.

The ATE estimates from these models provide critical insights into the
effects of green space exposure on individual happiness while accounting
for various confounding factors. The model without control variables
estimated ATE = 1.55, CI = {[}1.47, 1.63{]}, significantly
overestimating the treatment effect. Incorporating standard covariate
control reduced this estimate to ATE = 0.86, CI = {[}0.8, 0.92{]},
aligning more closely with the expected effect but still overestimating.
Most notably, the model that included interactions among baseline
exposure, outcome, and confounders yielded ATE = 0.29, CI = {[}0.27,
0.31{]}, approximating the true effect of 0.3. This finding underscores
the importance of including baseline values of the exposure and outcome
wherever these data are available.

\subsubsection{4.3 Recommendations for Conducting and Reporting Causal
Analyses with Longitudinal
Data}\label{recommendations-for-conducting-and-reporting-causal-analyses-with-longitudinal-data}

Longitudinal data offer strong advantages for causal inference by
enabling researchers to establish the relative timing of confounders,
treatments, and outcomes. The temporal sequence of events is crucial for
establishing causality because causality occurs in time. The following
recommendations aim to guide researchers in leveraging longitudinal data
effectively to conduct and report causal analyses:

\paragraph{1. Draw multiple causal
diagrams}\label{draw-multiple-causal-diagrams-1}

\begin{itemize}
\tightlist
\item
  \textbf{Identification problem diagram}: begin by constructing a
  causal diagram that outlines your initial assumptions about the
  relationships among variables, identifying potential confounders and
  mediators. This diagram should illustrate the complexity of the
  identification problem.
\item
  \textbf{Solution diagram}: next, create a separate causal diagram that
  proposes solutions to the identified problems. This may involve
  highlighting variables for conditioning to isolate the causal effect
  of interest or suggesting novel pathways for investigation. Having
  distinct diagrams for the problem and its proposed solutions clarifies
  your study's analytic strategy and theoretical underpinning.
\end{itemize}

Table~\ref{tbl-lg} provides an example of a table with multiple causal
diagrams clarifying potential sources of confounding threats and reports
strategies for addressing them.

\paragraph{2. Attempt longitudinal designs with at least three waves of
data}\label{attempt-longitudinal-designs-with-at-least-three-waves-of-data}

Incorporating data from at least three intervals considerably enhances
your ability to infer causal relationships. This approach allows for the
examination of temporal precedence and lagged effects. For example, by
adjusting for physical activity measured before the treatment, we can
ensure that physical activity does not result from a new initiation to
green spaces, which we establish by measuring green space access at
baseline. Establishing chronological order in the temporal sequence of
events allows us to avoid confounding problems 1-4 in \textbf{?@tbl-04}.

\paragraph{3. Calculate Average Treatment Effects for a clearly
specified target
population}\label{calculate-average-treatment-effects-for-a-clearly-specified-target-population}

Estimating the average treatment effect (ATE) across the entire study
population provides a comprehensive measure of the intervention's
effects. This step is crucial for understanding the treatment's overall
effect and generalising findings to broader populations.

\paragraph{4. Where causality is unclear, report results for multiple
causal
graphs}\label{where-causality-is-unclear-report-results-for-multiple-causal-graphs}

Given that the true causal structure may be complex and partially
unknown, analysing and reporting results under each plausible causal
diagram is prudent. This practice acknowledges the uncertainty inherent
in causal modelling and demonstrates the robustness of findings across
different theoretical frameworks.

\paragraph{5. Conduct sensitivity
analyses}\label{conduct-sensitivity-analyses}

Sensitivity analyses are essential for assessing the robustness of your
findings to various assumptions within the causal model. These analyses
can include simulations, as illustrated in Appendices C and D, to
examine bias arising of unmeasured confounding, model misspecification,
and alternative causal pathways on the study conclusions. Sensitivity
analyses help to identify the conditions under which the findings hold,
enhancing the credibility of the causal inferences. (For more about
addressing missing data, see:
(\citeproc{ref-bulbulia2024PRACTICAL}{Bulbulia 2024}).)

\paragraph{6. Address missing data at baseline and study
attrition}\label{address-missing-data-at-baseline-and-study-attrition}

Longitudinal studies often need help with missing data and attrition,
which can introduce bias and affect the validity of causal inferences.
Implement and report strategies for handling missing data, such as
multiple imputation or sensitivity analyses that assess the bias arising
from missing responses at the study's conclusion. (For more about
addressing missing data, see:
(\citeproc{ref-bulbulia2024PRACTICAL}{Bulbulia 2024})).

By following these recommendations, you will more effectively navigate
the inherent limitations of observational longitudinal data, improving
the quality of your causal inferences.

\subsection{4 Summary}\label{summary}

Although powerful aides, causal directed acyclic graphs may encourage
false confidence wherever causal questions are ill-defined, the
structures of the world are uncertain, data-quality are poor,
statistical estimators are inadequate, or statistical models are
misspecified.

\subsubsection{On the priority of
assumptions.}\label{on-the-priority-of-assumptions.}

You might wonder, ``If not from the data, where do our assumptions about
causality come from?'' This question will come up repeatedly throughout
the course. The short answer is that our assumptions are based on
existing knowledge. This reliance on current knowledge might seem
counterintuitive for buiding scientific knowledge-\/--- shouldn't we use
data to build knowledge, not the other way around? Yes, but it is not
that straightforward. Data often hold the answers we're looking for but
can be ambiguous. When the causal structure is unclear, it is important
to sketch out different causal diagrams, explore their implications,
and, if necessary, conduct separate analyses based on these diagrams.

Otto Neurath, an Austrian philosopher and a member of the Vienna Circle,
famously used the metaphor of a ship that must be rebuilt at sea to
describe the process of scientific theory and knowledge development.

\begin{quote}
Duhem has shown \ldots{} that every statement about any happening is
saturated with hypotheses of all sorts and that these in the end are
derived from our whole world-view. We are like sailors who on the open
sea must reconstruct their ship but are never able to start afresh from
the bottom. Where a beam is taken away a new one must at once be put
there, and for this the rest of the ship is used as support. In this
way, by using the old beams and driftwood, the ship can be shaped
entirely anew, but only by gradual reconstruction.
(\citeproc{ref-neurath1973}{Neurath 1973 p. 199})
\end{quote}

This quotation emphasises the iterative process that accumulates
scientific knowledge; new insights are cast from the foundation of
existing knowledge. Causal diagrams are at home in Neurath's boat. The
tradition of science that believes that knowledge develops from the
results of statistical tests applied to data should be resisted. The
data alone typically do not contain the answers we seek.

\newpage{}

\subsection{Appendix A: Glossary}\label{appendix-a-glossary}

\begin{table}

\caption{\label{tbl-experiments}Glossary}

\centering{

\glossaryTerms

}

\end{table}%

\subsection{Appendix B:}\label{appendix-b}

\subsection{Examples of common causal
questions}\label{examples-of-common-causal-questions}

\begin{table}

\caption{\label{tbl-common-interests}Common causal questions}

\centering{

\terminologycommoncausalinterests

}

\end{table}%

\subsection{Effect Modification}\label{effect-modification}

\begin{table}

\caption{\label{tbl-common-interests}representing effect modification}

\centering{

\terminologyeffectmodification

}

\end{table}%

\begin{table}

\caption{\label{tbl-common-interests}Common causal questions}

\centering{

\terminologyeffectmodificationtypes

}

\end{table}%

\newpage{}

\subsection{Appedix C:}\label{appedix-c}

\subsection{Time-varying Confounding: Causal
Mediation}\label{time-varying-confounding-causal-mediation}

\begin{table}

\caption{\label{tbl-mediation}Anatomy of bias in mediation analysis:
statistical SEM fails.}

\centering{

\mediationfull

}

\end{table}%

\newpage{}

\subsection{Appedix D}\label{appedix-d}

\subsubsection{Time-varying Confounding: Treatment Confounder
Feedback}\label{time-varying-confounding-treatment-confounder-feedback}

\begin{figure}

\centering{

\feedbackA

}

\caption{\label{fig-timevarying-amplification}Treatment-confounder
feedback: statistical SEM fails.}

\end{figure}%

\newpage{}

\subsubsection{Time-varying Confounding in the Absence of
Treatment-confounder
feedback}\label{time-varying-confounding-in-the-absence-of-treatment-confounder-feedback}

\begin{figure}

\centering{

\feedbackB

}

\caption{\label{fig-timevarying-nofeedback}Anatomy of bias in
treatment-confounder feedback}

\end{figure}%

\newpage{}

\subsection{Appendix E}\label{appendix-e}

Pearl's Do-Calculus and Structural Equation Models

\newpage{}

\phantomsection\label{refs}
\begin{CSLReferences}{1}{0}
\bibitem[\citeproctext]{ref-bareinboim2013general}
Bareinboim, E, and Pearl, J (2013) A general algorithm for deciding
transportability of experimental results. \emph{Journal of Causal
Inference}, \textbf{1}(1), 107--134.

\bibitem[\citeproctext]{ref-bulbulia2024PRACTICAL}
Bulbulia, J (2024) A practical guide to causal inference in three-wave
panel studies. \emph{PsyArXiv Preprints}.
doi:\href{https://doi.org/10.31234/osf.io/uyg3d}{10.31234/osf.io/uyg3d}.

\bibitem[\citeproctext]{ref-bulbulia2023}
Bulbulia, JA (2023) Causal diagrams (directed acyclic graphs): A
practical guide.

\bibitem[\citeproctext]{ref-diaz2021nonparametric}
Dı́az, I, Hejazi, NS, Rudolph, KE, and Der Laan, MJ van (2021)
Nonparametric efficient causal mediation with intermediate confounders.
\emph{Biometrika}, \textbf{108}(3), 627--641.

\bibitem[\citeproctext]{ref-Diaz2023}
Dı́az, I, Williams, N, and Rudolph, KE (2023) \emph{Journal of Causal
Inference}, \textbf{11}(1), 20220077.
doi:\href{https://doi.org/doi:10.1515/jci-2022-0077}{doi:10.1515/jci-2022-0077}.

\bibitem[\citeproctext]{ref-greenland2003quantifying}
Greenland, S (2003) Quantifying biases in causal models: Classical
confounding vs collider-stratification bias. \emph{Epidemiology},
300--306.

\bibitem[\citeproctext]{ref-hernan2023}
Hernan, MA, and Robins, JM (2023) \emph{Causal inference}, Taylor \&
Francis. Retrieved from
\url{https://books.google.co.nz/books?id=/_KnHIAAACAAJ}

\bibitem[\citeproctext]{ref-hernuxe1n2008a}
Hernán, MA, Alonso, A, Logan, R, \ldots{} Robins, JM (2008)
Observational studies analyzed like randomized experiments: An
application to postmenopausal hormone therapy and coronary heart
disease. \emph{Epidemiology}, \textbf{19}(6), 766.
doi:\href{https://doi.org/10.1097/EDE.0b013e3181875e61}{10.1097/EDE.0b013e3181875e61}.

\bibitem[\citeproctext]{ref-hernuxe1n2006}
Hernán, MA, and Robins, JM (2006) Estimating causal effects from
epidemiological data. \emph{Journal of Epidemiology \& Community
Health}, \textbf{60}(7), 578--586.
doi:\href{https://doi.org/10.1136/jech.2004.029496}{10.1136/jech.2004.029496}.

\bibitem[\citeproctext]{ref-hernan2017per}
Hernán, MA, Robins, JM, et al. (2017) Per-protocol analyses of pragmatic
trials. \emph{N Engl J Med}, \textbf{377}(14), 1391--1398.

\bibitem[\citeproctext]{ref-hernuxe1n2016}
Hernán, MA, Sauer, BC, Hernández-Díaz, S, Platt, R, and Shrier, I (2016)
Specifying a target trial prevents immortal time bias and other
self-inflicted injuries in observational analyses. \emph{Journal of
Clinical Epidemiology}, \textbf{79}, 70--75.

\bibitem[\citeproctext]{ref-hernuxe1n2022}
Hernán, MA, Wang, W, and Leaf, DE (2022) Target trial emulation: A
framework for causal inference from observational data. \emph{JAMA},
\textbf{328}(24), 2446--2447.
doi:\href{https://doi.org/10.1001/jama.2022.21383}{10.1001/jama.2022.21383}.

\bibitem[\citeproctext]{ref-holland1986}
Holland, PW (1986) Statistics and causal inference. \emph{Journal of the
American Statistical Association}, \textbf{81}(396), 945--960.

\bibitem[\citeproctext]{ref-hume1902}
Hume, D (1902) \emph{Enquiries Concerning the Human Understanding: And
Concerning the Principles of Morals}, Clarendon Press.

\bibitem[\citeproctext]{ref-imai2008misunderstandings}
Imai, K, King, G, and Stuart, EA (2008) Misunderstandings between
experimentalists and observationalists about causal inference.
\emph{Journal of the Royal Statistical Society Series A: Statistics in
Society}, \textbf{171}(2), 481--502.

\bibitem[\citeproctext]{ref-lash2020}
Lash, TL, Rothman, KJ, VanderWeele, TJ, and Haneuse, S (2020)
\emph{Modern epidemiology}, Wolters Kluwer. Retrieved from
\url{https://books.google.co.nz/books?id=SiTSnQEACAAJ}

\bibitem[\citeproctext]{ref-lauritzen1990}
Lauritzen, SL, Dawid, AP, Larsen, BN, and Leimer, H-G (1990)
Independence properties of directed {M}arkov fields. \emph{Networks},
\textbf{20}(5), 491--505.

\bibitem[\citeproctext]{ref-lewis1973}
Lewis, D (1973) Causation. \emph{The Journal of Philosophy},
\textbf{70}(17), 556--567.
doi:\href{https://doi.org/10.2307/2025310}{10.2307/2025310}.

\bibitem[\citeproctext]{ref-mcelreath2020}
McElreath, R (2020) \emph{Statistical rethinking: A {B}ayesian course
with examples in r and stan}, CRC press.

\bibitem[\citeproctext]{ref-montgomery2018}
Montgomery, JM, Nyhan, B, and Torres, M (2018) How conditioning on
posttreatment variables can ruin your experiment and what to do about
It. \emph{American Journal of Political Science}, \textbf{62}(3),
760--775.
doi:\href{https://doi.org/10.1111/ajps.12357}{10.1111/ajps.12357}.

\bibitem[\citeproctext]{ref-neal2020introduction}
Neal, B (2020) Introduction to causal inference from a machine learning
perspective. \emph{Course Lecture Notes (Draft)}. Retrieved from
\url{https://www.bradyneal.com/Introduction_to_Causal_Inference-Dec17_2020-Neal.pdf}

\bibitem[\citeproctext]{ref-neurath1973}
Neurath, O (1973) Anti-spengler. In M. Neurath and R. S. Cohen, eds.,
\emph{Empiricism and sociology}, Dordrecht: Springer Netherlands,
158--213.
doi:\href{https://doi.org/10.1007/978-94-010-2525-6_6}{10.1007/978-94-010-2525-6\_6}.

\bibitem[\citeproctext]{ref-pearl1988}
Pearl, J (1988) \emph{Probabilistic reasoning in intelligent systems:
Networks of plausible inference}, Morgan kaufmann.

\bibitem[\citeproctext]{ref-pearl1995}
Pearl, J (1995) Causal diagrams for empirical research.
\emph{Biometrika}, \textbf{82}(4), 669--688.

\bibitem[\citeproctext]{ref-pearl2009a}
Pearl, J (2009) \emph{Causality}, Cambridge University Press.

\bibitem[\citeproctext]{ref-pearl2022}
Pearl, J, and Bareinboim, E (2022) External validity: From do-calculus
to transportability across populations. In, 1st edn, Vol. 36, New York,
NY, USA: Association for Computing Machinery, 451--482. Retrieved from
\url{https://doi.org/10.1145/3501714.3501741}

\bibitem[\citeproctext]{ref-richardson2013}
Richardson, TS, and Robins, JM (2013a) Single world intervention graphs:
A primer. In, Citeseer.

\bibitem[\citeproctext]{ref-richardson2013swigsprimer}
Richardson, TS, and Robins, JM (2013b) Single world intervention graphs:
A primer. In \emph{Second UAI workshop on causal structure learning,
{B}ellevue, {W}ashington}, Citeseer. Retrieved from
\url{https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=07bbcb458109d2663acc0d098e8913892389a2a7}

\bibitem[\citeproctext]{ref-richardson2023potential}
Richardson, TS, and Robins, JM (2023) Potential outcome and decision
theoretic foundations for statistical causality. \emph{Journal of Causal
Inference}, \textbf{11}(1), 20220012.

\bibitem[\citeproctext]{ref-robins2010alternative}
Robins, JM, and Richardson, TS (2010) Alternative graphical causal
models and the identification of direct effects. \emph{Causality and
Psychopathology: Finding the Determinants of Disorders and Their Cures},
\textbf{84}, 103--158.

\bibitem[\citeproctext]{ref-rubin1976}
Rubin, DB (1976) Inference and missing data. \emph{Biometrika},
\textbf{63}(3), 581--592.
doi:\href{https://doi.org/10.1093/biomet/63.3.581}{10.1093/biomet/63.3.581}.

\bibitem[\citeproctext]{ref-rudolph2024mediation}
Rudolph, KE, Williams, NT, and Diaz, I (2024) {Practical causal
mediation analysis: extending nonparametric estimators to accommodate
multiple mediators and multiple intermediate confounders}.
\emph{Biostatistics}, kxae012.
doi:\href{https://doi.org/10.1093/biostatistics/kxae012}{10.1093/biostatistics/kxae012}.

\bibitem[\citeproctext]{ref-shpitser2016causal}
Shpitser, I, and Tchetgen, ET (2016) Causal inference with a graphical
hierarchy of interventions. \emph{Annals of Statistics}, \textbf{44}(6),
2433.

\bibitem[\citeproctext]{ref-stuart2018generalizability}
Stuart, EA, Ackerman, B, and Westreich, D (2018) Generalizability of
randomized trial results to target populations: Design and analysis
possibilities. \emph{Research on Social Work Practice}, \textbf{28}(5),
532--537.

\bibitem[\citeproctext]{ref-webster2021directed}
Webster-Clark, M, and Breskin, A (2021) Directed acyclic graphs, effect
measure modification, and generalizability. \emph{American Journal of
Epidemiology}, \textbf{190}(2), 322--327.

\bibitem[\citeproctext]{ref-westreich2010}
Westreich, D, and Cole, SR (2010) Invited commentary: positivity in
practice. \emph{American Journal of Epidemiology}, \textbf{171}(6).
doi:\href{https://doi.org/10.1093/aje/kwp436}{10.1093/aje/kwp436}.

\bibitem[\citeproctext]{ref-westreich2019target}
Westreich, D, Edwards, JK, Lesko, CR, Cole, SR, and Stuart, EA (2019)
Target validity and the hierarchy of study designs. \emph{American
Journal of Epidemiology}, \textbf{188}(2), 438--443.

\bibitem[\citeproctext]{ref-westreich2017}
Westreich, D, Edwards, JK, Lesko, CR, Stuart, E, and Cole, SR (2017)
Transportability of trial results using inverse odds of sampling
weights. \emph{American Journal of Epidemiology}, \textbf{186}(8),
1010--1014.
doi:\href{https://doi.org/10.1093/aje/kwx164}{10.1093/aje/kwx164}.

\bibitem[\citeproctext]{ref-westreich2013}
Westreich, D, and Greenland, S (2013) The table 2 fallacy: Presenting
and interpreting confounder and modifier coefficients. \emph{American
Journal of Epidemiology}, \textbf{177}(4), 292--298.

\end{CSLReferences}



\end{document}
