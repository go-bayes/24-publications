---
title: "Confounding in Experiments"
abstract: |
  Confounding bias arises when a treatment and outcome share a common cause. In randomised controlled trials (Experiments) treatment assignment is random. It would appear there can be no confounding bias in experiments. Here we use causal directed acyclic graphs to clarify seven structural sources of bias in randomised controlled trials. 

   **KEYWORDS**: *Causal Inference*; *Experiments*; *DAGs*;* *Evolution*; *Per Protocol Effect*; *Intention to Treat Effect*.
author: 
  - name: Joseph A. Bulbulia
    affiliation: Victoria University of Wellington, New Zealand
    orcid: 0000-0002-5861-2056
    email: joseph.bulbulia@vuw.ac.nz
    corresponding: no
editor_options: 
  chunk_output_type: console
format:
  pdf:
    sanitise: true
    keep-tex: true
    link-citations: true
    colorlinks: true
    documentclass: article
    classoption: [single column]
    lof: false
    lot: false
    geometry:
      - top=30mm
      - left=25mm
      - heightrounded
      - headsep=22pt
      - headheight=11pt
      - footskip=33pt
      - ignorehead
      - ignorefoot
    template-partials: 
      - /Users/joseph/GIT/templates/quarto/title.tex
    header-includes:
      - \input{/Users/joseph/GIT/latex/latex-for-quarto.tex}
date: last-modified
execute:
  echo: false
  warning: false
  include: true
  eval: true
fontfamily: libertinus
bibliography: /Users/joseph/GIT/templates/bib/references.bib
csl: /Users/joseph/GIT/templates/csl/camb-a.csl
---


```{r}
#| label: load-libraries
#| echo: false
#| include: false
#| eval: true

## WARNING SET THIS PATH TO YOUR DATA ON YOUR SECURE MACHINE. 
# pull_path <-
#   fs::path_expand(
#     #"/Users/joseph/v-project\ Dropbox/Joseph\ Bulbulia/00Bulbulia\ Pubs/DATA/nzavs_refactor/nzavs_data_23"
#     "/Users/joseph/Library/CloudStorage/Dropbox-v-project/Joseph\ Bulbulia/00Bulbulia\ Pubs/DATA/nzavs-current/r-data/nzavs_data_qs"
#   )
# 


push_mods <-  fs::path_expand(
  "/Users/joseph/Library/CloudStorage/Dropbox-v-project/data/nzvs_mods/24/church-prosocial-v7"
)


#tinytext::tlmgr_update()

# WARNING:  COMMENT THIS OUT. JB DOES THIS FOR WORKING WITHOUT WIFI
#source("/Users/joseph/GIT/templates/functions/libs2.R")
# # WARNING:  COMMENT THIS OUT. JB DOES THIS FOR WORKING WITHOUT WIFI
# source("/Users/joseph/GIT/templates/functions/funs.R")

#ALERT: UNCOMMENT THIS AND DOWNLOAD THE FUNCTIONS FROM JB's GITHUB

# source(
#   "https://raw.githubusercontent.com/go-bayes/templates/main/functions/experimental_funs.R"
# )
# 
# source(
#   "https://raw.githubusercontent.com/go-bayes/templates/main/functions/funs.R"
# )

# check path:is this correct?  check so you know you are not overwriting other directors
#push_mods

# for latex graphs
# for making graphs
library("tinytex")
library("extrafont")
library("tidyverse")
library("kableExtra")
#devtools::install_github("go-bayes/margot")
library(margot)
loadfonts(device = "all")
```



## Introduction

Here, we consider seven structural sources of bias in randomised controlled trials, henceforth "experiments." The examples are not exhaustive. We do not consider biases from selection into study, which may threaten generalisation. Nor do we consider censoring biases from study attrition, in which population characteristics of the restricted sample at the end-of-study differ from those of the sample at baseline in respect of treatment effects. Both types of sample/target population can invalidate experimental results. However such biases present as failure modes in external validity. Our focus here will be on threats to internal valid from confounding that relate the treatment to the outcome in the absence of confouding.  We will assume large samples such that random differences in the distribution of variables that may affect treatment do not come into play. We will also assume that the experimental designs are double-blind, that treatment conditions are the same across all arms, and that the investigators are careful and scupulous. No chance event, other than randomisation, is left to chance. 

"Doesn't randomisation, by its very nature, eliminate all systematic causes of treatment assignment and so of treatment assigment and outcome?" 

We assume the answer is yes. 

"Doesn't this mean that confounding is ruled out?"  

The answer is no.  Eight examples illustrate why. Understanding how confounding arises in experiments is important for experimental design, data-analysis, and inference. However, the examples clarify problems of general interest about how causality operates over time, confounding where the treatment affects attrition, non-compliance, and non-response, and differences between the effects of randomisation (intention to treat effects) and the causal effects of treatments themselves (per-protocol effects).

We begin by definining our terms.

### Terminology

**Confounding**: treatment and outcome are associated independently of causality or are disassociated in the presence of causality relative to causal question at hand.

**Intention-to-Treat Effect**: The effect of treatment assignment, analysed based on initial treatment assignment, reflecting real-world effectiveness but possibly obscuring mechanisms.

**Per-Protocol Effect**: The causal effect of treatment under full treatment adherence.


### Meaning of Symbols

We use the following conventions in our directed acyclic graphs:

**$A$**: Denotes the "treatment" or "exposure" - a random variable.

This is the variable for which we seek to understand the effect of intervening on it. It is the "cause."

**$\bar{A}$**: Denotes a sequence of treatments.

**$Y$**: Denotes the outcome or response, measured at the end of study.

It is the "effect."

**$L$**: Denotes a measured confounder or set of confounders.

**$U$**: Denotes an unmeasured confounder or confounders.

**$\mathcal{R}$**: Denotes a randomisation to treatment condition.

**$\mathcal{G}$**: Denotes a causal graph, here, either a causal DAG, Single World Intervention Graph. We will also use this symbol to denote a Single World Intervention Template, which is a function for generating Single World Intevention Graphs. This jargon will become clear when the graphical tools are explained. 


### Elements of causal graphs

**Node**: a node or vertex represents characteristics or features of units within a population on a causal diagram -- that is a "variable." In causal directed acyclic graphs, we draw nodes with respect to the *target population*, which is the population for whom investigators seek causal inferences [@suzuki2020]. Time-indexed node:  $X_t$ denotes relative chronology

**Edge without an Arrow** ($\association$): path of association, we do not assert causality.

**Arrow** ($\rightarrowNEW$): denotes causal relationship from the node at the base of the arrow (a 'parent') to the node at the tip of the arrow (a 'child'). In causal DAGS it is conventional to refrain from drawing an arrow from treatment to outcome to avoid asserting a causal path from $A$ to $Y$ because iyr purpose is to ascertain whether causality can be identified for this path. All other nodes and paths -- including the absence of nodes and paths -- is typically assumed.

**Red Arrow** ($\rightarrowred$): path of non-causal association between the treatment and outcome. Despite the arrows, this path is associational and may flow against time.

**Dashed Arrow** ($\rightarrowdotted$): denotes a true association between the treatment and outcome that becomes partially obscured when conditioning on a mediator, assuming $A$ causes $Y$.

**Dashed Red Arrow** ($\rightarrowdottedred$): highlights over-conditioning bias from conditioning on a mediator.

<!-- **Open Blue Arrow** ($\rightarrowblue$): highlights effect modification, which occurs when the levels of the effect of treatment vary within levels of a covariate. We do not assess the causal effect of the effect-modifier on the outcome, recognising that it may be incoherent to consider intervening on the effect-modifier. -->

**Boxed Variable** $\big(\boxed{X}\big)$: conditioning or adjustment for $X$. 

**Red-Boxed Variable** $\big(\boxedred{X}\big)$: highlights the source of confounding bias from adjustment.

**Dashed Circle** $\big( \circledotted{X}\big)$: no adjustment is made for a variable (implied for unmeasured confounders.)

**$\mathbf{\mathcal{R}}$**  $\big(\mathcal{R} \rightarrow A\big)$ randomisation into the treatment condition.


### Review of d-separation for Causal Identification on a Graph


::: {#tbl-fiveelementary}

```{=latex}
\terminologydirectedgraph
```
The five elementary structures of causality from which all causal directed acyclic graphs can be built.
:::


## Eight Example of Confounding Bias in Experiments


::: {#tbl-terminologyelconfoundersexperiments}
```{=latex}
\terminologyelconfoundersexperiments
```
Eight confounding biases in Randomised Controlled Trials.
:::


### Example 1: Post-treatment adjustment blocks treatment effect

@tbl-terminologyelconfoundersexperiments $\mathcal{G} 1.1$ illustrates the threat of confounding bias by conditioning on a post-treatment mediator. Imagine investigators are interested in whether the framing of an authority as religious or secular -- "source framing" -- affects affects subjective ratings of confidence in the authority -- "source credibility."  There are two conditions. A claim is presented from an authority. The content of the claim does not vary by condition. Participants are asked to rate the claim on a credibility scale. Next, imagine that the investigators decide they should control for religiosity.  Furthermore imagine there is a true effect of source-framing.  Finally, assume that the source framing not only affects source credibility but also affects relgiosity.  Perhaps viewing a religious authority makes religious people more religious. In this scenario, measuring religiosity following the exeperimental invertion will partially block the effect of the treatment on the outcome. It might make it appear that the treatment does not work for religious people, when in reality it works because it amplifies religiosity.  (Note that in this graph we assume that $L_1$ occurs before $Y_2$, however investigators may have measured $L_1$ after $Y_2$. Our time-index pertains to the occurance of the event, not of its measurement. This statement applies to all examples that follow.)


@tbl-terminologyelconfoundersexperiments $\mathcal{G} 1.2$clarifies a response: do not control post-treatment variables, here the intermediary effects of "religiosity".  If effect-modification by religiosity is scientifically interesting, measure religiosity before randomisation. Randomisation did not prevent confounding.



### Example 2: Post-treatment adjustment induces collider stratification bias

@tbl-terminologyelconfoundersexperiments $\mathcal{G} 2.1$ illustrates the threat of confounding bias by conditioning on a post-treatment collider.  Imagine the same experiment as in Example 1 and the same conditioning strategy in which religiosity is measured following the treatment. We imagine the treatment affects religiosity. However, in this example, imagine that religiosity has no causal effect on the outcome, source credibility. Finally, imagine that an unmeasured variable affects both the mediator, religiosity ($L_1$), and the outcome, source-credibility ($Y_2$). This unmeasured confounder might be religious education in childhood.  In this scenario, conditioning on the post-treatment variable religiosity will open a backdoor path between the treatment and outcome, leading to association in the absence of causation. Randomisation did not prevent confounding. 

@tbl-terminologyelconfoundersexperiments $\mathcal{G} 2.2$ clarifies a response: do not control post-treatment variables.

The point that investigators should not condition on post-treatment variables is worth developing using a common flaw in experimental designs: exclusion from 'attention checks.'' Consider that if an experimental condition affects attention and an unmesaured variable is a common cause of attention and the outcome, then selection on attention will induce confounding bias in a randomised experiment. For example, imagine that people are more attentive in the scientific authority design, because science is interesting -- whether or not one is religious, yet religion is not interesting whether or not one is religious. Suppose further that an unmeasured "altruistic disposition" affects both attention and ratings of source credibility.  By selecting on attention, investigators may unwittingly destroy randomisation. If attention is a scientifically interesting effect-modifier it should be measured before random assignment to treatment. 


### Example 3: Demographic measures at end of study induce collider stratification bias

@tbl-terminologyelconfoundersexperiments $\mathcal{G} 3.1$ illustrates the threat of confounding bias from adjusting for post-treatment variables, here, one affected by the treatment and outcome absent any unmeasured confounder. In our example, imagine both the treatment, source framing, and the outcome, source credibility, affect religiosity measured at the end of study.  Investigators measure religiosity at the end of study and include this measure as a covariate. However, doing so induces collider bias such that if both the treatment and outcome are positively associated with religiosity, the collider, they will be negatively associated with each other. Conditioning on the collider risks the illusion of a negative experimental effect in the absence of casuality. 


@tbl-terminologyelconfoundersexperiments $\mathcal{G} 3.2$ clarifies a response: again, do not control post-treatment variables! -- here "religiosity" measured after the end of study. If the scientific interest is in effect-modification or in obtaining statistical precision, measure covariates before randomisation.


### Example 4: Demographic measures at end of study condition on a collider that opens a back-door path. 

@tbl-terminologyelconfoundersexperiments $\mathcal{G} 4.1$ illustrates the threat of illustrates the threat of confounding bias by adjusting for post-treatment variables, here that is affect only by the treatment and an unmeasured cause of the outcome. Suppose source crediblity affects religiosity (religious people are reminded of thier faith), but there is no experimental affect of framing on credibility.  Imagine further that an unmeasured common cause of the covariate religiosity and the outcome source credibility. Again, this unmeasured confounder might be religious education in childhood.  In this scenario, conditioning on the post-treatment variable religiosity will open a backdoor path between the treatment and outcome, leading to association in the absence of causation. Again we find that that andomisation did not prevent confounding. 

@tbl-terminologyelconfoundersexperiments $\mathcal{G} 4.2$ clarifies a response. Again, unless investigators are able to rule out an effect of treatment, they should not condition on a post-treatment covarite. The covariates of interest should be measured before randomisation. 


### Example 5: Treatment affects attrition biasing measure of outcome


@tbl-terminologyelconfoundersexperiments $\mathcal{G} 5$ Suppose that the experimentally condition affects measurement error in self-reported source credibility $U_{\Delta Y}$.  For example, suppose that source framing has no effect on credibility.  However, those in the scientific authority condition are more likely to express credibilty for science from self-presentation bias. Likewise, perceiving the investigators to be irreligious, participants in the religious source framing condition might report less credibility for religious authorities than they secretly harbour. Directed measurement error from the treatment to the measurement error of the outcomes creates an association in the absence of true causality, which we denote by removing any arrow between the treatment $A$ and the true outcome $Y$.

@tbl-terminologyelconfoundersexperiments $\mathcal{G} 5$ reveals there is no easy solution to directed measurement error bias. If the magnitude of bias were known, investigators could apply adjustments. Additional experiments might be devised that are insensitive to directed measurement error bias. Investigators might compute sensitivity analysis to example how much measurement error bias would be required to explain away a results (refer to @linden2020EVALUE for relatively easy-to-implement sensitivity analysis.)  The point we make here is that randomisation does not prevent confounding by directed measurement error bias. Investigators must be vigilent. 


### Example 6: Per protocol effect lost in sustained treatments where treatment adherence is affected by a measured confounder

Setting aside self-inflicted injuries of post-treatment conditioning and directed measurement error, randomisation recovers unbiased causal effect estimates for randomisation into treatment.  Under perfect adherence, these effect estimates correspond to the causal effects of the treatments themselves. However, adherence is seldom perfect.  The following examples reveal challenges for recovering per-protocol effects in settings where there is imperfect adherence. @tbl-terminologyelconfoundersexperiments $\mathcal{G} 6-8$  are adapted from @hernan2017per. 


@tbl-terminologyelconfoundersexperiments $\mathcal{G} 6$ illustrates the threat for identifying the per-protocol effect in sustained treatments where treatment adherence is affected by a measured confounder.  Consider a sequential experiment that investigates the effects of sustained adherence to yoga on psychological distress, measured at the end of study.  Suppose that inflexible people are less likely to adhere the protocols set out in the experiment, and so do not. Suppose that flexibility is measured by indicator $L$.  If we do not codition on $L$ there is an open path from $A_1 \association L_0 \association U \association Y_2$.  Although investigators may recover the effect of randomisation into treatment, the per-protocol effect is confounded. 

@tbl-terminologyelconfoundersexperiments $\mathcal{G} 6$ also clarifies a response. Conditioning on $L_0$ and $L_1$ will block the backdoor path, leading to an unbiased per-protocol effect estimate. 



### Example 7: Per protocol effect lost in sustained treatments where past treatments affect measured confounder of future treatment adherence


@tbl-terminologyelconfoundersexperiments $\mathcal{G} 7$ illustrates the threat for identifying the per-protocol effect in sustained treatments where past treatments affect measured confounder of future treatment adherence.  Suppose that yoga affects flexibility. We should condition on pre-treatment measures of flexibility to identify the per-protocal effect. However, conditioning on the post-treatment measure of flexibilty, $\boxed{L_1}$ induces collider stratification bias. This path runs from $A_1 \association \L_1 \association U \association Y_3$. However, if we do not condition on $L_1$ there is an open backdoor path from $A_1 \association U \association Y_3$.  We cannot estimate a per-protocal effect by conditioning strategies.  


@tbl-terminologyelconfoundersexperiments $\mathcal{G} 7$ does not clarify the response.  However, in a sequential treatment with fixed strategies, in which there is sequential exchangeability -- or no unmeasured confounding at each time point -- valid estimators for the sequential treatments may be constructed (refer to @hernan2024WHATIF; @diaz2021nonparametric; @hoffman2023). Although we may niavely obtain an intention-to-treat effect estimate withouth special methods, infering an effect of doing yoga on well-being -- the per-protocal effect, requires special methods. These methods are not routinely used in the human sciences. 


### Example 8: Per protocol effect lost in sustained treatments because both measured and unmeasured confounders affect treatment adherence

@tbl-terminologyelconfoundersexperiments $\mathcal{G} 8$ llustrates the threat for identifying the per-protocol effect in sustained treatments where there is both measured and unmeasured confounders.  Suppose flexibility affects adherence, that yoga affects flexibilty, and that an unmeasured variable, say prejudice toward eastern spiritual practices affects adherence.  We have no measures for this variable. There is unmeasured confounding. 


If there were no effect of yoga on well-being except through flexibility, and if flexibility were not affected by the unmeasured antipathy toward eastern spiritual practices, and further if the effect of flexibilty on yoga at each time point were condititionally independent of all future counterfactual data, both for the treatments and the outcomes, then it might be possibile to construct special estimators that identify the per-protocol effect of yoga on well-being in the presence of unmeasured confounding that affects adherence (refer to @hernan2017per). Clearly we have come a long way from the ANOVAs routinely deployed in experimental studies. However, if we seek to understand the effect of yoga on well-being and not the effect of random-assignment to yoga on well-being, we require special estimators. 


## Conclusions

The examples we have considered here hardly exhaust threats to causal inference in experiments. Wherever the sample at the end of study differs in the distribution effect modifiers from the sample population at the start of study, results will not generalise as we hope. Such bias goes by different names, such as selection-bias or selection-restriction bias or sample restriction bias. We have not considered these threats. It does not take much imagination to imagine threats to valid inference beyond those considered here.  However, I hope the eight examples considered here persuade investigators of the following: 

First, confounding biases are possible in randomised experiments even when randomisation succeeds. 

Second, causal directed acyclic graphs are useful for clarifying these biases. 

Third, many such biases are self-inflicted. These self-inflicted biases arise from conditioning on variables that may be affected by treatment assignment. If an experiment consists of a single treatment, unless investigators are certain that treatments to do not affect covariates, covariate data should be collected before randomisation. 

Fourth, "attention checks" should not be used to select participants after treatments have been randomised. If attention is a relevant covariate, measures should be taken before randomisation. 

Fifth, investigators should not adopt naive practices of infering per-protocal effects from the portion of the sample that has followed experimental protocols. Not only is such selection nearly guaranteed to results in differences between the study population at the start and end of study, compromising external validity, we have considered that both measured and unmeasured confounders may invalidate per-protocol results for the retained sample.  

Sixth, methods for identifying causal effects in observational settings may be useful for the identification of causal effects in randomised experiments because after randomisation, every experiment becomes and observational study. 

Seventh, the points that we consider here for experiments apply to observational studies that have obtain psuedorandomisation through baseline adjustments. Standardly employed methods in the observational science such as structural equation models or multi-level models will encounter the same problems that arise in experimental steetings with sustained treatment strategies.  In an aphorism, every observational study becomes an observational study.  That is, sustained treatment strategies require sequential randomisation. Satisfying the assumptions for valid causal inferences is typically much more challenging than many investigators presently understand. 




{{< pagebreak >}}

## Funding

This work is supported by a grant from the Templeton Religion Trust (TRT0418) and RSNZ Marsden 3721245, 20-UOA-123; RSNZ 19-UOO-090. I also received support from the Max Planck Institute for the Science of Human History. The Funders had no role in preparing the manuscript or the decision to publish it.

## Acknowledgements

Errors are my own.


{{< pagebreak >}}



## Appendix A: Glossary


::: {#tbl-experiments}
```{=latex}
\glossaryTerms
```
Glossary
:::

