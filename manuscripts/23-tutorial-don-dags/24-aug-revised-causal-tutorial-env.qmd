---
title: "An Invitation to Causal Inference in Environmental Psychology"
abstract: |
  This chapter introduces causal inference within environmental psychology, underscoring its fundamental differences from traditional statistical analysis. The content is organised into four main sections:
  1. **Non-technical introduction**. Stating a causal question requires defining pre-specified contrasts between interventions experienced by an entire population. Because each individual can experience only one intervention, causal effects must be estimated using assumptions. We build intuitions for these assumptions by clarifying their satisfaction in randomised controlled experiments.
  2. **Causal Directed Acyclic Graphs (DAGs) tutorial**. Causal DAGs are potent tools for clarifying whether and how causal effects may be identified from data. We explain how they work. 
  3. **Practical examples**. We apply causal DAGs to common scenarios in observational environmental psychology.
  4. **Guidelines**. The main aim of this chapter is to motivate broader adoption of causal workflows.
authors: 
  - name: Joseph A Bulbulia
    orcid: 0000-0002-5861-2056
    email: joseph.bulbulia@vuw.ac.nz
    affiliation: 
      name: Victoria University of Wellington, New Zealand, School of Psychology, Centre for Applied Cross-Cultural Research
      department: Psychology/Centre for Applied Cross-Cultural Research
      city: Wellington
      country: New Zealand
      url: www.wgtn.ac.nz/cacr
  - name: Donald W Hine
    orcid: 0000-0002-3905-7026
    email: donald.hine@canterbury.ac.nz
    affiliation: 
      name: University of Canterbury, School of Psychology, Speech and Hearing
      city: Canterbury
      country: New Zealand
      url: https://profiles.canterbury.ac.nz/Don-Hine
keywords:
  - DAGS
  - Causal Inference
  - Confounding
  - Environmental
  - Longitudinal
  - Psychology
format:
  pdf:
    sanitize: true
    keep-tex: true
    link-citations: true
    colorlinks: true
    documentclass: article
    classoption: [singlecolumn]
    lof: false
    lot: false
    number-sections: false
    number-depth: 4
    highlight-style: github
    geometry:
      - top=30mm
      - left=20mm
      - heightrounded
    template-partials: 
      - /Users/joseph/GIT/templates/quarto/title.tex
    header-includes:
      - \input{/Users/joseph/GIT/latex/latex-for-quarto.tex}
date: last-modified
execute:
  echo: false
  warning: false
  include: true
  eval: true
fontfamily: libertinus
bibliography: /Users/joseph/GIT/templates/bib/references.bib
csl: ./camb-a.csl
---

```{r}
#| label: load-libraries
#| echo: false
#| include: false
#| eval: false
#  fig-pos: 'htb'
#   html:
#    html-math-method: katex

# Include in YAML for Latex
# sanitize: true
# keep-tex: true
# include-in-header:
#       - text: |
#           \usepackage{cancel}


# for making graphs
library("tinytex")
library(extrafont)
loadfonts(device = "all")


# libraries for jb (when internet is not accessible)
# read libraries
source("/Users/joseph/GIT/templates/functions/libs2.R")

# read functions
source("/Users/joseph/GIT/templates/functions/funs.R")

# read data/ set to path in your computer
pull_path <-
  fs::path_expand(
    "/Users/joseph/v-project\ Dropbox/data/current/nzavs_13_arrow"
  )

# for saving models. # set path fo your computer
push_mods <-
  fs::path_expand(
    "/Users/joseph/v-project\ Dropbox/data/nzvs_mods/00drafts/23-causal-dags"
  )


# keywords: potential outcomes, DAGs, causal inference, evolution, religion, measurement, tutorial
# 10.31234/osf.io/b23k7

```

Psychological scientists are taught that "correlation does not imply causation." By "correlation," we refer to statistical measures of association between variables. Most statistical techniques —from t-tests to structural equation models- estimate associations from data. Although we know that measuring associations does not imply causation, in observational settings, we often persist in reporting statistical associations as if they are tentative evidence for causation. The purpose of this chapter is to clarify why such reporting is confused and misleading, and to guide researchers toward better practices.

What do we mean by "causation?" Causation has been a topic of extensive interest and debate in philosophy [@lewis1973]. Here, we narrow our focus. We consider the assumptions under which it is possible to estimate causal effects from data and how to estimate them. In causal-effect estimation, or "causal inference," investigators seek to quantify the average differences across a specified population or sub-population (the "target population") that interventions would produce on well-defined outcomes. This requires comparing at least two states of the world: one where the population experiences a treatment and another where they do not. While the concept of causation in causal inference is more narrowly defined than causation itself, it draws intellectual inspiration from David Hume, who, in his *Enquiries Concerning Human Understanding* (1751), characterises the cause-effect relationship as follows:

  > "If the first object had not been, *the second never would have existed*." [@hume1902] (emphasis added).

This conceptualisation aligns closely with the counterfactual approach in causal inference, which considers what would have happened to an outcome in both the presence and the absence of a treatment.

Hume's definition relies on counterfactual thinking—specifically, comparing two mutually exclusive states of the world: one where an event occurs and one where it does not. For Hume, assessing causation requires not just observing events as they happen but also considering how the world might have differed had those events not occurred. Such comparisons, where we consider scenarios where treatments did not take place, are known as "counterfactual contrasts;" such contrasts are fundamental to causal-effect estimation. In modern causal inference, these contrasts are formalised within the potential outcomes framework, which estimates the average difference in outcomes between treated and control conditions had the entire population of interest been treated at different levels of the intervention.

Importantly, although statistically evaluating associations from data is essential for estimating average treatment effects, causation estimation cannot be derived from the study of associations alone. Every realisation in the data provides information about only one of the two potential outcomes necessary for a causal contrast. We may only obtain causal contrasts under assumptions that are not testable from the data. Moreover, a careful and systematic workflow is required. By the end of this chapter, you will understand why, without such a workflow, common analytic techniques—such as linear regression, correlation, and structural equation modelling—lack causal interpretations and may mislead investigators.

[**Part 1**](#section-part1) introduces the counterfactual framework of causal inference, focusing on the three fundamental assumptions necessary for estimating average causal effects. We build intuition for these concepts by considering randomised controlled trials, where these assumptions are met through enforced randomisation [@westreich2012berkson; @hernan2017per; @westreich2015; @robins2008estimation].

[**Part 2**](#section-part2) introduces causal Directed Acyclic Graphs (DAGs), powerful tools for visualising and addressing the assumption of conditional exchangeability (also known as the "no unmeasured confounders" assumption). Here, we introduce Judea Pearl's rules of d-separation [@pearl1995], which allow investigators to identify appropriate variables to adjust for confounding. Although most psychological scientists are aware that regression adjustment is commonly used to control confounding, they may not be fully aware of the formal criteria required to select appropriate adjustment variables. Understanding the formal criteria is essential because over-adjustment may introduce bias or reduce statistical power, leading to misleading conclusions.


[**Part 3**](#section-part3) examines seven practical examples that demonstrate how causal Directed Acyclic Graphs (DAGs) can be used to address causal questions. These examples serve as practical guides for translating causal questions into causal identification models that clarify whether and how causal effects can be estimated from data. Only after stating and addressing these identification assumptions should researchers develop statistical estimators and perform statistical analysis [@vansteelandt2012; @wager2018].

[**Part 4**](#section-part4) offers practical guidelines for environmental psychologists aiming to infer causal effects from observational data. Given that assumptions about causal relationships are often uncertain or subject to debate, we recommend developing multiple causal diagrams and reporting their corresponding analyses to evaluate different plausible causal pathways. This approach enhances transparency in how causal relationships are inferred.

We conclude by suggesting further readings and resources for those interested in learning more about causal inference.

## Part 1: An Overview of the Counterfactual Framework for Causal Inference {#section-part1}

### The Origins of Causal Inference

Causal inference began in the early 20$^{th}$ century with Jerzy Neyman's invention of the potential outcomes framework, initially developed for agricultural experiments. Neyman realised that understanding the causal effect of an experimental treatment required comparing potential outcomes under different treatment conditions, even though only one outcome could be observed for each unit [@neyman1923]. This framework laid the foundation for modern causal inference.

Donald Rubin extended this approach to observational settings into what is now known as the Neyman-Rubin Causal Model [@rubin1976; @holland1986]. James Robins advanced causal inference by introducing the mathematical and conceptual framework for understanding causal effects in settings where there are two or more sequential treatments over time [@robins1986]. Robins and his colleagues also developed statistical tools such as marginal structural models and structural nested models to enable quantitative assessment of time-varying treatments and time-varying confounding [@hernan2024WHATIF]. Computer scientist Judea Pearl significantly advanced the use of causal Directed Acyclic Graphs (DAGs) to evaluate the "no unmeasured confounding" assumption in causal inference, which is also known as "no unmeasured confounding", which is also referred to as "conditional exchangeability," "selection on observables," or "ignorability."  In Pearl’s framework, this is formalised through d-separation, a criterion used to check for "no open backdoor paths" in causal DAGs, ensuring that causal estimates are not biased by confounding [@pearl1995a; @pearl2009]. In [**Part 2**](#section-part2) we introduce the concept of causal identification, which occurs when, conditional on measured covariates, there is no unmeasured confounding. This concept is distinct from statistical identification. The rules of d-separation will be explained later, but for now, it is important to recognise that causal inference involves considerable specialised terminology.

Although terminology differs, the mathematical basis of causal inference evolved almost independently in the fields of biostatistics, economics, and computer science. This shared foundation, anchored in proofs, has enabled a remarkable agreement across disciplinary lines despite terminological differences. In every approach to causal inference, a causal effect is conceptualised as a contrast between two states of the world, only one of which may be observed on any individual -- a "counterfactual contrast" also known as a "causal estimand" [@hernan2024WHATIF]. Notably, prior to intervention, these scenarios are purely hypothetical. Post-intervention, only one scenario is actualised for each realised treatment, leaving the alternative as a non-observed counterfactual. For any individual unit to be treated, that only one of the two possible outcomes is realised underscores a critical property of causality: causality is **not directly observable** [@hume1902]. Causal inference, therefore, can only quantify causal effects by combining data with counterfactual simulation [@edwards2015; @bulbulia2023a]. The concept of a counterfactual data science --  may sound strange. However, anyone who has encountered a randomised experiment has encountered counterfactual data science. Before building intuitions for causal inference from the familiar example of experiments, let's first build intuitions for the idea that causal quantities are never directly observed. 

## The Fundamental Problem of Causal Inference: Counterfactual Comparisons in Environmental Psychology

Imagine you are faced with a significant life decision: enrolling in a graduate programme in environmental psychology in New Zealand or accepting a job offer from a leading renewable energy company. This choice will shape your future, influencing your lifestyle, income, and social network. Which option is best for you?

The challenge is that, once you choose one path, you cannot observe how your life would have unfolded on the other. If you go to graduate school, you will experience that outcome, but the outcome of taking the job remains unknown—and vice versa. This is **the fundamental problem of causal inference**: we can never observe both potential outcomes for the same individual, so the path not taken remains an unobservable "what if?" — a counterfactual that cannot be measured [@holland1986].

Again, counterfactual comparisons lie at the heart of causal inference, as they involve contrasting what actually happened with what would have happened under a different scenario. In environmental psychology, computing counterfactual contrasts is essential for understanding the effects of psychological and behavioural interventions on well-defined outcomes. However, the full data required to make such contrasts are inevitably partially missing and can only be recovered by assumptions [@edwards2015; @westreich2015].

### Causal Inference in Experiments: The Problem of Missing Counterfactuals

Consider a question relevant to environmental psychologists: What is the causal effect of access to green spaces on subjective happiness? Denote happiness by $Y$, where $Y_i$ represents the happiness of individual $i$. Assume that "subjective happiness" is a coherent concept. For now, assume that errors in its measurement are not systematically linked to access to green spaces. Suppose further that "ample access to green space" is represented as a binary variable: $A = 1$ for "ample access" and $A = 0$ for "lack of ample access." These conditions are mutually exclusive. While we simplify the treatment to a binary variable, the concepts apply to more complex or continuous treatments. Estimating causal effects always requires a contrast between well-defined treatment conditions. Importantly, defining clear causal questions is essential but often neglected in psychological science outside experimental work.

Imagine our aim is to compare potential outcomes under different treatment conditions. Specifically, we contrast the happiness of individuals with access to green space ($A = 1$) against those without ($A = 0$). The target population for this contrast should be explicit—for example, all New Zealand residents in 2024.

A clear causal question, framed as a counterfactual contrast—also known as a "causal estimand"—might be:

> *"Among New Zealand residents, does access to abundant green space increase self-perceived happiness compared to environments without such spaces?"*

Now, imagine—hypothetically and ethically—that we could randomise individuals to high or low green space access. Notice that even if such an experimental setup were possible, causal inference would face missing data in the potential outcomes. For each person assigned to treatment, only one potential outcome is observed, depending on the treatment they receive. The outcome they would have experienced under the alternative treatment is unobserved -- it remains counterfactual. Such missingness is the fundamental problem of causal inference, raised to the level of treatment groups. We only observe $Y_i(1)$ for individuals with $A_i = 1$ and $Y_i(0)$ for individuals with $A_i = 0$. The other potential outcome for each individual remains unobserved. However, although this missingness in the "full data" poses a challenge at the individual level, we can estimate the average treatment effect within the sampled population without needing to obtain individual-level causal effects.

Although individual causal effects are generally not recoverable from data, we can recover causal effect estimates by changing our causal question. For example, although randomised controlled experiments do not solve the fundamental problem of causal inference at the level of individuals, they may obtain consistent causal effect estimates for average treatment effects at the level of populations. Randomised experiments solve the missing data problem at the heart of causal inference by satisfying three fundamental assumptions required for obtaining average treatment effects. These are the (1) Conditional Exchangeability Assumption, (2) The Causal Consistency Assumption, and (3) The Positivity Assumption.

Before proceeding to explain these assumptions, we offer the following clarification: the concept of within-subject effects as understood in traditional statistical analyses does not directly apply in causal inference and may even appear incoherent within this framework. In conventional analyses, within-subject effects refer to changes observed within the same individual across different conditions or over time, using repeated measures to control for individual-specific variability. However, causal inference is fundamentally concerned with estimating causal effects based on hypothetical interventions and potential outcomes—scenarios that cannot be simultaneously realized for a single individual. Since an individual cannot both receive and not receive a treatment at the same time, we cannot observe both potential outcomes required to define the causal effect at the individual level. Instead, causal inference relies on comparisons of population averages under distinct treatments or treatment regimes. Therefore, what is often termed a 'within-subject effect" is more accurately understood as a within-subject research design in the context of causal inference. In causal inference, there is no coherent distinction for a within-person/between-person causal effect [@rohrer2023withinbetween]

#### Assumption 1: Conditional Exchangeability

First, we define the expected value of a treatment $A=a$ as the sum of individual counterfactual (or equivalently) "potential" outcomes for individuals within a specified population:


$$
\mathbb{E}[Y(a)] \equiv \frac{1}{n} \sum_{i=1}^n Y_i(a)
$$

Note that in causal inference, we assume these potential outcomes to be real, even if determined stochastically.

We next define the average treatment effect (ATE) as a contrast between the averages of the potential outcomes in each treatment condition:


$$
\text{ATE} = \mathbb{E}[Y(1)] - \mathbb{E}[Y(0)]
$$

Suppose that the individuals in a sample are representative of the population of interest—the target population. In this setting, randomisation ensures there is no common cause of the treatment and the potential outcomes that would be observed under treatment. That is, if treatment assignment is determined by chance, any difference in the average response within treatment groups is best explained by the treatment itself.

Mathematically, we express the absence of a common cause of treatment assignment and the potential outcomes under treatment:

$$
Y(a) \coprod A
$$

This notation means that the potential outcomes $Y(a)$ are independent of ($\coprod$) the treatment assignment $A$. Importantly, the observed outcomes need not be independent of treatment. Note that we do not require that exposure is independent of the actual outcomes under exposure ($Y|A$), which would be equivalent to requiring there can be no causal effects for treatment! Randomisation achieves unconditional independence, allowing us to estimate average causal effects because treatments remain independent of potential outcomes under treatment. Put differently, randomisation ensures that average treatment differences (or equivalently, differences in treatment averages) are the result of the treatments themselves rather than the effects of another variable. 

Importantly, we can relax the requirement for unconditional independence and allow randomisation to occur conditional on certain measurable features of the sampled population. For example, suppose we randomise treatment within different age cohorts such that older individuals have a greater probability of receiving the treatment than younger individuals. Assume that the treatment effect is constant across all ages. In this scenario, we would expect to see higher average treatment effects in older cohorts simply because a larger proportion of older individuals receive the treatment.

When randomisation occurs conditional on a measurable feature such as age, and the treatment effect is constant, differences in treatment probabilities across groups can lead to variations in observed average treatment effects. However, if we adjust for this difference in the probability of receiving treatment, or simply compare treatment effects within the different age strata, the bias from differential treatment group assignment disappears.

We use the symbol $L$ to denote measured variables that might be common causes of the treatment ($A$) and the treatment effect ($Y(a)$). We write that the potential or counterfactual outcome $Y(a)$ is independent of $A$, conditional on $L$:

$$
Y(a) \coprod A \mid L
$$

This assumption means that, within levels of $L$, the treatment assignment is as good as random. In experiments, randomisation ensures unconditional exchangeability ($Y(a) \coprod A$). However, to compute causal effects from data in which the treatments are not randomised, we must believe that within levels of measured covariates $L$, the treatment is as good as random. 

Practically, this involves measuring all common causes of $A$ and $Y$, and appropriately adjusting for $L$ in our statistical model. In observational studies, achieving conditional exchangeability is challenging because treatment assignment is not controlled, and there is no statistical test that will tell investigators whether treatment assignment is as good as random conditional on measured covariates. However, as we will explain in the next section, causal Directed Acyclic Graphs (causal DAGs) help investigators evaluate whether there exists a set of measured covariates $L$ such that conditioning on $L$ ensures that the potential outcomes are independent of the treatment -- that is, to evaluate whether the Conditional Exchangeability Assumption may be satisfied.

#### Assumption 2: Causal Consistency

Causal consistency requires that the potential outcome under the treatment actually received equals the observed outcome:

$$
Y_i = Y_i(a) \quad \text{if } A_i = a
$$

Note that in causal inference, we compare potential outcomes under at least two different treatments, say, $Y(a = 1)$ and $Y(a = 0)$ (in causal inference, we use a lowercase variable to note that the random variable $A$ is fixed to a certain level ($A = a$). To compute causal contrasts, the counterfactual or potential outcomes under treatment must be observable. The causal consistency assumption allows investigators to link counterfactual or potential outcomes to observed outcomes. It might seem obvious that if one receives a treatment, we can say that the observation of the outcome following treatment is no longer counterfactual—it is actual. However, for the causal consistency assumption to hold across a population, we must assume that the treatments are well-defined and consistently administered. Thus, the causal consistency assumption is a two-edged sword. On the one hand, we may use it—if other assumptions are satisfied—to compute causal contrasts. Simply put, causal consistency puts the factual into counterfactual. 

In controlled experiments, we may typically take causal consistency for granted. The investigators administer consistent treatments. However, in observational settings, no such control is ensured. For example, one reason it has been difficult to investigate the causal effects of weight loss from massive observational medical datasets is that there are many ways to lose weight—eating less and healthfully and exercising causes people to lose weight. However, one may also lose weight from smoking, psychological distress, stomach stapling, amputation, cancer, and famine. The mechanisms of the latter forms of weight loss are unhealthy. Thus, stating a causal contrast, for example, as the expected difference in all-cause mortality after five successive years of weight loss, is an invitation for confusion. The treatments that lead to weight loss in medical data are not comparable across all cases (refer to @vanderweele2013, @bulbulia2023). Even if we consistently estimate a causal effect, it might be unclear which effect we are estimating because our measures to not correspond to well-defined interventions. 

#### Assumption 3: Positivity

Positivity requires that every individual has a non-zero probability of receiving each level of the treatment, given their covariates $L$:

$$
P(A = a \mid L = l) > 0 \quad \text{for all } a, l
$$

This assumption ensures that we have data to compare treatment effects across all levels of $L$. For example, suppose we are interested in the causal effects of vasectomy on happiness. It would not make sense to include biological females in this study because biological females cannot have vasectomies [@westreich2010; @hernan2023].


### Summary

Causal inference fundamentally relies on statistical inference; however, statistics is only a part of a larger workflow that begins with defining a causal quantity. This involves specifying contrasts that would be observed if we had complete data for the population of interest, treated at different levels of the intervention [@ogburn2021]. The next step is to determine how these contrasts can be estimated from the available data.

Standard statistical practices, such as regression adjustment and structural equation modelling, often fail to accurately estimate causal effects. This is largely because these models typically do not explicitly define the treatments being compared, nor do they properly account for the causal structures that can bias treatment-outcome associations.

In the following section, we introduce causal directed acyclic graphs (causal DAGs) -- intuitive graphical tools that help researchers evaluate the complex conditional dependencies needed to satisfy the "no unmeasured confounders" assumption. This is done by inspecting relationships in a graph using a straightforward algorithm known as the rules of d-separation [@pearl1995; @pearl2009a]. Causal DAGs also reveal how standard practices in statistical modelling, such as over-adjustment in regression or mediation analysis in structural equation modelling, may inadvertently introduce bias through inappropriate variable adjustment. Such over-adjustment can obscure or distort true causal relationships.

In causal inference, non-causal associations are termed "spurious" because they lack coherent interpretation. It is often unclear how to interpret such associations, as they might imply no causal effect, a causal effect of a different magnitude, or even a causal effect in the opposite direction. By explicitly representing the causal structure and carefully selecting which variables to adjust for, causal DAGs can help environmental psychologists more effectively estimate causal effects within their research.

## Part 2: An Introduction to Causal Diagrams {#section-part2}

A causal directed acyclic graph (DAG) is a diagram that clarifies whether and how the assumption of conditional exchangeability (no unmeasured confounders) may be satisfied from data. A causal DAG is constructed with the aim of evaluating this assumption for a specific treatment variable (or set of treatment variables) and a specific outcome (or set of outcomes). When there are multiple treatments, the assumptions required for causal identification become increasingly complex. Here, we restrict our focus to the setting in which there is one treatment variable. The difficulty of ensuring identification for multiple treatments will become evident from this restricted setting.

It is important to note at the outset that in a causal DAG, we do not attempt to represent all causal relationships within a system, but only those that are relevant to the causal question of interest. This requires a conceptual shift from the use of graphs in the structural equation modelling (SEM) tradition, where the goal is to represent a system of statistical relationships obtained from data. Instead, causal DAGs focus on the minimal set of relationships necessary to address the causal question and assess whether pre-specified causal contrasts can be identified from data.

We begin with definitions.

#### 1. **Node**

A node represents a variable—a state of the world relevant to our research question. Again, it is helpful to keep the example of a randomised controlled experiment in mind when thinking about causality. In a simple randomised experiment, there are two states of the world that interest investigators. The experimental intervention, also called a "treatment" or "exposure". This is the condition into which participants are randomised. Here, we have defined interventions using the symbol $A$. In a causal DAG, we would simply draw the letter $A$. Note that the choice of symbol is arbitrary. It is only necessary that we define our terminology. The second variable of interest in a randomised experiment is the outcome. Here we have used the symbol $Y$ to denote an outcome. Again, the symbol itself is arbitrary. Not only are our conventions arbitrary, so too are our interests. We might be interested in how $Y$ affects $A$. In this case, $Y$ would be the intervention and $A$ would be the outcome. There is nothing to prevent us taking such an interest. However, as we shall see, the conditions in which we hope to identify the causal effects of variables will typically vary depending on which effect is of interest. For now, it is sufficient to understand that we represent variables as nodes on a graph, typically by using symbols to refer to interventions and outcomes that we must define.

#### 2. **Arrows (also known as Edges)**

Arrows indicate the direction and presence of causal relationships between nodes. They represent the assumed flow of causal influence from a "parent" (originating variable) to a "child" (receiving variable). For example, an arrow from $A \rightarrow Y$ suggests that $A$ causally influences $Y$.

Note, however, that when drawing a causal DAG, we will typically not draw an arrow linking $A \rightarrow Y$. This is because the purpose of a causal DAG is to evaluate whether there are paths linking $A$ and $Y$ absent any assumption of a causal path. We evaluate whether there are such paths by drawing all common causes of $A$ and $Y$, both measured and unmeasured. For example, if $L$ is a measured common cause of $A$ and $Y$, we would draw the graph $A \leftarrow L \rightarrow Y$. Such a graph would reflect our assumption that $A$ and $Y$ share a common cause $L$. Likewise, we might be interested in unmeasured common causes of the treatment and outcome. We might define such unmeasured common causes using the symbol $U$ and draw the graph $A \leftarrow U \rightarrow Y$.

#### 3. **Conditioning**

Recall that $Y(a) \coprod A \mid L$ is the conditional exchangeability assumption—or the assumption of "no unmeasured confounders." To infer causal effects, we must ensure that, conditional on a set of measured covariates $L$, the treatment assignment is essentially as good as random. We graphically denote that we "control for," or equivalently "condition on," or "adjust for," or equivalently "stratify by" a variable by enclosing it in a box. For example, $A \leftarrow \boxed{L} \rightarrow Y$ means that $L$ is included in a statistical model that aims to evaluate the causal effect of $A$ on $Y$. Again, we would not typically draw an arrow from $A$ to $Y$ when seeking to evaluate the conditional exchangeability assumption. This is because we want to understand whether there might be an association between $A$ and $Y$ in the absence of causation.

Before proceeding, there are two basic considerations to keep in mind when drawing a causal DAG.

First, causal DAGs are qualitative tools whose primary purpose is to evaluate the conditions under which causal effects may be estimated from data. We only include as much information in a causal DAG as is necessary to evaluate the assumption of conditional exchangeability. This means that we should not attempt to represent non-linear relationships, for example by drawing arrows intersecting arrows—a common misconception. Nor should we attempt to represent more than is necessary for evaluating the assumption of conditional exchangeability. That means omitting any variables from the graph that are irrelevant to evaluating this assumption. Put simply, a causal DAG is not meant to be a map of reality. It is a simple tool for investigating causal assumptions. Although it might be tempting to compare causal DAGs to Structural Equation Models, the purposes and interests have little in common.

Second, causal DAGs must be *acyclic*. This means that we draw nodes and arrows in a manner that respects the flow of time. Occasionally, students will worry that there are "reciprocal effects" between a treatment and an outcome. For example, access to greenspace might cause one to be happier, and being happier one might seek out greater access to greenspace. It might be tempting, then, to construct a causal graph with double arrows such that $A\leftarrow \rightarrow Y$. However, on reflection it should be clear that there is only one arrow to time, and as such, our graphs must respect that direction. To represent a cycle, we would expand the graph: Greenspace (Time 0) $\to$ Happiness (Time 2) $\to$ Greenspace (Time 3) $\to$ Happiness (Time 4). In this setting, which we assume to be nearly universal for most questions in environmental psychology, investigators should state the exposure and time point of interest, say Greenspace at Time 3, state the time point and outcome of interest, say Happiness at Time 4, and where data are available, control for past states of greenspace access and of happiness. That is, Greenspace at Time 0 (and before) and Happiness at Time 2 (and before) should be included in the confounder set $L$ when estimating the effect of $A$ (Greenspace at Time 3) on $Y$ (Happiness at Time 4) (refer to @vanderweele2020, @bulbulia2023).

Next, we consider how to use causal DAGs to evaluate whether and how conditional exchangeability may be satisfied.

### d-Separation

Having defined the essential components of a causal Directed Acyclic Graph, consider the rules by which we may evaluate a causal graph to ascertain whether the causal effect of one variable on another may be consistently estimated from data without confounding bias.

In causal diagrams, d-separation is a graphical criterion used to determine whether, and how, we may collect data and develop a statistical model in which the association we estimate between a treatment variable and an outcome variable reflects a causal association. In the mid-1990s, Judea Pearl proved that there is a set of simple rules by which we can inspect a causal diagram to evaluate whether the causal effect of a treatment on an outcome is "as good as random." These are called the rules of "d-separation" [@pearl1995; @pearl2009a].

When stating the rules of d-separation, we will depart from our convention of using $A$ to denote the treatment and $Y$ to denote the outcome. The reason for doing so is that we want to flexibly adjust our orientation depending on the specific causal effect in which we are interested. So we will think of three variables, $A$, $B$, and $C$, which we will imagine to be ordered in time, such that $A$ precedes $B$ and $B$ precedes $C$. We will describe the rules of d-separation by referring to three structural relationships, where "structural" means causal, and where the causal relationships are presented in a graph with nodes and arrows, such that arrows denote causal relationships.

#### 1. **Fork (Common Cause)**

Diagram: $B \leftarrow \boxed{A} \rightarrow C$

Suppose that $A$ is a common cause of $B$ and $C$, and that there are no additional common causes or direct causal paths between $B$ and $C$. Pearl proved that conditioning on $A$, which we denote $\boxed{A}$, removes the association between $B$ and $C$ due to the common cause $A$. Therefore, $B$ and $C$ are statistically independent given $A$, which in the potential outcomes framework we write:

$$
B \coprod C \mid A
$$

In Pearl's vocabulary, we say that $B$ and $C$ are "d-separated" conditional on $A$. This means that any statistical association between $B$ and $C$ observed in the data is due to the common cause $A$, and after conditioning on $A$, there is no remaining association between $B$ and $C$.

#### 2. **Chain (Mediator)**

Diagram: $A \rightarrow \boxed{B} \rightarrow C$

Suppose that we are interested in the causal effect of $A$ on $C$. Suppose furthermore that $B$ is an effect of $A$ as well as a cause of $C$—in other words, $B$ is a mediator of $A$'s effect on $C$. Pearl proved that conditioning on $B$, which we denote $\boxed{B}$, blocks the association between $A$ and $C$ along the path $A \rightarrow B \rightarrow C$. Thus, $A$ and $C$ are independent given $B$:

$$
A \coprod C \mid B
$$

Recall that earlier we said that we will typically not draw an arrow linking the treatment and the outcome of interest. The exception to this rule is when we are interested in whether our conditioning strategy will introduce bias such that a true causal relationship becomes distorted. Suppose that $A$ is randomised and that $C$ is the outcome of interest. To estimate the total effect of $A$ on $C$, Pearl's rules of d-separation imply that we must not condition on $B$. Doing so will introduce what is known as "mediator bias." Mediator bias is a species of overconditioning bias in which, by conditioning on a variable, we unwittingly distort a causal effect estimate. If $A \rightarrow B \rightarrow C$ were an accurate representation of reality, then we should not condition on $B$. This should be easy, right? In a recent study, @montgomery2018 found that 46.7% of the experimental studies published in leading experimental political science journals conditioned on a post-treatment variable. These and other self-inflicted injuries would be easily avoided were investigators simply to graph the relationships in their data (for discussion refer to @bulbulia_2024_experiments). Note that when data are cross-sectional—that is, collected at one time point—we often cannot evaluate whether a variable is a cause or an effect of another. We will return to the perils of cross-sectional data below.

#### 3. **Collider (Common Effect)**

Diagram: $A \rightarrow \boxed{C} \leftarrow B$

Suppose now that we are interested in estimating the causal effect of $A$ on $B$. Suppose there is no direct causal relationship between $A$ and $B$ (we do not draw an arrow from $A$ to $B$). Suppose further that $C$ is a common effect of $A$ and $B$, and that we condition on $C$. If we do not condition on $C$, then $A$ and $B$ are independent, or in Pearl's vocabulary, $A$ and $B$ are d-separated. However, Pearl proved that conditioning on a common effect of $A$ and $B$ may introduce a statistical dependence between $A$ and $B$, such that:

$$
A \coprod B
$$

Yet

$$
A \cancel{\coprod} B \mid C
$$

When a variable is the child of two parents, we call this variable a "collider." Here, $C$ is a collider of $A$ and $B$. According to Pearl's rules of d-separation, $A$ and $B$ are independent if we do not condition on $C$, yet dependent if we do. If we wish to estimate the causal effect of $A$ on $B$, the advice would be simple: do not condition on $C$. However, with only cross-sectional data, we often cannot be certain whether a variable that is a common effect of the treatment and the outcome is a common effect. Experimental studies that collect data after the outcomes have been measured and include these data in a statistical model—or use the data to select participants—may easily fall prey to collider biases (refer to @bulbulia_2024_experiments). However, Pearl's graphical rules make it clear that such practices must be avoided. A nice feature of Pearl's rules of d-separation is that they are supported by proofs.

We can restate the rules of d-separation as follows:

1. **Colliders block paths when unconditioned:** an open path (with no variables conditioned on) is blocked if it contains a collider—a node where two arrows converge. If $A \rightarrow \boxed{C} \leftarrow B$, then $A$ and $B$ are blocked from being associated with each other when $C$ is not conditioned upon.

2. **Conditioning on a collider opens a path:** conditioning on a collider opens a previously blocked path, potentially introducing an association between variables that are not causally related. If $A \rightarrow \boxed{C} \leftarrow B$, conditioning on $C$ allows information to flow between $A$ and $B$, leading to a possible association in the absence of a direct causal relationship.

3. **Conditioning on a descendant of a collider also opens the path:** conditioning on a descendant of a collider has a similar effect. If $A \rightarrow C \rightarrow \boxed{C'} \leftarrow B$, and $C \to C'$, then conditioning on $C'$ is akin to conditioning on $C$. The path $A \rightarrow C \rightarrow \boxed{C'} \leftarrow B$ becomes unblocked. Thus, conditioning on $C'$ opens an association between $A$ and $B$.

4. **Conditioning on non-collider nodes blocks the path:** if a path does not contain a collider, conditioning on any variable along that path blocks it. For instance, in the diagram $A \rightarrow \boxed{B} \rightarrow C$, conditioning on $B$ blocks the path from $A$ to $C$, rendering $A$ and $C$ conditionally independent given $B$ [@pearl2009a; @hernan2024WHATIF, p. 78].

Pearl proved these rules of d-separation in the 1990s, demonstrating that we may use causal directed acyclic graphs to evaluate the conditions under which consistent causal effect estimates are possible from data [@pearl1995; @pearl2009a]. Note again that the structures in a graph must be assumed and cannot generally be verified by the data alone.


### The Concept of a "Backdoor Path"

In causal diagrams, the concept of a *backdoor path* is central to understanding whether an association between treatment and outcome variables might be confounded by other variables. A *backdoor path* is any path between the treatment and the outcome that flows through a common cause of both variables, rather than through a direct causal link. In general, backdoor paths introduce confounding, which means that an association between the treatment and outcome variables is not purely the result of the causal effect of the treatment.

To illustrate how blocking a backdoor path works we return to our convention of using $A_1$ to represent the treatment and $Y_2$ to represent the outcome. We index the nodes with numbers to clarify the temporal order of causality. The numbers denote relative timing. A backdoor path is any non-causal path between $A_1$ and $Y_2$ that originates from $A_1$, passes through one or more other variables, and ends at $Y_2$. These paths include arrows that flow both into and out of the treatment or the outcome, such that the flow of influence "sneaks" in through the backdoor. For example in this causal DAG:

$$
A_1 \leftarrow L_0 \rightarrow Y_2
$$

$L_0$ is a common cause of both $A_1$ and $Y_2$. Therefore, the association between $A_1$ and $Y_2$ is confounded by $L_0$, as part of their association arises from the influence that $L_0$ exerts on both. The open backdoor path runs from $A_1$ to $L_0$ to $Y_2$.  Because this path is open we say that $A_1$ and $Y_2$ are "d-connected." 

#### Blocking Backdoor Paths

To estimate the causal effect of $A_1$ on $Y_2$, we need to block all backdoor paths between them. This can be done by conditioning on, or "adjusting for," a set of covariates that block all such paths. In our example, adjusting for $L_0$—which we graphically represent as $\boxed{L_0}$—blocks the backdoor path $A_1 \leftarrow \boxed{L_0} \rightarrow Y_2$ and removes the confounding introduced by $L_0$. Once this backdoor path is blocked, the association between $A$ and $Y$ can be attributed to the causal effect of $A$ on $Y$.

Formally, if all backdoor paths between $A$ and $Y$ are blocked by conditioning on a set of covariates $L$, then:

$$
Y_2(a) \coprod A_1 \mid L_0
$$

This is the conditional exchangeability assumption, which, as we observed in Part 1, must be satisfied for us to identify the causal effect of $A_1$ on $Y_2$. If all backdoor paths are closed between $A_1$ and $Y_2$ we say that $A_1$ and $Y_2$ are "d-separated."

#### Rules for Blocking Backdoor Paths

To block a backdoor path, we can condition on a variable that lies on the path, but the choice of variable must follow the rules of d-separation:

1. **Conditioning on a common cause:** if a variable is a common cause of both $A$ and $Y$, conditioning on this variable blocks the backdoor path. In our previous example, conditioning on $L_1$ blocks the path $A \leftarrow L_1 \rightarrow Y$.

2. **Avoid conditioning on a mediator:** conditioning on a mediator (a variable that lies on the causal path from $A$ to $Y$) introduces mediator bias. For example, if we condition on a variable $M$ in the path $A \rightarrow M \rightarrow Y$, we block the causal effect of $A$ on $Y$ and distort our estimate.

3. **Avoid conditioning on a collider:** conditioning on a collider can introduce spurious associations. As discussed earlier, if we condition on $C$ in the diagram $A \rightarrow C \leftarrow B$, we may induce an association between $A$ and $B$, even though no direct causal link exists between them.

### How to apply d-Separation to Causal DAGs

Now that we understand the essential components of a causal DAG, let us apply the concept of d-separation to a series of practical example.
<!-- #### Absence of Causality: Two Variables with No Arrows

When no arrows connect $A$ and $B$, we assume they do not share a causal relationship and are statistically independent. This means that knowing something about $A$ does not give us any information about $B$ and vice versa. Graphically, we represent this relationship as:

$$
\xorxALARGE
$$

That is, as two nodes without any linking arrow.

#### Causal Structure 1: Direct Causation Between Two Variables

A causal arrow ($A \to B$) signifies that changes in $A$ directly cause changes in $B$, creating statistical dependence between them. This direct causal link is graphically depicted as:

$$
\xtoxALARGE
$$

That is, as two nodes linked by an arrow.

#### **Rule 1: Ensure That the Treatment Precedes the Outcome** {#sec-four-rules}

Causality follows the arrow of time; an outcome occurs after the intervention that causes it. Therefore, the treatment ($A$) must occur before the outcome ($B$) in order to claim causality. This underscores the importance of temporal precedence in causal inference. Where the relative timing of events is unknown—as it often is in observational data—we cannot ensure that the treatment precedes the outcome unless we make additional assumptions. Such assumptions may be implausible or untestable. Where this is the case, we cannot rule out the possibility that the outcome precedes the treatment.


##### Motivating Example

Suppose we find an association between conservation behaviours and happiness. We might infer that conservation behaviours cause happiness. However, it is also possible that happy people are simply more likely to engage in conservation behaviours. If we do not have longitudinal data that shows treatment preceding the outcome, we cannot rule out this alternative explanation.

#### Causal Structure 2: The Fork Structure—Common Cause Scenario

The fork structure occurs when a variable $A$ influences both $B$ and $C$ ($A \rightarrow B$ and $A \rightarrow C$). This makes $A$ a common cause of $B$ and $C$. Graphically:


$$
\forkLARGE
$$

#### Rule 2: Condition on the Common Cause to Block Confounding of The Treatment/Outcome Association
To estimate the causal effect between $B$ and $C$, we need to adjust for the common cause $A$ (indicated by $\boxed{A}$). This adjustment blocks the confounding path and removes any non-causal association between $B$ and $C$.

##### Fork Structure: Motivating Example

Suppose data reveals that areas with higher rates of bicycle commuting also have lower levels of psychological distress. Does bicycle commuting directly reduce distress? Not necessarily. Sunshine hours ($A$) could be a common cause:

- Sunshine ($A$) encourages bicycle commuting ($B$).
- Sunshine ($A$) reduces psychological distress ($C$).

By adjusting for sunshine ($\boxed{A}$), the apparent link between bicycle commuting ($B$) and psychological distress ($C$) disappears. This is an example of a spurious association -- one that that does not arise from a direct causal relationship between $B$ and $C$ but rather from their common cause, $A$


#### Causal Structure 3: The Chain Structure (Mediators)

The chain structure ($A \rightarrow B \rightarrow C$) illustrates a scenario where $A$ affects $B$, and $B$ in turn affects $C$. In this case, $B$ acts as a mediator in the relationship between $A$ and $C$. Graphically:

$$
\chainLARGE
$$

#### Rule 3: Avoid Conditioning on the Mediator for Total Effect of the Treatment and Outcome

If we are interested in estimating the total causal effect of $A$ on $C$, we should not condition on the mediator $B$. Conditioning on $B$ would **block** the path from $A$ to $C$, making $A$ and $C$ appear independent, even though there is a causal relationship. 

Many psychological scientists are taught to adjust for a rich array of confounders without attending to hazards of over-adjustment. Consider the following example of how over-adjustment can distort causal pathways, potentially masking true relationships.


##### Conditioning on a Mediator: Motivating Example

Suppose we examine whether renovating green spaces ($A$) in urban areas reduces crime rates ($C$). The renovation improves community engagement ($B$), which then reduces crime. If we adjust for community engagement, it might look like the green space renovation has no effect on crime, when in fact it does, but indirectly through community engagement. This example illustrates the risks of **over-conditioning bias.** Over-conditioning bias occurs when adjusting for variables that either lie on the causal path between treatment and outcome or act as mediators or colliders. Such adjustment can distort the causal relationships, potentially blocking the effect of interest or introducing spurious associations.

Let us denote the relative timing of events using subscripts. Here, we assume that $A_1 \to B_2 \to C_3$. It is clear from the rules of d-separation that $A_{1} \rightarrow \boxed{B_2} \rightarrow C_{3}$ (conditioning on $B_2$) blocks the path from $A_1$ to $C_3$.  However, what if $B_0 \to A_1$ and $B_0 \to C_3$. The rules of d-separation tell us that to obtain an unbiased causal effect estimate of $A_1 \to C_3$ we must condition on $B_0$.  With cross-sectional data we cannot typically ensure the relative timing of confounders, treatments, and outcomes.  For this reason, we should collect time-series data to ensure that the relative timing of events assumed in our model is appropriate. 

In some cases, investigators may want to distinguish between direct and indirect effects, as is done in causal mediation analysis. However, performing causal mediation analysis requires making additional assumptions about the relationships among variables and about unmeasured confounding. These assumptions are often difficult to verify. Causal mediation analysis goes beyond the scope of our current discussion, however interested readers may refer to @vanderweele2015; @bulbulia2024swigstime.

#### Causal Structure 4: The Collider Structure: Conditioning on the Common Effect of a Treatment and Outcome.

The collider structure ($A \to C$, $B \to C$) occurs when two variables $A$ and $B$ independently cause a common effect $C$. Initially, $A$ and $B$ are independent. However, conditioning on the collider $C$ introduces a non-causal or "spurious" association between $A$ and $B$. Graphically:

$$
\immoralityLARGE
$$

This gives us two rules: 

#### Rule 4: Do Not Condition on a Collider of the Treatment and Outcome

When assessing the causal effect of $A$ on $B$, avoid conditioning on a collider ($C$) or its descendants, as it may introduce a misleading association between $A$ and $B$.


#### Rule 5: Do Not Condition on Descendants of Colliders of the Treatment and Outcome

Causal relationships induce associations. Conditioning on a descendant of a collider is similar to conditioning on its parent, as the descendant serves as a proxy for its parent. Therefore, we should avoid conditioning on a descendant of a collider (or mediator).

#### Conditioning on a Collider: Motivating Example

Consider a scenario where we want to determine whether access to green spaces ($A$) makes people happier ($B$). If we condition on health status ($C$), which is influenced by both $A$ and $B$, we may introduce a spurious association between $A$ and $B$. This occurs because conditioning on $C$ creates an artificial link between $A$ and $B$, even if no direct causal relationship exists between them.

For instance, among individuals with poor health (low $C$), those with greater access to green spaces ($A$) may appear less happy ($B$). When we stratify by $C$, a negative association between green space access and happiness might emerge, even if $A$ and $B$ are not causally related. Similarly, among healthy individuals (high $C$), those with less access to green spaces ($A$) may seem happier ($B$), again suggesting a negative association upon stratifying by $C$.

This example demonstrates the risks of over-conditioning bias. By ensuring that confounders precede both the treatment and the outcome, we can often avoid collider bias, though not always. In our upcoming discussion of M-bias, we will explore how adjusting for pre-treatment confounders may sometimes induce collider bias. Additionally, we will discuss when conditioning on post-treatment proxies for pre-treatment confounders ($L^\prime$) may be a practical strategy for confounding control.

For now, it is essential to recognise that collider bias is a form of over-conditioning bias. Its threat to valid causal inference can be understood by examining the basic relationships in a causal Directed Acyclic Graph (DAG). Over-conditioning bias has not been fully appreciated in traditional structural equation modelling approaches. Pearl’s rules of d-separation show that when two arrows converge on the same node, their parent nodes may become spuriously associated. To estimate any causal path, it must remain unblocked by other nodes and free from unmeasured common causes.
 -->


## Part 3: How to Use Causal Diagrams for Causal Identification Tasks—Worked Examples {#section-part3}

### Notation

In the previous section, we introduced causal Directed Acyclic Graphs (causal DAGs), which use specific symbols to represent key elements in causal inference [@pearl1995; @pearl2009; @greenland1999]. Here, we adopt the following notation to illustrate how to use causal DAGs:

- **Treatment ($A$)**: This represents an intervention or exposure we are interested in studying, such as giving people access to green space.
- **Outcome ($Y$)**: This is the variable of interest that may be affected by the treatment, such as subjective well-being. Recall that $Y(a)$ denotes the outcome $Y$ when the treatment $A$ is set to a specific value $a$.
- **Measured Confounders ($L$)**: These are variables that we have measured and that may influence both the treatment and the outcome, such as age, gender, or income.
- **Measured Descendants of Confounders Not Affected by Treatment or Outcome ($L^\prime$)**: These are variables that are effects (descendants) of confounders, which we have measured and are not affected by the treatment or outcome. By adjusting for such variables, we can account for the effects of confounders; in other words, these descendants can serve as proxies for the confounders.
- **Unmeasured Confounders ($U$)**: These are variables that we have not directly observed but that may influence both the treatment and the outcome, potentially leading to a non-causal or "spurious" association between them.

Table @tbl-04 provides seven worked examples that demonstrate how to use causal diagrams in practice. In these examples, we focus on the question of whether access to green space affects happiness. We explore how different assumptions about (i) the structure of the world and (ii) the observational data collected can influence strategies for controlling confounding and our confidence in the results. Each example corresponds to a row in the table.

::: {#tbl-04}
```{=latex}
\terminologyelconfoundersLONG

Putting causal directed acyclic graphs (DAGs) to work.
```
:::

### 1. Confounding by a Common Cause

Table @tbl-04, Row 1, describes the problem of confounding by a common cause. We encountered this issue in [Part 2](#section-part2). Confounding arises when there is a variable or set of variables, denoted by $L_0$, that influences both the treatment ($A_1$) and the outcome ($Y_2$). Because $L_0$ is a common cause of both $A_1$ and $Y_2$, it can create a statistical association between them that does not reflect a causal relationship.

For example, in the context of green spaces, consider people who live closer to green spaces (treatment $A_1$) and their experience of improved happiness (outcome $Y_2$). A common cause might be socioeconomic status ($L_0$). Individuals with higher socioeconomic status might have the financial means to afford housing near green spaces and simultaneously have better access to healthcare and lifestyle choices that contribute to greater happiness. Thus, although data may show a statistical association between living closer to green spaces ($A_1$) and greater happiness ($Y_2$), this association might not reflect a direct causal relationship due to confounding by socioeconomic status ($L_0$).

To address confounding by a common cause, we adjust for the confounder in our statistical model. This adjustment can be done through methods such as regression analysis or more advanced techniques like inverse probability of treatment weighting and marginal structural models (see @hernan2024WHATIF). Adjusting for $L_0$ effectively **blocks the backdoor path** from the treatment to the outcome. In other words, conditioning on $L_0$ leads to d-separation of $A_1$ and $Y_2$ because $\boxed{L_0}$ closes the backdoor path from $A_1$ to $Y_2$.

Table @tbl-04, Row 1, Column 3, emphasises that a confounder must precede both the treatment and the outcome in time. Although in cross-sectional data (data collected at a single point in time) it is sometimes clear whether a confounder precedes the treatment (e.g., a person’s country of birth), often the relative timing of events is unclear. Therefore, we must draw several causal diagrams to consider the implications of different assumptions. Below, we return to the idea that investigators should draw multiple causal DAGs and perform multiple analyses.

### 2. Mediator Bias

Table @tbl-04, Row 2, presents a problem of mediator bias. Let’s reconsider whether proximity to green spaces ($A_0$) affects happiness ($Y_2$), and suppose that physical activity is a mediator, denoted by $L_1$.

Imagine that living close to green spaces ($A_0$) increases physical activity ($L_1$), which in turn improves happiness ($Y_2$). If we mistakenly treat physical activity ($L_1$) as a confounder and adjust for it in our analysis, we will bias our estimate of the total effect of green space proximity ($A_0$) on happiness ($Y_2$). This bias occurs because adjusting for $L_1$ blocks part of the causal pathway from $A_0$ to $Y_2$, effectively breaking the link between them.

Again, this bias is known as mediator bias, and as mentioned in [Part 2](#section-part2), experiments are not immune to such over-adjustment biases. (Recall that @montgomery2018 found that over half of all published research in political science experiments were guilty of conditioning on a post-treatment variable.)

To avoid mediator bias when estimating the total causal effect, we should never condition on a mediator. The surest way to prevent this problem is to ensure that data collection for covariates we assume to be confounders ($L_0$) occurs before data collection of the treatment ($A_1$), which in turn occurs before data collection of the outcome ($Y_2$) [@vanderweele2020]. We present this solution in Table @tbl-04, Row 2, Column 3.

### 3. Confounding by Collider Stratification (Conditioning on a Common Effect)

Table @tbl-04, Row 3, illustrates a problem known as collider bias. Recall that this type of bias occurs when we condition on a variable that is a common effect (a "collider") of both the treatment ($A_1$) and the outcome ($Y_2$), denoted by $L_3$.

Suppose that access to green spaces ($A_1$) and happiness ($Y_2$) are actually independent; that is, there is no causal relationship between them. Furthermore, assume that physical health ($L_3$) is affected by both access to green space and happiness. In other words, $L_3$ is a common effect of $A_1$ and $Y_2$.

If we condition on $L_3$—for example, by including it as a covariate in our analysis—we may inadvertently introduce an association between $A_1$ and $Y_2$ where none exists. This happens because knowing the value of $L_3$ gives us information about both $A_1$ and $Y_2$, creating a spurious association between them. This phenomenon is known as collider stratification bias.

To avoid collider bias, it is important to carefully consider which variables we adjust for in our analysis. Specifically, we should avoid conditioning on variables that are common effects of the treatment and outcome. Instead, we should focus on measuring and adjusting for variables that are common causes (confounders) of both the treatment and the outcome.

In practice, we can mitigate the risk of collider bias by ensuring the following:

  -	Measure all common causes ($L_0$) of the treatment ($A_1$) and outcome ($Y_2$) before the treatment occurs.
  -	Ensure that the outcome ($Y_2$) is measured after the treatment ($A_1$) occurs.


### 4. Confounding by Conditioning on a Descendant of a Confounder

Table @tbl-04, Row 4, presents a situation where conditioning on a descendant (an effect) of a confounder can introduce bias. This happens when we adjust for a variable that is influenced by an unmeasured confounder.

For instance, imagine that poor health is an unmeasured confounder that affects both access to green spaces ($A_1$) and happiness ($Y_2$). Suppose further that poor health is an outcome of both the treatment and outcome. To avoid confounding, we must measure poor health before the $A_1$ and $Y_2$.  Suppose further we have data on doctor visits ($L_4$), which are a consequence of poor health, but the count of doctors visits is compiled after the outcome has occurred. If we condition on doctor visits ($L_4$), we are effectively conditioning on a descendant of the unmeasured confounder (poor health at Time 3). This can introduce bias into our analysis because it can open a backdoor path between the treatment and outcome, leading to confounding. The solution here is clear. Do not condition on $L_4$. Rather, ensure that the the confounders are not effects of the treatment. If the data are not available then draw several causal graphs to clarify how estimation is compromised (see: [Part 4](#section-part4)). 


### 5. M-Bias: Conditioning on a Pre-Exposure Collider {#section-mbias}

Table @tbl-04, Row 5, presents a form of bias known as M-bias, which arises from conditioning on a pre-exposure collider. This bias combines the collider structure and the fork (common cause) structure, revealing that it is possible to induce confounding even if all variables have been measured before the treatment.

In the causal diagram, the collider structure is evident in the path $U_Y \to L_0 \leftarrow U_A$. Conditioning on $L_0$ opens a path between $U_Y$ and $U_A$. As a result, $U_Y$ becomes associated with $U_A$, creating a backdoor path from the treatment ($A_1$) to the outcome ($Y_2$) through $U_A$ and $U_Y$. This leads to confounding, even though we have adjusted for $L_0$.

How might this confounding play out in a real-world setting?

In the context of green spaces, consider that an individual’s level of physical activity ($L_0$) is influenced by an unmeasured factor related to their propensity to live near green spaces ($A_1$)—say, their childhood upbringing ($U_A$). Suppose further that another unmeasured factor—say, a genetic predisposition ($U_Y$)—increases both physical activity ($L_0$) and happiness ($Y_2$). Here, physical activity ($L_0$) does not affect the decision to live near green spaces ($A_1$) or happiness ($Y_2$) directly but is a descendant of unmeasured variables that do.

If we condition on physical activity ($L_0$) in this scenario, we create the bias just described—M-bias. By adjusting for $L_0$, we open a path between $U_A$ and $U_Y$, which introduces confounding between $A_1$ and $Y_2$.

How can we respond to this problem? The solution is straightforward: if $L_0$ is not a common cause of $A_0$ and $Y_0$, or a descendent of a common cause, $L-0$ should not be included in our causal model. In terms of the conditional exchangeability principle, here we find that $A_1$ and $Y_2(a)$ are independent ($A_1 \coprod Y_2(a)$), but they become dependent when we condition on $L_0$ ($A_1 \cancel{\coprod} Y_2(a) \mid L_0$). Therefore, again, we should not condition on $L_0$—that is, do not control for physical activity in this case [@cole2010].

### 6. Conditioning on a Descendant May Sometimes Reduce Confounding {#section-conditioning-on-descendents}

In Table @tbl-04, Row 6, we encounter a causal diagram where an unmeasured confounder creates a backdoor path between the treatment and the outcome. Here, we explore how we can use the rules of d-separation to find unexpected strategies for controlling confounding.

Returning to our green space example, suppose there is an unmeasured genetic factor ($U$) that influences both an individual’s preference for living near green spaces ($A_1$) and their happiness ($Y_2$). If such an unmeasured confounder exists, it seems we cannot obtain an unbiased estimate of the causal effect of green space access on happiness because we cannot adjust for $U$.

However, imagine there is a variable $L_3'$, a trait that emerges later in life as a result of the genetic factor $U$. Even though $L_3'$ is expressed after the treatment and outcome have occurred, we can measure it. By controlling for $L_3'$, we can help close the backdoor path between the treatment and outcome. This works because $L_3'$ serves as a proxy for the unmeasured confounder $U$. By adjusting for $L_3'$, we partially account for the influence of $U$.

Therefore, effective confounding control does not always require measuring pre-exposure variables. This example shows that sometimes we can control for confounding by conditioning on a post-outcome variable, which is not intuitive. Although the common cause ($U$) must occur before the treatment and outcome, its proxy ($L_3'$) does not have to. If we can measure the proxy but not the confounder itself, we should consider conditioning on the post-treatment proxy of a pre-treatment confounder.

### 7. Confounding Control with Three Waves of Data: Estimating an “Incident Exposure” Effect

Table @tbl-04, Row 7, addresses another scenario involving unmeasured confounding. Here, we use the rules of d-separation to develop a data collection and modeling strategy that can significantly reduce the impact of unmeasured confounders.

By collecting data on both the treatment and the outcome at an initial baseline time point (e.g., time 0) and controlling for their baseline values, we can mitigate confounding. When we include the baseline measurements of the treatment ($A_0$) and the outcome ($Y_0$) in our analysis, any unmeasured association between the treatment at a later time ($A_1$) and the outcome at a subsequent time ($Y_2$) would need to be independent of these baseline measurements. Including these baseline variables, along with other measured covariates, provides considerable control over confounding [@vanderweele2020].

Moreover, this approach offers an additional benefit. By controlling for the baseline exposure ($A_0$), we focus on individuals who have changed their level of exposure between the baseline and the follow-up period. This allows us to estimate the causal effect of initiating or changing access to green space, known as the incident exposure effect. This effect is more informative for understanding the impact of changes in exposure over time.

This strategy better emulates a "target trial"—an approach where observational data are organized to mimic a hypothetical randomized experiment starting at a defined "time zero" (see @hernán2016; @danaei2012; @vanderweele2020; @bulbulia2022). Without controlling for the baseline treatment, we could only estimate a prevalent exposure effect, which reflects the association with existing levels of exposure rather than changes in exposure. If initial exposure to green space had caused some people to become less happy, we would not detect this effect without considering changes over time.

Finally, by controlling for both the baseline treatment ($A_0$) and the baseline outcome ($Y_0$), we further reduce the potential for unmeasured confounding. For an unmeasured confounder to bias our estimates, it would need to affect both the treatment and the outcome independently of their baseline values. Collecting repeated measures over time allows us to use past states of the treatment and outcome to exert more robust control over unmeasured confounding.

### Summary

All forms of confounding bias stem from combinations of the basic causal structures outlined above—absence of cause, causality, forks (common causes), chains (mediators), and colliders (common effects). Understanding these elements allows us to identify potential confounders based on the assumptions encoded in a causal diagram.

There are three essential observations we wish to reiterate:

1. **Causal DAGs Structure Assumptions but Do Not Prove Causation**. Causal directed acyclic graphs (DAGs) provide a framework for structuring assumptions about causal relationships so that investigators may clarify whether causal effects can be identified from data, and how. The data do not typically determine which Causal DAG investigators should chose, as multiple causal diagrams are typically consistent with the data. The strength of causal DAGs lies in helping researchers understand the implications of their assumptions. Therefore, causal DAGs should be created in collaboration with domain experts, because experts will typically have a better understanding of which assumptions are important and credible. When experts disagree, multiple causal diagrams should be proposed to reflect differing assumptions.
2. **Temporal Order is Crucial in Causal Inference**. Causal inference relies on the assumption that the treatment precedes the outcome in time. Cross-sectional data—that is, data collected at a single point in time—typically lack the temporal resolution investigators require to establish the order of events. Without knowing whether the exposure occurred before the outcome, it’s difficult to make causal inferences.
3. **Longitudinal Data are Valuable but Not Sufficient**: Longitudinal data, which follow the same subjects over multiple time points, allow for a clearer temporal ordering of events. However, longitudinal data alone are insufficient to establish causality. Although we should seek longitudinal data to confirm that confounders precede the treatment and that the treatment precedes the outcome, longitudinal data are not a cure-all. As we have seen, obtaining a causal effect estimate requires a workflow that begins by stating a clearly defined causal question, with reference to a counterfactual contrast between specified treatments applied to a target population. We must then ensure that all assumptions required for causal inference are satisfied, particularly that treatment effects can be identified from the data. Only after we have stated our causal question and checked that our data support the necessary causal assumptions can we proceed to statistical analysis.

## Part 4.  Practical Guide For Constructing Causal Diagrams and Reporting Results When Causal Structure is Unclear {#section-part4}


#### 1. Clarify the Research Question and Target Population

Before drawing a causal diagram, we must state the problem it addresses and the population to whom it applies. Causal identification strategies may vary by question. We have seen that if $A\to B\to C; A\to B; A\to C$  the confounding control strategy for evaluating $A \to C$ differs from that for $B \to C$. Again, if our interest is in estimating $B \to C$ then reporting a coefficient for $A$ would be  ill-advised because the path $A\to C$ contains the mediator $B$; see @westreich2013; @mcelreath2020; @bulbulia2023.

### 2 Evaluate Bias in the Absence of a Treatment Effect

Before attributing any statistical association to causality, we must eliminate non-causal sources of correlation. We do this by, first, by drawing the treatment ($A$) and outcome ($Y$) on our causal diagram with no arrow linking them. As mentioned in [Part 2](#section-part2), do not draw an arrow between $A$ and $Y$ because we are evaluating bias in the absence of a treatment effect. Second, we identify all common causes of $A$ and $Y$, and consider whether they are measured or unmeasured. Third, we use the rules of d-separation to evaluate whether conditioning on measured common causes of $A$ and $Y$ will block all backdoor paths that create indirect, non-causal associations between $A$ and $Y$. That is, we consider whether $A$ and $Y$ are d-separated by the measured common causes of $A$ and $Y$. If they are, then we have successfully blocked all backdoor paths and can proceed to evaluate bias in the presence of a treatment effect. If they are not, then we have failed to block all backdoor paths and our estimate of the causal effect of $A$ on $Y$ will be biased. 


#### 3. Draw the Most Recent Common Causes of Exposure and Outcome

Include all common causes (confounders) of both the exposure and the outcome in your diagram, whether measured or unmeasured. Where possible, group functionally similar common causes into a single variable (e.g., $L_0$ for demographic variables).

#### 4. Include All Ancestors of Measured Confounders

Add any ancestors (precursors) of measured confounders associated with the treatment, the outcome, or both. Simplify the causal diagram by grouping similar variables. For example, suppose we believe that both age and income are common causes of both the treatment and the outcome. We may represent this belief by grouping age and income into a single variable, $L_0 = \{age,~income\}$.

#### 5. Explicitly State Assumptions About Relative Timing

Annotate the temporal sequence of events using subscripts (e.g., $L_0$, $A_1$, $Y_2$). 

Note that it is imperative that causal DAGs are acyclic. The graph: $A \to Y \to A \to Y$ is not acyclic. **However, the demand for an acyclic graph does not imply that dynamic causal relationships are precluded from causal analysis.** For example $L_0 \to Y_1 \to  A_1 \to L_2 \to A_3 \to Y_5$ is an acyclic graph. Here, past states of the confounders, treatment, and outcome are depicted as affecting their future states.  By indexing the relative timing of states we ensure that the causal diagram is acyclic. 

Note this example further illustrates the potential difficulties when investigating multiple treatments. Suppose we are interested in causal contrasts for multiple treatments over time, for example the contrast between $A_1 = 0, A_3 = 0$ and $A_1 = 1, A_3 = 1$, with the outcome $Y_5$ measured at the end of study. Notice that $L_2$ is a common cause of $A_3$ and $Y_5$. We must condition on $L_2$ to block the back-door path from $A_3$ to $Y_5$. However, were we to condition on $L_2$, we would induce mediator bias because $L_2$ blocks the path from $A_1$ to $Y_5$.  We cannot use standard regression, multi-level regression, or structural equation modelling to estimate the time-varying treatment regime of interest. We instead require special methods (refer to @hernan2024WHATIF; @bulbulia2024swigstime).

#### 6. Arrange Temporal Order Visually

Arrange your diagram to reflect the temporal progression of causality, either left-to-right or top-to-bottom. This enhances comprehension of causal relations. Establishing temporal ordering is vital for evaluating identification problems, as discussed in [**Part 3**](#sec-part3).

#### 7. Box Variables Adjusted for Confounding

Mark variables for adjustment (e.g., confounders) with boxes or another easy to understand convention. Be clear about this and other conventions. 

#### 8. Present Paths Structurally, Not Parametrically

Focus on whether paths exist, not their functional form (linear, non-linear, etc.). Parametric descriptions are not relevant for bias evaluation in a causal diagram. For an explanation of causal interaction and diagrams, see @bulbulia2023.

#### 9. Minimise Paths to Those Necessary for the Causal Identification Problem

Reduce clutter by including only paths critical for the specific question (e.g., back-door paths, mediators).

#### 10. Consider Potential Unmeasured Confounders

Use domain expertise to identify potential unmeasured confounders and represent them in your diagram. This proactive step helps anticipate and address *all* possible sources of confounding bias.

#### 11. State Your Graphical Conventions

Establish and explain the graphical conventions used in your diagram in your manuscript (e.g., using red to highlight open back-door paths). Consistency in symbol use enhances interpretability, while explicit descriptions improve accessibility and understanding. 

#### 12. Prepare Sensitivity Analyses. 

Because unmeasured confounding cannot be ruled out, investigators should implement sensitivity analyses to determine how dependent conclusions are on specific assumptions or parameters within your causal model. A relatively simple sensitivity analysis is VanderWeele's E-value [@vanderweele2017]

### Specific Advice for Causal Analysis and Reporting in Cross-Sectional Designs


::: {#tbl-cs}

```{=latex}
\examplecrosssection
```
Cross-sectional designs typically require multiple causal DAGS where the temporal order of variables cannot be ensured.
:::



### Recommendations for Conducting and Reporting Causal Analyses with Cross-Sectional Data

When analysing and reporting analyses with cross-sectional data, researchers face the challenge of making causal inferences without the benefit of temporal information. 

The following recommendations aim to guide researchers in navigating these challenges effectively:

#### 1. **Draw multiple causal diagrams**

As shown in @tbl-cs, draw multiple causal diagrams to represent different theoretical assumptions about the relationships and timing of variables relevant to an identification problem. If some causal pathways cannot be ruled out, clarify the implications of assigning variables the roles for which consensus or which the time ordering of the data do not resolve. 

#### 2. **Perform and report analyses for each assumption**

Conduct and transparently report separate analyses for each scenario your causal diagrams depict. This practice ensures that your study is theoretically grounded for each model. Presenting results from each analytical approach and the underlying assumptions and statistical methods promotes a balanced interpretation of findings. 

#### 3. **Report divergent findings**

Approach conclusions with caution, especially when findings suggest differing practical implications. Acknowledge the limitations of cross-sectional data in establishing causality and the potential for alternative explanations. Do not over-sell.

#### 4. **Identify avenues for future research**

Target future research that might clarify ambiguities. Consider the design of longitudinal studies or experiments capable of clarifying lingering uncertainties.

#### 5. **Supplement observational data with simulated data** 

Leverage data simulation to understand the complexities of causal inference. Simulating data based on various theoretical models allows researchers to examine the effect of different assumptions on their findings. This method tests analytical strategies under controlled conditions, assessing the robustness of conclusions against assumption violations or unobserved confounders.



### Specific Advice for Causal Analysis and Reporting in Longitudinal Designs


Longitudinal designs offer a substantial advantage over cross-sectional designs for causal inference because sequential measurements allow us to capture causation and quantify its magnitude. We typically do not need to assert timing as in cross-sectional data settings. Because we know when variables have been measured, we can reduce ambiguity about the directionality of causal relationships. For instance, tracking changes in "happiness" following changes in access to green spaces over time can more definitively suggest causation than cross-sectional snapshots.

Despite this advantage, longitudinal researchers nevertheless face assumptions regarding the absence of unmeasured confounders or the stability of measured confounders over time. These assumptions must be explicitly stated.  As with cross-sectional designs, wherever assumptions differ, researchers should draw different causal diagrams that reflect these assumptions and subsequently conduct and report separate analyses.  Supplementary materials D and E provide examples of how to conduct and report analyses for multiple causal diagrams. The following summarises our advice. 



#### 1. Draw multiple causal diagrams

   - **Causal Identification problem diagram**: begin as usual by stating a causal question and population of interest. Then constructing a causal diagram that outlines your assumptions about the relationships among variables relevant for ensuring conditional exchangeability between the treatment and outcome.

   - **Solution diagram**: next, create a separate causal diagram that proposes solutions to the identified problems. Having distinct diagrams for the problem and its proposed solutions clarifies your study's analytic strategy and theoretical underpinning.

@tbl-lg provides an example of a table with multiple causal diagrams clarifying potential sources of confounding threats and reports strategies for addressing them. 

::: {#tbl-lg}

```{=latex}
\examplelongitudinal
```
Use causal DAGs to report both the causal identification problem and its solution.
:::


#### 2. Attempt longitudinal designs with at least three waves of data

Incorporating repeated measures data from at least three time intervals considerably enhances your ability to infer causal relationships. For example, by adjusting for physical activity measured before the treatment, we can ensure that physical activity does not result from a new initiation to green spaces, which we establish by measuring green space access at baseline. Establishing chronological order allows us to avoid confounding problems 1-4 in @tbl-04. 


#### 3. Where causality is unclear, report results for multiple causal graphs

Given that the true causal structure may be complex and partially unknown, analysing and reporting results under each plausible causal diagram is prudent. 

#### 4. Address missing data at baseline and study attrition

Longitudinal studies often need help with missing data and attrition, which can introduce bias and affect the validity of causal inferences. Implement and report strategies for handling missing data, such as multiple imputation or sensitivity analyses that assess the bias arising from missing responses at the study's conclusion. (For more about addressing missing data, see: [@bulbulia2024PRACTICAL]). 

By following these recommendations, you will more effectively navigate the inherent limitations of observational longitudinal data, improving the quality of your causal inferences.


### On the Differences Between Methods for Causal Inference And The Statistical Modelling Tradition 

In traditional statistical methods such as regression analysis and structural equation modelling (SEM), the focus often lies in estimating associations among variables, sometimes treating all variables symmetrically within the model. However, causal inference shifts the focus to estimating pre-specified causal contrasts, defined explicitly in terms of interventions and outcomes measured on a target population. This approach takes us beyond measuring associations in the data to understanding the effects of hypothetical and actual interventions. 

Here, we have focussed the place of counterfactual contrasts between  well-defined outcomes experienced by a target population under different levels of treatment.  It is important to emphasise that the other variables in a model are typically treated as nuisance variables—variables that are not of direct interest, but must be accounted for to obtain valid causal effect estimates. In causal inference, nuisance parameters often include confounders that need to be adjusted to block non-causal associations between a treatment of interest and the outcomes of interest. However, the coefficients associated with these nuisance variables generally lack a causal interpretation. For example, consider a causal pathway where $L \to A \to Y$, and we are interested in estimating the effect of $A$ on $Y$ while controlling for $L$. Although controlling for $L$ is necessary to obtain an unbiased estimate of the causal effect of $A$ on $Y$, the coefficient of $L$ in the model does not have a straightforward causal interpretation and reporting it may be misleading, especially if $A$ mediates the effect of $L$ on $Y$.

By concentrating on pre-specified causal contrasts and appropriately handling nuisance parameters, causal inference methods enable clarity for pre-specified causal contrasts of interest. This approach departs from traditional statistical methods by emphasising the estimation of specific causal effects rather than modelling the entire system of associations. It acknowledges that not all parameters in a model can simultaneously be of equal interest; some parameters primarily serve to adjust for confounding or other biases and should not be interpreted causally.

Understanding and properly addressing nuisance parameters are critical steps in a causal inference workflow. The observation that nuisance parameters typically lack causal interpretation suggests that the standard practice of reporting all model coefficients may be unwise or even misleading (see @westreich2013; @mcelreath2020; @bulbulia2023; @Lonati_Lalive_Efferson_2024). When multiple treatments are of interest, it is essential to ensure that the identification assumptions required for each causal effect are satisfied—a task that can be more challenging than it appears (see @vanderweele2015; @hernan2008aObservationalStudiesAnalysedLike; @robins1986; @robins1992; @robins2008estimation; @bulbulia2024eigstime).

Our questions are nearly always causal. We want to know what would happen, on average, if we were to intervene in the world. In the observational psychological sciences, obtaining such understanding requires a paradigm shift from traditional statistical modelling. Such a paradigm shift has become established in economics, computer science, and epidemiology, and they are making strong inroads in political science. However, in observational psychological sciences, including environmental psychology, methods for causal inference remain rare. We hope this chapter inspires readers to begin developing robust causal workflows capable of addressing their causal questions and those that have long animated the field's scientific interests but which have yet to be coherently addressed. 



## Where to Go Next?

There are many good resources available for learning causal directed acyclic graphs [@rohrer2018; @greenland1999; @glymour2008causal; @hernan2024WHATIF; @cinelli2022; @barrett2021; @mcelreath2020; @greenland1999; @suzuki2020; @pearl2009a; @pearl_greenland_2007; @major2023exploring; @greenland1999; @morgan2014].  For those just getting started on causal diagrams, we recommend Miguel Hernan's free course here: [https://www.edx.org/learn/data-analysis/harvard-university-causal-diagrams-draw-your-assumptions-before-your-conclusions, accessed 10 June 2024](https://www.edx.org/learn/data-analysis/harvard-university-causal-diagrams-draw-your-assumptions-before-your-conclusions). For those seeking a slightly more technical but still accessible introduction to causal inference and causal DAGs, we recommend Brady Neal's introduction to causal inference course and textbook, both freely available here [https://www.bradyneal.com/causal-inference-course, accessed 10 June 2024](https://www.bradyneal.com/causal-inference-course). For those interested in causal inference in experiments refer to @hernan2017per, @montgomery2018; @bulbulia_2024_experiments. For those interested in causal mediation analysis and time-varying treatments refer to @robins1986; @robins1992; @robins2008estimation; @vanderlaanRobins2003CensoringLongitudinal; @diaz2021nonparametric; @williams2021; @hoffman2023;@bulbulia2024swigstime. Moreover, the fifth segment of Miguel Hernan's free online course covers time-varying treatments:  [https://www.edx.org/learn/data-analysis/harvard-university-causal-diagrams-draw-your-assumptions-before-your-conclusions, accessed 10 June 2024](https://www.edx.org/learn/data-analysis/harvard-university-causal-diagrams-draw-your-assumptions-before-your-conclusions). On the pitfalls of traditional path modelling refer to @rohrer2022PATH. We have set aside complications arising from measurement error. For more on this topic see @hernan2009MEASUREMENT; @hernan2024WHATIF; @bulbulia2024wierd; @vanderweele2022; @vanderweele2022a.


{{< pagebreak >}}

## Funding

This work is supported by a grant from the Templeton Religion Trust (TRT0418). JB received support from the Max Planck Institute for the Science of Human History. The funders had no role in preparing the manuscript or deciding to publish it.

## Contributions

DH proposed the chapter. JB developed the approach and wrote the first draft. Both authors contributed substantially to the final work.



<!-- 

### The Five Elementary Rules For Evaluating Confounding

To review, causal diagrams allow researchers to visualise and systematically identify potential confounders and strategies for adjusting for them. There are five basic graphical structures:

#### 1. **Causality Absent**  $A$ does not cause $B$: absent any common causes, there is no statistical association between them.

$$\xorxA$$ 

#### 2. **Causality Present**  $A$ causes $B$: absent conditioning that blocks them, $A$ and $B$ will be statistically associated.

$$\xtoxA$$

#### 3. **The Fork Structure** $A$ causes $B$ and $A$ causes $C$: absent conditioning on $A$, $B$ and $C$ will be statistically associated. Conditional on $A$, $B$ and $C$ will be independent.

$$\forkTINY$$

#### 4. **The Chain Structure**  $A$ causes $B$ and $B$ causes $C$: absent conditioning on $B$, $A$ and $C$ will be statistically associated. Conditioning on $B$, $A$ and $C$ will be independent. 

$$\chainTINY$$ 

#### 5. **A Collider Structure**  $A$ causes $C$ and $B$ causes $C$: absent conditioning on $C$, $A$ and $B$ will be statistically independant. Conditioning on $C$, $A$ and $B$ will be statistically associated. 

$$\immoralityTINY$$

From these five elementary structures, we discovered four rules that allow us to use these structures to evaluate confounding and its control:

#### 1. **The Fork Rule** 

When a common cause influences treatment and outcome, condition on the common cause to avoid bias.

#### 2. **The Chain Rule**

  (i)  For total effect estimates, avoid conditioning on mediators within the causal path.
  (ii) For mediation analysis, ensure potential confounders do not introduce bias. )(Note: mediation analysis is complex [@vanderweele2015; @vansteelandt2012; @bulbulia2023].)

#### 3. **The Collider Rule**

Conditioning on a common effect opens a path between the two variables that cause it.  

#### 4. **The Proxy Rule**

Conditioning on a descendant is a proxy for conditioning on its parent.  -->

{{< pagebreak >}}

## References

::: {#refs}
:::


