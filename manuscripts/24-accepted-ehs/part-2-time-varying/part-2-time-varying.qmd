---
title: 'Methods in Causal Inference Part 2: Interaction, Mediation, and Time-Varying Treatments'
abstract: |
 The analysis of 'moderation', 'interaction', 'mediation', and 'longitudinal growth' is widespread in the human sciences, yet subject to confusion. To clarify these concepts, it is essential to state causal estimands, which requires specifying counterfactual contrasts for a target population on an appropriate scale. Once causal estimands are defined, we must consider their identification. I employ causal directed acyclic graphs (causal DAGs) and Single World Intervention Graphs to elucidate identification workflows. I show that when multiple treatments exist, common methods for statistical inference, such as multi-level regressions and statistical structural equation models, cannot typically recover the causal quantities we seek. By properly framing and addressing causal questions of interaction, mediation, and time-varying treatments, we can expose the limitations of popular methods and guide researchers to a clearer understanding of the causal questions that animate our interests.
 
 **KEYWORDS**: *DAGs*; *Mediation*; *Moderation*; *SWIGs*; *Time-varying Treatments*

author: 
  - name: Joseph A. Bulbulia
    affiliation: Victoria University of Wellington,  NEW ZEALAND
    orcid: 0000-0002-5861-2056
    email: joseph.bulbulia@vuw.ac.nz
    corresponding: no
editor_options: 
  chunk_output_type: console
format:
  pdf:
    sanitise: true
    keep-tex: true
    link-citations: true
    colorlinks: true
    documentclass: article
    classoption: [single column]
    lof: false
    lot: false
    geometry:
      - top=30mm
      - left=25mm
      - heightrounded
      - headsep=22pt
      - headheight=11pt
      - footskip=33pt
      - ignorehead
      - ignorefoot
    template-partials: 
      - /Users/joseph/GIT/templates/quarto/title.tex
    header-includes:
      - \input{/Users/joseph/GIT/latex/latex-for-quarto.tex}
date: last-modified
pdf-engine: lualatex
execute:
  echo: false
  warning: false
  include: true
  eval: true
fontfamily: libertinus
bibliography: /Users/joseph/GIT/templates/bib/references.bib
csl: /Users/joseph/GIT/templates/csl/apa-7.csl
---


```{r}
#| label: load-libraries
#| echo: false
#| include: false
#| eval: true

## WARNING SET THIS PATH TO YOUR DATA ON YOUR SECURE MACHINE. 
# pull_path <-
#   fs::path_expand(
#     #'/Users/joseph/v-project\ Dropbox/Joseph\ Bulbulia/00Bulbulia\ Pubs/DATA/nzavs_refactor/nzavs_data_23'
#     '/Users/joseph/Library/CloudStorage/Dropbox-v-project/Joseph\ Bulbulia/00Bulbulia\ Pubs/DATA/nzavs-current/r-data/nzavs_data_qs'
#   )
# 


push_mods <-  fs::path_expand(
  '/Users/joseph/Library/CloudStorage/Dropbox-v-project/data/nzvs_mods/24/church-prosocial-v7'
)


#tinytext::tlmgr_update()

# WARNING:  COMMENT THIS OUT. JB DOES THIS FOR WORKING WITHOUT WIFI
#source('/Users/joseph/GIT/templates/functions/libs2.R')
# # WARNING:  COMMENT THIS OUT. JB DOES THIS FOR WORKING WITHOUT WIFI
# source('/Users/joseph/GIT/templates/functions/funs.R')

#ALERT: UNCOMMENT THIS AND DOWNLOAD THE FUNCTIONS FROM JB's GITHUB

# source(
#   'https://raw.githubusercontent.com/go-bayes/templates/main/functions/experimental_funs.R'
# )
# 
# source(
#   'https://raw.githubusercontent.com/go-bayes/templates/main/functions/funs.R'
# )

# check path:is this correct?  check so you know you are not overwriting other directors
#push_mods

# for latex graphs
# for making graphs
library('tinytex')
library('extrafont')
library('tidyverse')
library('kableExtra')
#devtools::install_github('go-bayes/margot')
library(margot)
loadfonts(device = 'all')
```


## Introduction  {#id-introduction}  

The young Charles Darwin was a keen fossil hunter and amateur geologist. In August 1831, he accompanied the geologist Adam Sedgwick to the Glyderau mountain range in northern Wales.

> We spent many hours in Cwm Idwal, ... but neither of us saw a trace of the wonderful glacial phenomena all around us; we did not notice the plainly scored rocks, the perched boulders, the lateral and terminal moraines. Yet these phenomena are so conspicuous that ... a house burnt down by fire did not tell its story more plainly than did this valley. If it had still been filled by a glacier, the phenomena would have been less distinct than they now are. [@darwin1887life: p.25]

This 'striking instance of how easy it is to overlook phenomena, however conspicuous' [@darwin1887life: p.25] is cited in cultural evolution to emphasise the importance of theory for organising observations [@wilson2008evolution]. However, the importance of theory to scientific discovery carries broader relevance: it applies to the statistical methods scientists routinely apply to the data they collect. Without a clear framework that relates statistical models to observations, the understanding we seek from our data remains elusive.

Across many human sciences, we apply statistical models to data and report 'moderation', 'interaction', 'mediation', and 'longitudinal growth'. How are we to interpret the results of these models? It is often unclear. The confidence with which investigators report findings does not make interpretation any clearer. 

Answering a causal question requires a careful workflow that begins by defining the causal quantities and the target population of interest. We specify a causal quantity, or estimand, as the contrast between counterfactual outcomes under two (or more) clearly defined interventions. Mathematical proofs establish that we can consistently estimate the size of this counterfactual difference using data, provided certain assumptions are met (discussed below). The subsequent steps in a causal inferential workflow involve assessing the credibility of these necessary assumptions, constructing appropriate estimators, and obtaining relevant data. Only after these steps should statistical analysis be attempted. Without adhering to a causal inferential workflow, the associations derived from statistical models will reflect true causal relationships only by chance, regardless of the sophistication of our statistical methods [@westreich2013].

There is good news. Progress in the health sciences, computer science, and economics has led to a common vocabulary and robust workflows that enable investigators to formulate causal questions addressable with data. These advancements allow for the evaluation of assumptions necessary for obtaining consistent estimates, the construction of valid statistical estimators, and the application of statistical models at the end of the workflow. This conceptual framework, grounded in mathematical proofs, enables investigators to clarify, communicate, and evaluate causal questions using observational data. The consensus that has emerged in causal inference over the past several decades is, in my view, as transformative for the human sciences as the theory of glaciation was for geology, or as Darwin's theory of evolution was for biology. By refreaming questions of interaction, mediation, and time-varying treatments as causal questions, I aim to clarify the framework’s interest, relevance, and power.

Several excellent resources are available that clarify workflows for causal inference, from stating causal questions to communicating results [@hernan2024WHATIF; @vanderweele2015; @tlverse_handbook; @grf2024; @morgan2014; @montgomery2018; @neal2020introduction; @pearl2009a].

Here, my ambition is focussed.

[Part 1](#id-sec-1) considers how to ask causal questions when our interest is in comparing effect magnitudes between groups (effect modification).

[Part 2](#id-sec-2) considers how to ask causal questions when our interest is in evaluating the joint effects of two independent interventions (interaction).

[Part 3](#id-sec-3) considers how to ask causal questions when our interest is in evaluating the joint effects of two dependent interventions (mediation analysis).

[Part 4](#id-sec-4) considers how to ask causal questions when our interest is in evaluating two or more sequential treatments of the same kind (time-varying treatments).

I begin with a brief introduction to key concepts and terminology.


### Fundamental Assumptions for Causal Inference

Consider indicators $A$ and $Y$ measuring states of the world. Let $A$ denote the 'treatment' or 'exposure' and $Y$ the 'outcome'.  For unit $i$, we say that $A_i$ causes $Y_i$ if changing $A_i$ from one level, say $A_i = a^*$, to another level, $A_i = a$, leads to a different outcome for $Y_i$. We assume $A_i$ occurs before $Y_i$. To compare these outcomes, we use the notation $Y_i(\tilde{a})$, which represents the outcome for unit $i$ under the treatment level $A_i = \tilde{a}$. To determine whether $Y_i(\tilde{a})$ quantitatively differs under two treatment levels on the difference scale, we would compute the contrast $Y_i(a^*) - Y_i(a)$. If $Y_i(a^*) - Y_i(a) \neq 0$, we would say there is a causal effect of $A$ on $Y$ for individual $i$. 

Note that, for any given application of $A$ for unit $i$, we can only observe one level of treatment. Therefore, we refer to $Y_i(a^*) - Y_i(a) \neq 0$ as a counterfactual contrast, or equivalently, as a contrast of potential outcomes. Because an individual may only receive one of two treatments at any given time, individual causal effects cannot generally be observed. However, when certain assumptions are satisfied, we may compute average treatment effects by aggregating individual observations under different treatment conditions. For a binary treatment, the difference in the average of the potential outcomes under two different treatment levels for the population from which a sample is drawn may be expressed as the difference in mean outcomes: $\mathbb{E}[Y(1)] - \mathbb{E}[Y(0)]$ or equivalently as the average of the differences of the potential outcomes: $\mathbb{E}[Y(1) - Y(0)]$.

This counterfactual contrast represents the quantity obtained from an ideally conducted randomised controlled trial, where any common cause of the treatment and the outcome would occur only by chance. There are three fundamental assumptions for computing average treatment effects that, although generally satisfied in an ideally randomised experiment, may not be satisfied in observational or 'real world' data:

1. **Causal Consistency**: Treatment levels remain consistent within the treatment arms to be compared. There must be at least two arms.
2. **(Conditional) Exchangeability**: Covariates that might affect outcomes under treatment are balanced across all arms (implied by randomisation).
3. **Positivity**: Each covariate that might affect treatment in the target population has a non-zero probability of being observed within each treatment condition (refer to @westreich2010; @bulbulia2023a).

Note that we speak of an 'ideal' experiment because real-world experiments may fail these assumptions [@hernan2017per; @bulbulia_2024_experiments]. Our interest here is restricted to observational or 'real world' data.


### Schematic Workflow for Inferring Causal Effects from Real-World Data Before Stating Statistical Estimators and Performing Statistical Analysis

In causal inference, we do not apply statistical models to data until after we have stated a causal question and considered whether and how the question may be identified from the data. We take the following steps *before* considering a statistical estimator or estimation:


1. **State a well-defined treatment.**
   Clearly define the treatment (or equivalently exposure) that states the hypothetical intervention to which all population members will be exposed. For example, the treatment 'weight loss' is a vaguely stated intervention because there are many ways one can lose weight -- exercise, diet, depression, cancer, amputation, and others [@hernan2024WHATIF]. The intervention 'weight loss by at least 30 minutes of vigorous exercise each data' is more clearly defined [@hernan2008aObservationalStudiesAnalysedLike]. 

2. **State a well-defined outcome.**
   Specify the outcome measure so that a causal contrast is interpretable. For example, 'well-being' is arguably a vaguely stated outcome.  However, the outcome, 'psychological distress measured one year after the intervention using the Kessler-6 distress scale [@kessler2002]' is more clearly defined. 

3. **Clarify the target population.**
   Define the population to whom the results will generalise. The eligibility criteria for a study will define the source population from which units in the study are sampled. However, sampling from the source population may yield a study population that differs from the source population in variables that modify the effects of treatment [@dahabreh2019; @dahabreh2019generalizing; @stuart2018generalizability; @bulbulia2024wierd]. Investigators may also seek to generalise beyond the source population, which requires additional assumptions and may require additional knowledge [@deffner2022; @bareinboim2013general; @westreich2017transportability; @dahabreh2019; @pearl2022external]. 


4. **Evaluate whether treatment groups, conditional on measured covariates, are exchangeable.**
   The potential outcomes must be independent of treatments conditional on measured covariates [@neal2020introduction; @morgan2014; @angrist2009mostly; @hernan2024WHATIF].

5. **Ensure treatments to be compared satisfy causal consistency.**
   The versions of treatments over which a causal effect is estimated must be independent of the potential outcomes to be compared conditional on measured covariates [@vanderweele2013; @hernan2024WHATIF; @vanderweele2013]. 

6. **Check if the positivity assumption is satisfied.**
   There must be a non-zero probability of receiving each treatment level at every level of covariate required to satisfy the conditional exchangeability assumption, and if there are many versions of treatment, to satisfy the causal consistency assumption [@westreich2010]. 

7. **Ensure that the measures relate to the scientific questions at hand.** 
In particular, evaluate structural features of measurement error bias [@hernan2024WHATIF; @vanderweele2012MEASUREMENT; @bulbulia2024wierd].

8. **Consider strategies to ensure the study group measured at the end of the study represents the target population.**
   If the study population differs in the distribution of variables that modify the effect of a treatment on the outcome at both the beginning and end of treatment, the study will be biased when there is a treatment effect; as such, investigators must develop strategies to address attrition, non-response, and structural sources of bias from measurement error [@hernan2017SELECTIONWITHOUTCOLLIDER; @hernan2017per; @hernan2004STRUCTURAL; @bulbulia2024wierd].

9. **Clearly communicate the reasoning, evidence, and decision-making that inform steps 1-8.**
   Provide transparent and thorough documentation of the decisions in steps 1-8. This includes detailing investigators' causal assumptions and any disagreements about these assumptions [@ogburn2021].

### Conventions Used in This Article

@tbl-terminologylocalconventions reports our variables. @tbl-terminologygeneral describes our graphical conventions. Here, we use two types of graphical tools to clarify causal questions: causal directed acyclic graphs and Single World Intervention Graphs. (Refer to supplementary materials **S1** for a glossary for commonly used causal inference terms).

::: {#tbl-terminologylocalconventions}
```{=latex}
\terminologylocalconventions
```
Terminology
:::

::: {#tbl-terminologygeneral}
```{=latex}
\terminologygeneral
```
Elements of Causal Graphs 
:::

Throughout, for clarity, we repeat our definitions and graphical conventions as they are used.  To begin, we define the following:

- **Node**: or equivalently a 'variable,' denotes properties or characteristics of units within a population. In causal directed acyclic graphs, we draw nodes with respect to features in a *target population*, which is the population for whom we seek causal inferences [@suzuki2020]. A time-indexed node, $X_t$, allows us to index measurements within time intervals $t \in 1\dots T$, denoting relative chronology. If relative timing is not known, we may use $X_{\phi t}$. The directions of arrows on a causal directed acyclic graph imply causation, and causation implies temporal order.

- **Arrow** ($\rightarrowNEW$): Denotes a causal relationship from the node at the base of the arrow (a 'parent') to the node at the tip of the arrow (a 'child'). In causal directed acyclic graphs, we refrain from drawing an arrow from treatment to outcome to avoid asserting a causal path from $A$ to $Y$. Our purpose is to ascertain whether causality can be identified for this path. All other nodes and paths, including the absence of nodes and paths, are typically assumed.

- **Boxed Variable** $\boxed{X}$: Denotes conditioning or adjustment for $X$. 


Judea Pearl demonstrated that causal dependencies in a directed acyclic graph could be evaluated using observable probability distributions according to rules known as 'd-separation' [@pearl1995; @pearl2009a].

The rules, presented in @tbl-terminologydirectedgraph, are as follows:

\begin{enumerate}[a)]
     \item  {\bf Fork rule} ($B \leftarrowNEW \boxed{A} \rightarrowNEW C$): $B$ and $C$ are independent when conditioning on $A$: ($B \coprod C \mid A$).
     \item  {\bf Chain rule} ($A \rightarrowNEW \boxed{B} \rightarrowNEW C$): Conditioning on $B$ blocks the path between $A$ and $C$: ($A \coprod C \mid B$).
     \item  {\bf Collider rule} ($A \rightarrowNEW \boxed{C} \leftarrowNEW B$): $A$ and $B$ are independent until conditioning on $C$, which introduces dependence: ($A \cancel{\coprod} B \mid C$). 
 \end{enumerate}

From d-separation, Pearl derived a 'backdoor adjustment theorem', which provides a general identification algorithm given structural assumptions encoded in a causal directed acyclic graph [@pearl1995, @pearl2009a]: In a causal directed acyclic graph (causal DAG), a set of variables $L$ satisfies the backdoor adjustment theorem relative to the treatment $A$ and the outcome $Y$ if $L$ blocks every path between $A$ and $Y$ that contains an arrow pointing into $A$ (a backdoor path). Formally, $L$ must:

1. not be a descendant of $A$;
2. block all backdoor paths from $A$ to $Y$.

If $L$ satisfies these conditions, the causal effect of $A$ on $Y$ is identified by conditioning on $\boxed{L}$[@pearl2009a].  In what follows, I will assume readers are familiar with causal directed acyclic graphs. Accessible introductions to causal directed acyclic graphs can be found in @pearl2009a; @barrett2021; @mcelreath2020; @neal2020introduction; @hernan2024WHATIF; @bulbulia2023. (For an introduction to Single World Intervention Graphs, used below, refer to @richardson2013; @richardson2013swigsprimer.) 


::: {#tbl-terminologydirectedgraph}
```{=latex}
\terminologydirectedgraph
```
Five elementary causal structures in a causal directed acyclic graph.
:::


{{< pagebreak >}}

## Part 1: Interaction as 'Effect Modification' {#id-sec-1}

We have said that in causal inference, we must explicitly define our causal question before applying statistical models to data. What question might the analysis of interaction answer?  In causal inference, we think of interaction in two ways:

1. **Interaction as Effect Modification from a Single Intervention**: We want to understand how an intervention varies in its effect across the strata of the target population in which we are interested. For example, we might ask: does the one-year effect of attending weekly religious service differ among people born in Australia compared with people born in Egypt? Note that here, we do not imagine intervening on birthplace.

2. **Interaction as Joint Intervention**: We want to understand whether the combined effects of two treatments administered together differ from the separate effect of each treatment acting alone. For example, we might ask: does the one-year effect of attending weekly religious service and the one-year effect of being at least one standard deviation above population average wealth differ from not attending any religious service and being at the population average in wealth?  Here there are two interventions that might act individually, separately, or in concert.

[Part 1](#id-sec-1) considers interaction as effect modification. Readers who do not wish to use the 'effect modification' may prefer the term 'moderation.' 


### Effect Modification

First, we define the 'sharp-null hypothesis' as the hypothesis that there is no effect of the exposure on the outcome for any unit in the target population. Unless the sharp-null hypothesis is false, there may be effect modification (@bulbulia2024wierd). Clearly, the variability of causal effect modification cannot be assessed from descriptive measures of individuals in one's sample. Such heterogeneity must be modelled (refer to @grf2024; @vansteelandt2022a). Alternatively, we might seek to compare whether the effects of treatments vary by strata within the target population. For example we may ask whether effects vary by culture group membership, gender or another grouping variable. 

::: {#tbl-terminologyeffectmodification}
```{=latex}
\terminologyeffectmodification
```
Graphical conventions we use for representing effect modification.
:::

@tbl-terminologyeffectmodification describes conventions to clarify how to ask a causal question of effect modification. 
We assume no confounding of the treatment on the outcome and that $A$ has been randomised (i.e. $\mathcal{R} \rightarrowNEW A$). As such, we will not use causal directed acyclic graphs to evaluate a treatment effect. We will assume $\mathcal{R}  \to A \to Y$. 

To sharpen focus on our interest in effect modification, we will not draw a causal arrow from the direct effect modifier $F$ to the outcome $Y$. This convention is specific to this article. (Refer to @hernan2024WHATIF, pp. 126-127, for a discussion of 'noncausal' arrows.)

::: {#tbl-terminologyeffectmodificationtypes}
```{=latex}
\terminologyeffectmodificationtypes
```
Effect Modification
:::

In @tbl-terminologyeffectmodificationtypes $\mathcal{G}_1$, we represent that $F$ is a direct effect modifier for the effect of $A$ on $Y$. The open arrow indicates that we are not attributing causality to $F$. Because our estimand does not involve intervening on $Z$, there is no need to close its backdoor paths. Note that if $F$ were to affect $A$, we could still estimate the effect modification of $A$ on $Y$ because $F$ has no causal interpretation. However, if $A$ were to cause $F$, and $F$ were to cause $Y$, then by the chain rule (recall @tbl-terminologygeneral $\mathcal{G}_4$), conditioning on $F$ would bias the effect estimate of $A$ on $Y$.

In @tbl-terminologyeffectmodificationtypes $\mathcal{G}_2$, we represent that $F$ is an unobserved direct effect modifier of $A$ to $Y$. When the distribution of direct effect modifiers $F$ differs between two populations and effect modification is non-linear, marginal treatment effects between populations will generally differ and will not easily transport from one population to another. The concept of an average treatment effect has no meaning without a population over which the effect marginalises. This point, although obvious, has profound implications when investigators seek to assess whether their research generalises; refer to @hernan2024WHATIF, @bulbulia2024wierd. For example, if the study population differs in the distribution of features that modify a treatment effect, and no correction is applied,  effect estimates will be biased for the target population in at least one measure of effect [@greenland2009commentary; @lash2020; @bulbulia2024wierd]

We present two candidate effect modifiers in @tbl-terminologyeffectmodificationtypes $\mathcal{G}_3$. Notice that whether a variable is an effect modifier also depends on which other variables are included in the model. Here, $F$ is a direct effect modifier and $G$, a descendant of $F$, is an indirect effect modifier. Suppose we were interested in whether treatment effects vary (on the difference scale) within levels of $F$. For example, let $F$ denote childhood deprivation, $G$ denote educational achievement, $A$ denote a government educational initiative, and $Y$ denote recycling. If we were to condition on $F$, we would not observe effect modification by education $G$ for the effect of the government initiative $A$ on recycling behaviour $Y$: $\boxed{F}$ blocks the path $G \association \boxed{F} \association Y$.

We present the same causal structure in @tbl-terminologyeffectmodificationtypes $\mathcal{G}_4$. However, we do not condition on the direct effect modifier $F$, but rather condition only on $G$, the indirect effect modifier. In this scenario, we would find that the effectiveness of the government initiative $A$ on recycling behaviour $Y$ varies by educational achievement $G$. Thus, we would observe $G$ as an effect modifier because this path is open: $G \association F \association Y$.

In @tbl-terminologyeffectmodificationtypes $\mathcal{G}_5$, suppose we add another variable to our model, depression, denoted by $B$. We imagine $B$ to be a stable trait or that investigators measured childhood depression (that is, $B$ precedes $G$). Suppose we do not condition on the direct effect modifier $F$ (childhood deprivation), but we condition on educational attainment ($G$) and depression ($B$). In this graph, $G$ is a collider of $F$ and $B$. Thus, conditioning on $G$ (but not $F$) opens a path from $B \association G \association Z \association Y$. The investigators would find evidence for effect modification by depression on the effectiveness of the government intervention $A$ on recycling ($Y$). However, they should not interpret this result to mean that if levels of depression were to change within the population the treatment effect would change. $B$ is not causally related to $Y$ in this scenario. Here, association is not causation.

In @tbl-terminologyeffectmodificationtypes $\mathcal{G}_6$, we will not find evidence for effect modification for $B$ and $G$ because conditioning on $F$ blocks the flow of information that was open in $\mathcal{G}_4$ and $\mathcal{G}_5$. This again underscores the relativity of effect modification to (1) the structure of causality in the world and (2) an investigator's statistical modelling strategy.

These examples reveal the power—and simplicity—of causal diagrams to 'transform the obvious'. Using causal directed acyclic graphs and Pearl's rules of d-separation, it is clear that the analysis of effect modification cannot be conducted without reference to an assumed causal order and an explicit statement about which variables within that order investigators have included in their statistical models [@vanderweele2012]. Investigators and policymakers may make incorrect policy decisions if they do not understand the relevance of effect modification to such parameters. It is important to remember that when evaluating evidence for effect modification, we are not assessing the effects of intervening on variables other than the treatment. Instead, we qualitatively evaluate whether treatment effects vary across subgroups. For more on effect modification, refer to @vanderweele2012; @vanderweele2007; @suzuki2013counterfactual.



### Example Showing Scale Dependence of Effect Modification

Suppose investigators are interested in whether treatment varies across levels of another variable, an effect modifier. We next illustrate how causal inferences about the presence or absence of effect modification depend on the scale that is used to measure the contrast. We show that an effect modifier on the ratio scale may not be an effect modifier on the difference scale, and vice versa.

Recall individual treatment effects are not observed. Assume a binary treatment is randomised, and we have $A = a \in \{0,1\}$. Investigators are interested in comparing the magnitude of this treatment effect across two levels of $F = f \in \{0,1\}$. 

We define the average treatment effects for each group under each intervention as follows: 
$$
\mathbb{E}[Y \mid A = 0, F = 1] = \mu_{01}, \quad \mathbb{E}[Y \mid  A = 1, F = 1] = \mu_{11}
$$
$$
\mathbb{E}[Y \mid  A = 0, F = 0] = \mu_{00}, \quad \mathbb{E}[Y \mid A = 1, F = 0] = \mu_{10}
$$

The treatment effect for each group on the difference scale (absolute scale) is given:

$$
\text{ATE}_{F = 0} = \mu_{10} - \mu_{00}
$$

$$
\text{ATE}_{F = 1} = \mu_{11} - \mu_{01}
$$

The treatment effect on the ratio scale (relative scale) for each group is:

$$
\text{RR}_{F = 0} = \frac{\mu_{10}}{\mu_{00}}
$$
$$
\text{RR}_{F = 1} = \frac{\mu_{11}}{\mu_{01}}
$$

We say there is effect modification on the difference scale if:
$$
\text{ATE}{F = 1} \neq \text{ATE}{F = 0} \implies \mu_{11} - \mu_{01} \neq \mu_{10} - \mu_{00}
$$

We say there is effect modification on the ratio scale if:
$$
\text{RR}{F = 1} \neq \text{RR}{F = 0} \implies \frac{\mu_{11}}{\mu_{01}} \neq \frac{\mu_{10}}{\mu_{00}}
$$

We have stated each causal question in relation to well-defined causal contrast and population, here defined by membership in $F$.

Imagine we obtain the following estimates from our study:

Outcomes under A = 0:

- $\mu_{00} = 5$
- $\mu_{01} = 15$

Outcomes under A = 1:

- $\mu_{10} = 10$
- $\mu_{11} = 20$

Next, we calculate the treatment effects on the difference and ratio scales for each group:

**Difference Scale:**

$$
\text{ATE}_{F = 0} = \mu_{10} - \mu_{00} = 10 - 5 = 5
$$
$$
\text{ATE}_{F = 1} = \mu_{11} - \mu_{01} = 20 - 15 = 5
$$

Both groups have the same treatment effect on the difference scale, $\text{ATE}_{F = 0} = \text{ATE}_{F = 1} = 5$. investigators conclude there is no evidence for effect modification on the difference scale.

**Ratio Scale:**
$$
\text{RR}_{F = 0} = \frac{\mu_{10}}{\mu_{00}} = \frac{10}{5} = 2.00
$$
$$
\text{RR}_{F = 1} = \frac{\mu_{11}}{\mu_{01}} = \frac{20}{15} \approx 1.33
$$

The treatment effect on the ratio scale is different between the two groups, $\text{RR}_{F = 0} = 2 \neq \text{RR}_{F = 1} \approx 1.33$. Hence, investigators find evidence for effect modification on the ratio scale. 

The discrepancy in evidence for effect modification depending on the scale we choose arises because the two scales measure different aspects of the treatment effect: the absolute difference in outcomes versus the relative change in outcomes. Parallel considerations apply to the analysis of interaction, where we imagine a joint intervention. For this reason, it is important to state the causal effect scale of interest in advance of estimation [@bulbulia2023]. We next consider interaction as a joint intervention.

## Part 2: Interaction {#id-sec-2}

### Introducing Single World Intervention Graphs

When evaluating evidence for interaction, we must assess whether the combined effects of two treatments differ from the unique effects of each treatment relative to a baseline where neither treatment is administered. Understanding multiple interventions can be facilitated by using Single World Intervention Graphs (SWIGs) [@richardson2013].

SWIGs employ Pearl's rules of d-separation but offer additional benefits by graphically representing the complex factorisations required for identification, presenting distinct interventions in separate graphs.  The first advantage is **greater precision and clarity**: SWIGs allow us to consider identification conditions for each counterfactual outcome individually. Such precision is useful because identification conditions may differ for one, but not another, of the treatments to be compared. Node-splitting also makes it easier to determine identification conditions that are obscured in causal directed acylic graphs, for an example refer to supplementary materials **S2**. The second advantage **Single World Intervention Graphs unify the potential outcomes framework with Pearl's structural causal model framework**: any causal relationship that can be represented in a causal directed acyclic graph can also be represented in a Single World Intervention Graph [@richardson2013swigsprimer].

::: {#tbl-swigtable}
```{=latex}
\swigtable
```
Single World Interventions Graphs ($\mathcal{G}_{3-4}$) present separate causal diagrams for each treatment to be contrasted. A Single World Intervention Template ($\mathcal{G}_{2}$) is a 'graph value function' that produces the individual counterfactual graphs [@richardson2013]. On the other hand, causal directed acyclic graphs, such as $\mathcal{G}_1$, require positing interventional distributions. The formalism underpinning these interventional distributions is mathematically equivalent to formalism underpinning the potential outcomes framework, assuming the errors of the underlying structural causal models that define the nodes on which interventions occur are independent [@richardson2013]. Single World Intervention Graphs (SWIGs), however, permit the comparison of distinct interventions in our causal diagram without requiring that the non-parametric structural equations that correspond to nodes on a causal graph have independent error structures. This is useful when attempting to identify the causal effects of sequential treatments, refer to supplementary materials **S2**.
:::

#### Single World Intervention Graphs Work by Node-Splitting

We create a Single World Intervention Graph by 'node-splitting' at each intervention such that the random variable that is intervened upon is presented on one side and the level at which the random variable is fixed is presented on the other.

Consider a template graph @tbl-swigtable $\mathcal{G}$. Applying node-splitting to $A$ involves creating separate graphs for each value of $A$ to be contrasted.

1. **SWIG for $A$ = 0**: Denoted as $\mathcal{G}_{A=0}$, this graph shows the hypothetical scenario where $A$ is set to 0.
2. **SWIG for $A$ = 1**: Denoted as $\mathcal{G}_{A=1}$, this graph shows the hypothetical scenario where $A$ is set to 1.

In these graphs, the node corresponding to the treatment $A$ is split, relabelled with the random and fixed component, and then each node that follows is labelled with the fixed component until the next intervention. Here, $Y(\tilde{a})$ is the only variable to follow $A$ and it is relabelled either $Y(0)$ or $Y(1)$ corresponding to whether $A=1$ or $A=0$; hence $Y(\tilde{a})$ is relabelled as either $Y(0)$ or $Y(1)$. Note that we do not place both $Y(0)$ and $Y(1)$ on the same Single World Intervention Graph because the variables are not jointly observed. Hence, we evaluate identification for $Y(0)\coprod A = 0| L$ and $Y(1)\coprod A = 1 | L$ separately.


### Interaction as a Joint Intervention

We now use Single World Intervention Graphs (SWIGs) to clarify the concept of causal interaction as a joint intervention.

Consider two treatments, denoted as $A$ and $B$, and a single outcome, $Y$. A joint intervention causal interaction examines whether the combined effect of $A$ and $B$ on $Y$ (denoted as $Y(a,b)$) is greater than, less than, or equal to the effect of each individual treatment. What does this mean?

First, we obtain the expected outcomes when the entire target population is treated at each level of the treatments to be compared. These potential outcomes are illustrated in @tbl-interactionpuzzle:

- @tbl-interactionpuzzle $\mathcal{G}_1$: Neither treatment $A$ nor treatment $B$ is given.
- @tbl-interactionpuzzle $\mathcal{G}_2$: Both treatment $A$ and treatment $B$ are given.
- @tbl-interactionpuzzle $\mathcal{G}_3$: Treatment $A$ is given, and treatment $B$ is not given.
- @tbl-interactionpuzzle $\mathcal{G}_4$: Treatment $A$ is not given, and treatment $B$ is given.

By comparing these expected outcomes, we can determine the presence and nature of causal interaction between treatments $A$ and $B$ with respect to the outcome $Y$.

::: {#tbl-interactionpuzzle}
```{=latex}
\interactionpuzzle
```
:::


### Example

Consider the effect of beliefs in big gods (exposure $A$) and a culture's monumental architecture (exposure $B$) on social complexity (outcome $Y$). Both interventions have equal status; we are not investigating effect modification of one by the other. The interventions must be well-defined, with clear measures for 'big gods', 'monumental architecture', and 'social complexity' at specified time intervals after the interventions are first observed.

We need to define the population of interest and choose the appropriate scale to assess the individual and combined effects of $A$ and $B$. Suppose our population consists of societies of primary urban genesis [@wheatley1971pivot]. We look for evidence of causal interaction on the additive (difference) scale. Evidence for interaction is present if the following inequality holds:

- $\mathbb{E}[Y(1,1)]:$ Mean outcome for those jointly exposed to both treatments.
- $\mathbb{E}[Y(1,0)]:$ Mean outcome for those exposed only to big gods.
- $\mathbb{E}[Y(0,1)]:$ Mean outcome for those exposed only to monumental architecture.
- $\mathbb{E}[Y(0,0)]:$ Mean outcome for those exposed to neither treatment.

Evidence for interaction on the additive scale exists if:

$$
\left( \mathbb{E}[Y(1,1)] - \mathbb{E}[Y(0,0)] \right) - \left[ \left( \mathbb{E}[Y(1,0)] - \mathbb{E}[Y(0,0)] \right) + \left( \mathbb{E}[Y(0,1)] - \mathbb{E}[Y(0,0)] \right) \right] \neq 0
$$

Simplifying:

$$
\mathbb{E}[Y(1,1)] - \mathbb{E}[Y(1,0)] - \mathbb{E}[Y(0,1)] + \mathbb{E}[Y(0,0)] \neq 0
$$

A positive value indicates a synergistic (super-additive) interaction, while a negative value indicates a sub-additive interaction. A value close to zero suggests no interaction on the additive scale.

To identify these causal effects, we need to adjust for all confounders of the relationships between $A$, $B$, and $Y$. This includes any variables that influence:

- Both $A$ and $Y$,
- Both $B$ and $Y$,
- Both $A$ and $B$,
- Or all three variables simultaneously.

As with effect modification, evidence for causal interaction may differ depending on the measurement scale used [@vanderweele2014; @vanderweele2012]. For most policy settings, the additive scale is recommended because it directly relates to differences in outcome levels, which are often more actionable.

Note that if $A$ and $B$ potentially influence each other over time, we would need to collect longitudinal data and estimate causal effects using mediation analysis. Indeed, if there has been a co-evolution of religious culture, monumental architecture, and social complexity—as archaeologists have long reported [@decoulanges1903; @wheatley1971pivot]—mediation analysis may be more appropriate. However, the requirements for causal mediation analysis are more stringent than those for causal interaction analysis, which we will consider next.

## Part 3: Causal Mediation Analysis {#id-sec-3}

In 1992, Robins and Greenland clarified the objectives of interpretable causal mediation analysis: to decompose the total effect into natural direct and indirect effects within a set of hypothetical interventions, contrasting their counterfactual outcomes [@robins1992]. This landmark paper has been to mediation analysis of what 'On the Origin of Species' has been to evolutionary biology [@darwin1859origin]. However, mediation analysis in the human sciences remains rife with confusion. The primary source of this confusion is the application of statistical models to data without first defining the causal quantities of interest. Associations derived from mediation analysis do not necessarily imply causation and are typically uninterpretable. This section considers how to formulate causal questions in mediation analysis.

### Defining a Mediation Analysis Estimand

To understand causal mediation, we deconstruct the total effect into natural direct and indirect effects.

Again, the total effect of treatment $A$ on outcome $Y$ is defined as the difference between potential outcomes when the treatment is applied versus when it is not. The estimand for the total (or average, or 'marginal') treatment effect is given:

$$
\text{Total Treatment Effect} = \mathbb{E}[Y(1)] - \mathbb{E}[Y(0)]
$$

The total effect can be further decomposed into direct and indirect effects, addressing questions of mediation. The potential outcome $Y(1)$, considering the mediator, expands to:

$$ 
\mathbb{E}[Y(1)] = \mathbb{E}[Y(1, M(1))]
$$

This considers the effect of the exposure $A = 1$ and the mediator at its natural value when $A = 1$. Similarly, the potential outcome $\mathbb{E}[Y(0)]$, considering the mediator, expands to:

$$ 
\mathbb{E}[Y(0)] = \mathbb{E}[Y(0, M(0))]
$$

This quantity denotes the effect of exposure $A = 0$ and the mediator at its natural value when $A = 0$.

Next, we clarify our estimand by decomposing the Total Effect (TE) into the Natural Direct Effect (NDE) and the Natural Indirect Effect (NIE).

**Natural Direct Effect (NDE)** is the effect of the treatment on the outcome while maintaining the mediator at the level it would have been if the treatment had not been applied:

$$
\text{Natural Direct Effect} = \textcolor{blue}{\mathbb{E}[Y(1, M(0))]} - \mathbb{E}[Y(0, M(0))]
$$

Here, the counterfactual quantities not directly realised in the data are highlighted in blue: $\textcolor{blue}{\mathbb{E}[Y(1, M(0))]}$. Notice we add this term to the potential outcomes when $A = 0$, recalling $\mathbb{E}[Y(0, M(0))] = Y(0)$.

**Natural Indirect Effect (NIE)** is the effect of the exposure on the outcome that is mediated. To obtain these quantities, we compare the potential outcome $Y$ under treatment, where the mediator assumes its natural level under treatment, with the potential outcome when the mediator assumes its natural value under no treatment:

$$
\text{Natural Indirect Effect} = \mathbb{E}[Y(1, M(1))] - \textcolor{blue}{\mathbb{E}[Y(1, M(0))]}
$$

Here, the counterfactual quantities not directly realised in the data are again highlighted in blue: $\textcolor{blue}{\mathbb{E}[Y(1, M(0))]}$. Notice we subtract this term from the potential outcomes when $A = 1$, recalling $\mathbb{E}[Y(1, M(1))] = \mathbb{E}[Y(1)]$.

By rearranging this decomposition, we find that the total effect (TE) is the sum of the NDE and NIE. This is shown by adding and subtracting the term $\textcolor{blue}{\mathbb{E}[Y(1, M(0))]}$ to our equation:

$$
\text{Total Effect (TE)} = \underbrace{\bigg\{\mathbb{E}[Y(1, M(1))] - \textcolor{blue}{\mathbb{E}[Y(1, M(0))]}\bigg\}}_{\text{Natural Indirect Effect (NIE)}} + \underbrace{\bigg\{\textcolor{blue}{\mathbb{E}[Y(1, M(0))]} - \mathbb{E}[Y(0, M(0))]\bigg\}}_{\text{Natural Direct Effect (NDE)}}
$$

::: {#tbl-mediationpuzzle}
```{=latex}
\mediationpuzzle
```
In causal mediation, the quantities that we require to obtain natural direct and indirect effects, namely $\mathbb{E}[Y\big(1,M(0)\big)]$, cannot be experimentally observed because we cannot treat someone and observe the level of their mediator if they were not treated. 
:::

@tbl-mediationpuzzle presents a conceptual challenge for causal mediation analysis. Suppose we randomise a binary treatment $A \in \{0,1\}$. Although randomising $A$ does not ensure there is no confounding of the mediator/outcome path, we assume no unmeasured confounding for either the treatment or the mediator. (We will relax this assumption in the next section.)

@tbl-mediationpuzzle $\mathcal{G}_1$ is a Single World Intervention Template (SWIT), which generates Single World Intervention Graphs (SWIGs) for each condition.

@tbl-mediationpuzzle $\mathcal{G}_2$ presents counterfactual outcomes for condition $A = 0$; here, the natural value of $M$ is $M(a = 0)$, and the counterfactual outcome is given by $Y\big(\textcolor{cyan}{0}, M(\textcolor{cyan}{0})\big)$.

@tbl-mediationpuzzle $\mathcal{G}_3$ presents counterfactual outcomes for condition $A = 1$; here, the natural value of $M$ is $M(a = 1)$, and the counterfactual outcome is given by $Y\big(\textcolor{cyan}{1}, M(\textcolor{cyan}{1})\big)$.

These Single World Intervention Graphs clarify that we cannot identify natural direct and indirect effects from observations on individual units under treatment because $\mathbb{E}[Y(1, M(0))]$ is not observable. [@vanderweele2015; @vansteelandt2012; @valeri2014; @vanderweele2014a; @shi2021; @steen2017]. Expressing these quantities requires a counterfactual framework. Here, we see that a counterfactual formulation of mediation analysis has made the familiar strange. However, under assumptions, we can sometimes recover natural direct and indirect effects from data [@vanderweele2015], given that our interest is in contrasts obtained for the target population, not for individuals, where we assume no causal effects are directly observed.


### Assumptions of Causal Mediation Analysis

@tbl-medationassumptions $\mathcal{G}_1$ presents a Single World Intervention Template (SWIT) that specifies the assumptions required for inferring natural direct and indirect effects. This template highlights that, when estimating natural mediated effects, we only intervene on the treatment. Therefore, we must infer the mediated effect of the treatment under the condition that the mediator is set to the level it would naturally take under the control condition.

Additionally, @tbl-medationassumptions $\mathcal{G}_1$ also clarifies the assumptions needed for inferring controlled direct effects, where the mediator is fixed to a level specified by the investigators. In this scenario, we obtain causal contrasts by fixing variables to specific states.

Consider the hypothesis that cultural beliefs in 'Big Gods' influence social complexity, with political authority mediating. Assuming we have well-defined interventions and outcomes, what requirements are necessary to decompose this causal effect into natural direct and indirect effects?

::: {#tbl-medationassumptions}
```{=latex}
\mediationassumptionsswig
```
**Assumptions of Causal Mediation Analysis**
:::

1. **No Unmeasured Exposure-Outcome Confounder**

   This requirement is expressed as:
   $$
   Y(a, m) \coprod A \mid L
   $$
   After accounting for the covariates in set $L$, there must be no unmeasured confounders influencing cultural beliefs in Big Gods ($A$) and social complexity ($Y$). For example, if our study examines the causal effect of cultural beliefs in Big Gods on social complexity, and the covariates in $L$ include factors such as geographic location and historical context, we need to ensure that these covariates effectively block any confounding paths between $A$ and $Y$. The relevant path in @tbl-medationassumptions $\mathcal{G}_1$ is the confounder of the path $A \rightarrow Y$.

2. **No Unmeasured Mediator-Outcome Confounder**

   This requirement is expressed as:
   $$
   Y(a, m) \coprod M \mid Z
   $$
   After controlling for the covariate set $Z$, we must ensure that no other unmeasured confounders affect political authority ($M$) and social complexity ($Y$). For instance, if trade networks affect both political authority and social complexity, we must account for trade networks to block the path linking our mediator and outcome. The relevant path in @tbl-medationassumptions $\mathcal{G}_1$ is the confounder of the path $M \rightarrow Y$.

3. **No Unmeasured Exposure-Mediator Confounder**

   This requirement is expressed as:
   $$
   M(a) \coprod A \mid Q
   $$
   After controlling for the covariate set $Q$, we must ensure that no additional unmeasured confounders affect cultural beliefs in Big Gods ($A$) and political authority ($M$). For example, the capability to construct large ritual theatres may influence both the belief in Big Gods and the level of political authority. If we have indicators for this technology measured before the emergence of Big Gods (these indicators being $Q$), we must assume that accounting for $Q$ closes the backdoor path between the exposure and the mediator. The relevant path in @tbl-medationassumptions $\mathcal{G}_1$ is the confounder of the path $A \rightarrow M$.

4. **No Mediator-Outcome Confounder Affected by the Exposure**

   This assumption requires that there are no unmeasured confounders of the mediator-outcome relationship that are themselves affected by the exposure $A$. Such confounders cannot be adjusted for without introducing bias.

   **Clarification:**

   - There must be no variables $Z$ that both affect $M$ and $Y$, are affected by $A$, and are not appropriately controlled for.
   - This assumption cannot be easily expressed using independence notation but is crucial for the identification of natural direct and indirect effects.

   The relevant path in @tbl-medationassumptions $\mathcal{G}_1$ involves confounders of the $M \rightarrow Y$ path that are influenced by $A$.

Satisfying Assumption 4 imposes considerable demands on causal mediation analysis. When the exposure influences a confounder of the mediator and the outcome, we face a dilemma. Without adjusting for this confounder, a backdoor path between the mediator and the outcome remains open, introducing bias. However, adjusting for it can block part of the effect of the exposure on the mediator, also leading to bias. In this setting, we cannot recover the natural direct and indirect effects from observational data. We may need to settle for investigating controlled direct effects, estimate jointly mediated effects of $Z$ and $M$ together, or consider alternative methods as suggested by VanderWeele and others [@vanderweele2015; @vanderweele2014effect; @vo2024recanting; @vanderweele2017mediation; @Diaz2023; @robins2010alternative].

Notice that even when Assumptions 1–4 are satisfied, natural direct effect estimates and natural indirect effect estimates require conceptualizing a counterfactual that is never directly observed on any individual, namely:
$$
Y\big(1, M(0)\big)
$$
Such effects are only identified in distribution (refer to @vanderweele2015).

### Controlled Direct Effects

Consider another identification challenge, as described in template @tbl-medationassumptions $\mathcal{G}_1$. Suppose we aim to understand the effect of a stringent pandemic lockdown ($A$) on psychological distress ($Y$), focusing on trust in government ($M$) as a mediator. Further, suppose that pandemic lockdowns may plausibly influence attitudes towards the government through pathways that also affect psychological distress. For instance, people might trust the government more when it provides income relief payments ($Z$), which may also reduce psychological distress.

Under the rules of d-separation, conditioning on income relief payments ($Z$) could block necessary causal paths:

- **If we adjust for $Z$:**

  - We might block part of the effect of $A$ on $M$ and $Y$ that operates through $Z$, potentially biasing the estimation of the natural indirect effect.
  - The paths $A \rightarrow Z \rightarrow M$ and $A \rightarrow Z \rightarrow Y$ are blocked.

- **If we do not adjust for $Z$:**

  - $Z$ acts as an unmeasured confounder of the $M \rightarrow Y$ relationship since $Z$ influences both $M$ and $Y$.
  - The path $M \leftarrowred Z \rightarrowred Y$ remains open, introducing bias.

In such a scenario, it is not feasible to consistently decompose the total effect of the exposure (pandemic lockdowns) on the outcome (psychological distress) into natural indirect and direct effects. However, if all other assumptions hold, we might obtain an unbiased estimate for the controlled direct effect of pandemic lockdowns on psychological distress at a fixed level of government trust.

@tbl-medationassumptions $\mathcal{G}_2$ presents the weaker assumptions required to identify a controlled direct effect. We might examine the effect of the pandemic lockdown if we could intervene and set everyone's trust in government to, say, one standard deviation above the baseline, compared with fixing trust in government to the average level at baseline. We might use modified treatment policies that specify interventions as functions of the data. For instance, we might investigate interventions that 'shift' only those whose mistrust of government was below the mean level of trust at baseline and compare these potential outcomes with those observed. Asking and answering precisely formulated causal questions such as this might lead to clearer policy advice, especially in situations where policymakers can influence public attitudes towards the government; see [@williams2021; @díaz2021; @hoffman2022; @hoffman2023].

In any case, I hope this discussion of causal mediation analysis clarifies that it would be unwise to simply examine the coefficients obtained from structural equation models and interpret them as meaningful in statistical mediation analysis. To answer any causal question, we must first state it with respect to clearly defined counterfactual contrasts and a target population. Once we state our causal question, we find we have no guarantees that the coefficients from statistical models are straightforwardly interpretable [@vanderweele2015].

For those interested in statistical estimators for causal mediation analysis, I recommend visiting the CMAverse website [https://bs1125.github.io/CMAverse/articles/overview.html](https://bs1125.github.io/CMAverse/articles/overview.html), accessed 13 December 2023. This excellent resource provides comprehensive documentation, software, and practical examples, including sensitivity analyses. Next, we will consider more complex scenarios that involve feedback between treatments and confounders across multiple time points—settings in which traditional statistical methods also fail to provide valid causal inferences.

## Part 4: Time-fixed and Time-Varying Sequential Treatments (Treatment Strategies, Modified Treatment Policies) {#id-sec-4}

Our discussion of causal mediation analysis focused on how effects from two sequential treatments -- the initial treatment and that of a mediator affected by the treatment  -- may combine to affect an outcome.

This concept can be expanded to investigate the causal effects of multiple sequential exposures, referred to as 'treatment regimes', 'treatment strategies', or 'modified treatment policies'. Researchers often use longitudinal growth and multi-level models in many human sciences, where longitudinal data are collected. How may we identify such effects? What do they mean? 

As before, to answer a causal question, we must first clearly state it. This involves specifying the counterfactual contrast of interest, including the treatments to be compared, the scale on which the contrast will be computed, and the target population for whom inferences are valid. Without such clarity, our statistical models are often uninterpretable.

### Worked Example: Does Marriage Affect Happiness?

Richard McElreath considers whether marriage affects happiness and provides a simulation to clarify how age structure complicates causal inferences [@mcelreath2020 pp. 123-144]. We expand on this example by first clearly stating a causal question and then considering how time-varying confounding invalidates the use of standard estimation methods such as multi-level regression.

Let $A_t = 1$ denote the state of being married at time $t$ and $A_t = 0$ denote the state of not being married, where $t \in \{0, 1, \tau\}$ and $\tau$ is the end of the study. $Y_\tau$ denotes happiness at the end of study. For simplicity, assume this concept is well-defined and measured without error.

The table below reveals four treatment strategies and six causal contrasts that we may estimate for each treatment strategy combination.

**Treatment Strategies:**


| Regime           | Counterfactual Outcome |
|------------------|------------------------|
| Always married   | $Y(1,1)$           |
| Never married    | $Y(0,0)$           |
| Divorced         | $Y(1,0)$           |
| Gets married     | $Y(0,1)$           |


: Table outlines four fixed treatment regimens and six causal contrasts in time-series data where exposure varies {#tbl-regimens-marriage}


**Causal Contrasts:**

| Comparison                          | Counterfactual Outcome              |
|-------------------------------------|-------------------------------------|
| Always married vs. Never married    | $\mathbb{E}[Y(1,1) - Y(0,0)]$            |
| Always married vs. Divorced         | $\mathbb{E}[Y(1,1) - Y(1,0)]$            |
| Always married vs. Gets married     | $\mathbb{E}[Y(1,1) - Y(0,1)]$            |
| Never married vs. Divorced          | $\mathbb{E}[Y(0,0) - Y(1,0)]$            |
| Never married vs. Gets married      | $\mathbb{E}[Y(0,0) - Y(0,1)]$            |
| Divorced vs. Gets married           | $\mathbb{E}[Y(1,0) - Y(0,1)]$            |


: Table outlines four fixed treatment regimens and six causal contrasts in time-series data where exposure varies. {#tbl-regimens-marriage-contrasts}


To answer our causal question, we need to:

1. **Specify Treatments**: Define the treatment strategies being compared (e.g., always married vs. never married).
2. **Define the Contrast**: State the counterfactual contrast of interest (e.g., $\mathbb{E}[Y(1,1) - Y(0,0)]$).
3. **Identify the Population**: Specify the population for which the inferences are valid (e.g., adults aged 20-40).

::: {#tbl-swigtabledeveloped}
```{=latex}
\swigtabledeveloped
```
Single World Intervention Graph for Sequential Treatments.
:::

### Time-Varying Confounding with Treatment-confounder Feedback


@tbl-swigtabledeveloped $\mathcal{G}_1$ and @tbl-swigtabledeveloped $\mathcal{G}_2$ represent two subsets of possible confounding structures for a treatment regime conducted over two intervals. Covariates in $L_t$ denote measured confounders, and $U$ denotes unmeasured confounders. $A_t$ denotes the treatment, 'Marriage Status,' at time $t$. $Y$ denotes 'Happiness' measured at the end of the study. 

Consider the structure of confounding presented in @tbl-swigtabledeveloped $\mathcal{G}_1$. To close the backdoor path from $A_1$ to $Y$, we must condition on $L_0$. To close the backdoor path from $A_2$ to $Y$, we must condition on $L_2$. However, $L_2$ is a collider of treatment $A_1$ and unmeasured confounders, such that conditioning on $L_2$ opens a backdoor path between $A_1$ and $Y$. This path is highlighted in red: $A_1 \associationred L_2({\mathbf{\tilde{a_1}}}) \associationred U \associationred ({\mathbf{\tilde{a_1}, \tilde{a_2}}})$.

If @tbl-swigtabledeveloped $\mathcal{G}_1$ faithfully represents causality, it might seem that we cannot obtain valid inferences for any of the six causal contrasts we have defined. Indeed, using standard methods, we could not obtain valid causal inferences. However, @robins1986 first described a consistent estimation function that can be constructed where there is time-varying confounding [@robins2008estimation; @hernan2004STRUCTURAL].

@tbl-swigtabledeveloped $\mathcal{G}_3$ presents a Single World Intervention Template that clarifies how identification may be obtained in fixed treatment regimes where there is time-varying confounding as observed in @tbl-swigtabledeveloped $\mathcal{G}_1$. When constructing a Single World Intervention Graph (or Template), we obtain factorisations for counterfactual outcomes under a specific treatment regime by employing 'node-splitting,' such that all nodes following an intervention are relabelled as counterfactual states under the preceding intervention. After node-splitting, a fixed intervention is no longer a random variable. Thus, under fixed treatment regimes, the counterfactual states that follow an intervention are independent of the states that occur before node-splitting if there are no backdoor paths into the random partition of the node that has been split. 

If all backdoor paths are closed into the random partitions of the nodes on which interventions occur, we can graphically verify that the treatment is independent of the counterfactual outcome for that intervention node. Where there are multiple interventions, we ensure sequential exchangeability at the following node—which we likewise split and relabel—by closing all backdoor paths between the random portion of the following treatment node. We have sequential independence if, for each intervention node, all backdoor paths are closed (refer to @robins2010alternative; @richardson2013swigsprimer; @richardson2023potential).

The Single World Intervention Template @tbl-swigtabledeveloped $\mathcal{G}_3$ makes it clear that sequential identification may be obtained. $A_1$ is d-separated from $Y$ by conditioning on $L_0$; $A_2$ is d-separated from $Y$ by conditioning on $L_2$. 

Notice that we cannot estimate the combined effect of a treatment strategy over $A_1$ and $A_2$ by employing regression, multi-level regression, statistical structural equation models, or propensity score matching. However, special estimators may be constructed (refer to @robins1986; @robins2008estimation; @vanderlaan2011; @diaz2021nonparametric). For recent reviews of special estimators refer to @hernan2024WHATIF; @chatton2020; @vanderlaan2018; @chatton2024causal).

### Time-Varying Confounding *without* Treatment-Confounder Feedback

Consider how we may have time-varying confounding in the absence of treatment-confounder feedback. Suppose we are interested in computing a causal effect estimate for a two-treatment 'marriage' intervention on 'happiness'. We assume that all variables are well-defined, that 'marriage' can be intervened upon, that we have specified a target population, and that our questions are scientifically interesting. Here, we focus on the challenges in addressing certain causal questions with time-varying confounding without treatment confounder feedback. @tbl-swigtabledeveloped $\mathcal{G}_1$ presents such a structure of time-varying confounding.

Let $U_{AL}$ denote an over-confident personality, an unmeasured variable that is causally associated with decisions to marry early and with income. We do not suppose that $U_{AL}$ affects happiness. Taken in isolation, $U_{AL}$ is not a confounder.

Let $L_t$ denote income. Suppose that income affects whether one stays married. For example, suppose it takes wealth to divorce. We can sharpen focus on risks of time-varying confounding by assuming that income itself does not affect happiness. Even with this weaker assumption, we shall see a problem arises.

Let $U_{AY}$ denote a common cause of income, $L_2$, and of happiness at the end of the study, $Y$. An example of such a confounder might be educational opportunity, which by supposition affects both income and happiness. @tbl-swigtabledeveloped $\mathcal{G}_2$ presents the confounding structure for this problem in its simplest form. To declutter, we remove the baseline node, $L_0$, which we assume to be measured.

Notice that in this example there is no treatment-confounder feedback. We have not imagined that marriage affects wealth, or even that wealth affects happiness. Nevertheless, there is confounding. To obtain valid causal inference for the effect of $A_2$ on $Y$, we must adjust for $L_2$ [otherwise: $A_t\associationred L_t \associationred U_{AL}\associationred Y(a)$]. However, $L_2$ is a collider of $U_{AL}$ and $U_{AY}$. In this setting, adjusting for $L_2$ opens the path:


$$
A_1 \associationred U_{AL} \associationred L_2({\mathbf{\tilde{a_1}}})  \associationred U_{AY} \associationred Y({\mathbf{\tilde{a_1}, \tilde{a_2}}})
$$

We have confounding without treatment-confounder feedback (refer to @hernan2024WHATIF)

@tbl-swigtabledeveloped $\mathcal{G}_4$ clarifies that sequential exchangeability can be obtained in the fixed treatment regime. To estimate the effect of $A_2$ on $Y$, we must condition on $L_2$. When estimating the effect of $A_1$ on $Y$, all backdoor paths are closed because $L_2({\mathbf{\tilde{a_1}}})$ is a collider, and $A_0 \coprod Y({\mathbf{\tilde{a_1}, \tilde{a_2}}})$. Note that because a Singal World Intervention Template does not represent the joint distributions of more than one treatment, treatment sequence, or time-varying treatment, to evaluate the conditional independences we must specify the interventions of interest for $A_1$ and $A_2$. That is, we would need to evaluate at least two Single World Intervention Graphs that correspond to the treatment level we wish to contrast. 
Note further that because there is time-varying confounding, we cannot use standard estimators such as multi-level regressions or structural equation models. Estimation must occur stepwise (refer, for example to @diaz2021nonparametric, @williams2021.]

<!-- 
As an aside, note that we could substitute $U_{AL}$ with a variable $U_{AA}$ (not on any graph) -- that relates to decisions to marry at each time interval. Candidate confounders might be: 'prefers not being married', 'prefers marriage', 'prefers stability', or 'prefers change'. The structure of the problem would remain the same: there would be time-varying confounding without treatment-confounder feedback.  -->

### Confounding Under Dynamic Treatment Strategies (Modified Treatment Policies)


Suppose we were interested in the population average effect of divorce on happiness if divorce was only permitted for those with incomes. 

This question is easy to ask but deceptively difficult to answer. For example, we cannot fit an interaction of time $\times$ social status $\times$ marriage status because marital status might affect personal wealth. Yet even if marriage did not affect personal wealth, as in @tbl-swigtabledeveloped $\mathcal{G}_4$, regression would not produce valid estimates for the counterfactual question we asked.


Suppose further that we want to consider the following fixed treatment strategy. Define the treatment policy as follows:

$g^\phi(\cdot)$: remain married for at least two additional years beyone baseline:

$$
A_t^{+}(\mathbf{g}^\phi) = \mathbf{g}^\phi (A_{t}) = \begin{cases} 
   a_{1} = 1 & \\ 
   a_{2} = 1 &   
\end{cases}
$$

This regime is identified. The setting is identical to @tbl-swigtabledeveloped $\mathcal{G}_3$ with no unmeasured variables and no arrow from $A_1$ to $L_2$.

However, every causal contrast requires a comparison of at least two interventions.

Next, define the treatment policy $g^{\lambda}(\cdot)$ as divorce only if one's personal wealth is at least 50% greater than average and one would have divorced in the absence of intervention; otherwise, enforce marriage:

$$
A_t^{+}(\mathbf{g}^{\lambda}) = \mathbf{g}^{\lambda}(A_{t}) = \begin{cases} 
   a_{1} = 0 & \text{if income} > 1.5 \times  \mu_{\text{income}} \& A_1 = 0 \\ 
   a_{2} = 0 & \text{if income} > 1.5 \times  \mu_{\text{income}} \& A_2 = 0 \\ 
   a_{t} = 1 & \text{otherwise} 
\end{cases}
$$

Notice that for the treatment policy $\mathbf{g}^\lambda(\cdot)$, treatment is computed as a function of income at both the natural value of $A_t$ and of wealth $L_t$. Again, to declutter our graphs, we leave $L$ at baseline off the graph, noting that adjustment at baseline does not change the confounding structure.

Template @tbl-swigtabledeveloped $\mathcal{G}_5$ presents this confounding structure. To convey the dependence of the fixed node on covariate history under treatment, we use @richardson2013's conventions and draw a dashed line ($\rightarrowdottedgreen$) to indicate paths from the variables on which the time-varying treatment regime depends on the deterministic portion of the intervention node. This strategy clarifies that setting the treatment level requires information about prior variables, including the 'natural value of the treatment' in the absence of any intervention [@young2014identification].

The reason for noting these dependencies on a Single World Intervention Graph (SWIG) is that such dependencies impose stronger identification assumptions. At every time $t$, $A_t$ must be conditionally independent not only of the potential outcome at the end of the study but also of any future variable that might lead to a non-causal association between future treatments and the potential outcome at the end of the study. We clarify this additional requirement for *strong sequential exchangeability* in the next section.

#### Identification of Dynamic Time-Varying Treatment Strategies using an Extension of Robin's Dynamic G-formula


Supplementary materials **S3** describes Richardson and Robins extension of @robins1986 dynamic g-formula. Essentially, the algorithm can be stated as follows.

**Step 1**: where $\mathbf{g}(\cdot)$ is a modified treatment policy, identify all the variables that influence the outcome $Y(\mathbf{g})$, excluding those that are current or past treatment variables or covariates.

**Step 2**: For each treatment at time $t$, check if the treatment is independent of the variables identified in **Step 1**, after accounting for past covariates and treatments, in each Single World Intervention Graph where the treatment values are fixed. This step amounts to removing the dotted green arrows from the dynamic Single World Intervention Graph in @tbl-swigtabledeveloped $\mathcal{G}_5$, and doing so gives us @tbl-swigtabledeveloped $\mathcal{G}_4$. For each time point, we recover a set of future counterfactual variables that includes the potential outcome for the treatment regime under consideration, $Y(\mathbf{g})$, and other variables that the treatment might affect, including the natural value of future treatments. All backdoor paths must be closed to each member of this set of counterfactual variables. We call the more stringent assumptions required for identification in time-varying treatments (or equivalently longitudinal modified treatment policies [@diaz2023lmtp]): **strong sequential exchangeability**.



Where:

1. **$\mathbb{Z}_t(\mathbf{a}^*)$**: denotes the subset of vertices in $\mathcal{G}(\mathbf{a}^*)$ corresponding to $\mathbb{Z}_t(\mathbf{g})$.
2. **$A_t(\mathbf{a}^*) = a^*_t$**: denotes the specific value of the treatment variable at time $t$ under the intervention $\mathbf{a}^*$.
3. **$\bar{\mathbb{L}}_t(\mathbf{a}^*)$**: denotes the set of covariates up to time $t$ under the intervention $\mathbf{a}^*$.
4. **$\bar{\mathbb{A}}_{t-1}(\mathbf{a}^*)$**: denotes the set of past treatment variables up to time $t-1$ under the intervention $\mathbf{a}^*$.
:

Applying @richardson2013's dynamic extended g-formula, we obtain the following sets of future variables for which each current treatment must be independent:

$$
\begin{aligned}
\mathbb{Z}(\mathbf{g}) &= \{A_1, L_1(\mathbf{g}), A_1(\mathbf{g}), A_2(\mathbf{g}), Y(\mathbf{g})\} \\
\mathbb{Z}_1(\mathbf{g}) &= \{A_1(\mathbf{g}), L_1(\mathbf{g}), Y(\mathbf{g})\} \\
\mathbb{Z}_2(\mathbf{g}) &= \{Y(\mathbf{g})\}
\end{aligned}
$$

Having determined which variables must remain conditionally independent of each treatment in a sequence of dynamic treatments to be compared, we then consider whether strong sequential exchangeability holds. We do this by inspecting template @tbl-swigtabledeveloped $\mathcal{G}_4$ (recall this is @tbl-swigtabledeveloped $\mathcal{G}_5$ without the dashed green arrows). On inspection of $\mathcal{G}_4$ (the dynamic SWIG without dashed green arrows), we discover that this dynamic treatment strategy is not identified because we have the following open backdoor path:

$$
A_1 \associationred U_{AL} \associationred L_2(\mathbf{g})
$$

We also have:

$$
A_1 \associationred U_{AL} \associationred L_2(\mathbf{g}) \associationred A_2(\mathbf{g})
$$

Strong sequential exchangeability fails for $A_1$. We might consider lowering our sights and estimating a fixed or time-varying treatment strategy that can be identified.

Note that certain time-varying treatment strategies impose weaker assumptions than time-fixed strategies. For example, with a continuous intervention, we might consider intervening only if the observed treatment does not reach a specific threshold, such as:

$$
\mathbf{g}^{\phi} (A_i) = \begin{cases}  
\mu_A & \text{if } A_i < \mu_A \\ 
A_i & \text{otherwise} 
\end{cases}
$$

This is a weaker intervention than setting everyone whose natural value of treatment is above this threshold to precisely the threshold's value:

$$
\mathbf{g}^{\lambda} (A_i) = \begin{cases}   
\mu_A & \text{if } A_i \neq \mu_A \\ 
A_i & \text{otherwise} 
\end{cases}
$$

Whereas $\mathbf{g}^{\lambda}$ sets everyone in the population to the same treatment level, $\mathbf{g}^{\phi}$ sets only those below a certain threshold to a fixed level but does not estimate treatment effects for those above [@hoffman2023]. We can also write stochastic treatment functions [@diaz2012population; @vanderweele2014a; @young2014identification; @diaz2021nonparametric], see supplementary materials **S4**.

Of course, the details of every problem must be developed in relation to the scientific context and the practical questions that address gaps in present science. However, causal inference teaches us that the questions we ask—seemingly coherent and tractable questions such as whether marriage makes people happy—demand considerable scrutiny to become interpretable. When such questions are made interpretable, causal inference often reveals that answers may elude us, regardless of the quality and abundance of our data, or even if we randomise interventions. Modest treatment functions, however, might be more credible and useful for many scientific and practical questions. Such functions often cannot be estimated using the models routinely taught in the human sciences, such as multi-level modelling and statistical structural equation modelling.

## Conclusions {#id-sec-5}

Philosophical interests in causality are ancient. Democritus once declared, "I would rather discover one cause than gain the kingdom of Persia" [@freeman1948ancilla]. Hume provided a general account of causality by referencing counterfactuals: "... where, if the first object had not been, the second never would have existed" [@hume1902]. However, it was not until Jerzy Neyman's master's thesis that a quantitative analysis of causality was formalised [@neyman1923]. Remarkably, Neyman's work went largely unnoticed until the 1970s, when Harvard statistician Donald Rubin formalised what became known as the 'Rubin Causal Model' (also the Rubin-Neyman Causal Model) [@holland1986; @rubin1976].

In 1986, Harvard statistician James Robins extended the potential outcomes framework to time-varying treatments, laying the foundation for powerful new longitudinal data science methods [@robins1986]. Judea Pearl introduced directed acyclic graphs, making identification problems transparent and accessible to non-specialists [@pearl1995]. Robins and Richardson extended Pearl's graphical models to evaluate counterfactual causal contrasts on graphs, building on Robins' earlier work. Concurrently, the causal revolution in economics opened new, fertile frontiers in causal data sciences. By the early 2000s, targeted learning frameworks were being developed [@vanderlaan2011], along with causal mediation analysis methods [@robins1992; @pearl2009a; @vanderweele2015; @vanderweele2014a; @Diaz2023; @rudolph2024mediation; @vansteelandt2012], and techniques for analysing time-varying treatments [@robins1986; @robins1999; @young2014identification; @richardson2013; @diaz2012population; @robins2008estimation; @shpitser2022multivariate; @richardson2023potential].

Readers should note that the causal inference literature contains vigorous debates at the horizons of discovery. However, there is a shared consensus about the foundations of causal inference and a common conceptual and mathematical vocabulary within which to express disagreements and accumulate progress—a hallmark of a productive science. Old debates resolve and new debates arise, the hallmark of a vibrant science.

Despite the progress and momentum of the causal revolution in certain human sciences, many areas have yet to participate and benefit. The demands for researchers to acquire new skills, coupled with the intensive requirement for data collection, have significant implications for research design, funding, and the accepted pace of scientific publishing. To foster essential changes in causal inference education and practice, the human sciences need to shift from a predominantly output-focused, correlation-reporting culture to a slow, careful, creative culture that promotes retraining and funds time-series data collection. Such investments are worthwhile. Much as Darwin's theory transformed the biological sciences from speculative taxonomy, causal inference is slowly but steadily transforming the human sciences from butterfly collections of correlations to causal inferential sciences capable of addressing the causal questions that animate our curiosities.


{{< pagebreak >}}

## Acknowledgements

I am grateful to Dr. Inkuk Kim for checking previous versions of this manuscript and offering feedback, to two anonymous reviewers and the editors, Charles Efferson and Ruth Mace, for their constructive feedback. 

Any remaining errors are my own.


## Conflict of Interest

The author declares no conflicts of interest


## Financial Support

This work is supported by a grant from the Templeton Religion Trust (TRT0418) and RSNZ Marsden 3721245, 20-UOA-123; RSNZ 19-UOO-090. I also received support from the Max Planck Institute for the Science of Human History. The Funders had no role in preparing the manuscript or deciding to publish it.


## Research Transparency and Reproducibility

No data were used in this manuscript.




{{< pagebreak >}}

## References

::: {#refs}
:::





