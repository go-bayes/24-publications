---
title: "The Weirdest Causal Inferences in the World"
abstract: |
  Human scientists ask and answer questions about humans. Many of these questions are causal. This study clarifies two failure modes in causal inference. (1) Measurement error bias occurs when there is a discrepancy between a variable's true value and its observed value. (2) Sample-restriction bias arises when the association between cause and effect in a study population does not reflect the causal association in the target population. We use causal directed acyclic graphs (causal DAGs) to show how these threats to valid inference relate to each other. Our discussion addresses concerns that psycho-social datasets often draw entirely from 'Western, Educated, Industrialized, Rich, and Democratic (WEIRD)' populations. We provide simple graphical tools to help investigators evaluate when sample restriction is a feature and when it is a bug.

   **KEYWORDS**: *Causal Inference*; *Comparative*; *Cross-Cultural*;  *DAGs*;* *Evolution*,  *Experiments*; *Measurement Error**; *Selection Bias*; 
author: 
  - name: Joseph A. Bulbulia
    affiliation: Victoria University of Wellington, New Zealand
    orcid: 0000-0002-5861-2056
    email: joseph.bulbulia@vuw.ac.nz
    corresponding: no
editor_options: 
  chunk_output_type: console
format:
  pdf:
    sanitise: true
    keep-tex: true
    link-citations: true
    colorlinks: true
    documentclass: article
    classoption: [single column]
    lof: false
    lot: false
    geometry:
      - top=30mm
      - left=25mm
      - heightrounded
      - headsep=22pt
      - headheight=11pt
      - footskip=33pt
      - ignorehead
      - ignorefoot
    template-partials: 
      - /Users/joseph/GIT/templates/quarto/title.tex
    header-includes:
      - \input{/Users/joseph/GIT/latex/latex-for-quarto.tex}
date: last-modified
execute:
  echo: false
  warning: false
  include: true
  eval: true
fontfamily: libertinus
bibliography: /Users/joseph/GIT/templates/bib/references.bib
csl: /Users/joseph/GIT/templates/csl/camb-a.csl
---


```{r}
#| label: load-libraries
#| echo: false
#| include: false
#| eval: true

## WARNING SET THIS PATH TO YOUR DATA ON YOUR SECURE MACHINE. 
# pull_path <-
#   fs::path_expand(
#     #"/Users/joseph/v-project\ Dropbox/Joseph\ Bulbulia/00Bulbulia\ Pubs/DATA/nzavs_refactor/nzavs_data_23"
#     "/Users/joseph/Library/CloudStorage/Dropbox-v-project/Joseph\ Bulbulia/00Bulbulia\ Pubs/DATA/nzavs-current/r-data/nzavs_data_qs"
#   )
# 


push_mods <-  fs::path_expand(
  "/Users/joseph/Library/CloudStorage/Dropbox-v-project/data/nzvs_mods/24/church-prosocial-v7"
)


#tinytext::tlmgr_update()

# WARNING:  COMMENT THIS OUT. JB DOES THIS FOR WORKING WITHOUT WIFI
#source("/Users/joseph/GIT/templates/functions/libs2.R")
# # WARNING:  COMMENT THIS OUT. JB DOES THIS FOR WORKING WITHOUT WIFI
# source("/Users/joseph/GIT/templates/functions/funs.R")

#ALERT: UNCOMMENT THIS AND DOWNLOAD THE FUNCTIONS FROM JB's GITHUB

# source(
#   "https://raw.githubusercontent.com/go-bayes/templates/main/functions/experimental_funs.R"
# )
# 
# source(
#   "https://raw.githubusercontent.com/go-bayes/templates/main/functions/funs.R"
# )

# check path:is this correct?  check so you know you are not overwriting other directors
#push_mods

# for latex graphs
# for making graphs
library("tinytex")
library("extrafont")
library("tidyverse")
library("kableExtra")
#devtools::install_github("go-bayes/margot")
library(margot)
loadfonts(device = "all")
```


## Introduction

Human scientists ask and answer questions. To anchor answers in facts, we collect data.

Most human scientists work in what Joseph Henrich, Steven Heine, and Ara Norenzayan have termed 'WEIRD' societies: 'Western, Educated, Industrialized, Rich, and Democratic Societies' [@henrich2010weirdest]. Unsurprisingly, WEIRD samples are over-represented in human science datasets [@sears1986college; @arnett2008neglected]. @henrich2010weirdest illustrate how WEIRD samples differ from non-WEIRD samples in areas such as spatial cognition and perceptions of fairness, while showing continuities in basic emotions recognition, positive self-views, and motivation to punish anti-social behaviour. Because science seeks generalisation wherever it can, @henrich2010weirdest urge that sampling from non-WEIRD populations is desirable.

For certain questions, however, investigators might want to restrict sampling further. For example, suppose investigators are interested in the psychological effects of vasectomy on optimism. An efficient design will restrict eligibility to those who have not already had vasectomy. There might be other eligibility conditions. It might be desirable to extend findings to the global eligible population. However, findings sampling from the global population now need not extend to the ancestral past. Where relevant scientific knowledge permits, it may be credible to generalise with sample weights. Experimental designs may impose nested or sequential restrictions, benefiting efficiency or ethics.

Sometimes, howeever, investigators need to restrict samples. For instance, studying the psychological effects of vasectomy on optimism would efficiently restrict eligibility to those who have not had a vasectomy. Other eligibility conditions might apply. Extending findings to the global eligible population can be desirable, but such findings might not extend to the ancestral past. Where relevant scientific knowledge allows, generalisation with sample weights may be credible. Experimental designs may impose nested or sequential restrictions for efficiency or ethics.

When is restriction desirable and when not? It depends on the question.  Here we assume the questions are causal. 


**Part 1** uses causal diagrams to clarify five structural features of measurement-error bias. Understanding measurement error bias is essential in all research. In comparative research, we must distinguish threats arising from measurement error bias, sample restriction independently of measurement error bias, and threats combining both biases.

**Part 2** introduces the concept of 'target validity', focusing on threats arising from censoring bias. Unlike measurement error bias, which threatens validity irrespective of sampling restriction bias, sample restriction bias is relative to a target population. We focus on **censoring bias**, a form of sample restriction occurring within a study.

**Part 3** considers target validity when there is a mismatch between the sample population at baseline and the target population. This threat to target validity is often termed 'external validity'. In comparative research, we assume populations are drawn from a larger superpopulation. We evaluate the following threats to valid inference, imagining a comparison between "WEIRD" and "NOT-WEIRD" groups:

- Mismatch between sample populations and the target stratum population.
- Measurement error: If measurements work differently within strata of the superpopulation, comparative results will be invalid. Understanding measurement error bias clarifies the first failure mode: findings do not generalise because they are invalid for the comparative superpopulation.
- Variation in effect modifiers between the sample populations to be compared.

We begin with a brief overview of causal inference and causal diagrams. 



### Background: what is causality

To quantify a causal effect we must contrast the world as it has been realised -- which is, in principle, observable -- with the world as it might have been otherwise -- which is, in principle, not observable.

Consider a binary treatment variable $A \in \{0,1\}$ representing the randomised administration of a vaccine for individuals $i$ in the set $\{1, 2, \ldots, n\}$. $A_i = 1$ denotes administration in the vaccine condition and $A_i = 0$ denotes administration in the control condition. The potential outcomes for each individual are denoted as $Y_i(0)$ and $Y_i(1)$. These are the outcomes that are yet to be realised before administration. For this reason they are called 'potential' or 'counterfactual' outcomes. For an individual $i$ we can define a causal effect as a contrast between the outcome as it would have been observed in response to one level of an intervention and the outcome as it would have been observed in response to another level of the intervention. Such a contrast for the $i^{th}$ individual can be expressed on the difference scale as:

$$
\delta_i = Y_i(1) - Y_i(0)
$$

where $\delta_i$ defines the difference in respect of some predefined measure $Y$ in a scenario in which the treatment is received compared with a scenario in which the treatment is not received, and $\delta_i \neq 0$ denotes a causal effect of $A$ on $Y$ for unit $i$. Similarly $\delta_i = \frac{Y_i(1)}{Y(0)}\neq 1$ denotes a causal effect of treatment $A$ for unit $i$ on the risk-ratio scale. For any unit $i$ these quantities cannot be computed from observational data.

Suppose Alice is given vaccine: $A_{\text{Alice}} = 1$. If we assume the realised outcome $Y_{Alice}| A = 1$ is equal to the counterfactual outcome $Y_{Alice}(1)$ then for Alice $Y_{Alice}(1)$ is observed but $Y_{Alice}(0)$ remains counterfactual, and missing. Similarly, if Bob is not given vaccine, for Bob $Y_{Bob}(0)$ is observed but $Y_{Bob}(1)$ is not. That we cannot observe individual level causal effects is called the *Fundamental Problem of Causal Inference* [@rubin1976; @holland1986]. This problem has long puzzled philosophers[@hume1902; @lewis1973], However, although individual causal effects are generally unobservable, we may sometimes recover average causal effects by treatment group. 

### How we obtain average causal effect estimates from ideally conducted randomised experiments

The Average Treatment Effect (ATE), $\Delta_{ATE}$, measures the difference in outcomes between treated and control groups such that,

$$
\Delta_{ATE} = \mathbb{E}[Y(1)] - \mathbb{E}[Y(0)]
$$

where $\mathbb{E}[Y(1)]$ and $\mathbb{E}[Y(0)]$ are the average outcomes in the treatment and control groups, respectively.

In a randomised experiment, we estimate $\Delta_{ATE}$ by considering both observed and unobserved outcomes:

$$
\text{ATE} = \left(\mathbb{E}[Y(1)|A = 1] + \mathbb{E}[Y(1)|A = 0]\right) - \left(\mathbb{E}[Y(0)|A = 0] + \mathbb{E}[Y(0)|A = 1]\right)
$$

Effective randomisation ensures that potential outcomes are similarly distributed across both groups. Therefore, the average outcomes can be expected to be equal across different treatment conditions:

$$
\widehat{\mathbb{E}}[Y(0) | A = 1] = \widehat{\mathbb{E}}[Y(0) | A = 0], \quad \widehat{\mathbb{E}}[Y(1) | A = 1] = \widehat{\mathbb{E}}[Y(1) | A = 0]
$$

This provides an unbiased estimate of the Average Treatment Effect $\widehat{\text{ATE}}$,

$$
\widehat{\text{ATE}} = \widehat{\mathbb{E}}[Y | A = 1] - \widehat{\mathbb{E}}[Y | A = 0]
$$

Randomised controlled experiments are powerful because they evenly distribute potential explanatory factors across treatment groups. We cannot observe Alice or Bob's individual causal effects. However, for the groups to which they have been randomised, we recover average causal effects by a Sherlock-Holmes process of inference by elimination. In an ideally conducted randomised experiment, randomisation rules out every explanation for treatment group differences except the treatment.

Note that in the context of our imagined experiment $\widehat{\text{ATE}}$ applies to the population from which the experimental participants were drawn as calculated on the difference scale. An more explict notation would define this effect estimate by referencing it's scale and population: $\widehat{\text{ATE}}^{a'-a}_{\text{S}}$, where $a'-a$ denotes the difference scale, and $S$ denotes source population. We will return to this point in Part 2, but it is important to build intuition early that in causal inference we must specifying a scale of contrast and population for whom a causal effect estimate.

### To obtain average causal effect estimates from observational studies requires three fundamental assumptions

An observational study aims to estimate the average treatment effects in a setting where researchers do not control treatments or randomise the treatment assignments. We may only consistently obtain and estimate the counterfactual contrasts under strict assumptions. There are three fundamental assumptions for obtaining from observational data the counterfactual quantities required to compute causal contrasts.

#### Assumption 1. Causal Consistency

Causal consistency states that the observed outcome for each individual under the treatment they actually received is equal to their potential outcome under that treatment. This means if an individual $i$ received treatment $A_i = 1$, then their observed outcome $Y_i$ is the same as their potential outcome under treatment, denoted as $Y_i(1)$. Similarly, if they did not receive the treatment ($A_i = 0$), their observed outcome is the same as their potential outcome without treatment, denoted as $Y_i(0)$, such that:

$$
Y_i = A_i \cdot Y_i(1) + (1 - A_i) \cdot Y_i(0)
$$

where: 

- $Y_i$ is the observed outcome for individual $i$; 
- $A_i$ is the treatment status for individual $i$, with $A_i = 1$; indicating treatment received and $A_i = 0$ indicating no treatment; 
- $Y_i(1)$ and $Y_i(0)$ are the potential outcomes for individual $i$ under treatment and no treatment, respectively.

The causal consistency assumption necessary to linking the theoretical concept of potential outcomes -- the target quantities of interest -- with observable data (see @bulbulia2023a).

#### Assumption 2. Conditional exchangeability (or ignorability):

Conditional exchangeability states that given a set of measured covariates $L$, the potential outcomes are independent of the treatment assignment. That is, once we control for $L$, the treatment assignment $A$ is as good as random with respect to the potential outcomes:

$$
Y(a) \coprod A | L
$$

where:

-   $Y(a)$ represents the potential outcomes for a particular treatment level $a$.
-   $\coprod$ denotes conditional independence.
-   $A$ represents the treatment levels to be contrasted.
-   $L$ represents the measured covariates.

Under the conditional exchangeability assumption, any differences in outcomes between treatment groups can be attributed to the treatment. Note that the conditional exchangeability assumption requires that all confounding variables that affect both the treatment assignment $A$ and the potential outcomes $Y(a)$ are measured and included in $L$ (For further clarification, see Appendix A).

#### Assumption 3. Positivity

The positivity assumption requires that every individual in the population has a non-zero probability of receiving each treatment level, given their covariates. More formally,

$$
0 < Pr(A = a | L = l) < 1, \quad \forall a \in A, \, \forall l \in L \, \text{ such that } \, Pr(L = l) > 0
$$

where: 
- $A$ is the treatment or exposure variable. 
- $L$ is a vector of covariates. 
- $a$ and $l$ represent specific values of treatment and covariates, respectively.

The positivity assumption in causal inference, essential for ensuring valid estimates of treatment effects, requires that every individual has a non-zero chance of receiving each treatment across all covariate patterns $L$.[^positivity]

[^positivity]:In practice, verifying this assumption faces two main challenges:
a.  **Data sparsity**: certain covariate combinations are rare or unobserved in the data, making it difficult to empirically confirm positivity for these groups.
b.  **Model dependence**: as a result of data sparsity researchers rely on statistical models to estimate treatment probabilities for all covariate patterns, but these assessments are only as reliable as the models used, which may be subject to misspecification or inaccuracies.

#### Additional assumptions

There are additional practical and data assumptions for valid causal inference (see @bulbulia2023a). It is important to note that these assumptions are theoretical and often challenging to verify in practice. For example, the assumption of no unmeasured confounders (implicit in conditional exchangeability) is particularly challenging because it involves variables that are not observed. Note that even in ideally conducted randomised experiments, the fundamental assumptions of causal inference must be satisfied (see  @imai2008misunderstandings).

### Fundamental Assumptions for Causal Inference

We cannot generally observe individual causal effects, but we can compute average treatment effects by aggregating individual observations by treatment conditions. For a binary treatment, we express this as the difference in mean outcomes by treatment condition: $E[Y(1)] - E[Y(0)]$ or the mean difference in outcomes by treatment condition $E[Y(1) - Y(0)]$. This counterfactual contrast represents the quantity obtained for a sample population from an ideally conducted randomised controlled trial — an 'experiment'. There are three fundamental assumptions for computing average treatment effects:

1. **Causal Consistency**: Treatment levels remain consistent within the treatment arms to be compared (implied by 'control'). There must be at least two arms.
2. **(Conditional) Exchangeability**: Covariates that might affect outcomes under treatment are balanced across all arms (implied by 'randomisation').
3. **Positivity**: Each covariate that might affect treatment in the target population has a non-zero probability of being observed within each treatment condition to be compared (implied by randomisation and a clearly defined target population).

Although experiments often deviate from the ideal, potentially failing these assumptions, in the ideal experiment, these assumptions are satisfied. In observational or 'real-world' settings, none of these assumptions are guaranteed. Moreover, only the positivity assumption can be verified by data.

<!-- ### Workflow for Inferring Causal Effects from Real-World Data

1. **State a well-defined intervention.**
2. **State a well-defined outcome.**
3. **Clarify the target population.**
4. **Ensure treatments to be compared satisfy causal consistency.**
5. **Evaluate whether treatment groups, conditional on measured covariates, are exchangeable.** Differences must be ignorable, confounding covariates across treatment levels must be balanced, all backdoor paths between treatments and outcomes must be closed, treatments and outcomes must be d-separated, and there must be no unmeasured confounding. The goal is to ensure non-random 'real-world' data can emulate a randomised controlled experiment.
6. **Check if the positivity assumption is satisfied.**
7. **Clearly communicate the reasoning, evidence, and decision-making that inform steps 1-6.** -->

### Graphical Conventions

**$A$**: Denotes the "treatment" or "exposure" - a random variable.

This is the variable for which we seek to understand the effect of intervening on it. It is the "cause."

**$Y$**: Denotes the outcome or response, measured at the end of study.

It is the "effect."

**$L$**: Denotes a measured confounder or set of confounders.

**$U$**: Denotes an unmeasured confounder or confounders.

<!-- **$\mathcal{R}$**: Denotes a randomisation to treatment condition. -->


**Node**: a node or vertex represents characteristics or features of units within a population on a causal diagram -- that is a "variable." In causal directed acyclic graphs, we draw nodes with respect to the *target population*, which is the population for whom investigators seek causal inferences [@suzuki2020]. Time-indexed node:  $X_t$ denotes relative chronology

**Arrow** ($\rightarrowNEW$): denotes causal relationship from the node at the base of the arrow (a 'parent') to the node at the tip of the arrow (a 'child'). In causal DAGS it is conventional to refrain from drawing an arrow from treatment to outcome to avoid asserting a causal path from $A$ to $Y$ because iyr purpose is to ascertain whether causality can be identified for this path. All other nodes and paths -- including the absence of nodes and paths -- is typically assumed.

**Red Arrow** ($\rightarrowred$): path of non-causal association between the treatment and outcome. Despite the arrows, this path is associational and may flow against time.

**Dashed Arrow** ($\rightarrowdotted$): denotes a true association between the treatment and outcome that becomes partially obscured when conditioning on a mediator, assuming $A$ causes $Y$.

**Dashed Red Arrow** ($\rightarrowdottedred$): highlights over-conditioning bias from conditioning on a mediator.

**Open Blue Arrow** ($\rightarrowblue$): highlights effect modification, which occurs when the levels of the effect of treatment vary within levels of a covariate. We do not assess the causal effect of the effect-modifier on the outcome, recognising that it may be incoherent to consider intervening on the effect-modifier.

**Boxed Variable** $\big(\boxed{X}\big)$: conditioning or adjustment for $X$. 

**Red-Boxed Variable** $\big(\boxedred{X}\big)$: highlights the source of confounding bias from adjustment.

**Dashed Circle** $\big( \circledotted{X}\big)$: no adjustment is made for a variable (implied for unmeasured confounders.)

<!-- **$\mathbf{\mathcal{R}}$**  $\big(\mathcal{R} \rightarrow A\big)$ randomisation into the treatment condition. -->

### Causal Directed Acyclic Graphs (causal DAGs)

In the 1990s, Judea Pearl showed that we can evaluate causal dependencies using observable probability distributions [@pearl1995; @pearl2009a]. He also demonstrated that causal directed acyclic graphs (causal DAGs) clarify the conditional dependencies among variables [@pearl1995]. Based on assumptions about causal structure, researchers can identify causal effects from joint distributions of observed data.

Pearl developed graphical rules known as d-separation [@pearl1995]:

- **Fork rule** ($B \leftarrowNEW \boxed{A} \rightarrowNEW C$): $B$ and $C$ are independent when conditioned on $A$ ($B \coprod C \mid A$).
- **Chain rule** ($A \rightarrowNEW \boxed{B} \rightarrowNEW C$): Conditioning on $B$ blocks the path between $A$ and $C$ ($A \coprod C \mid B$).
- **Collider rule** ($A \rightarrowNEW \boxed{C} \leftarrowNEW B$): $A$ and $B$ are independent until conditioned on $C$, which introduces dependence ($A \cancel{\coprod} B \mid C$).

These rules lead to the backdoor criterion and 'backdoor adjustment' theorem, which provide algorithms for identifying causal effects based on the structural assumptions encoded in a causal DAG [@pearl1995]. We use the symbol $\mathcal{G}$ to name a graph.

Consider the following graphs from @tbl-terminologygeneral:

- **$\mathcal{G}_1$**: If $A$ and $B$ are not causally related and share no common causes, $A$ and $B$ will not be statistically related.
- **$\mathcal{G}_2$**: If $A$ causes $B$, and they share no common causes or their common causes are accounted for, $A$ and $B$ will be statistically related.
- **$\mathcal{G}_3$**: If $A$ causes $B$ and $A$ causes $C$, then conditioning on $A$ allows us to estimate the effect of $B$ on $C$.
- **$\mathcal{G}_4$**: If $A$ causes $B$ and $B$ causes $C$, conditioning on $B$ obscures the true causal effect of $A$ on $C$, making $A$ independent of $C$.
- **$\mathcal{G}_5$**: If $A$ causes $C$ and $B$ causes $C$, conditioning on $C$ associates $A$ and $B$, despite no direct causal effect.

If we assume that the variables in the graph correspond to Structural Causal Models, all causal relationships can be defined by the elementary structures presented above.

### Review of d-separation for Causal Identification on a Graph

::: {#tbl-terminologygeneral}
```{=latex}
\terminologydirectedgraph
```
Elements of Causal Graphs 
:::



::: {#tbl-terminologygeneral}
```{=latex}
\terminologyeffectmodification
```
Elements of Causal Graphs 
:::

@tbl-terminologygeneral clarifies how to ask a causal question of effect modification. We assume no confounding of the treatment on the outcome and that $A$ has been randomised ($\mathcal{R} \rightarrowNEW A$). We assume $\mathcal{R}  A \rightarrowNEW Y$.

To focus on effect modification, we do not draw a causal arrow from the direct effect modifier $F$ to the outcome $Y$. This convention is specific to this article (refer to @hernan2024WHATIF, pp. 126-127, for a discussion of 'noncausal' arrows).


## Part 1 Measurement Error Bias


::: {#tbl-terminologymeasurementerror}
```{=latex}
\terminologymeasurementerror
```
Six Structural Sources of Measurement Error Bias
:::

### Example 1: Uncorrelated errors under sharp null: no treatment effect

@tbl-terminologymeasurementerror $\mathcal{G}_1$ illustrates uncorrelated non-differential measurement error under the 'sharp-null,'' which arises when the error terms in the exposure and outcome are independent. In this setting the structure of measurement error is not expected to produce bias. 


For example, consider a study investigating a causal effect of beliefs in big Gods on social complexity in ancient societies. Imagine that societies either randomly omitted or inaccurately recorded details about their beliefs in big Gods and their social complexities. This might happen from the varying preservation of records across cultures, unrelated to the actual beliefs or social complexities. In this scenario, the errors in historical record for beliefs in big Gods and for social complexity would be independent. Such errors may generally not introduce bias when there is no true effect.


### Example 2: Uncorrelated errors under treatment effect biases true effects toward the null.

@tbl-terminologymeasurementerror $\mathcal{G}_2$ illustrates uncorrelated non-differential measurement error, that is bias that arises when the error terms in the exposure and outcome are independent (information bias). In this setting, bias will typically attenuate a true treatment effect. 

Consider again the example of a study investigating a causal effect of beliefs in big Gods on social complexity in ancient societies, were there are uncorrelated errors in the treatment and outcome. In this case, measurement error will make it seem that the true causal effects of beliefs in big Gods is smaller that it is, or perhaps even that such an effect is absent. 



### Example 3: Correlated errors Non-Differential (Undirected) Measurement Errors

@tbl-terminologymeasurementerror $\mathcal{G}_3$ illustrates the structure of correlated non-differential (un-directed) measurement error bias, which arises when the error terms of the treatment and outcome share a common cause.  

Consider an example: imagine that societies with more sophisticated record-keeping systems tend to offer more precise and comprehensive records both of beliefs in big Gods and of social complexity. In this setting, it is the record-keeping systems that give an illusion of a relationship between big Gods and social complexity. This might occur without any effect of big-God beliefs on the measurement of social complexity or vice versa. Nevertheless, the correlated sources of error for both the exposure and outcome may suggest causation in its absence. 



### Example 4: Uncorrelated Directed Measurement Error: Exposure affects error of outcome


@tbl-terminologymeasurementerror $\mathcal{G}_4$  illustrates the structure of uncorrelated differential (or directed) measurement error, in the when a non-causal path is opened linking  the treatment, the outcome, or a common cause of the treatment an outcome. 

Keeping with our previous example, imagine that beliefs in big Gods lead to inflated records of social complexity in a culture's record keeping. This might happen because the record keepers in societies that believe in big Gods prefer societies to reflect the grandeur of their big Gods. Suppose further that cultures lacking beliefs in big Gods prefer Bacchanalian-style feasting to record keeping. In this scenario, societies with record keepers who believe in big Gods would appear to have more social complexity than equally complex societies without such record keepers 



### Example 5: Uncorrelated Directed error: Outcome affects error of exposure


@tbl-terminologymeasurementerror $\mathcal{G}_5$ illustrates the structure of uncorrelated differential (or directed) measurement error, this time when the outcome affects the recording ot the treatment that preceeded the outcome. 

Consider if 'history is written by the victors' how might this affect measurement error bias?  Suppose that social complexity causes beliefs in big Gods. Perhaps kings make big Gods after the image of kings. If the kings prefer a history in which big Gods were historically present, this might bias the historical record, opening a path of association that reverses the order of causation. 

 
### Example 6: Directed error: outcome affects error of exposure


@tbl-terminologymeasurementerror $\mathcal{G}_6$ illustrates the structure of correlated differential (directed) measurement error, which occurs when the exposure affects levels of already correlated error terms.

Suppose social complexity produces a flattering class of religious elites who tend to produce vainglorious depictions of kings and their dominions, and also of the extend and scope of their societies beliefs in big Gods. For example, such elites might tend to downplay widespread cultural practices of worshiping lesser gods, inflate population estimates, and overstate their the range of their economies. In this scenario the errors of the exposure and of the outcome are both correlated and differential.


We limit the biases of measurement error by reducing error in our measures.  Often, specialist knowledge can guide the expected direction of measurement error associations, positive or negative (see: @suzuki2020, @vanderweele2010, and @vanderweele2007a). In some situations, researchers might use causal diagrams with signed paths to refine causal inferences, as suggested by @vanderweele2012. These techniques extend beyond the scope of this study. The point of these examples is to demonstrate how causal diagrams can clarify sources of confounding from measurement bias.


## Part 2: Selection-Restriction Bias From Right-Censoring (Attrition)

There is much confusion about the topic of 'selection bias', however, there need not be.  Some of this confusion is terminological. So we being by avoiding the term 'selection bias', and clarify our meanings.

**Unit/individual**: an entity, such as an object, person, or culture. We will use the term 'individual' in place of the more general term 'unit'. Think, 'row' in one's dataset. 

- **Variable**: a feature of an individual, transient or permenant. 'John was sleepy but is no longer.''Alice was born in December.'

- **Treatment**: an event that might change a variable. 'John was sleepy, we intervened with coffee, he's wide awake.' 'Alice was born in December; there's nothing to change that.' 

- **Measurement**: a recorded trace of a variable, such as a column in one's dataset.

- **Measurement error**: a misalignment between the true state of a variable and its recorded state. 'Alice was born 30/Nov, her mother, preoccupied, lost track of time.'  In Part 2, we considered a typology for structural sources of measurement.

**Population**: abstraction from statistics, denotes the set of all indivuals defined by certain features. John belongs to the set of all individuals who ignore instructions. 

- **Super-population**: abstraction, the population of all possible individuals of a given kind, another abstract but useful concept. John and Alice belong to a super-population of hominins.

- **Restricted population**: we say population $p$ is restricted relative to another population $P$, if the individuals $\in p$ share some but not all features of $P$. 'The living' are a restriction of hominins.

- **Target population**: a restriction of the super-population whose features interests investigators. An investigator who defines their interests is a member of the population of 'good investators.'

- **Source population**: the population from which the study's sample is drawn. Investigators wanted to recruit from a general population but recruited from the pool of first-year university psychology students conscripts. 

- **Baseline sample population**: the abstract set of individuals from which the units in one's study at treatment assigned belong, e.g. 'the set of all first-year university psychology students conscript might end up in this study'. To simplify we will think of the baseline population as the *source population.*

- **Selection into the sample**: the process by which individuals are included in a population or sample. Selection occurs, and is under investigator control, when a target population is defined from a super-population, or when investigators apply eligibility criteria for inclusion in the analytic sample. Selection into the sample is often out of investigator control. Investigators might aspire to answering questions about all of humanity but find themselves limited to undergraduate samples. Investigators might sample from a source population, but recover an analytic sample that differs from it in ways they cannot measure, such as mistrust of scientists. There is typically attrition of an analytic sample over time, and this is not typically fully within investigator control. 

- **Censored sample population**: the population from which the censored units are drawn. Censoring is uninformative if, for everyone in the baseline population, there is no effect of treatment (the sharp causal null hypothesis). Censoring is informative if there is an effect of the treatment, and this effect varies in at least one stratum of the baseline population. Note that uninformative censoring does not ensure valid inference for the target population even when valid inference is ensured for the baseline population. If the baseline population differs in the distribution of those features that modify the effect of the treatment, and no correction is applied, unbiased effect estimates for the baseline population will nevertheless be biased for the target population in at least one measure of effect [@greenland2009commentary; @lash2020]. This is why it is important for investigators to state a causal effect of interest with respect to *the full data* that includes the counterfactual quantities for the treatments to be compared in a clearly defined target population and with a specific causal contrast [@westreich2017].

- **Marginal effect**: synonym for the average treatment effect.

- **Intention-to-treat effect**: the effect of random treatment assignment.

- **Per-protocol effect**: the effect of adherence to a randomly assigned
treatment assignment if adherence were perfect [@hernan2017per].
We have no guarantee that the intention-to-treat effect will be
the same as the per-protocol effect. A safe assumption is that:
$\widehat{ATE}_{\text{target}}^{\text{Per-Protocol}} \ne \widehat{ATE}_{\text{target}}^{\text{Intention-to-Treat}}$.


When evaluating evidence for causality, in addition to specifying their causal contrast, effect measure, and target population, they should
specify whether they are estimating an intention-to-treat or per-protocol effect [@hernán2004; @tripepi2007]. 


Please note that terminology slighly varies for these concepts (see: @dahabreh2021study; @imai2008misunderstandings; @cole2010generalizing; @westreich2017transportability). A clear decomposition of key concepts need to assess generalisability is given in @imai2008misunderstandings. For a less technical, pragmatically useful discussion see:  @stuart2018generalizability.


**In Part 2 we will assume that the baseline sample population is the target population, and focus on biases arising from the (right)-censoring of the sample population.



::: {#tbl-terminologycensoring}
```{=latex}
\terminologycensoring
```
Five Structural Sources of Right-Censoring Bias
:::


### Example 1:  Confounding by common cause of treatment and attrition

@tbl-terminologycensoring $\mathcal{G}_1$ illustrates confounding by common cause of treatment and outcome in the censored such that the potential outcomes of the population at baseline $Y(a)$ may differ from those of the censored population at the end of study $Y'(a)$ such that $Y'(a) \neq Y(a)$. 


Suppose investigators are interested in whether religious service attendance affects volunteering. Suppose that an unmeasured variable, loyality, affects religious service attendance, attrition, and volunteering.  The structure of this bias reveals an open backdoor path from from the treatment to the outcome. 

Recall our measurement error causal diagrams. The structure we observe here is one of correlated measurement errors (@tbl-terminologymeasurementerror $\mathcal{G}_3$). In this example, attrition may exacerbate measurement error bias. 



### Example 2: Treatment affects censoring

@tbl-terminologycensoring $\mathcal{G}_2$ illustrates confounding bias in which the treatment affects the censoring process. 

Consider a study investigating the effects of mediation on volunteering. Suppose there is no treatment effect of meditation on volunteering, but that meditation affects social desirability, and that social desirability affects both attrition and reported volunteering. 


Recall our measurement error causal diagrams. The structure we observe here is one of directed uncorrelated measurement error (@tbl-terminologymeasurementerror $\mathcal{G}_4$). Randomisation ensures no backdoor paths.  However, if the intervention affects both attrition and bias in the outcome, this may exacerbate measurement error bias.  


Consider a study investigating the effects of mediation on well-being. Suppose there is no treatment effect but that Buddha-like detachment increases attrition. If $\mathcal{G}_4$ faithfully represents reality, there will be no bias in the treatment effect estimate. That is, there will be no risk that attrition will induce the appearance of a causa effect in its absence. 


### Example 3: No treatment effect when outcome causing censoring 

@tbl-terminologycensoring $\mathcal{G}_3$ illustrates the structure of bias when there is no treatment effect yet the outcome affects censoring. 


 If $\mathcal{G}_3$ faithfully represents reality, there will be no bias in the treatment effect estimate from attrition, even if there is measurement error in the outcome. Recall again our measurement error causal diagrams. The structure we observe here is one of undirected uncorrelated measurement error (@tbl-terminologymeasurementerror $\mathcal{G}_1$). Here, we would not expect attrition to induce or exaggerate the appearance of a causal effect in its absence, for the same reason that uncorrelated treatment errors. However $\mathcal{G}_3$ assumes a sharp null (no arrow from $A\to Y$ for any individual). If we believed in the sharp null we would not be motivated to conduct the experiment.  




### Example 4: Treatment effect when outcome causes censoring and there is a true treatment effect

@tbl-terminologycensoring $\mathcal{G}_4$ illustrates the structure of bias when the outcome affects censoring in the presence of a treatment effect. In contrast to the previous example, here there is scope for confounding bias. 

Consider again a study investigating the effects of mediation on well-being. This time suppose that Buddha-like detachment affects attrition. As such, the investigators will observe a restricted range of effect in the sample at the end of study compared to the sample at the start of the study.  The structure of bias in this example is not one of measurement error.  Rather, there is confounding of the per-protocal effect of the meditation on well-being.  Note that if the randomisation were successful, the 'intent-to-treat' effect would be unbiased.

 
### Example 5: Treatment effect and effect-modifiers differ in censored (restriction bias without confounding)


@tbl-terminologycensoring $\mathcal{G}_5$ represents a setting in which there is a true treatment effect, but the distribution of effect-modifiers -- variables that interact with the treatment -- differ among the sample at baseline and the sample at the end of study. Knowing nothing else, we might expect this setting to be standard.  Where measured variables are sufficient to predict attrition, that is, where missingness is at random, we can obtain valid estimates for a treatment effect by inverse probability of treatment weighting [@cole2008; @leyrat2021]. In this approach, the sample gives more weight to under-represented individuals owing to drop-out. As with missing data imputation, IPW with censoring weights also assumes that we can correctly model the missingness from the observed data [@shiba2021].However, if missingness is not completely at random, then identification may be compromised [@tchetgen2017general; @malinsky2022semiparametric].   

{{< pagebreak >}}


## Part 3: Selection-Restriction Bias at Baseline (Left-Censoring)
###  Sample-Restriction Bias Considered as Collider-Restriction Bias

::: {#tbl-terminologyselectionrestrictionclassic}
```{=latex}
\terminologyselectionrestrictionclassic
```
Collider-Stratification bias at start of study ('M-bias')
:::



@tbl-terminologyselectionrestrictionclassic $\mathcal{G}_1$ illustrates an example of sample restriction bias at baseline in which there is collider-restriction bias

Suppose investigators what to estimate the causal effects of regular physical activity, $A$, and heart health $Y$, in adults. Their design:

**Target population**: all adults in a given city.

**Source population**: adults visiting a network of community health centers for routine check-ups.

**Selection for Study ($T = 1$)**: participants are selected based on their willingness to participate in a health survey conducted at these centers. Suppose the analytic sample is representative of this source population, and the source and target poulation converge, such $Pr(D = 1) = Pr(S = 1) = Pr(T)$

Suppose there are two unmeasured variables: 

1. Health Awareness, $U1$, an unmeasured variable that influences both the probability of participating in the study,$\boxed{S = 1}$ and being physically active, $A$.  We assume that people with higher health awareness are both more likely to engage in physical activity and to participate in health-related studies.

2. Socioeconomic Status (SES), $U2$, an unmeasured variable that influences both the probability of participating in the study, $\boxed{S = 1}$ and heart health, $Y$. We assume that individuals with higher SES have better access to healthcare and are more likely to participate in health surveys; they also tend to have better heart health from healthy lifestyles: joining expensive gyms, juicing, long vacations, and the like.
 
As presented in @tbl-terminologyselectionrestrictionclassic $\mathcal{G}_1$, there is collider restriction bias from conditioning on $T=1$ 

1. **$U1$**: because individuals with higher health awareness are more likely to be both physically active and participate in the study, the subsample over-represents physically active individuals. This overestimates the prevalence of physical activity, setting up a bias in overstating the potential benefits of physical activity on heart health in the general population.

2. **$U2$**: because individuals with higher health awareness may have better heart health from SES-related factors, this opens a confounding path from physical activity and heart health through the selected sample, setting up the investigators for the potentially erroneous inference that physical activity has a greater positive impact on heart health than it actually does in the general population. The actual effect of physical activity on heart health in the general population might be less pronounced than observed.


It might seem as though researchers need to sample from the target population, the sub-sample will not work.  However, by adjusting for health awareness or SES, or a proxy of either, researchers may block the open path between. @tbl-terminologyselectionrestrictionclassic $\mathcal{G}_2$ presents this sollution. This strategy will only provide an unbiased effect estimate for the population if either there is no causal effect for all strata of the selected sample (the sharp null hypothesis) or there are no interactions between the distribution of effect modifiers in the sample population and the target population.  The next series of examples illustrates challenges to obtaining valid causal effect estimates in the presence of interactions. 


### Sample-Restriction Bias Without Collider-Restriction Bias

#### Generalisability

We say a study's findings generalise to a target population if the effects observed in the study group are also valid for the target group for structurally valid reasons (i.e. non-accidentally).

Clearly, the similarity of the source population to the target population in study-relevant characteristics enhances the applicability of the findings to the target population.

Suppose we sample randomly from the target population where,

-   $n_S$ denotes the size of the study sample $S$.
-   $N_T$ denotes the total size of the target population $T$.
-   $\hat{ATE}_{S}$ denotes the estimated average treatment effect in the study sample.
-   $\hat{ATE}_{T}$ denotes the estimated average treatment effect in the target population.

Assuming the rest of a causal inference workflow goes to plan (randomisation succeeds, there is no measurement error, no model misspecification, etc), as the random sample size $n_S$ increases over the target population $N_T$, the estimated treatment effect in $S$ converges to that in $T$:

$$
\lim_{n_S \to N_T} \hat{ATE}_{S} = \hat{ATE}_{T}
$$

#### Transportability

Sometimes the target population is the general population of interest from which researchers have sample. Where results are valid, they will generalise to this general population. Often the population of interest is often some 'trial eligible' population, not a general population.

For example, suppose our scientific question pertains to a restricted population, such as first year North American university students. If we were to successful sample from globally diverse population, the sample treatment effect might not transport to the trail eligible population of North American university students. In this setting, researchers should be discouraged from sampling from a global population, no matter what the public incentives, because doing so will lead to bias, or a need for corrections.

However, for many scientific questions, the target population differs from the source population. Although the concept of a 'target population' is critical for interpreting our results, this concept cannot be expressed absolutely. Rather, it is relative to the scientific interests and purposes at hand. The closer we sample from the target population the less results will rely on methods for adjustment, which carry the burdens of model misspecification bias.

Suppose we have not randomly sampled from the target population such that $S$ is not a subset of $T$.

Define,

-   $\hat{ATE}_{S}$ as the estimated average treatment effect in the study sample $S$.
-   $\hat{ATE}_{T}$ as the estimated average treatment effect in the target population $T$.
-   $f(S, R)$ as the mapping function over $S$ using a measured set of variables $R$ that permits valid projection of the source ATE to the population ATE.

$$
\hat{ATE}_{S} \xrightarrow{f(S, R)} \hat{ATE}_{T}
$$

All problems in adjusting for sampling bias are problems of obtaining a satisfactory function for such transportation.

**Source bias or "Sample-Restriction-at-Baseline Bias** occurs when the sample populationg (source population) does not accurately represent the group of interest (target population) in the distribution of variables that modify treatments effects.  Such bias occurs because the selection into the study occurs on an effect modifier for the effect of the exposure on the outcome. Note that although the causal effect of $A\to Y$ is unbiased for the exposed and unexposed in the source population, the effect estimate does not generalise to the exposed and unexposed in the target population. Although there is no confounding, causal inferences in this scenario do not generalise as we might hope [@suzuki2016; @suzuki2014; @suzuki2020]. 

**Appendix B** provides a simulation that illustrates the bias.  **Appendix C** provides a mathmatical explanation.

::: {#tbl-terminologyselectionrestrictionbaseline}
```{=latex}
\terminologyselectionrestrictionbaseline
```
The association in the population of selected individuals differs from the causal association for the target population. Hernán calls this scenario "selection bias off the null" [@hernán2017]. Lu et al. call this scenario "Type 2 selection bias" [@lu2022]. We call this bias "Sample-Restriction Bias at Baseline."
:::


### Problem 1:   Target population is not WEIRD; sample population is WEIRD


@tbl-terminologyselectionrestrictionbaseline $\mathcal{G}_{1.1}$  presents a scenario in which there is source bias for the population parameter. When that sample we obtain at baseline differs from the target population, and where the distributions of variables that modify treatment differ between the sample and target populations, effect-estimates may be biased, even in the absence of confounding bias. 

Example: we want to take 'the difficult steps to building a broader, richer, and better-grounded understanding of our species' but have sampled from a pool of undergraduate students.  For many questions, causal effect estimates in this sample population will not generalise to the target popultion. 

We might decide to adjust expectations, and not attempt species-level generalisations. 

Or we might decide to sample more widely from our local population, and, where possible, weight the sample to better recover the population parameters of interest for our target sample. To obtain weighted estimates for the target population, we must first defined by eligibility criteria.  If the baseline population differs from the target population, where sample weights for the distribution of covariates are available for the *target population*, these should be applied to the baseline population [with caution, given potential for model mis-specification, see @stuart2015.]

Let $\widehat{ATE}_{target}$ denote the population average treatment
effect for the target population. Let $\widehat{ATE}_{\text{restricted}}$ denote the average treatment effect at the end of treatment. Let $W$ denote a set of variables upon which the restricted and target populations structurally differ. We say that
results *generalise* if we can guarantee that:

$$
\widehat{ATE}_{target} =  \widehat{ATE}_{restricted} 
$$

or if there is a known function such that:

$$
ATE_{target}\approx  f_W(ATE_{\text{restricted}}, W)
$$

In most cases, $f_W$ will be unknown, as it must account for potential
heterogeneity of effects and unobserved sources of bias. For further
discussion on this topic, see: @imai2008misunderstandings;
@cole2010generalizing; @stuart2018generalizability



 Suppose we take the bold option and sample from the species. @tbl-terminologyselectionrestrictionbaseline $\mathcal{G}_{1.2}$ projects this scenario.  What would this recover?  If there is considerable heterogeneity, then we might not know how to interpret the average treatment effect that we recover.  Put differently, we might not be able to interpret how the quantity we obtain relates to WEIRD population, or any specific culture for that matter. To obtain such effects we would need to model treatment heterogeneity. If we seek explicity comparative models we will need to ensure validity for every sample that we compare. 



### Example 2:  Target population is WEIRD; sample population is not WEIRD

@tbl-terminologyselectionrestrictionbaseline $\mathcal{G}_{2.1}$ presents a scenario in which there the source population does not meet eligibility criteria. Consider again the question of whether vascectomy affects a sense of meaning and purpose in life.  Suppose further we want to evaluate effects in New Zealand among men over the age of 40 who have no prior history of vasectomy, who are in relationships with heterosexual partners. The target population is weirder than WEIRD, in the sense it must be narrower.  We should not sample from young children, the elderly, and any who do not qualify.  For many scientific questions, a narrow population us desirable. 


Note that @tbl-terminologyselectionrestrictionbaseline $\mathcal{G}_{2.1}$ is identical to @tbl-terminologycensoring $\mathcal{G}_5$ -- right-censoring bias with effect modifiers in an otherwise unconfounded study. The problem is formally the same: source-target population mismatch. However, whereas in the right-censoring setting, we imagined no source bias at baseline -- mismatch arose from attrition -- here there is a mismatch at baseline. 

@tbl-terminologyselectionrestrictionbaseline $\mathcal{G}_{2.2}$ presents a solution. Ensure eligibility criteria are scientifically relevant and feasible. Sample from this eligible population. With caution, apply survey or other weights where these weights enable a closer approximate to the distributions of effect-modifiers in the target population. 


### Example 3: Correlated measurement error of covariates and outcome in the absence of a treatment effect

@tbl-terminologyselectionrestrictionbaseline $\mathcal{G}_{3.1}$ considers the threats to target validity from correlated measurement errors in the target population arising from structured errors. Even if the treatment is measured with out error, there may be multiple sources of error leading to association without causation. 

Suppose the structures of correlated error correspond to features of cultural units. Suppose further that investigators develop a plan for a cross-cultural investigation to clarify the relationship between interventions on religious service attendance ($A$) and an outcome $Y$ charitable giving. The investigators plan to obtain measures of covariates $L$ sufficient to control for confounding. Suppose the investigators observe religious attendance so that it is not measured with error [as did @shaver2021comparison], yet there is heterogeneity in the measurement of covariates $L$ and the outcome $Y$.  For example, if charitable giving measures are included as baseline covariates in $L$, measurement errors at baseline will be correlated with outcome measures. Perhaps in certain cultures, religious serive is under-reported (associated with witchcraft), in other cultures, it is over reported. Suppose further that true covariates affect the treatment and outcome. As shown in @tbl-terminologyselectionrestrictionbaseline $\mathcal{G}_{3.1}$, multiple paths of bias are opened. 

Moreover, because measurements are causally related to the phenomena they record, investigators cannot apply statistical tests to verify whether measures are recorded with error [@vanderweele2022; @vansteelandt2022].  Whether the phenomena that investigtors hope measure are functionally equivalent across cultural settings remains unknown. Of course, we all want to understand the species, however, asking coherent omparative questions in the human sciences is not as easy as lying. Including multiple cultures into a single analysis imposes considerable burdens on investigators. 



@tbl-terminologyselectionrestrictionbaseline $\mathcal{G}_{3.2}$ provides sensible solution: restrict one's study to those cultures where causality can be identified.  Democritus wrote, 'I would rather discover one cause than gain the kingdom of Persia' [@freeman1948ancilla].' Paraphrasing Democritus we might say, 'I should rather discover one WIERD cause than the kingdom of associational comparative research'. 



### Example 4: Correlated measurement error of effect-modifiers for an overly ambitious target population

@tbl-terminologyselectionrestrictionbaseline $\mathcal{G}_{4.1}$ considers the threats to target validity from correlated measurement errors in the target population arising from structured errors linking measurements for the effect modifiers. Even if the treatment is randomised so that there are no open backdoor paths, and even if the treatment and outcome are measured without error, investigators will not be able to obtain valid estimates for treatment-effect heterogeneity from their data, nor will they be able to apply target-sample weights (such as census weights) to obtain valid estimates for the populations in which the measurement errros of effect modifiers are manifest. 

@tbl-terminologyselectionrestrictionbaseline $\mathcal{G}_{4.2}$ suggest that where measures of effect-modification are uncertain, best to consider settings in which the measurements are reliable -- whether or note the settings are WEIRD.


## Conclusions

- Measurement first 
The trail of the serpant of measurement error bias runs across every study. Challenges for managing this serpant are greatly augmented where structures of measurement error may open pathways of bias linking treatments and outcomes in absence of causality.  


### Building intuition for why definitions are insufficient for understanding selection and source biases

When considering how source/target population mismatch imperiles science, it is tempting to seek general definitions, typologies, and identification criteria by which to spot the snakes.

Recently, a host of institutional diversity and inclusion initiatives have been developed that commend researchers to obtain data from global samples. In my view, the motivation for these mission statements is ethically laudable. The injunction for a broader science of humanity also accords with institutional missions. For example, the scientific mission of the American Psychological Association (APA) is 'to promote the advancement, communication, and application of psychological science and knowledge to benefit society and improve lives.' The APA does not state that it wants to understand and benefit only North Atlantic Societies.[^1]. It is therefore tempting to use such a mission statement as an ideal by which to evaluate the samples used in human scientific research.

[^1]: https://www.apa.org/pubs/authors/equity-diversity-inclusion

Suppose we agree that promoting a globally diverse science makes ethical sense. Does the sampling of globally diverse populations always advance this ideal? It is easy find examples in which restricting our source population makes better scientific sense. Suppose we are interested in the psychological effects of restorative justice among victims of violent crime. Here, it would make little scientific sense to sample from a population that has not experienced violent crime. Nor would it make ethical sense. The scientific question, which my have important ethical importance, is not served by casting a wider net. Suppose our the interest were to investigate the health effects of calorie restriction. It might be unethical to include children or the elderly. It make little sense to investigate the psychological impact of vasectomy in biological female or historectomy in biological males

In the cases we just considered, the scientific questions pertained to a sub-sample of the human population and so could be sensibly restricted. However even for questions that relate to all of humanity, sampling from all of humanity might be undesirable. For example, if we were interested in the effects of a vaccine on disease, sampling from one population might be as good as sampling from all. Sampling from one population might spare time and expense, which come with opportunity costs. We might conclude that sampling universally, where unnecessary, is wasteful and unethical.

We might agree with our mission statements in judging that ethical aspirations must guide research at every phase. Yet, mistaking our aspirations for sampling directives would result in wasteful science. If such waste is avoidable, then the result is unethical science.

I present this examples to remind ourselves of the importance of addressing questions of sampling in relation to its context.

During the past twenty years, the causal data sciences, also known as causal inference, have enabled tremendous clarity for questions of research design and analysis. The following hopes to clarify that it is only within these workflows that we are able to understand the threats to causal inferences from selection and source bias, and strategies for addressing them.

{{< pagebreak >}}

## Funding

This work is supported by a grant from the Templeton Religion Trust (TRT0418) and RSNZ Marsden 3721245, 20-UOA-123; RSNZ 19-UOO-090. I also received support from the Max Planck Institute for the Science of Human History. The Funders had no role in preparing the manuscript or the decision to publish it.

## Acknowledgements

Errors are my own.


{{< pagebreak >}}



## Appendix A: Glossary


::: {#tbl-experiments}
```{=latex}
\glossaryTerms
```
Glossary
:::

{{< pagebreak >}}


## Appendix B: R Simulation to Clarify Why The Distribution of Effect Modifiers Matter For Estimating Treatment Effects For A Target Population

First, we load the `stdReg` library, which obtains marginal effect estimates by simulating counterfactuals under different levels of treatment [@sjölander2016]. If a treatment is continuous, the levels can be specified. 

We also load the `parameters` library, which creates nice tables [@parameters2020].


```{r}
# to obtain marginal effects
if (!requireNamespace("stdReg", quietly = TRUE)) install.packages("stdReg")
library(stdReg)

# to create nice tables
if (!requireNamespace("parameters", quietly = TRUE)) install.packages("parameters")
library(parameters)
```

Next, we write a function to simulate data for the sample and and target populations. 

We assume the treatment effect is the same in the sample and target population. We will assume that the coefficient for the effect-modifier and the coefficient for interaction are the same.  We assume no unmeasured confounding throughout the study.  We assume only selective attrition of one effect modifier such that the baseline population differs from the sample population at the end of the study.   

That is: **the distribution of effect modifiers is the only respect in which the sample will differ from the target population.**

This function will generate data under a range of scenarios.[^margot]

[^margot]: See documentation in the `margot` package: @margot2024



```{r}
# function to generate data for the sample and population, 
# along with precise sample weights for the population, there are differences 
# in the distribution of the true effect modifier but no differences in the treatment effect 
# or the effect modification.all that differs between the sample and the population is 
# the distribution of effect-modifiers.


# reproducability
set.seed(123)

# simulate the data -- you can use different parameters
data <- margot::simulate_ate_data_with_weights(
  n_sample = 10000,
  n_population = 100000,
  p_z_sample = 0.1,
  p_z_population = 0.5,
  beta_a = 1,
  beta_z = 2.5,
  noise_sd = 0.5
)

skimr::skim(data)
```

We have generated both sample and population data. 

Next, we verify that the distributions of effect modifiers differ in the sample and in the target population:

```{r}
# obtain the generated data
sample_data <- data$sample_data
population_data <- data$population_data


# check imbalance
table(sample_data$z_sample) # type 1 is rare
table(population_data$z_population) # type 1 is common
```



The sample and population distributions differ. 

Next, consider the question: "What are the differences in the coefficients that we obtain from the study population at the end of study, as compared with the those we would obtain for target population?"  

First, we obtain the regression coefficients for the sample. They are as follows:


```{r}
# model coefficients sample
model_sample  <-
  glm(y_sample ~ a_sample * z_sample, data = sample_data)

# summary
parameters::model_parameters(model_sample, ci_method = "wald")
```

We next obtain the regression coefficients for the weighted regression of the sample.   Notice that the coefficients are virtually the same:

```{r}
# model the sample weighted to the population, again note that these coefficients are similar 
model_weighted_sample <-
  glm(y_sample ~  a_sample  * z_sample,
      data = sample_data,
      weights = weights)

# summary
summary(parameters::model_parameters(model_weighted_sample, ci_method =
                                       "wald"))
```


We might be tempted to infer that weighting wasn't relevant to the analysis. However, we'll see that such an interpretation would be a mistake.


Next, we obtain model coefficients for the population. Note again there is no difference -- only narrower errors owing to the large sample size. 


```{r}
# model coefficients population -- note that these coefficients are very similar. 
model_population <-
  glm(y_population ~ a_population * z_population, data = population_data)

parameters::model_parameters(model_population, ci_method = "wald")
```


Again, there is no difference. That is, we find that all model coefficients are practically equivalent. The different distribution of effect modifiers does not result in different coefficient values for the treatment effect, the effect-modifier "effect," or the interaction of effect modifier and treatment. 

Consider why this is the case: in a large sample where the causal effects are invariant -- as we have simulated them to be -- we will have good replication in the effect modifiers within the sample, so our statistical model can recover the *coefficients* for the population without challenge. 

However, *in causal inference, we are interested in the marginal effect of the treatment. That is, we seek an estimate for the counterfactual *contrast* in which everyone in a pre-specified population was subject to one level of treatment compared with a counterfactual condition in which everyone in a population was subject to another level of the same treatment. 

**When the sample population differs in the distribution of effect modifiers from the target population effect, the marginal effect estimates will typically differ.**

To see this, we use the `stdReg` package to recover marginal effect estimates, comparing (1) the sample ATE, (2) the true oracle ATE for the population, and (3) the weighted sample ATE.  We will use the outputs of the same models above. The only difference is that we will calculate marginal effects from these outputs. We will contrast a difference from an intervention in which everyone receives treatment = 0 with one in which everyone receives treatment = 1, however, this choice is arbitrary, and the general lessons apply irrespective of the estimand.

First, consider this Average Treatment Effect for the sample population. 

```{r}
# What inference do we draw?  We cannot say the models are unbiased for the marginal effect estimates. 
# regression standardisation 
library(stdReg) # to obtain marginal effects 


# obtain sample ate
fit_std_sample <-
  stdReg::stdGlm(model_sample, data = sample_data, X = "a_sample")

# summary
summary(fit_std_sample,
        contrast = "difference",
        reference = 0)
```

The treatment effect is given as a 1.06 unit change in the outcome across the sample population, with a confidence interval from 1.04 to 1.08. 


Next, we obtain the true (oracle) treatment effect for the population under the same intervention.

```{r}
## note the population effect is different

#obtain true ate
fit_std_population <-
  stdReg::stdGlm(model_population, data = population_data, X = "a_population")

# summary
summary(fit_std_population,
        contrast = "difference",
        reference = 0)
```


Note, the true treatment effect is a 1.25 unit change in the population, with a confidence bound between 1.24 and 1.26. This is well outside the ATE that we obtain from the sample population!



Next, consider the ATE in the weighted regression, where the sample was weighted to the target population's true distribution of effect modifiers. 

```{r}
## next try weights adjusted ate where we correctly assign population weights to the sample
fit_std_weighted_sample_weights <- stdReg::stdGlm( model_weighted_sample, 
    data = sample_data, 
    X = "a_sample")

# this gives us the right answer
summary(fit_std_weighted_sample_weights, 
    contrast = "difference", 
    reference = 0)


# Moral of the story. When we marginalise over the entire sample we need to weight estimates to the target population. 
```


We find that we obtain the population-level causal effect estimate with accurate coverage by weighting the sample to the target population. So with appropriate weights, our results generalise from the sample to the target population.


## Lessons 

- Regression coefficients do not clarify the problem of sample/target population mismatch -- or selection bias as discussed in this manuscript.
- The correct advice to investigators is that they should not rely on regression coefficients when evaluating the biases that arise from sample attrition. This advice applies to both methods that the authors use to investigate threats of bias. That is, to implement this advice, the authors must first take it.
- Generally, observed data are insufficient for assessing threats. Observed data do not clarify structural sources of bias, nor do they clarify effect-modification in the full counterfactual data condition in which all receive the treatment and all do not receive the treatment (at the same level).
- To properly assess bias, one would need access to the counterfactual outcome—what would have happened to the missing participants had they not been lost to follow-up or had they responded. Again, the join distributions over "full data" are inherently unobservable [@vanderlaan2011]. 
- In simple settings like the one we just simulated, we may address the gap between the sample and target population using methods such as modelling the censoring (e.g., censoring weighting). However, we never know what setting we are in or whether it is simple—such modelling must be handled with care. There is a large and growing epidemiology literature on this topic (see, for example, @li2023non).
