---
title: 'Methods in Causal Inference Part 1: Causal Diagrams and Confounding'
abstract: |
  Causal inference requires contrasting counterfactual states of the world under pre-specified interventions. Obtaining counterfactual contrasts from data relies on explicit assumptions and careful, multi-step workflows. Causal diagrams are powerful tools for clarifying whether and how the counterfactual contrasts we seek can be identified from data. Here, I explain how to use causal directed acyclic graphs (causal DAGs) to determine whether and how causal effects can be identified from 'real-world' non-experimental observational data. I offer practical tips for reporting and suggest ways to avoid common pitfalls.
  **KEYWORDS**: *Causal Inference*; *Culture*; *DAGs*; *Evolution*; *Tutorial*
author: 
  - name: Joseph A. Bulbulia
    affiliation: Victoria University of Wellington, NEW ZEALAND
    orcid: 0000-0002-5861-2056
    email: joseph.bulbulia@vuw.ac.nz
    corresponding: no
editor_options: 
  chunk_output_type: console
format:
  pdf:
    sanitise: true
    keep-tex: true
    link-citations: true
    colorlinks: true
    documentclass: article
    classoption: [single column]
    lof: false
    lot: false
    geometry:
      - top=30mm
      - left=25mm
      - heightrounded
      - headsep=22pt
      - headheight=11pt
      - footskip=33pt
      - ignorehead
      - ignorefoot
    template-partials: 
      - /Users/joseph/GIT/templates/quarto/title.tex
    header-includes:
      - \input{/Users/joseph/GIT/latex/latex-for-quarto.tex}
date: last-modified
execute:
  echo: false
  warning: false
  include: true
  eval: true
fontfamily: libertinus
pdf-engine: lualatex
bibliography: /Users/joseph/GIT/templates/bib/references.bib
csl: /Users/joseph/GIT/templates/csl/apa-7.csl
---
```{r}
#| label: load-libraries
#| echo: false
#| include: false
#| eval: true

## WARNING SET THIS PATH TO YOUR DATA ON YOUR SECURE MACHINE. 
# pull_path <-
#   fs::path_expand(
#     #'/Users/joseph/v-project\ Dropbox/Joseph\ Bulbulia/00Bulbulia\ Pubs/DATA/nzavs_refactor/nzavs_data_23'
#     '/Users/joseph/Library/CloudStorage/Dropbox-v-project/Joseph\ Bulbulia/00Bulbulia\ Pubs/DATA/nzavs-current/r-data/nzavs_data_qs'
#   )
# 


push_mods <-  fs::path_expand(
  '/Users/joseph/Library/CloudStorage/Dropbox-v-project/data/nzvs_mods/24/church-prosocial-v7'
)


#tinytext::tlmgr_update()

# WARNING:  COMMENT THIS OUT. JB DOES THIS FOR WORKING WITHOUT WIFI
#source('/Users/joseph/GIT/templates/functions/libs2.R')
# # WARNING:  COMMENT THIS OUT. JB DOES THIS FOR WORKING WITHOUT WIFI
# source('/Users/joseph/GIT/templates/functions/funs.R')

#ALERT: UNCOMMENT THIS AND DOWNLOAD THE FUNCTIONS FROM JB's GITHUB

# source(
#   'https://raw.githubusercontent.com/go-bayes/templates/main/functions/experimental_funs.R'
# )
# 
# source(
#   'https://raw.githubusercontent.com/go-bayes/templates/main/functions/funs.R'
# )

# check path:is this correct?  check so you know you are not overwriting other directors
#push_mods

# for latex graphs
# for making graphs
library('tinytex')
library('extrafont')
library('tidyverse')
library('kableExtra')
#devtools::install_github('go-bayes/margot')
library(margot)
loadfonts(device = 'all')
```


## Introduction {#id-sec-introduction}

Human research begins with two fundamental questions:

1. What do I want to know?
2. For which population does this knowledge generalise?

In the human sciences, our questions are typically causal. We aim to understand the effects of interventions on certain variables. However, many researchers report non-causal associations, collecting data, applying complex regressions, and reporting coefficients. We often speak of covariates as 'predicting' outcomes. Yet, even when our models predict well, it remains unclear how these predictions relate to the scientific questions that sparked our interest. Our predictions lack meaning and fail to address our core scientific questions.

Some say that association cannot imply causation. However, our experimental traditions reveal that when interventions are controlled and randomised, the coefficients we recover from statistical models can permit causal interpretations.

Despite familiarity with experimental protocols, many researchers struggle to emulate randomisation and control with non-experimental or 'real-world' data. Though we use terms such as 'control' and employ sophisticated adjustment strategies, such as multilevel modelling and structural equation models, our practices are not systematic. We often overlook that what we take as control can undermine our ability to consistently estimate causal effects [@montgomery2018]. Although the term 'crisis' is overused, the state of causal inference across many human sciences, including experimental sciences, has much headroom for improvement. 'Room for headroom' applies to poor experimental designs that unintentionally weaken causal claims [@hernan2017per; @bulbulia_2024_experiments; @montgomery2018]. Fortunately, recent decades have seen considerable progress in causal data science, commonly called 'causal inference', or 'CI'. The progress has transformed those areas of health science, economics, political science, and computer science that have adopted it. Causal inference provides methods for obtaining valid causal inferences from data through careful, systematic workflows.

Within the workflows of causal inference, causal directed acyclic graphs (causal DAGs)—are powerful tools for evaluating whether and how causal effects can be identified from data. My purpose here is to explain where these tools fit within causal inference workflows and to illustrate several practical applications. I focus on causal directed acyclic graphs (causal DAGs) because they are relatively easy to use and clear for most applications. However, causal DAGs can be misused. I will consider common pitfalls and how to avoid them.

In [Part 1](#id-sec-1), I review the conceptual foundations of causal inference. The basis of all causal inference lies in counterfactual contrasts. Although there are slightly different philosophical approaches to counterfactual reasoning, it is widely agreed that to infer a causal effect is to contrast counterfactual for a well defined population under different levels of intervention. The overview I present here builds on the Neyman-Rubin potential outcomes framework of causal inference [@holland1986] as it has been extended for longitudinal treatments by epidemiologist James Robins [@robins1986]. 

In [Part 2](#id-sec-2), I describe how causal directed acyclic graphs (causal DAGs) allow investigators to evaluate whether and how causal effects may be identified from data using assumptions encoded in a causal DAG. I outline five elementary graphical structures from which all causal relations may be derived; these structures form the building blocks of every causal directed acyclic graphs. I then examine five rules that clarify whether and how investigators may identify causal effects from data under the structural (or equivalently causal) assumptions that a causal DAG encodes.

In [Part 3](#id-sec-3), I apply causal directed acyclic graphs to seven common identification problems, showing how repeated-measures data collection addresses these problems. I then use causal diagrams to explain the limitations of repeated-measures data collection for identifying causal effects, tempering enthusiasm for easy solutions from repeated-measures designs. 

In [Part 4](#id-sec-4), I offer practical suggestions for creating and reporting causal directed acyclic graphs in scientific research. Where there is ambiguity or debate about how a treatment may be related to an outcome independently of causality, I suggest that investigators report multiple causal diagrams and conduct distinct analyses for each.


## Part 1: Causal Inference as Counterfactual Data Science {#id-sec-1}

The first step in answering a causal question is to ask it [@hernán2016].

1. What causal quantity do I want to learn from the data?
2. For which population does this knowledge generalise?

Causal diagrams come after we have stated a causal question and have clarified the population for whom we hope to obtain valid causal inferences -- the 'target population'. We begin by considering what is required to state a causal question and to define a target population precisely.

### The Fundamental Problem of Causal Inference: Missing Counterfactual Observations

To ask a causal question, we must consider the concept of causality itself. Consider an intervention, $A$, and its effect, $Y$. We say that $A$ causes $Y$ if altering $A$ would lead to a change in $Y$ [@hume1902; @lewis1973]. If altering $A$ would not change $Y$, we say that $A$ has no causal effect on $Y$.

In causal inference, we aim to use data to quantitatively contrast the potential outcomes in response to different levels of a well-defined intervention. Commonly, we refer to such interventions as 'exposures' or 'treatments;' we refer to the possible effects of interventions as 'potential outcomes.'

Consider a binary treatment variable $A \in \{0,1\}$. For each unit $i$ in the set $\{1, 2, \ldots, n\}$, when $A_i$ is set to 0, the potential outcome under this condition is denoted $Y_i(0)$. Conversely, when $A_i$ is set to 1, the potential outcome is denoted $Y_i(1)$. We refer to the terms $Y_i(1)$ and $Y_i(0)$ as 'potential outcomes' because, until realised, the effects of interventions describe counterfactual states.

Suppose that each unit $i$ receives either $A_i = 1$ or $A_i = 0$. The corresponding outcomes are realised as $Y_i|A_i = 1$ or $Y_i|A_i = 0$. For now, we assume that each realised outcome under that intervention is equivalent to one of the potential outcomes required for a quantitative causal contrast, such that $[(Y_i(a)|A_i = a)] = (Y_i|A_i = a)$. Thus, when $A_i = 1$, $Y_i(1)|A_i = 1$ is observed. However, when $A_i = 1$, it follows that $Y_i(0)|A_i = 1$ is not observed:

$$
Y_i|A_i = 1 \implies Y_i(0)|A_i = 1~ \text{is counterfactual}
$$

Conversely:

$$
Y_i|A_i = 0 \implies Y_i(1)|A_i = 0~ \text{is counterfactual}
$$

We define $\delta_i$ as the individual causal effect for unit $i$ and express the individual causal effect as:

$$
\delta_i = Y_i(1) - Y_i(0)
$$

Notice that at the level of the individual, a causal effect is a contrast between treatments one of which is excluded by the other at any given time. That individual causal effects cannot be identified from observations is known as '*the fundamental problem of causal inference*' [@rubin1976; @holland1986].


### Identifying Causal Effects Using Randomised Experiments

Although it is not typically feasible to compute individual causal effects, under certain assumptions, it may be possible to estimate *average* treatment effects, also called 'marginal effects' by contrasting the outcomes of observed treatments among individuals who have been randomly assigned, perhaps conditional on measured covariates, to the treatment levels that investigators wish to compare. We define an average treatment effect (ATE) as the difference between the expected or average outcomes observed under treatment where treatment has been randomly assigned, perhaps conditionally, on measured covariates. Consider a binary treatment, $A \in \{0,1\}$. We write the average treatment effect as a contrast in the expected means of a population all of whose members are exposed to two levels of treatment: 

$$
\text{Average Treatment Effect} = \mathbb{E}[Y(1)] - \mathbb{E}[Y(0)]
$$

This is our pre-specified estimand for our target population. Note that a challenge remains in computing these treatment-group averages, given that individual causal effects are unobservable: each treatment to be compared is not administered to every member of the population from which a sample is drawn. We can frame the problem by referring to the *full data* required to compute this estimand — that is, in terms of the complete counterfactual dataset where the missing potential outcomes, inherent in observational data, were somehow available for everyone in the target population. The text highlighted in red denotes inherently missing responses over the joint distribution of the full counterfactual dataset. Suppose that 50% of the sample is randomly assigned to each treatment condition. We find that for each treatment condition, half the observations over the joint distribution of the counterfactual data are inherently unobservable:

$$
\text{Average Treatment Effect} = \left(\underbrace{\underbrace{\mathbb{E}[Y(1)|A = 1]}_{\text{observed for } A = 1} + \underbrace{\textcolor{red}{\mathbb{E}[Y(1)|A = 0]}}_{\textcolor{red}{\text{unobserved for } A = 0}}}_{\text{effect among treated}}\right) - \left(\underbrace{\underbrace{\mathbb{E}[Y(0)|A = 0]}_{\text{observed for } A = 0} + \underbrace{\textcolor{red}{\mathbb{E}[Y(0)|A = 1]}}_{\textcolor{red}{\text{unobserved for } A = 1}}}_{\text{effect among untreated}}\right)
$$

Although the fundamental problem of causal inference remains at the individual level, randomisation allows investigators to recover the treatment group averages. When investigators randomise units into treatment conditions, ensuring full adherence and a sufficiently large sample to rule out chance differences in group composition, we can generally attribute differences in treatment group averages to the treatment itself. That is, randomisation implies:

$$
\mathbb{E}[Y(0) | A = 1] = \mathbb{E}[Y(0) | A = 0]
$$

and

$$
\mathbb{E}[Y(1) | A = 1] = \mathbb{E}[Y(1) | A = 0]
$$

If we assume:

$$ 
\mathbb{E}[Y(1) | A = 1] = \mathbb{E}[Y | A = 1]
$$

and

$$
\mathbb{E}[Y(0) | A = 0] = \mathbb{E}[Y | A = 0]
$$

it follows that the average treatment effect of a randomised experiment can be computed:

$$
\text{Average Treatment Effect} = \widehat{\mathbb{E}}[Y | A = 1] - \widehat{\mathbb{E}}[Y | A = 0]
$$

It is evident that we do not require the joint distribution over the full data (i.e., the counterfactual data) to obtain these averages. Rather, randomisation allows us to obtain a contrast of averages (or equivalently the average of contrasts) from the observed data. 

There are four critical aspects of how ideally randomised experiments enable the estimation of average treatment effects worth highlighting.

First, we must specify a population for whom they seek to generalise their results. We refer to this population as the *target population*. If the study population differs from the target population in the distribution of covariates that interact with the treatment, we will have no guarantees our results will generalise (for discussions of sample/target population mismatch, refer to @imai2008misunderstandings; @westreich2019target; @westreich2017; @pearl2022; @bareinboim2013general; @stuart2018generalizability; @webster2021directed).

Second, because the units in the study sample at randomisation may differ from the units in the study after randomisation, we must be careful to avoid biases that arise from sample/population mismatch over time [@hernan2004STRUCTURAL; @bulbulia2024wierd]. If there is sample attrition or non-response, the treatment effect we obtain for the sample may differ from the treatment effect in the target population.

Third, a randomised experiment recovers the causal effect of random treatment assignment, not of the treatment itself, which may differ if some participants do not adhere to their treatment (even if they remain in the study) [@hernan2017per]. The effect of randomised assignment is called the 'intent-to-treat effect' or equivalently the 'intention-to-treat effect'. The effect of perfect adherence is called the 'per-protocol effect' [@hernan2017per; @lash2020]. To obtain the per-protocol effect for randomised experiments, methods for causal inference in observational settings must be applied [@hernan2017per; @bulbulia_2024_experiments].

Fourth, I have presented the average treatment effect on the additive scale, that is, as an additive difference in average potential outcomes for the target population under two distinct levels of treatment. However, depending on the scientific question at hand, investigators may wish to estimate causal effects on the risk-ratio scale, the rate-ratio scale, the hazard-ratio scale, or another scale. Where there are interactions such that treatment effects vary across different strata of the population, an estimate of the causal effect on the risk difference scale will differ in at least one stratum to be compared from the estimate on the risk ratio scale [@greenland2003quantifying; @vanderweele2012]. The sensitivity of treatment effects in the presence of interactions to the scale of contrast underscores the importance of pre-specifying a scale for the causal contrast investigators hope to obtain.

Fifth, investigators may unintentionally spoil randomisation by adjusting for indicators that might be affected by the treatment, outcome, or both, by excluding participants using attention checks, by collecting covariate data that might be affected by the experimental conditions, by failing to account for non-response and loss-to-follow-up, and by committing any number of other self-inflicted injuries [@bulbulia_2024_experiments]. Unfortunately, such practices of self-inflicted confounding are widespread [@montgomery2018]. Notably, causal directed acyclic graphs are useful for describing risks to valid causal identification in experiments (refer to @hernan2017per), a topic I consider elsewhere [@bulbulia_2024_experiments].

In observational studies, investigators might wish to describe the target population of interest as a restriction of the study sample population. For example, investigators might wish to estimate the average treatment effect only in the population that received the treatment [@greifer2023; @greifer2023a]. This treatment effect is sometimes called the average treatment effect in the treated (ATT) and may be expressed as:

$$
\text{Average Treatment Effect in the Treated} = \mathbb{E}[Y(1) - Y(0) \mid A = 1]
$$

Consider that if investigators are interested in the average treatment effect in the treated, counterfactual comparisons are deliberately restricted to the sample population that was treated. That is, the investigators will seek to obtain the average of the missing counterfactual outcomes for the treated population if everyone in that population were, perhaps contrary to fact, treated, without necessarily obtaining the counterfactual outcomes for the untreated population. This difference in focus may imply different assumptions and analytic workflows. Supplementary materials **S2** describes an example for which the assumptions required to estimate the average treatment effect in the treated might be preferred. In what follows, we will use the term ATE as a placeholder to mean the average treatment effect, or equivalently the `marginal effect', for a target population on a pre-specified scale of causal contrast, where we assume that this effect estimate pertains to the source population from which the analytic sample was randomly drawn (under the assumption of random sampling, which, as with most assumptions, need not hold [@dahabreh2019generalizing; @dahabreh2019].

Setting aside the important detail that the 'average treatment effect' requires considerable care in its specification, it is worth pausing to marvel at how an ideally conducted randomised controlled experiment provides a means for identifying inherently unobservable counterfactuals. It does so by using a Sherlock-Holmes method of inference by elimination of confounders, which randomisation balances across treatments.

When experimenters observe a difference in average treatment effects, and all else goes right, they may infer that the distribution of potential outcomes differs by treatment because randomisation exhausts every other explanation. If the experiment is valid, experimenters are entitled to this inference because randomisation balances the distribution of potential confounders across the treatment groups to be compared.

However, we lack guarantees for balance in the confounders outside idealised randomised experiments. Unfortunately, randomised experiments cannot address many scientifically important questions. This bitter constraint is familiar to evolutionary human scientists. We typically confront 'What if?' questions that are rooted in the unidirectional nature of human history.
However, understanding how randomisation obtains the missing counterfactual outcomes that we require to consistently estimate average treatment effects clarifies the tasks of causal inference in non-experimental settings [@hernán2008a; @hernán2006; @hernán2022]: we want to ensure balance in the variables that might affect outcomes under treatment in the treatment groups to be compared. 

Next, we examine basic causal identification assumptions in greater detail. We do so because using causal diagrams without understanding these assumptions may lead to unwarranted false confidence.

### Fundamental Assumptions Required for Causal Inference in the Potential Outcomes Framework

Three fundamental identification assumptions must be satisfied to consistently estimate causal effects from data. These assumptions are typically satisfied in properly executed randomised controlled trials but not in real-world studies where randomised treatment assignment is absent.

#### Assumption 1: Causal Consistency

We satisfy the causal consistency assumption when, for each unit $i$ in the set $\{1, 2, \ldots, n\}$, the observed outcome corresponds to one of the specific counterfactual outcomes to be compared such that:

$$
Y_i^{observed}|A_i = 
\begin{cases} 
Y_i(a^*) & \text{if } A_i = a^* \\
Y_i(a) & \text{if } A_i = a
\end{cases}
$$

The causal consistency assumption implies that the observed outcome at the specific treatment level that an individual receives equates to that individual's counterfactual outcome at the observed treatment level. Although this assumption would appear straightforward, outside ideally controlled randomised experiments, treatment conditions typically vary, and treatment heterogeneity poses considerable challenges to satisfying this assumption. Refer to supplementary materials **S3** for further discussion on how investigators may satisfy the causal consistency assumption in real-world settings.

#### Assumption 2: Positivity

We satisfy the positivity assumption if there is a non-zero probability of receiving each treatment level within each stratum of covariate required to ensure conditional exchangeability of treatments (assumption 3). Where $A$ is the treatment and $L$ is a vector of covariates sufficient to ensure no unmeasured confounding, we say positivity is achieved if:

$$
0 < Pr(A = a | L = l) < 1, \quad \text{for all } a, l \text{ with } Pr(L = l) > 0
$$

There are two types of positivity violation:

1. **Random non-positivity**: When a treatment is theoretically possible but specific treatment levels are not represented in the data, random non-positivity is the only identifiability assumption verifiable with data.
2. **Deterministic non-positivity**: When the treatment is implausible by nature, such as a hysterectomy in biological males.

Satisfying the positivity assumption can present considerable data challenges [@westreich2010; @bulbulia2023a]. For instance, if we wanted to estimate a one-year causal effect of weekly religious service attendance on charitable donations, controlling for baseline attendance, and the natural transition rate to weekly service attendance is low, the effective sample size for the treatment condition may be insufficient. Where the positivity assumption is violated, causal diagrams will be of limited utility because observations in the data do not support valid causal inferences. Supplementary materials **S2** presents a worked example illustrating this difficulty in a cultural evolutionary context.)

#### Assumption 3: Conditional Exchangeability (also 'No Unmeasured Confounding', 'Conditional Ignorability', 'd-separation')

We satisfy the conditional exchangeability assumption if the treatment groups are conditionally balanced in the variables that could affect the potential outcomes. In experimental designs, random assignment facilitates this assumption. In observational studies, more effort is required to control for any covariate that could account for observed correlations between $A$ and $Y$ without a causal effect of $A$ on $Y$.

Let $\coprod$ denote independence, and let $L$ denote the set of covariates necessary to ensure this conditional independence. Conditional exchangeability is satisfied when:

$$
Y(a) \coprod A | L \quad \text{or equivalently} \quad A \coprod Y(a) | L
$$

If we assume that the positivity and consistency assumptions also hold, we may compute the average treatment effect (ATE) on the difference scale:

$$
\text{Average Treatment Effect} = \mathbb{E}[Y(1) | L] - \mathbb{E}[Y(0) | L]
$$

In randomised controlled experiments, exchangeability is unconditional. We would only adjust our statistical model by interacting the treatment with pre-treatment variables to improve efficiency [@lin2012regressexperiments] or diminish threats to valid randomisation from chance imbalances [@hernan2024WHATIF]. However, it would be confusing to think of such an adjustment as 'control.'

In real-world observational studies, where measured covariates are sufficient to ensure conditional exchangeability across the treatment groups to be compared -- also called, 'no unmeasured confounding' or 'ignorability' --  we may obtain valid estimates for an average treatment effect by conditioning on the densities of measured confounders by treatment group. Where $A = a$ and $A = a^*$ are the treatment levels we seek to contrast:

$$
\widehat{\text{ATE}} =  \sum_l \big( \mathbb{E}[Y(a^*) \mid L] - \mathbb{E}[Y(a) \mid L] \big) \times Pr(L)
$$

By causal consistency, we obtain:

$$
\widehat{\text{ATE}} =  \sum_l \big( \mathbb{E}[Y \mid A = a^*, L] - \mathbb{E}[Y \mid A = a, L] \big) \times Pr(L)
$$

For continuous covariates $L$, we have:

$$
\widehat{\text{ATE}} = \int \big( \mathbb{E}[Y \mid A = a^*, L] - \mathbb{E}[Y \mid A = a, L] \big) dP(L)
$$

We may now state the primary function of a causal directed acyclic graph (causal DAG), which is to identify sources of bias that may lead to an association between an exposure and outcome in the absence of causation. Causal DAGs visually encode features of a causal order necessary to evaluate the assumptions of conditional exchangeability, or equivalently of 'no-unmeasured confounding', or equivalently of 'ignorability' -- or equivalently of 'd-separation' (explained next).  Although causal directed acyclic graphs may be useful for addressing other biases such as measurement error and target-population restriction bias (also called 'selection bias') [@hernan2024WHATIF; @bulbulia2024wierd], it is important to understand that causal directed acyclic graphs are specifically designed to evaluate the assumptions of conditional exchangeability or 'd-separation', any other use is strictly 'off-label'.

Finally, it is important to emphasise that without randomisation, we typically cannot ensure there is no-unmeasured confounding[@stuart2015; @greifer2023]. For this reason, causal data science workflows typically include sensitivity analyses to determine how much unmeasured confounding would be required to compromise a study’s findings [@vanderweele2017]. Moreover, even if investigators do not represent unmeasured common causes of treatment and exposure in the causal DAGs they craft for observational studies, we should assume there are umeasured common causes and plan sensitivity analyses.

### Summary of Part 1

Causal data science is distinct from ordinary data science. The initial step involves formulating a precise causal question that clearly defines a treatment or sequence of treatments, the outcome or outcomes to be contrasted under treatment, and a population of interest called the target population. We must then satisfy the three fundamental assumptions required for causal inference, assumptions that are implicit in the ideal of a randomised controlled experiment:

Causal consistency: Outcomes at the treatment levels to be compared must align with their counterfactual counterparts.
- **Positivity**: Each treatment must have a non-zero probability across all covariates.
- **Conditional exchangeability**: There should be no unmeasured confounding, meaning treatment assignment is ignorable conditional on measured confounders, or equivalently, that treatment groups are conditionally exchangeable.


{{< pagebreak >}}



## Part 2: How Causal Directed Acyclic Graphs Clarify the Conditional Exchangeability Assumption {#id-sec-2}

Next, I will introduce causal directed acyclic graphs (DAGs). I will start by explaining the meaning of the symbols used. Refer to supplementary materials S1 for a glossary of common causal inference terms.

### Variable Naming Conventions

::: {#tbl-terminology}
```{=latex}
\terminologylocalconventionssimple
```
Variable naming conventions
:::
- **$X$**: Denotes a random variable without reference to its role.

- **$A$**: Denotes the 'treatment' or 'exposure'—a random variable. This is the variable for which we seek to understand the effect of intervening on it. It is the 'cause.'

- **$A=a$**: Denotes a fixed 'treatment' or 'exposure.' The random variable $A$ is set to level $A=a$.

- **$Y$**: Denotes the outcome or response of an intervention. It is the 'effect.'

- **$Y(a)$**: Denotes the counterfactual or potential state of $Y$ in response to setting the level of the treatment to a specific level, $A=a$. The outcome $Y$ as it would be observed when, perhaps contrary to fact, treatment $A$ is set to level $A=a$. Different conventions exist for expressing a potential or counterfactual outcome, such as $Y^a$, $Y_a$.

- **$L$**: Denotes a measured confounder or set of confounders. This set, if conditioned upon, ensures that any differences between the potential outcomes under different levels of the treatment are the result of the treatment and not the result of a common cause of the treatment and the outcome. Mathematically, we write this independence:

$$
Y(a) \coprod A \mid L
$$

- **$U$**: Denotes an unmeasured confounder or confounders. $U$ is a variable or set of variables that may affect both the treatment and the outcome, leading to an association in the absence of causality, even after conditioning on measured covariates:

$$
Y(a) \cancel{\coprod} A \mid L \quad \text{[because of unmeasured } U]
$$

- **$F$**: Denotes a modifier of the treatment effect. $F$ alters the magnitude or direction of the effect of treatment $A$ on an outcome $Y$.

- **$M$**: Denotes a mediator, a variable that transmits the effect of treatment $A$ on an outcome $Y$.

- **$\bar{X}$**: Denotes a sequence of variables, for example, a sequence of treatments.

- **$\mathcal{R}$**: Denotes a randomisation to treatment condition.

- **$\mathcal{G}$**: Denotes a graph, here, a causal directed acyclic graph.

Note that investigators use a variety of different symbols. There is no unique right way to create a causal directed acyclic graph, except that meaning must be clear and the graph must be capable of identifying relationships of conditional and unconditional independence between the treatment and outcome. Although directed acyclic graphs are accessible tools, general graphical models such as 'Single World Intervention Graphs,' which allow for the explicit representation of counterfactual dependencies, may be preferable for investigators to estimate causal effects under multiple interventions [@richardson2013; @bulbulia2024swigstime].



### Conventions We Use in This Article to Create Causal Directed Acyclic Graphs

The conventions we use to describe components of our causal graphs are given in @tbl-general.

::: {#tbl-general}
```{=latex}
\terminologygeneraldags
```
Nodes, Edges, Conditioning Conventions.
:::

- **Node**: a node or vertex represents characteristics or features of units within a population on a causal diagram -- that is a 'variable.' In causal directed acyclic graphs, we draw nodes with respect to the *target population*, which is the population for whom investigators seek causal inferences [@suzuki2020]. Time-indexed node:  $X_t$ denotes relative chronology; $X_{\customphi{t}}$ is our convention for indicating that timing is assumed, perhaps erroneously.  

- **Edge without an Arrow** ($\association$): path of association, causality not asserted.

- **Red Edge without an Arrow** ($\associationred$): confounding path: ignores arrows to clarify statistical dependencies. 

- **Arrow** ($\rightarrowNEW$): denotes causal relationship from the node at the base of the arrow (a parent) to the node at the tip of the arrow (a child). We typically refrain from drawing an arrow from treatment to outcome to avoid asserting a causal path from $A$ to $Y$ because the function of a causal directed acyclic graph is to evaluate whether causality can be identified for this path.

- **Red Arrow** ($\rightarrowred$): path of non-causal association between the treatment and outcome. Path is associational and may run against arrows.

- **Dashed Arrow** ($\rightarrowdotted$): denotes a true association between the treatment and outcome that becomes partially obscured when conditioning on a mediator, assuming $A$ causes $Y$.

- **Dashed Red Arrow** ($\rightarrowdottedred$): highlights over-conditioning bias from conditioning on a mediator.

-  **Open Blue Arrow** ($\rightarrowblue$): Highlights effect modification, occurring when the treatment effect levels vary within covariate levels. We do not assess the causal effect of the effect modifier on the outcome, recognising that intervening on the effect modifier may be incoherent. This is an off-label convention we use to clarify our interest in effect modification within strata of a covariate when there is a true treatment effect. However, it is possible to replace these open blue arrows with ordinary nodes and explain that the edges are drawn not for identification but for evaluating generalisations [see @bulbulia2024swigstime].

- **Boxed Variable** $\boxed{X}$: conditioning or adjustment for $X$. 

- **Red-Boxed Variable** $\boxedred{X}$: highlights the source of confounding bias from adjustment.

- **Dashed Circle** $\circledotted{X}$: no adjustment is made for a variable (implied for unmeasured confounders.)

- **$\mathbf{\mathcal{R}}$** randomisation, for example, randomisation into treatment: $\mathcal{R} \rightarrow A$.

- **Presenting Temporal Order**: Causal directed acyclic graphs must be — as truth in advertising implies— *acyclic.* Directed edges or arrows define ancestral relations. No descendant node can cause an ancestor node. Therefore causal diagrams are, by default, sequentially ordered.

Nevertheless, to make our causal graphs more readable, we adopt the following conventions:

1. The layout of a causal diagram is structured from left to right to reflect the assumed sequence of causality as it unfolds.
2. We often index our nodes using $X_t$ to indicate their relative timing and chronological order, where $t$ represents the time point or sequence in the timeline of events.
3. Where temporal order is uncertain or unknown, we use the notation $X_{\phi t}$ to propose a temporal order that is uncertain.

Typically, the timing of unmeasured confounders is unknown, except that they occur before the treatments of interest; hence, we place confounders to the left of the treatments and outcomes they are assumed to affect, but without any time indexing.

Again, temporal order is implied by the relationship of nodes and edges. However, explicitly representing the order in the layout of one's causal graph often makes it easier to evaluate, and the convention representing uncertainty is useful, particularly when the data do not ensure the relative timing of the occurrence of the variable in a causal graph.

More generally, investigators use various conventions to convey causal structures on graphs. Whichever convention we adopt must be clear.

Finally, note that all nodes and paths on causal graphs—including the absence of nodes and paths—are asserted. Constructing causal diagrams requires expert judgment of the scientific system under investigation. It is a great power given to those who construct causal graphs, and *with great power comes great responsibility to be transparent.* When investigators are unclear or there is debate about which graphical model fits reality, they should present multiple causal graphs. Where identification is possible in several candidate causal graphs, they should perform and report multiple analyses.

### How Causal Directed Acyclic Graphs Relate Observations to Counterfactual Interventions

#### Ancestral Relations in Directed Acyclic Graphs

We define the relation of 'parent' and 'child' on a directed acyclic graph as follows:

1. Node $A$ is a **parent** of node $B$ if there is a directed edge from $A$ to $B$, denoted $A \rightarrow B$.
2. Node $B$ is a **child** of node $A$ if there is a directed edge from $A$ to $B$, denoted $A \rightarrow B$.

It follows that a parent and child are **adjacent nodes** connected by a directed edge.

We denote the set of all parents of a node $B$ as $\text{pa}(B)$.

In a directed acyclic graph, the directed edge $A \rightarrow B$ indicates a statistical dependency where $A$ may provide information about $B$. In a causal directed acyclic graph, the directed edge $A \rightarrow B$ is interpreted as a causal relationship, meaning $A$ is a direct cause of $B$.


We further define the relations of **ancestor** and **descendant** on a directed acyclic graph as follows:

1. Node $A$ is an **ancestor** of node $C$ if there exists a directed path from $A$ to $C$. Formally, $A$ is an ancestor of $C$ if there exists a sequence of adjacent nodes $(A, B_1, B_2, \ldots, B_t, C)$ such that $A \rightarrow B_1 \rightarrow B_2 \rightarrow \cdots \rightarrow B_t \rightarrow C$.
2. Node $C$ is a **descendant** of node $A$ if there exists a directed path from $A$ to $C$. Formally, $C$ is a descendant of $A$ if there exists a sequence of adjacent nodes $(A, B_1, B_2, \ldots, B_t, C)$ such that $A \rightarrow B_1 \rightarrow B_2 \rightarrow \cdots \rightarrow B_t \rightarrow C$.

It follows that a node can have multiple ancestors and multiple descendants.

#### Markov Factorisation and the Local Markov Assumption

@pearl2009a p.52 asks us to imagine the following. Suppose we have a distribution $P$ defined on n discrete variables, $X_1, X_2, \dots, X_n$. By the chain rule, the joint distribution for variables $X_1, X_2, \dots, X_n$ on a graph can be decomposed into the product of $n$ conditional distributions such that we may obtain the following factorisation:

$$
\Pr(x_1, \dots, x_n) = \prod_{j=1}^n \Pr(x_j \mid x_1, \dots, x_{j-1})
$$

We translate nodes and edges on a graph into a set of conditional independences that a graph implies over statistical distributions.

According to **the local Markov assumption**, given its parents in a directed acyclic graph, a node is said to be independent of all its non-descendants. Under this assumption, we obtain what Pearl calls Bayesian network factorisation, such that:

$$
\Pr(x_j \mid x_1, \dots, x_{j-1}) = \Pr(x_j \mid \text{pa}_j)
$$

This factorisation greatly simplifies the calculation of the joint distributions encoded in the directed acyclic graph (causal or non-causal) by reducing complex factorisations of the conditional distributions in $\mathcal{P}$ to simpler conditional distributions in the set $\text{PA}_j$, represented in the structural elements of a directed acyclic graph [@lauritzen1990; @pearl1988; @pearl1995; @pearl2009a].

#### Minimality Assumption

The minimality assumption combines (a) the local Markov assumption with (b) the assumption that adjacent nodes on the graph are dependent. This is needed for causal directed acyclic graphs because the local Markov assumption permits that adjacent nodes may be independent [@neal2020introduction].

#### Causal Edges Assumption

The causal edges assumption states that every parent is a direct cause of their children. Given the minimality assumption, the causal edges assumption allows us to interpret the conditional dependence between variables on a graph based on the causal relationships encoded by the arrangement of nodes and edges [@neal2020introduction].

#### Compatibility Assumption

The compatibility assumption ensures that the joint distribution of variables aligns with the conditional independencies implied by the causal graph. This assumption requires that the probabilistic model conforms to the graph's structural assumptions. Demonstrating compatibility directly from data is challenging, as it involves verifying that all conditional independencies specified by the causal directed acyclic graph (DAG) are present in the data. Therefore, we typically assume compatibility rather than empirically proving it [@pearl2009a].

#### Faithfulness

A causal diagram is considered faithful to a given set of data if all the conditional independencies present in the data are accurately depicted in the graph. Conversely, the graph is faithful if every dependency implied by the graph's structure can be observed in the data [@hernan2024WHATIF]. Faithfulness ensures that the graphical representation of relationships between variables accords with empirical evidence [@pearl2009a].

We may distinguish between **weak faithfulness** and **strong faithfulness**:

- **Weak faithfulness** allows for the possibility that some observed independencies might occur by chance, such as chance cancellation of effects among multiple causal paths.
- **Strong faithfulness** assumes that all observed statistical relationships directly reflect the underlying causal structure, with no difference left to chance.

The faithfulness assumption, whether weak or strong, is not directly testable from observed data [@pearl2009a].

#### d-separation

In a causal diagram, a path is 'blocked' or 'd-separated' if a node along it interrupts causation. Two variables are d-separated if all paths connecting them are blocked, making them conditionally independent. Conversely, unblocked paths result in 'd-connected' variables, implying potential dependence [@pearl1995; @pearl2009a]. (Note that 'd' stands for 'directional'.)

The rules of d-separation are as follows:

1. **Fork rule** ($B \leftarrow \boxed{A} \rightarrow C$): $B$ and $C$ are independent when conditioning on $A$ ($B \coprod C \mid A$).
2. **Chain rule** ($A \rightarrow \boxed{B} \rightarrow C$): Conditioning on $B$ blocks the path between $A$ and $C$ ($A \coprod C \mid B$).
3. **Collider rule** ($A \rightarrow \boxed{C} \leftarrow B$): $A$ and $B$ are independent unless conditioning on $C$, which introduces dependence ($A \cancel{\coprod} B \mid C$).

Judea Pearl proved d-separation in the 1990s [@pearl1995; @pearl2009a].


It follows from d-separation that:

1. An open path (no variables conditioned on) is blocked only if two arrows point to the same node: $A \rightarrow C \leftarrow B$. The node of common effect (here $C$) is called a *collider*.
2. Conditioning on a collider does not block a path; thus, $A \rightarrow \boxed{C} \leftarrow B$ can lead to an association between $A$ and $B$ in the absence of causation.
3. Conditioning on a descendant of a collider does not block a path; thus, if $C \rightarrow \boxed{C'}$, then $A \rightarrow \boxed{C'} \leftarrow B$ is open.
4. If a path does not contain a collider, any variable conditioned along the path blocks it; thus, $A \rightarrow \boxed{B} \rightarrow C$ blocks the path from $A$ to $C$ [@pearl2009a; @hernan2024WHATIF, p. 78].


#### Backdoor Adjustment

From d-separation, Pearl was able to define a general identification algorithm for causal identification, called the 'backdoor adjustment theorem' [@pearl2009a].

Let us shift to the general notation that we will use in the following examples. Where $A$ denotes the treatment, $Y$ denotes the outcome, and $L$ denotes a set (or subset) of measured covariates. In a causal directed acyclic graph (causal DAG), we say that a set of variables $L$ satisfies the backdoor adjustment theorem relative to the treatment $A$ and the outcome $Y$ if $L$ blocks every path between $A$ and $Y$ that contains an arrow pointing into $A$ (a backdoor path). Formally, $L$ must satisfy two conditions:

1. No element of $L$ is a descendant of $A$.
2. $L$ blocks all backdoor paths from $A$ to $Y$.

If $L$ satisfies these conditions, the causal effect of $A$ on $Y$ can be estimated by conditioning on $\boxed{L}$ [@pearl2009a].


## Front Door Path Criterion

Pearl also proves a 'front-door adjustment' criterion, which is rarely used in practice but is worth understanding for its conceptual value. The front-door criterion is useful when we cannot estimate the causal effect of $A$ on $Y$ and there is unmeasured confounding by $U$. Suppose further, that there is a mediator, $M$, that fully mediates the effect of $A$ on $Y$. If $A\to M$ is unconfounded and $M \to Y$ is unconfounded, $A \to Y$ may be identified by estimating the separate identifiable paths through $M$. The front-door criterion is not widely used because requires measuring an appropriate mediator that fully captures the causal effect. However, understanding the front-door adjustment helps develop intuition for how estimating causal effects may be possible when there is unmeasured confounding [@pearl2009a].  

#### Pearl's Structural Causal Models


In the potential outcomes framework, we represent interventions by setting variables to specific levels, e.g., setting the treatment to a specific value $A = \tilde{a}$. We have noted that counterfactual outcomes are conceived as the outcomes that would occur if, perhaps contrary to fact, an individual's treatment was set to a specific level. We use the convention $Y_i(a)$ or equivalently $Y_i^{a}$ to denote the counterfactual or 'potential' outcome for individual $i$ when that individual's treatment is set to $A_i = a$. Because we assume individual treatments to be independent and identically distributed (i.i.d.), we drop the subscripts when describing the potential outcomes for multiple individuals under specific levels of treatment. Thus, we write $Y(a)$ or $Y^a$ as shorthand for:

$$
\mathbb{E}[Y(a)] \equiv \frac{1}{n} \sum_{i=1}^n Y_i(a)
$$

As noted above, we say that conditional exchangeability is satisfied if the potential outcomes are independent of the treatment assignment conditional on the measured covariates:

$$
A \coprod Y(\tilde{a}) \mid L
$$

It is worth considering that causal directed acyclic graphs do not directly represent counterfactual outcomes. Instead, they evaluate whether causality can be identified from hypothetical interventions on the variables represented in a graph. Formally, causal directed acyclic graphs rely on Judea Pearl's do-calculus [@pearl2009a], which relies on the concept of an 'interventional distribution'. Under Pearl's do-calculus, any node in a graph can be intervened upon. Nodes and edges in a causal diagram correspond to non-parametric structural equations or what Pearl calls 'structural causal models' [@pearl2009a]. Note that non-parametric structural equations are causal-structural models. They are fundamentally different from statistical structural equation models that are employed in many human sciences. **Please do not confuse non-parametric structural equation models with statistical structural equation models** [@vanderweele2015]. In a causal directed acyclic graph, non-parametric structural equations represent the underlying causal mechanisms without making specific parametric assumptions about the functional forms of relationships. It is important to note that non-parametric structural equations, also known as structural causal models, are mathematical representations of the causal relationships between variables in a system. These equations describe the functional relationships between variables without specifying the particular functional form or the probability distributions of the variables. In contrast, statistical structural equation models, commonly used in the social sciences and psychology, make specific assumptions about the functional form of the relationships (e.g., linear, polynomial, or exponential) and the probability distributions of the variables (e.g., normal, Poisson, or binomial). Statistical structural equation models model observed data. Non-parametric structural equations state the assumed causal structure of the system -- we do not use non-parametric structural equation models to do statistics.  When we employ statistical structural equation models or any other statistical model, we must first state the assumed functional relationships that we maintain (under expert advice) hold for the data. We must do so without making assumptions about the functional form of the statistical model we will eventually employ -- statistics come later, only after we have evaluated whether the causal effect we seek may be identified with data. Pearl's do-calculus and the rules of d-separation are based on non-parametric structural equations, which provide a flexible and generalisable framework for causal inference [@pearl2009a]. 

Pearl's structural causal models work as follows. 

Let $L$ denote the common causes of treatment $A$ and outcome $Y$:

- The node $L$ in the corresponding directed acyclic graph (DAG) $\mathcal{G}$ corresponds to the non-parametric structural equation: $L = f_L(U_L)$, where $f_L$ is an unspecified function and $U_L$ represents the exogenous error term or unmeasured factors affecting $L$.
- The treatment node $A$ in $\mathcal{G}$ is associated with the non-parametric structural equation: $A = f_A(L, U_A)$, where $f_A$ is an unspecified function, $L$ represents the common causes, and $U_A$ represents the exogenous error term or unmeasured factors affecting $A$.
- The outcome node $Y$ in $\mathcal{G}$ is associated with the non-parametric structural equation: $Y = f_Y(A, L, U_Y)$, where $f_Y$ is an unspecified function, $A$ represents the treatment, $L$ represents the common causes, and $U_Y$ represents the exogenous error term or unmeasured factors affecting $Y$.

In Pearl's formalism, we assume that $U_L$, $U_A$, and $U_Y$ are independent exogenous random variables. That is, we assume there are no direct arrows linking $A$ to $Y$ except through the common cause node $L$. Causal diagrams allow us to factorise the joint distribution of $L$, $A$, and $Y$ as a product of conditional probability distributions.

Define $O$ as a distribution of independent and identically distributed observations such that $O = (L, A, Y)$. The true distribution $P_O$ is factorized as:
$$
P_O = P_O(Y \mid A, L) P_O(A \mid L) P_O(L)
$$

Where:
- $P_O(L)$ is the marginal distribution of the covariates $L$.
- $P_O(A \mid L)$ is the conditional distribution of the treatment given the covariates.
- $P_O(Y \mid A, L)$ is the conditional distribution of the outcome given the treatment and covariates.

Pearl's do-calculus allows us to evaluate the consequences of intervening on variables represented in a causal DAG to interpret probabilistic dependencies and independencies in the conditional and marginal associations presented on a graph.

Here, we have developed counterfactual contrasts using the potential outcomes framework. The potential outcomes framework considers potential outcomes to be fixed and real (even if assigned non-deterministically). Pearl develops counterfactual contrasts using operations on structural functionals, referred to as 'do-calculus'. In Pearl's framework, we obtain counterfactual inference by assuming that the nodes in a causal directed acyclic graph correspond to a system of structural equation models, such as those we just described.

Mathematically, potential outcomes and counterfactual interventions are equivalent, such that:

$$
\overbrace{\Pr(Y(a) = y)}^{\text{Potential Outcomes Framework}} \equiv \overbrace{\Pr(Y = y \mid do(A = a))}^{\text{Do-Calculus}}
$$

where the left-hand side of the equivalence is the potential outcomes framework formalisation of a potential outcome recovered by causal consistency, and the right-hand side is given by Pearl's do-calculus, which, as just mentioned, formalises interventional distributions on nodes of a graph that correspond to structural causal models.

In practice, whether one uses Pearl's do-calculus or the potential outcomes framework to interpret causal inferences is often irrelevant to identification results. However, there are theoretically interesting debates about edge cases. For example, Pearl's structural causal models permit the identification of contrasts that cannot be falsified under any experiment [@richardson2013]. Because advocates of non-parametric structural equation models treat causality as primitive, they are less concerned with the requirement for falsification [@pearl2009a; @diaz2021nonparametric; @Diaz2023; @rudolph2024mediation]. Additionally, the potential outcomes framework allows for identification in settings where the error terms in a structural causal model are not independent [@bulbulia2024swigstime].

I have presented the potential outcomes framework because it is easier to interpret, more general, and—to my mind—clearer and more intellectually compelling (moreover, one does not need to be a verificationist to adopt it). However, for nearly every practical purpose, the do-calculus and 'po-calculus' (potential outcomes framework, refer to @shpitser2016causal) are both mathematically and practically equivalent. And remember, the nodes and edges in a causal directed acyclic graph correspond to non-parametric structural equations: these equations represent causal mechanisms *without* making specific assumptions about the functional form of the assumed causal relationships encoded in the causal DAG.  As currently employed, the statistical structural equation models (SEMs) used in human sciences often make implausible (or even incoherent) causal assumptions [@vanderweele2015; @bulbulia2024swigstime].  It is generally useful to draw a causal directed acyclic graph (causal DAG) before considering a statistical structural equation model (SEM).


### The Five Elementary Structures of Causality

::: {#tbl-fiveelementary}

```{=latex}
\terminologydirectedgraph
```
The five elementary structures of causality from which all causal directed acyclic graphs can be built.
:::


@tbl-fiveelementary presents five elementary structures of causality from which all causal directed acyclic graphs are built. These elementary structures can be assembled in different combinations to clarify the causal relationships presented in a causal directed acyclic graph.


{{< pagebreak >}}

### The Five Elementary Rules for Causal Identification

@tbl-terminologyconfounders describes five elementary rules for identifying conditional independence using directed acyclic causal diagrams.

::: {#tbl-terminologyconfounders}
```{=latex}
\terminologyelconfounders
```
Five elementary rules for causal identification.
:::

There are no shortcuts to reasoning about causality. Each causal question must be asked in the context of a specific scientific question, and each causal graph must be built under the best lights of domain expertise. However, the following five elementary rules for confounding control are implied by the theorems that underpin causal directed acyclic graphs. They may be a useful start for evaluating the prospects for causal identification across a broad range of settings.

1. **Ensure That Treatments Precede Outcomes**: This rule is a logical consequence of our assumption that causality follows the arrow of time and that a causal directed acyclic graph is faithful to this ordering. However, the assumption that treatments precede outcomes may be easily violated where investigators cannot ensure the relative timing of events from their data.

Note that this assumption does not raise concerns in settings where past outcomes may affect future treatments. Indeed, an effective strategy for confounding control in such settings is to condition on past outcomes, and where relevant, on past treatments as well. For example, if we wish to identify the causal effect of $A_1$ on $Y_2$, and repeated-measures time series data are available, it may be useful to condition such that $\boxed{A_{-1}} \to \boxed{Y_0} \to A_1 \rightarrow Y_2$. Critically, the relations of variables must be arranged sequentially without cycles.

Causal directed acyclic graphs must be acyclic. Yet most processes in nature include feedback loops. However, there is no contradiction as long as we represent these loops as sequential events. To estimate a causal effect of $Y$ on $A$, we would focus on: $\boxed{Y_{-1}} \to \boxed{A_0} \to Y_1 \rightarrow A_2$. Departing from conventions we have previously used to label treatments and outcomes, here $Y$ denotes the treatment and $A$ denotes the outcome. 

2. **Condition on Common Causes or Their Proxies**: This rule applies to settings in which the treatment $A$ and the outcome $Y$ share common causes. By conditioning on these common causes, we block the open backdoor paths that could introduce bias into our causal estimates. Controlling for these common causes (or their proxies) helps to isolate the specific effect of $A$ on $Y$. Note that we do not draw a path from $A \to Y$ in this context because it represents an interventional distribution. In a causal directed acyclic graph, conditioning does not occur on interventional distributions. We do not box $A$ and $Y$.

3. **Do Not Condition on a Mediator when Estimating Total Effects**: This rule applies to settings in which the variable $L$ is a mediator of $A \to Y$. Recall Pearl's backdoor path criterion requires that we do not condition on a descendant of the treatment. Here, conditioning on $L$ violates the backdoor path criterion, risking bias for a total causal effect estimate. We must not condition on a mediator if we are interested in total effect estimates. Note we draw the path from $A \to Y$ to underscore that this specific overconditioning threat occurs in the presence of a true treatment effect. Over-conditioning bias can operate in the absence of a true treatment effect. This is important because conditioning on a mediator might create associations without causation. In many settings, ensuring accuracy in the relative timing of events in our data will prevent the self-inflicted injury of conditioning on a common effect of the treatment.

4. **Do Not Condition on a Collider**: This rule applies to settings in which $L$ is a common effect of $A$ and $Y$. Conditioning on a collider may invoke a spurious association. Again, the backdoor path criterion requires that we do not condition on a descendant of the treatment. We would not be tempted to condition on $L$ if we knew that it was an effect of $A$. In many settings, ensuring accuracy in the relative timing of events in our data will prevent the self-inflicted injury of conditioning on a common effect of the treatment and outcome.

5. **Proxy Rule: Conditioning on a Descendant Is Akin to Conditioning on Its Parent**: This rule applies to settings where $L'$ is an effect from another variable $L$. The graph considers when $L'$ is downstream of a collider. Here again, in many settings, ensuring accuracy in the relative timing of events in our data will prevent the self-inflicted injury of conditioning on a common effect of the treatment and outcome.

### Summary Part 2

We use causal directed acyclic graphs to represent and evaluate structural sources of bias. We do not use these causal graphs to represent the entirety of the causal system in which we are interested, but rather only those features necessary to evaluate conditional exchangeability, or equivalently to evaluate d-separation. Moreover, causal directed acyclic graphs should not be confused with the structural equation models employed in the statistical structural equation modelling traditions (refer also to @rohrer2022PATH). Although Pearl's formalism is built upon 'Non-Parametric Structural Equation Models,' the term 'Structural Equation Model' can be misleading. Causal directed acyclic graphs are structural models that represent assumptions about reality, they are not statistical models. We use structural causal models to evaluate identifiability. We create causal graphs before we embark on statistical modelling. They aim to clarify how to write statistical models by elucidating which variables we must include in our statistical models and, equally important, which variables we must exclude to avoid invalidating our causal inferences. All causal graphs are grounded in our assumptions about the structures of causation. Although it is sometimes possible -- under assumptions -- to automate causal discovery [@peters2016causal] we cannot fully dispense with assumption because the causal structures of the world are underdetermined by the data [@robins1999association; @quine1981theories]. 

THe distinction between structural and statistical models is fundamental because absent clearly defined causal contrasts on well-defined treatments, well-defined outcomes, and well-defined populations, and absent carefully evaluated assumptions about structural sources of bias in the relationship between treatments and outcomes, the statistical structural equation modelling tradition offers no guarantees that the coefficients investigators recover are interpretable. Misunderstanding this difference between structural and statistical models has led to considerable confusion across the human sciences [@vanderweele2015; @vanderweele2022;@vanderweele2022a; @bulbulia2022; @bulbulia2024swigstime].


## Part 3. How Causal Directed Acyclic Graphs Clarify the Importance of Timing of Events Recorded in Data {#id-sec-3}

::: {#tbl-elementary-chronological-hyg}
```{=latex}
\terminologychronologicalhygeine
```
Causal DAGs illustrate how ensuring the relative timing of the occurrence of variables of interest addresses common forms of bias when estimating causal effects.
:::

As noted in the previous section, the five elementary rules of confounding control reveal the importance of ensuring accurate timing in the occurrence of the variables whose structural features a causal directed acyclic graph encodes. We begin by considering seven examples of confounding problems resolved when accuracy in the timing of the occurrence of variables is ensured.  These examples refer to causal graphs in @tbl-elementary-chronological-hyg. We use the symbol $\mathcal{G}$ to denote a graph. We use the convention: $\mathcal{G}_{\{\text{row}\}\{.\}\{1 = \text{problem}; 2 = \text{solution}\}}$ to indicate a causal directed acyclic graph in the table. 

### Example 1: Reverse Causation

@tbl-elementary-chronological-hyg $\mathcal{G}_{3.1}$ illustrates bias from reverse causation. Suppose we are interested in the causal effect of marriage on well-being. If we observe that married people are happier than unmarried people, we might erroneously infer that marriage causes happiness, or happiness causes marriage (refer to @mcelreath2020).

@tbl-elementary-chronological-hyg $\mathcal{G}_{3.2}$ clarifies a response. Ensure that the treatment is observed before the outcome is observed. Note further that the treatment, in this case, is not clearly specified because 'marriage' is unclear. There are at least four causal contrasts we might consider when thinking of 'marriage', namely:

1. $Y(0, 0)$: The potential outcome when there is no marriage.
2. $Y(0, 1)$: The potential outcome when there is a shift to marriage from no marriage.
3. $Y(1, 0)$: The potential outcome under divorce.
4. $Y(1, 1)$: The potential outcome from marriage prevalence.

Each of these four outcomes may be contrasted with the others, yielding six unique contrasts. Which do we wish to consider? 'What is the causal effect of marriage on happiness?' is ill-defined. This question does not uniquely state which of the six causal contrasts to consider. The first step in causal inference is to state a well-defined causal question in terms of interventions and outcomes to be compared. For a worked example refer to @bulbulia2024swigstime.

### Example 2: Confounding by Common Cause

@tbl-elementary-chronological-hyg $\mathcal{G}_{3.2}$ illustrates confounding by common cause. Suppose there is a common cause, $L$, of the treatment, $A$, and outcome, $Y$. In this setting, $L$ may create a statistical association between $A$ and $Y$, implying causation in its absence. Most human scientists will be familiar with the threat to inference in this setting: a 'third variable' leads to a statistical association between treatment and outcome absent causation.

Suppose that smoking, $L$, is a common cause of both yellow fingers, $A$, and cancer, $Y$. Here, $A$ and $Y$ may show an association without causation. If investigators were to scrub the hands of smokers, this would not affect cancer rates.

@tbl-elementary-chronological-hyg $\mathcal{G}_{3.2}$ clarifies a response. Condition on the common cause, smoking. Within strata of smokers and non-smokers, there will be no association between yellow fingers and cancer.

### Example 3: Mediator Bias

@tbl-elementary-chronological-hyg $\mathcal{G}_{3.1}$ illustrates mediator bias. Conditioning on the effect of treatment blocks the flow of information from treatment to outcome, biasing the total effect estimate.

Suppose investigators are interested in whether cultural 'beliefs in big Gods' $A$ affect social complexity $Y$. Suppose that 'economic trade', $L$, is both a common cause of the treatment and outcome. To address confounding by a common cause, we must condition on economic trade. However, timing matters. If we condition on measurements that reflect economic trade after the emergence of beliefs in big Gods, we may bias our total effect estimate.

@tbl-elementary-chronological-hyg $\mathcal{G}_{3.2}$ clarifies a response. Ensure that measurements of economic trade are obtained for cultural histories before big Gods arise. Do not condition on post-treatment instances of economic trade.

### Example 4: Collider Bias

@tbl-elementary-chronological-hyg $\mathcal{G}_{4.1}$ illustrates collider bias. Imagine a randomised experiment investigating the effects of different settings on individuals' self-rated health. In this study, participants are assigned to either civic settings (e.g., community centres) or religious settings (e.g., places of worship). The treatment of interest, $A$, is the type of setting, and the outcome, $Y$, is self-rated health. Suppose there is no effect of setting on self-rated health. However, suppose both setting and rated health independently influence a third variable: cooperativeness. Specifically, imagine religious settings encourage cooperative behaviour, and at the same time, individuals with better self-rated health are more likely to engage cooperatively. Now suppose the investigators decide to condition on cooperativeness, which in reality is the common effect of $A$ and the outcome $Y$. Their rationale might be to study the effects of setting on health among those who are more cooperative or perhaps to 'control for' cooperation in the health effects of religious settings. By introducing such 'control', the investigators would inadvertently introduce collider bias, because the control variable is a common effect of the treatment and the outcome. If both $A$ and $Y$ are positively associated with $L$, $A$ and $Y$ will be negatively associated with each other. However, such an association is a statistical artefact. Were we to intervene on $A$, $Y$ would not change. 

@tbl-elementary-chronological-hyg $\mathcal{G}_{4.2}$ clarifies a response. If the worry is that cooperativeness is a confounder, ensure that cooperativeness is measured before the initiation of exposure to religious settings.

### Example 5: Collider Proxy Bias

@tbl-elementary-chronological-hyg $\mathcal{G}_{5.1}$ illustrates bias from conditioning on the proxy of a collider. Consider again the scenario described in $\sec 3.4$, but instead of controlling for cooperativeness, investigators control for charitable donations, a proxy for cooperativeness. Here, because the control variable is a descendant of a collider, conditioning on the proxy of the collider is akin to conditioning on the collider itself.

@tbl-elementary-chronological-hyg $\mathcal{G}_{5.2}$ clarifies a response. Do not condition on charitable donations, an effect of treatment.

### Example 6: Post-Treatment Collider Stratification Bias

@tbl-elementary-chronological-hyg $\mathcal{G}_{6.1}$ illustrates post-treatment collider stratification bias. Consider again an experiment investigating the effect of religious service on self-rated health. Suppose we measure 'religiosity' after the experiment, along with other demographic data. Suppose further that religious setting affects religiosity, as does an unmeasured confounder, such as childhood deprivation. Suppose that childhood deprivation affects self-reported health. Although our experiment ensured randomisation of the treatment and thus ensured no unmeasured common causes of the treatment and outcome, conditioning on the post-treatment variable 'religiosity' opens a back-door path from the treatment to the outcome. This path is $A_0 \associationred L_1 \associationred U \associationred Y_2$. We introduced confounding into our randomised experiment.

@tbl-elementary-chronological-hyg $\mathcal{G}_{6.2}$ clarifies a response. Do not condition on a variable that the treatment may affect (refer to @cole2010 for a discussion of theoretical examples; refer to @montgomery2018 for evidence of the widespread prevalence of post-treatment adjustment in published political science experiments; refer also to @bulbulia_2024_experiments).

### Example 7: Conditioning on Past Treatments and Past Outcomes to Control for Unmeasured Confounders

@tbl-elementary-chronological-hyg $\mathcal{G}_{7.1}$ illustrates the threat of unmeasured confounding. In 'real world' studies, this threat is ubiquitous.

@tbl-elementary-chronological-hyg $\mathcal{G}_{7.2}$ clarifies a response. With at least three repeated measurements, investigators may greatly reduce unmeasured confounding by controlling for past measurements of the treatment as well as past measurements of the outcome. With such control, any unmeasured confounder must be orthogonal to its effects at baseline (refer to @vanderweele2020). Moreover, controlling for past treatments allows investigators to estimate an incident exposure effect over a prevalence exposure effect. The prevalence exposure effect describes the effect of current or ongoing exposures (treatments) on outcomes. This effect risks leading to erroneous conclusions. The incident exposure effect targets initiation into treatment, which is typically the effect we obtain from experiments. To obtain the incident exposure effect, we generally require that events in the data can be accurately classified into at least three relative time intervals (refer to @hernán2016; @danaei2012; @vanderweele2020; @bulbulia2022).

## Summary Part 3

The examples in Part 3 reveal that the ability to order treatments, outcomes, and their common causes on a timeline is necessary for obtaining valid inferences. When timing is ensured, we can use Pearl's backdoor path adjustment algorithm to evaluate identification, subject to the assumptions encoded in a causal directed acyclic graph.

{{< pagebreak >}}

## Part 4  How Causal Directed Acyclic Graphs Clarify The Insufficiency of Ensuring The Timing of Events Recorded in Data For Causal Identification {#id-sec-4} 

We next present a series of illustrations that clarify ordering variables in time is insufficient insurance against confounding biases. All graphs in [Part 4](#id-sec-4) refer to @tbl-chronology-notenough.

::: {#tbl-chronology-notenough}
```{=latex}
\terminologychronologicalhygeineNOTENOUGH
```
Common confounding scenarios in which ordering of variable timing is insufficient for causal identification.
:::



### Example 1: M-bias

@tbl-chronology-notenough $\mathcal{G}_{1.1}$ illustrates the threat of over-conditioning on pre-treatment variables -- 'M-bias'. Suppose we want to estimate the effect of religious service attendance on charitable donations. We obtain time-series data and include a rich set of covariates, including baseline measures of religious service and charity. Suppose there is no treatment effect. Suppose further that we condition on loyalty measures, yet loyalty neither affects religious service attendance nor charitable giving. However, imagine that loyalty is affected by two unmeasured confounders. Furthermore, imagine that one's childhood upbringing (an unmeasured variable) affects both loyalty and inclinations to religious service but not charitable giving. $U_A$ denotes this unmeasured confounder. Furthermore, suppose wealth affects loyalty and charitable giving but not religious service. $U_Y$ denotes this unmeasured confounder. In this setting, because loyalty is a collider of the unmeasured confounders, conditioning on loyalty opens a path between treatment and outcome. This path is $A \associationred U_A \associationred U_Y \associationred Y$.

@tbl-chronology-notenough $\mathcal{G}_{1.2}$ clarifies a response. If we are confident that $\mathcal{G}_{1.1}$ describes the structural features of confounding, we should not condition on loyalty.

### Example 2: M-bias Where the Pre-treatment Collider is a Confounder

@tbl-chronology-notenough $\mathcal{G}_{2.1}$ illustrates the threat of incorrigible confounding. Imagine the scenario in $\mathcal{G}_{1.1}$ and $\mathcal{G}_{1.2}$ but with one change. Loyalty is indeed a common cause of religious service attendance (the treatment) and charitable giving (the outcome). If we do not condition on loyalty, we have unmeasured confounding. This is bad. If we condition on loyalty, as we have just considered, we also have unmeasured confounding. This is also bad.

@tbl-chronology-notenough $\mathcal{G}_{2.2}$ clarifies a response. Suppose that although we have not measured wealth, we have measured a surrogate of wealth, say neighbourhood deprivation. Conditioning on this surrogate is akin to conditioning on the unmeasured confounder; we should adjust for neighbourhood deprivation.

### Example 3: Opportunities for Post-treatment Conditioning for Confounder Control

@tbl-chronology-notenough $\mathcal{G}_{3.1}$ illustrates the threat of unmeasured confounding. Suppose we are interested in whether curiosity affects educational attainment. The effect might be unclear. Curiosity might increase attention but it might also increase distraction. Consider an unmeasured genetic factor $U$ that influences both curiosity and educational attainment, say anxiety. Suppose we do not have early childhood measures of anxiety in our dataset. We have unmeasured confounding. This is bad.

@tbl-chronology-notenough $\mathcal{G}_{3.2}$ clarifies a response. Suppose $U$ also affects melanin production in hair follicles. If grey hair is an effect of a cause of curiosity, and if grey hair cannot be an effect of educational attainment, we could diminish unmeasured confounding by adjusting for grey hair in adulthood. This example illustrates how conditioning on a variable that occurs after the treatment has occurred, or even after the outcome has been observed, may prove useful for confounding control. When considering adjustment strategies, it is sometimes useful to consider adjustment on post-treatment confounders, however, it must be clear that the confounder is not affected by the treatment.

### Example 4: Residual Confounding After Conditioning on Past Treatments and Past Outcomes

@tbl-chronology-notenough $\mathcal{G}_{4}$ illustrates the threat of confounding even after adjusting for baseline measures of the treatment and the outcome. Imagine that childhood deprivation, an unmeasured variable, affects both religious service attendance and charitable giving. Despite adjusting for religious status and charitable giving at baseline, childhood deprivation might influence changes in one or both variables over time. This can create a longitudinal association between religious service attendance and charitable giving without a causal relationship. Strictly speaking, the causal effect cannot be identified. We may estimate an effect and perform sensitivity analyses to check how much unmeasured confounding would be required to explain way an effect (refer to @linden2020EVALUE); we may also seek negative controls (refer to @hernan2024WHATIF).

### Example 5: Intermediary Confounding in Causal Mediation

@tbl-chronology-notenough $\mathcal{G}_5$ illustrates the threat of treatment confounding in causal mediation. Imagine that the treatment is randomised; there is no treatment-outcome confounding. Nor is there treatment-mediator confounding. $\mathcal{R} \to A$ ensures that backdoor paths from the treatment to the outcome are closed. We may obtain biased results despite randomisation because the mediator is not randomised. Suppose we are interested in whether the effects of COVID-19 lockdowns on psychological distress were mediated by levels of satisfaction with the government. Suppose that assignment to COVID-19 lockdowns was random, and that time series data taken before COVID-19 provides comparable population-level contrasts. Despite random assignment to treatment, assume that there are variables that may affect both satisfaction with the government and psychological distress. For example, job security or relationship satisfaction might plausibly function as common causes of the mediator (government satisfaction) and the outcome (psychological distress). To obtain valid inference for the mediator-outcome path, we must control for these common causes.

@tbl-chronology-notenough $\mathcal{G}_5$ reveals the difficulty in decomposing the total effect of COVID-19 on psychological distress into the direct effect of COVID-19 that is not mediated by satisfaction with the government and the indirect effect that is mediated. Let us assume that confounders of the mediator-outcome path are themselves potentially affected by the treatment. In this example, imagine that COVID-19 lockdowns affect relationship satisfaction because couples are trapped in "captivity." Imagine further that COVID-19 lockdowns affect job security, which is reasonable if one owns a street-facing business. If we adjust for these intermediary variables along the path between the treatment and outcome, we will partially block the treatment-mediator path. This means that we will not be able to obtain a natural indirect effect estimate that decomposes the effect of the treatment into that part that goes through the intermediary path $A \associationred V \associationred M \associationred Y$ and that part that goes through the mediated path independently of $V$, namely $A \associationred V \associationred M \associationred Y$. However, it may be possible to estimate controlled direct effects—that is, direct effects when the mediator is fixed to different levels  [@greenland1999; @vanderweele2015;  @shpitser2022multivariate], or to obtain approximations of the natural direct effect [refer to @Diaz2023; @stensrud2023conditional; @bulbulia2024swigstime].

### Example 6: Treatment Confounder Feedback in Sequential Treatments

@tbl-chronology-notenough $\mathcal{G}_6$ illustrates the threat of treatment confounder feedback in sequential treatment regimes. Suppose we are interested in whether beliefs in big Gods affect social complexity. Suppose that beliefs in big Gods affect economic trade and that economic trade may affect beliefs in big Gods and social complexity. Suppose the historical record is fragmented such that there are unmeasured variables that affect both trade and social complexity. Even if these unmeasured variables do not affect the treatment, conditioning on the $L$ (a confounder) and sequential treatment opens a backdoor path $A \associationred L \associationred U \associationred Y$. We have confounding.

@tbl-chronology-notenough $\mathcal{G}_6$ reveals the difficulty of sequentially estimating causal effects. To estimate an effect requires special estimators under the assumption of sequential randomisation for fixed treatments and the assumption of strong sequential randomisation for time-varying treatments—that is, for treatments whose present levels depend on the levels of past treatments and and measured confounders affected by those treatments [@robins1986; @hernan2004STRUCTURAL; @vanderlaan2011; @vanderlaan2018; @haneuse2013estimation; @young2014identification; @rotnitzky2017multiply; @richardson2013; @diaz2021nonparametric; @williams2021; @hoffman2023].

Importantly, we have six potential contrasts for the two sequential treatments: beliefs in big Gods at both time points vs. beliefs in big Gods at neither time point; beliefs in big Gods first, then lost vs. never believing in big Gods at both:

| Type     | Description                             | Counterfactual Outcome |
|----------|-----------------------------------------|------------------------|
| Regime   | Always believe in big Gods              | $Y(1,1)$               |
| Regime   | Never believe in big Gods               | $Y(0,0)$               |
| Regime   | Believe once first, then scepticism     | $Y(1,0)$               |
| Regime   | Start with scepticism, then believe     | $Y(0,1)$               |
| Contrast | Always believe vs. Never believe        | $E[Y(1,1) - Y(0,0)]$   |
| Contrast | Always believe vs. Treat once first     | $E[Y(1,1) - Y(1,0)]$   |
| Contrast | Always believe vs. Treat once second    | $E[Y(1,1) - Y(0,1)]$   |
| Contrast | Never believe vs. Treat once first      | $E[Y(0,0) - Y(1,0)]$   |
| Contrast | Never believe vs. Treat once second     | $E[Y(0,0) - Y(0,1)]$   |
| Contrast | Believe once first vs. Believe once second | $E[Y(1,0) - Y(0,1)]$ |

: Table outlines four fixed treatment regimens and six causal contrasts in time-series data where treatments vary over time. {#tbl-regimens}

We can compute six causal contrasts for these four fixed regimens, as shown in @tbl-regimens.

A limitation of directed acyclic causal diagrams is that we do not project factorisations of the counterfactual contrasts onto the graphs themselves. To evaluate counterfactual identification, using Single World Intervention Graphs can be helpful[@robins2010alternative; @richardson2013swigsprimer; @richardson2023potential]. I consider intermediate confounding in more detail in @bulbulia2024swigstime.

### Example 7: Collider Stratification Bias in Sequential Treatments

@tbl-chronology-notenough $\mathcal{G}_7$ illustrates the threat of confounding bias in sequential treatments even without treatment-confounder feedback. Assume the setting is  $\mathcal{G}_6$ with two differences. First, assume that the treatment, beliefs in big Gods, does not affect trade networks. However, assume that an unmeasured confounder affects both the beliefs in big Gods and the confounder, trade networks. Such a confounder might be openness to outsiders, a feature of ancient cultures for which no clear measures are available. We need not imagine that treatment affects future states of confounders for time-varying confounding. It would be sufficient to induce bias for an unmeasured confounder to affect the treatment and the confounder, in the presence of another confounder that affects both the confounder and the outcome.

@tbl-chronology-notenough $\mathcal{G}_7$ reveals the challenges of sequentially estimating causal effects. Yet again, to estimate causal effects here requires special estimators, under the assumption of sequential randomisation for fixed treatments, and the assumption of strong sequential randomisation for time-varying treatments [@robins1986; @hernan2004STRUCTURAL; @vanderlaan2011; @vanderlaan2018; @haneuse2013estimation; @young2014identification; @rotnitzky2017multiply; @richardson2013; @diaz2021nonparametric; @williams2021; @hoffman2023]. We note again that a specific causal contrast must be stated, and we must ask, for which cultures do our causal effect estimates generalise.

Readers should be aware that merely applying currently popular tools of time-series data analysis—multi-level models and structural equation models—will not overcome the threats of confounding in sequential treatments. Applying models to data will not recover consistent causal effect estimates. Again, space constraints prevent us from discussing statistical estimands and estimation here (refer to @bulbulia2024PRACTICAL).

### Summary Part 4

Directed acyclic graphs reveal that ensuring the timing of events in one's data does not ensure identification. In some cases, certain mediated effects cannot be identified by any data, as we discussed in the context of mediation analysis with intermediate confounding. However, across the human sciences, we often apply statistical models to data and interpret the outputs as meaningful. Causal diagrams demonstrate that standard approaches, no matter how sophisticated, are often hazardous and can lead to misleading conclusions.

## Part 5. Creating Causal Diagrams: Pitfalls and Tips {#id-sec-5}

The primary interest of causal diagrams is to address **identification problems**. Pearl's backdoor adjustment theorem proves that if we adopt an adjustment set such that $A$ and $Y$ are d-separated, and furthermore do not condition on a variable along the path from $A$ to $Y$, then association is causation.

Here is how investigators may construct safe and effective directed acyclic graphs.

1. **Clarify the causal question and target population**

  An identification strategy is relative to the question at hand. The adjustment criteria for estimating an effect of  $A$  on  $Y$  will generally differ from those for estimating an effect of  $Y$  on  $A$. Before attempting to draw any causal diagram, state the problem your diagram addresses and the population to whom it applies. Additionally, when adopting a specific identification strategy for a treatment or set of treatments, the coefficients we obtain for the other variables in the model will often be biased causal effect estimates for those variables.

   Moreover, the *coefficients obtained from statistical models developed to estimate causal effects will typically not have a marginal interpretation* [@cole2008; @vanderweele2009a; @chatton2020]. This implication has wide-ranging consequences for scientific reporting. For example, if regression coefficients are reported at all, they should come with clear warnings against interpreting them as having any causal meaning or interpretation [@westreich2013; @mcelreath2020; @bulbulia2023]. Powerful machine learning algorithms treat these parameters as a nuisance, and in many cases, coefficients cannot be obtained. Referees of human science journals need to be alerted to this fact and retrained accordingly.

2. **Consider whether the three fundamental assumptions for causal inference may be satisfied**

   Merely possessing data, even if the data are richly detailed time-series data, does not mean our causal questions will find answers. Along with identification, we must also consider the causal consistency and positivity assumptions, refer to [Part 1](#id-sec-1).

3. **Clarify the meanings of symbols and conventions**

   It is fair to say that the state of terminology in causal inference is a dog's breakfast (for a glossary, refer to supplement **S1**). Meanings and conventions vary not only for terminology but also for causal graphical conventions. For example, whereas we have denoted unmeasured confounders using the variable $U$, those who follow Pearl will often draw a bi-directional arrow. Although investigators will have their preferences, there is generally little substantive interest in one's conventions, only that they are made clear, frequently repeated (as I have done repeatedly in the key for each graph table), and applied correctly.

4. **Include all common causes of the treatment and outcome**

   Once we have stated our causal question, we are ready to create a draft of our causal graph. This graph should incorporate the most recent common causes (parents) of both the treatment and the outcome, or, where measures are not available, measures for available proxies.

   Where possible, aggregate functionally similar common causes and label them with a single node. For example, all baseline confounders that are a common cause of the treatment and outcome might be labelled $L_0$.  Time-varying confounders might be labelled $L_1, L_2, \dots L_{\tau -1}$, where $Y_\tau$ is the outcome at the end of study.

   How do we determine whether a variable is a common cause of the treatment and the outcome? We might not always be in a position to know. Remember that a causal directed acyclic graph (DAG) asserts structural assumptions. Expertise in crafting causal diagrams does not guarantee expertise in encoding plausible structural assumptions! Therefore, creating and revising causal DAGs should involve topic specialists. Additionally, the decision-making processes should be thoroughly documented in published research, even if this documentation is placed in supplementary materials.

5. **Consider potential unmeasured confounders**

   We leverage domain expertise not only to identify measured sources of confounding but also—and perhaps most importantly—to identify potential unmeasured confounders. These should be included in our causal diagrams. Because we cannot guard against all unmeasured confounding, it is essential to perform sensitivity analyses and to consider developing multiple analytic strategies to provide multiple channels of evidence for the question at hand, such as instrumental variables, negative control treatments, negative control outcomes, and mendelian randomisation [@angrist2009mostly; @smith2022combining]. 

6. **Ensure the causal directed acyclic graph is acyclic and practice good chronological hygiene**

   Although not strictly necessary, it may be useful to annotate the temporal sequence of events using subscripts (e.g., $L_0$, $A_1$, $Y_2$), as we have done here. Moreover, it is a great help to your audience (and to yourself) to spatially order your directed acyclic graph to reflect the progression of causality in time—either left-to-right or top-to-bottom. What might be called 'chronological hygiene' will considerably enhance comprehensibility, and allow you to spot errors you might otherwise miss -- such as worrying about whether a post-treatment variable is a confounder -- it is not -- and we should not condition on an effect of the treatment if our interest is in a total treatment effect. Note there are other post-treatment biases to worry about, such as directed measurement error bias [@bulbulia2024wierd], however, it is perilous to fix such biases through adjustment.

7. **Represent paths structurally, not parametrically**

   Whether a path is linear is unimportant for causal identification—and remember causal diagrams are tools for causal identification. Focus on whether paths exist, not their functional form (linear, non-linear, etc.).

   Consider a subway map of Paris. We do not include all the streets on this map, all noteworthy sites, or a detailed overview of the holdings by room in the Louvre. We use other maps for these purposes.  Remember, the primary function of a causal diagram is to ensure d-separation. If a causal diagram is to be useful, it must remove almost every detail about the reality it assumes.

8. **Minimise paths to those necessary for addressing an identification problem**

   Reduce clutter; only include paths critical for a specific question (e.g., backdoor paths, mediators). For example, in @tbl-chronology-notenough $\mathcal{G} 6$ and @tbl-chronology-notenough $\mathcal{G} 7$, I did not draw arrows from the first treatment to the second treatment. Although I assume that such arrows exist, drawing them was not, in these examples, relevant to evaluating the identification problem at hand.

9. **When Temporal Order is Unknown, Explicitly Represent This Uncertainty on Your Causal Diagram**

   In many settings, the relevant timing of events cannot be ascertained with confidence. To address this, we adopt the convention of indexing nodes with uncertain timing using $X_{\phi t}$ notation. Although there is no widely adopted convention for representing uncertainty in timing, our primary obligation is to be clear.

10. **Create, Report, and Deploy Multiple Graphs**

   Causal inference hinges on assumptions, and experts might disagree. When the structure of reality encoded in a causal graph is uncertain or debated, investigators should produce multiple causal diagrams that reflect these uncertainties and debates.

   By stating different assumptions and adopting multiple modelling strategies that align with these assumptions, we might find that our causal conclusions are robust despite differences in structural assumptions. Even when different structural assumptions lead to opposing causal inferences, this knowledge can guide future data collection to resolve these differences. The primary goal of causal inference, as with all science, is to truthfully advance empirical understanding. Assertions are poor substitutes for honesty. Rather than asserting a single causal directed graph, investigators should follow the implications of several.


11. **Use Automated Identification Algorithms such as `daggity` with Care**

   Automated software can assist with identification tasks, such as factorising complex conditional independencies. However, automated software may not converge on identifying the optimal set of confounders in the presence of intractable confounding.

   Consider Tyler VanderWeele's **modified disjunctive cause criterion**. @vanderweele2019 recommends obtaining a maximally efficient adjustment, termed a 'confounder set.' A member of this set is any variable that can reduce or remove structural sources of bias. The strategy is as follows:
    
   a. Control for any variable that causes the treatment, the outcome, or both.

   b. Control for any proxy of an unmeasured variable that is a shared cause of both the treatment and outcome.

   c. Define an instrumental variable as a variable associated with the treatment but not influencing the outcome independently, except through the treatment. Exclude any instrumental variable that is not a proxy for an unmeasured confounder from the confounder set [@vanderweele2019].

   VanderWeele's modified disjunctive cause criterion is an excellent strategy for selecting an optimal confounder set. However, this set might not remove all structural sources of confounding bias in most observational settings. As such, an automated algorithm might reject it. This rejection could be unwise because, in non-randomised treatment assignments, we should often include relations of unmeasured confounding in our causal graphs. Rejecting causal inferences in observational settings entirely would be imprudent, as many examples closely approximate randomised control trials [@hernan2016; @hernan2006estimating; @hernan2008aObservationalStudiesAnalysedLike].

   For example, consider @tbl-chronology-notenough $\mathcal{G}_{2.1}$, where we encountered intractable confounding. What if there were no proxy for an unmeasured confounder? Should we condition on the measured confounder and induce M-bias, leave the backdoor path from the measured confounder open, or not attempt causal inferences at all? The answer depends on assumptions about the relative strength of confounding in the causal diagram. Rather than relying on a generic strategy, we require subject-specialist expertise, sensitivity analyses, and multiple causal identification strategies [@smith2022combining].


12. **Clarify Assumptions about Structural Bias from Measurement Error and Target Population Restriction (also known as 'Selection Bias')**

   Space constraints prevented us from examining how causal directed acyclic graphs can clarify structural biases from measurement error and restrictions of the target population in the sample population at the start and end of the study. We can (and should) examine structural features of bias in these settings. For an overview, refer to @bulbulia2024wierd.

## Conclusions {#id-sec-6} 

#### Limitations

**First, I have focused on the application of causal diagrams to confounding bias; however, there are other biases that threaten causal inference besides confounding biases.** Causal directed acyclic graphs can also be extended to evaluate measurement-error biases and some features of target population restriction bias (also called 'selection restriction bias'). Valid causal inferences require addressing all structural sources of bias. This work does not aim for complete coverage of how causal diagrams may be useful for off-label applications other than assessing d-separation, but it hopes to stimulate curiosity [@hernan2017SELECTIONWITHOUTCOLLIDER; @liu2023application; @hernan2024WHATIF; @hernan2009MEASUREMENT; @vanderweele2012MEASUREMENT; @bulbulia2024wierd].

**Second, I have not reviewed other graphical tools for identification, such as Single World Intervention Graphs**. Although causal directed acyclic graphs are powerful tools for addressing identification problems, they are not the only graphical tools researchers use to investigate causality. For example, @robins1986 developed the 'finest fully randomized causally interpreted structured tree graph (FFRCISTG),' which has been more recently revived and simplified in Single World Intervention Graphs (refer to @richardson2013swigsprimer). These graphs explicitly factorise counterfactual states, which can be helpful for identification in complex longitudinal settings. For some, representing counterfactual states on a graph is more satisfying, as it allows inspection of the conditional independence of expectations over $Y(a^*)$ and $Y(a)$ separately. Refer to @bulbulia2024swigstime for use cases.

**Third, I have not reviewed workflows downstream of causal identification**. This article does not cover statistical estimands, statistical estimation, and the interpretation and reporting of causal inferences, which come downstream of causal graphs in causal inference workflows. Rapid developments in machine learning offer applied researchers new tools for handling model misspecification [@vanderlaan2018; @van2012targeted; @diaz2021nonparametric; @williams2021; @hoffman2023] and assessing treatment effect heterogeneity [@athey2019; @athey2021; @wager2018; @vansteelandt2022a]. Those interested in workflows for causal inference in panel studies might consider @vanderweele2020. The workflows in my research group can be found here: @bulbulia2024PRACTICAL. For general approaches, I recommend: [https://tlverse.org/tmle3/, accessed 10 June 2024](https://tlverse.org/tmle3/). However, readers should be aware that workflows for statistical designs and estimation are quickly evolving.

Nevertheless, after precisely stating our causal question, the most difficult and important challenge is considering whether and how it might be identified in the data. The 'statistical models first' approach routinely applied in most human sciences is soon ending. This approach has been attractive because it is relatively easy to implement—the methods do not require extensive training—and because the application of statistical models to data appears rigorous. However, if the coefficients we recover from these methods have meaning, this is typically accidental. Without a causal framework, these coefficients are not just uninformative about what works and why; they lack any meaning [@ogburn2021].

There are many good resources available for learning causal directed acyclic graphs [@rohrer2018; @hernan2024WHATIF; @cinelli2022; @barrett2021; @mcelreath2020; @greenland1999; @suzuki2020; @pearl2009a; @major2023exploring; @greenland1999; @morgan2014]. This work aims to add to these resources, first by providing additional conceptual orientation to the frameworks and workflows of causal data science, highlighting the risks of applying causal graphs without this understanding; second, by using causal diagrams to emphasise the importance of ensuring relative timing for the variables whose causal relationships are represented on the graph; third, by employing causal diagrams to clarify the limitations of longitudinal data for certain questions in causal mediation and time-varying confounding under time-varying treatments, which remain topics of confusion in many human sciences (see @bulbulia2024swigstime for a detailed explanation). 

For those just getting started on causal diagrams, I recommend Miguel Hernan's free course here: [https://www.edx.org/learn/data-analysis/harvard-university-causal-diagrams-draw-your-assumptions-before-your-conclusions, accessed 10 June 2024](https://www.edx.org/learn/data-analysis/harvard-university-causal-diagrams-draw-your-assumptions-before-your-conclusions) 

For those seeking a slightly more technical but still accessible introduction to causal inference and causal DAGs, I recommend Brady Neal's introduction to causal inference course and textbook, both freely available here [https://www.bradyneal.com/causal-inference-course, accessed 10 June 2024](https://www.bradyneal.com/causal-inference-course).


### Neurath's Boat: On the Priority of Assumptions in Science

We might wonder, if not from the data, where do our assumptions about causality come from?  We have said that our assumptions must come from expert knowledge. Our reliance on expert knowledge might seem counterintuitive for building scientific knowledge. Shouldn't we use data to build scientific knowledge, not the other way around? Isn't scientific history a record of expert opinions being undone?

The Austrian philosopher Otto Neurath famously described scientific progress using the metaphor of a ship that must be rebuilt at sea:

> ... every statement about any happening is saturated with hypotheses of all sorts and these in the end are derived from our whole world-view. We are like sailors who on the open sea must reconstruct their ship but are never able to start afresh from the bottom. Where a beam is taken away a new one must at once be put there, and for this the rest of the ship is used as support. In this way, by using the old beams and driftwood, the ship can be shaped entirely anew, but only by gradual reconstruction. [@neurath1973, p.199]

Neurath emphasises the iterative process of accumulating scientific knowledge; new insights are formed from the foundation of existing knowledge [@godfrey2006strategy; @godfrey2009theory; @quine1981theories].

Causal diagrams are at home in Neurath's boat. We should resist the tradition of science that believes knowledge develops solely from the results of statistical tests applied to data. The data have never fully contained the answers we seek. When reconstructing knowledge, we have always relied on assumptions. Causal graphs enable us to make these assumptions explicit and to understand what we obtain based on them.


{{< pagebreak >}}

## Acknowledgements

I am grateful to Dr. Inkuk Kim for checking previous versions of this manuscript and offering feedback, to two anonymous reviewers and the editors, Charles Efferson and Ruth Mace, for their constructive feedback. 

## Conflict of Interest

The author declares no conflicts of interest


## Financial Support

This work is supported by a grant from the Templeton Religion Trust (TRT0418) and RSNZ Marsden 3721245, 20-UOA-123; RSNZ 19-UOO-090. I also received support from the Max Planck Institute for the Science of Human History. The Funders had no role in preparing the manuscript or deciding to publish it.

## Research Transparency and Reproducibility

No data were used in this manuscript



{{< pagebreak >}}


## References

::: {#refs}
:::



