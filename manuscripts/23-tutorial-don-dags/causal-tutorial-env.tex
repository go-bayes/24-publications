% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames,x11names}{xcolor}
%
\documentclass[
  singlecolumn]{article}

\usepackage{amsmath,amssymb}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage[]{libertinus}
\ifPDFTeX\else  
    % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\usepackage[top=30mm,left=20mm,heightrounded]{geometry}
\setlength{\emergencystretch}{3em} % prevent overfull lines
\setcounter{secnumdepth}{-\maxdimen} % remove section numbering
% Make \paragraph and \subparagraph free-standing
\ifx\paragraph\undefined\else
  \let\oldparagraph\paragraph
  \renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
  \let\oldsubparagraph\subparagraph
  \renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi

\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{241,243,245}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.40,0.45,0.13}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\BuiltInTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\ExtensionTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.28,0.35,0.67}{#1}}
\newcommand{\ImportTok}[1]{\textcolor[rgb]{0.00,0.46,0.62}{#1}}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\NormalTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\RegionMarkerTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.07,0.07,0.07}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}

\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
% definitions for citeproc citations
\NewDocumentCommand\citeproctext{}{}
\NewDocumentCommand\citeproc{mm}{%
  \begingroup\def\citeproctext{#2}\cite{#1}\endgroup}
\makeatletter
 % allow citations to break across lines
 \let\@cite@ofmt\@firstofone
 % avoid brackets around text for \cite:
 \def\@biblabel#1{}
 \def\@cite#1#2{{#1\if@tempswa , #2\fi}}
\makeatother
\newlength{\cslhangindent}
\setlength{\cslhangindent}{1.5em}
\newlength{\csllabelwidth}
\setlength{\csllabelwidth}{3em}
\newenvironment{CSLReferences}[2] % #1 hanging-indent, #2 entry-spacing
 {\begin{list}{}{%
  \setlength{\itemindent}{0pt}
  \setlength{\leftmargin}{0pt}
  \setlength{\parsep}{0pt}
  % turn on hanging indent if param 1 is 1
  \ifodd #1
   \setlength{\leftmargin}{\cslhangindent}
   \setlength{\itemindent}{-1\cslhangindent}
  \fi
  % set entry spacing
  \setlength{\itemsep}{#2\baselineskip}}}
 {\end{list}}
\usepackage{calc}
\newcommand{\CSLBlock}[1]{\hfill\break\parbox[t]{\linewidth}{\strut\ignorespaces#1\strut}}
\newcommand{\CSLLeftMargin}[1]{\parbox[t]{\csllabelwidth}{\strut#1\strut}}
\newcommand{\CSLRightInline}[1]{\parbox[t]{\linewidth - \csllabelwidth}{\strut#1\strut}}
\newcommand{\CSLIndent}[1]{\hspace{\cslhangindent}#1}

\usepackage{booktabs}
\usepackage{longtable}
\usepackage{array}
\usepackage{multirow}
\usepackage{wrapfig}
\usepackage{float}
\usepackage{colortbl}
\usepackage{pdflscape}
\usepackage{tabu}
\usepackage{threeparttable}
\usepackage{threeparttablex}
\usepackage[normalem]{ulem}
\usepackage{makecell}
\usepackage{xcolor}
\input{/Users/joseph/GIT/templates/latex/custom-commands.tex}
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\AtBeginDocument{%
\ifdefined\contentsname
  \renewcommand*\contentsname{Table of contents}
\else
  \newcommand\contentsname{Table of contents}
\fi
\ifdefined\listfigurename
  \renewcommand*\listfigurename{List of Figures}
\else
  \newcommand\listfigurename{List of Figures}
\fi
\ifdefined\listtablename
  \renewcommand*\listtablename{List of Tables}
\else
  \newcommand\listtablename{List of Tables}
\fi
\ifdefined\figurename
  \renewcommand*\figurename{Figure}
\else
  \newcommand\figurename{Figure}
\fi
\ifdefined\tablename
  \renewcommand*\tablename{Table}
\else
  \newcommand\tablename{Table}
\fi
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Listing}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}
\makeatother
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\@ifpackageloaded{subcaption}{}{\usepackage{subcaption}}
\makeatother
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same} % disable monospaced font for URLs
\hypersetup{
  pdftitle={Causal Inference in Environmental Psychology},
  pdfkeywords={DAGS, Causal
Inference, Confounding, Environmental, Psychology, Panel},
  colorlinks=true,
  linkcolor={blue},
  filecolor={Maroon},
  citecolor={Blue},
  urlcolor={Blue},
  pdfcreator={LaTeX via pandoc}}

\title{Causal Inference in Environmental Psychology}
\author{Joseph A. Bulbulia \and Donald W Hine}
\date{2024-02-15}

\begin{document}
\maketitle
\begin{abstract}
This chapter offers an introduction to causal inference within
environmental psychology, emphasising the utility of causal diagrams
(Directed Acyclic Graphs -- or DAGs) for addressing causal questions. It
begins by outlining foundational concepts of causal inference and
clarifying assumptions necessary for estimating causal effects.
Subsequent sections describe how to construct causal diagrams tailored
to identifying causal effects in observational research settings. The
chapter is structured into three parts: an introduction to principles of
causal inference, a guide on crafting causal diagrams, and a discussion
of their practical applications. It aims to be useful to researchers who
hope to address causal questions from observational data.
\end{abstract}

\subsection{Introduction}\label{introduction}

Causal inference attempts to quantify the magnitude of causal relations
from data. In the simplest case, we evaluate whether an intervention or
``treatment'' on one variable leads, on average, to subsequent changes
in another variable, known as the ``outcome'', and if so, by how much.
The objective of causal inference is not only to determine whether
cause-effect relationships are present but also to estimate their
magnitude using data (\citeproc{ref-cook2002experimental}{Cook \emph{et
al.} 2002}).

It is often thought that the capacity for evaluating causal
relationships is a complex cognitive process unique to humans. However,
capacities for causal understanding span the plant and animal kingdoms
among organisms that vary in levels of conscious awareness and
complexity (\citeproc{ref-mancuso2018revolutionary}{Mancuso 2018}). For
example, plants and microorganisms have genetic predispositions to
recognise and react to environmental conditions critical for their
reproduction and survival. Birds and rodents demonstrate the ability to
learn, albeit not as complexly as larger mammals and primates. Over
millennia, humans have developed diverse frameworks for causal
understanding, ranging from the law of karma in Hindu, Jain, and
Buddhist traditions to Aristotle's four causes, early empirical and
experimental approaches by Francis Bacon and John Locke, Newtonian
mechanics, and others.

Although the capacity to infer cause and effect is common to many
lineages and has evolved to high levels of sophistication in human
cultures, methods for quantitatively estimating magnitudes of causal
effect from data are much less common. Arguably, such capacities emerged
only recently with the development of randomised controlled experiments.
Specialist methods for quantifying magnitudes of causal influence from
``real-world'' data are even more recent. Although such methods are
routinely taught in modern epidemiology (\citeproc{ref-lash2020}{Lash
\emph{et al.} 2020}) and economics
(\citeproc{ref-angrist2009mostly}{Angrist and Pischke 2009}), many
social scientists do not routinely employ methods for observational
causal inference (\citeproc{ref-morgan2014}{Morgan and Winship 2014}).
Thus, in our more specialised sense of ``causal inference,'' that of
quantifying magnitudes of effect, causal inference is far from
universal. This chapter aims to familiarise environmental psychologists
with a fundamental understanding of these newly developed specialist
methods.

We believe there is considerable value in understanding these more
recently developed methods for causal inference in observational
settings and for routinely employing them. Introductory undergraduate
psychology courses teach that correlations between variables do not
necessarily indicate causation. The question arises: can valid causal
inferences ever be drawn from observational data, and if so, how?
Although randomised experiments are often considered to be the ``gold
standard'' for generating causal inferencess, they can be costly and
ethically challenging. Moreover, even the use of randomised controlled
experiments do not always ensure valid causal insights due to factors
such as treatment noncompliance, differential attrition patterns across
treatment conditions and sampling bias
(\citeproc{ref-hernan2017per}{Hernán \emph{et al.} 2017};
\citeproc{ref-montgomery2018}{Montgomery \emph{et al.} 2018}).

Observational data, being more plentiful, offers potentially valuable
resources for accelerating knowledge in environmental psychology,
provided causal insights can be gleaned from them. However, many
researchers continue to attempt to draw causal inferences from such data
with statistical models that are not fit for purpose. Indeed routinely
employed practices that attempt to ``control for'' the confounding of
cause and effect, such as covariance analysis, can exacerbate bias
(\citeproc{ref-hernan2023}{Hernan and Robins 2023};
\citeproc{ref-robins1986}{Robins 1986};
\citeproc{ref-westreich2013}{Westreich and Greenland 2013}). Thus, on
one side, social scientists assess correlations and state that
correlations are not causation. This is unsatisfactory because we
typically want to understand causation. On the other hand, we continue
to draw hesitant causal conclusions from observed correlations using
hedging language (\citeproc{ref-bulbulia2022}{Bulbulia 2022}). However,
lacking appropriate methods, we typically lack any entitlement to such
hedging language. Indeed widespread practices that attempt to
``control'' point in the opposite direction of truth. In short, the
social sciences that have yet to incorporate advances in causal
inference are arguably perpetuating what might be called a ``causality
crisis'' (\citeproc{ref-bulbulia2023a}{Bulbulia \emph{et al.} 2023}).

The advances in causal inference during the past two decades, which
emerged in the disciplines of biostatistics and computer science, offer
hope for progress in across all the social sciences including psychology
(\citeproc{ref-vanderweele2015}{VanderWeele 2015}). Incorporating
methods for causal inference into environmental psychology research
offers particularly strong potential for scientific progress because
many of the questions that environmental psychologists hope to answer
cannot be approached through randomised controlled experiments. This
chapter aims to lay a foundation of understanding that will inspire
environmental psychologists to develop the knowledge and skills to help
us obtain the more reliable and robust causal understanding that nearly
all of us seek.

\subsection{Overview}\label{overview}

\textbf{Part 1} introduces the potential outcomes framework for causal
inference (\citeproc{ref-hernan2023}{Hernan and Robins 2023}). This
section describes the three core assumptions underpinning causal
inference, grounding these concepts within the familiar context of
randomised experiments. Our experience is that building intuitions about
causal inference from an understanding of experimental designs helps to
demystify the assumptions central to causal analysis and provides clear
guidelines for data collection in observational studies. It also reveals
that experiments obtain causal inferences by assumptions. Understanding
how these assumptions work is not only useful for improving standards of
experimental design and inference (\citeproc{ref-hernan2017per}{Hernán
\emph{et al.} 2017}; \citeproc{ref-robins2008estimation}{Robins and
Hernan 2008}; \citeproc{ref-westreich2012berkson}{Westreich 2012};
\citeproc{ref-westreich2015}{Westreich \emph{et al.} 2015}), but also
for improving design and analysis standards for observational studies.

\textbf{Part 2} introduces Directed Acyclic Graphs (DAGs) -- or causal
diagrams -- as powerful tools for examining causal assumptions
(\citeproc{ref-pearl2009a}{Pearl 2009b}). Although an exhaustive review
of causal diagrams' capabilities is beyond our overview, we present
fundamental strategies for constructing and interpreting these diagrams,
which we hope will prove valuable for addressing many questions within
environmental psychology. We shall see that the elementary building
blocks of causal diagrams can be derived from four basic graphical
structures.

\textbf{Part 3} uses causal diagrams to investigate strategies for
causal identification in seven scenarios, revealing the power of causal
diagrams to clarify strategies for data collection and modelling. Some
of the strategies that causal diagrams suggest accord with intuition.
However, some do not. By drawing our assumptions and evaluating the
implications of these structures according to four basic rules, we
discover that new prospects for observational data collection and
analysis may be disclosed. Causal diagrams take us to places that
intuition does not. Although they are grounded in a rigorous system of
mathematical proofs, employing them requires no mathematical
calculations. This is a great advantage because it makes causal
inference widely accessible to a broad bandwidth of scientists and
practitioners.

\textbf{Part 4} provides a `how to' guide for employing Directed Acyclic
Graphs in two distinct use cases: (1) cross-sectional designs; (2)
repeated-measures longitudinal designs.

\subsection{Part 1: An Overview of the Potential Outcomes Framework for
Causal
Inference}\label{part-1-an-overview-of-the-potential-outcomes-framework-for-causal-inference}

The potential outcomes framework for causal inference originated in the
work of Jerzy Neyman for the purpose of evaluating the effectiveness of
agricultural experiments (\citeproc{ref-neyman1923}{Neyman 1923}). It
was later extended by Harvard statistician Donald Rubin, who
demonstrated the framework may also facilitate causal inferences in
non-experimental settings (\citeproc{ref-rubin1976}{Rubin 1976}). Jamie
Robins further generalised this framework to assess confounding in
complex scenarios involving multiple treatments and time-varying factors
(\citeproc{ref-robins1986}{Robins 1986}). A core concept within this
framework is that of a ``counterfactual contrast'' or ``estimand.'' To
quantitatively assess the magnitude of causality requires contrasting
how the world would have turned under two or more states, corresponding
to different levels of intervention or treatment or sequence of
treatments whose effects researchers hope to measure. Notably, before
any intervention, these states remain counterfactual. This is perhaps
obvious. We have yet to intervene. However, after any intervention, for
every instance of a treatment that is applied, at most only one of the
two states of the world to be contrasted is realised. The other state
remains counterfactual. Philosophers who have puzzled over the nature of
causation have long realised that causality is never directly observed
(\citeproc{ref-hume1902}{Hume 1902}). In a sense, we may think of causal
inference as a form of counterfactual data science
(\citeproc{ref-bulbulia2023a}{Bulbulia \emph{et al.} 2023};
\citeproc{ref-edwards2015}{Edwards \emph{et al.} 2015})

To understand the significance of counterfactual contrasts in causal
inference, consider a situation where you are at a crossroads in life,
facing a significant decision. You are soon graduating from university
and have been accepted into your ideal graduate programme in
Environmental Psychology at the University of Canterbury; you are making
preparations for a relocation to Christchurch, New Zealand.
Concurrently, you receive a compelling job offer from Acme Nuclear
Fuels, a pioneer in renewable energy solutions. Each option would lead
you on a distinct path, affecting your daily life, income, social
circles, romantic relationships, and perhaps ultimately your sense of
life's purpose. Which decision will lead to a better overall life for
you?

In causal inference, it can be useful to clarify conceptually
challenging concepts using mathematical notation. As mentioned, we will
see that the practical application of causal diagrams does not rely on
mathematical notation or calculations. If you find the mathematics
unappealing, you may proceed directly up the elevator of causal
inference via the elevator of causal diagrams. However, for some, it can
be helpful to express ideas symbolically using elementary mathematics,
to climb a ladder slowly, to understand the basis of how causal
inference works at each step, and perhaps only then to discard the
ladder of mathematical concepts that built this understanding.

Formally, let \(D\) denote the decision, where \(D = 1\) denotes the
decision to attend graduate school and \(D=0\) denotes the decision to
embark on a career in industry. We denote the two potential outcomes -
the counterfactual states of the world under different levels of
decision using the notation \(Y_{\text{you}}(1)\) and
\(Y_{\text{you}}(0)\). Again, these outcomes describe the hypothetical
scenarios resulting from each decision. We assume there is a
fact-of-matter. Were you to choose, your life would turn out one way
perhaps similarly, perhaps differently, to the other way. Conceptually,
to quantitatively attach a \emph{magnitude} to the causal effect of your
choice, we must measure the difference
\(Y_{\text{you}}(1) - Y_{\text{you}}(0)\). However, this difference is
fundamentally unobservable. Once you make a decision, the alternative
scenario remains unknowable. We can illustrate this as follows:

\[
(Y_{\text{you}}|D_{\text{you}} = 1) = Y_{\text{you}}(1) \quad \text{implies} \quad Y_{\text{you}}(0)|D_{\text{you}} = 1~ \text{is counterfactual}.
\]

Similarly, the equation applies in reverse when the choice is \(A = 0\):

\[
(Y_{\text{you}}|D_{\text{you}} = 0) = Y_{\text{you}}(0) \quad \text{implies} \quad Y_{\text{you}}(1)|D_{\text{you}} = 0~ \text{is counterfactual}.
\]

Of course, you regularly make principled decisions about your life based
on past experiences, instincts, and knowledge. Nevertheless, the
\emph{data} that you require to quantitatively compare life outcomes
under one decision as opposed to the other is not available. Life, as it
would have unfolded under the option you do not select, remains
counterfactual -- it cannot be directly measured. A quantitative causal
contrast here is not a matter of factual data science. This example,
although contrived, perhaps resonates with similar crossroads you have
encountered in your life. The dilemmas that you faced at these
crossroads underscore what is known as ``The fundamental problem of
causal inference'' (\citeproc{ref-holland1986}{Holland 1986};
\citeproc{ref-rubin2005}{Rubin 2005}): for any individual case, we
cannot observe the potential outcomes that we require to quantify the
magnitude of an individual causal contrast.

The fundamental problem of causal inference never goes away. However, by
collecting, organising, and aggregating data under certain assumptions,
we can obtain valid quantitative causal contrasts from data to estimate
\emph{average treatment effects}. To clarify these assumptions, we next
consider how experiments attach magnitudes to missing counterfactual
outcomes to obtain average treatment effects.

\subsubsection{Understanding Relationships of Cause and Effect Through
Intervention
Outcomes}\label{understanding-relationships-of-cause-and-effect-through-intervention-outcomes}

Let us transition to an example of relevance to environmental
psychology, estimating the average causal effect of easy access to urban
green spaces on psychological well-being, with a focus on subjective
happiness, hereafter referred to as ``happiness.'' We assume this
outcome is measurable and represent it with the letter \(Y\).

For simplicity, we classify the intervention ``ample access to green
space'' as a binary variable. Define \(A = 1\) as ``having ample access
to green space'' and \(A = 0\) as ``lacking ample access to green
space.'' We assume these conditions are mutually exclusive. This
simplification does not limit the generality of our conclusions; the
points we make about experiments apply to continuous treatments as well.
It is crucial in causal inference to specify the population for whom we
seek to evaluate causal effects, or the ``target population.'' In this
case, our target population is residents of New Zealand in the 2020s.

A preliminary causal question -- defined as a causal contrast or
``estimand'' might therefore be:

``In New Zealand, does proximity to abundant green spaces increase
self-perceived happiness compared to environments lacking such spaces?''

For the sake of argument, suppose that it would be unethical to
experimentally randomise individuals into different green-space access
conditions, but we choose to overlook this ethical consideration. Assume
we could assign people randomly to high and low green space access
without objection or harm.

The first point to note in the context of causal inference, as alluded
to earlier, is that even well-designed experiments confront the
challenge of missing values in the potential outcomes. Once an
individual is assigned to one treatment condition, we cannot observe
that individual's outcome for the condition not assigned. The
fundamental problem of causal inference remains constant: for each
individual, we can only observe one of the potential outcomes at any
given time. Breaking down the Average Treatment Effect (ATE) into
observed and unobserved outcomes yields the following equation:

\[
\text{Average Treatment Effect} = \left(\underbrace{\underbrace{\mathbb{E}[Y(1)|A = 1]}_{\text{observed}} + \underbrace{\mathbb{E}[Y(1)|A = 0]}_{\text{unobserved}}}_{\text{treated}}\right) - \left(\underbrace{\underbrace{\mathbb{E}[Y(0)|A = 0]}_{\text{observed}} + \underbrace{\mathbb{E}[Y(0)|A = 1]}_{\text{unobserved}}}_{\text{untreated}}\right).
\]

In this expression, \(\mathbb{E}[Y(1)|A = 1]\) represents the average
outcome when the treatment is given, which is observable. However,
\(\mathbb{E}[Y(1)|A = 0]\) represents the average outcome if the
treatment had been given to those who were actually untreated, which
remains unobservable. Similarly, the quantity \(\mathbb{E}[Y(0)|A = 1]\)
also remains unobservable.

It is hopefully evident from this brief application of the potential
outcomes framework to experiments that the fundamental problem of causal
inference is an ever-present concern even in experiments. For each
participant, it is impossible to determine the outcome they would have
experienced under an alternative treatment condition, just as you cannot
quantitatively describe the life you would have led had you chosen the
job at Acme Nuclear Fuels instead of attending the University of
Canterbury.

\subsubsection{Causal Inference in
Experiments}\label{causal-inference-in-experiments}

How do experiments manage to estimate average treatment effects despite
the inherent challenges? The solution involves addressing the concept of
``confounding.'' Consider the concept of ``confounding by common
cause.'' This occurs when one or more variables causally affect both the
intervention under study (the ``treatment'' or ``exposure'') and the
outcome of interest, leading to a non-causal association between the
treatment and outcome. By ``non-causal,'' we mean that were we to
intervene in the treatment but not the confounder, the outcome would not
change. The common cause creates a misleading or exaggerated
relationship that may be mistakenly interpreted as causal. For instance,
when assessing the impact of access to green space on happiness, it is
possible that the association could be entirely explained by income. If
so, then an observed association between access to green space and
happiness would be entirely misleading. Were we to relocate low-income
individuals to high-access green areas (hopefully not against their
will), we might not affect subjective happiness at all. Thus, accurately
identifying and adjusting for confounding by common cause is crucial for
determining the true causal relationship between two variables, ensuring
that the observed association is not merely a result of extraneous
influences.

\paragraph{Balance of Confounders Removes
Confounding}\label{balance-of-confounders-removes-confounding}

In experimental designs, random assignment of treatment effectively
eliminates any systematic relationship between treatment conditions and
the distribution of confounders that can affect both the treatment and
the outcome. Simply put, randomisation creates a balance in confounders
across treatment groups. While factors that might influence the outcome
do not vanish, the balance ensured by successful randomisation allows us
to disregard the phenomenon of \emph{confounding}. This is because the
distribution of confounders will be, on average, identical across the
treatment groups, supporting the assumption that any systematic
difference in average outcomes between these groups owes to the
treatment itself.

We denote the set of all possible confounders by the letter \(L\). In
causal inference, the absence of confounding can be articulated in one
of two equivalent ways. Where the symbol \(\coprod\) denotes
independence:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  The potential outcomes, given the treatment and confounders, are
  conditionally independent: \(Y(a) \coprod A \mid L\).
\item
  The treatment assignment, given the potential outcomes and
  confounders, is conditionally independent: \(A \coprod Y(a) \mid L\).
\end{enumerate}

To some, the mathematical formalism, which involves the symbol
\(\coprod\) to denote the statistical independence of the term to either
side of this symbol, might seem complex. However, this compact notion
for describing statistical independence will be helpful later when we
explore \emph{how} causal diagrams enable us to confidently develop
causal inference strategies for observational data that aim to emulate
experimental randomisation. If our treatment \(A\) is not independent of
the potential outcome \(Y(a)\) after controlling for confounders \(L\),
that is, if \(A\cancel\coprod Y(a)|L\), we cannot obtain an unbiased
estimate for the causal effect of treatment on outcome. Moreover if
\(A\coprod Y(a)\) yet \(A\cancel \coprod Y(a)|L\), then we can only
obtain consistent causal estimation by \emph{avoiding} controlling for
\(L\). The compact mathematical notation allows us to appreciate why
causal diagrams work -- how they specifically rely on the assumptions
about the relationships of measured and unmeasured variables to the
treatment and outcome. Again, the application of causal diagrams to
scientific problems does not rely on mastery of probability theory. If
you find the notation unfamiliar and confusing you may safely ignore it
because your causal diagrams encode these concepts in a purely spatial
representation.

However, it is essential to understand that the heart of every strategy
for causal inference -- whether experimental or non-experimental -- is
the attempt to achieve a balance across treatment conditions in the
variables that might influence the outcome. ``Under the hood'' such
balance implies statistical independence of the treatment and the
potential outcomes, conditional on covariates \(L\). Randomisation
ensures \(L\) is balanced on the treatments, allowing us to infer that
successful randomisation ensures \(A \coprod Y(a)\).

Thus, the first key principle for causal inference embodied in
randomised experiments may be summarised as follows:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Conditional Exchangeability}: There is statistical
  independence between potential outcomes and treatment assignment,
  given all measured confounders, ensuring observed group differences in
  the effects of the treatment are attributable to the treatment itself.
  Note that in a randomised controled experiment, we have
  \emph{unconditional exchangability}. Randomisation ensures balance.
\end{enumerate}

\paragraph{Control Ensures Consistent Treatments are Administered Across
All
Confounders}\label{control-ensures-consistent-treatments-are-administered-across-all-confounders}

Two further principles allow researchers to infer average treatment
effects without observing individual treatment effects. These principles
arise from \emph{control}, not randomisation:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  \textbf{Causal Consistency}: This principle allows us to assume that
  the observed outcomes in experimental conditions correspond to the
  potential outcomes under the received treatment. To develop intuition
  for this, we turn to simple mathematical notation:
\end{enumerate}

For any individual \(i\), the observed outcome \(Y_i\) given their
treatment status \(A_i = 1\) is equal to their potential outcome under
treatment, denoted as \(Y_i(1)\). Similarly, when the treatment status
for the same individual \(i\) is \(A_i = 0\), the observed outcome
\(Y_i\) is equal to the potential outcome under no treatment, denoted as
\(Y_i(0)\). More compactly:

\[
\begin{aligned}
Y_{i}(1) &= (Y_{i}|A_{i} = 1) \quad \text{(Potential outcome observed if treated)} \\
Y_{i}(0) &= (Y_{i}|A_{i} = 0) \quad \text{(Potential outcome observed if untreated)}
\end{aligned}
\]

When the exchangeability assumption is satisfied, we can extend our
analysis from comparisons of treatment effects observed in the data to
comparisons of the average treatment effects as if the entire population
had been treated or had been untreated:

\[
\begin{aligned}
\text{Average Treatment Effect (ATE)} &= \mathbb{E}[Y(1)] - \mathbb{E}[Y(0)] \\
&= \mathbb{E}(Y|A=1) - \mathbb{E}(Y|A=0)
\end{aligned}
\]

Here, \(\mathbb{E}[Y(1)]\) and \(\mathbb{E}[Y(0)]\) denote the expected
outcomes if the entire population were treated or untreated,
respectively.

Thus, by the assumptions of causal consistency and conditional
exchangeability, randomised controlled experiments recover average
treatment effects as contrasts in the average of the outcomes in
treatment groups (or equivalently as the average of the differences of
the treatment outcome averages -- the two quantities are mathematically
equivalent.)

Note that the causal consistency assumption is generally satisfied by
the \emph{control} that experimentalists exert over randomised
controlled experiments. In an experiment, treatment groups receive
equivalent treatments. This is almost self-evident. However, as we will
see, in real-world data, the assumption of consistent treatments is much
harder to satisfy. As it turns out, however, one may consistently
estimate causal effects if the many versions of the treatment are
conditionally independent of the potential outcomes under these
treatments.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  \textbf{Positivity}: There is one more assumption required to obtain
  valid causal contrasts from data. We must assume that there is a
  non-zero probability of receiving each treatment level within
  covariate-defined subgroups.
\end{enumerate}

\[
P(A = a | L= l) > 0
\]

This assumption is also met by the \emph{control} that experimentalists
exert over randomised controlled experiments and is rarely stated
explicitly. However, in observational settings, this condition must be
verified to avoid extrapolating results beyond observed data.

\subsubsection{Why Satisfying the Fundamental Assumptions of Causal
Inference in Observational Settings is
Difficult}\label{why-satisfying-the-fundamental-assumptions-of-causal-inference-in-observational-settings-is-difficult}

In observational research, researchers do not control the treatment
allocation. The goal is to replicate a controlled experimental
environment as closely as possible. However, obtaining balance in
confounders across the treatments to be compared introduces challenges.

\subparagraph{The Conditional Exchangeability Assumption in
Observational
Settings}\label{the-conditional-exchangeability-assumption-in-observational-settings}

Achieving conditional exchangeability is challenging in observational
studies. This condition requires the groups being compared to be similar
in every aspect except for the treatment. Consider the example of the
effect of living near green spaces on subjective happiness. In
real-world data, individuals with access to green spaces may differ from
those without access in several ways:

\textbf{Socioeconomic status}: there might be a link between an
individual's economic background and their proximity to green spaces.
More affluent individuals could afford housing in areas with better
access to high-quality green spaces.

\textbf{Age demographics}: different age groups may have different
preferences regarding green spaces. Younger individuals or families with
children might prioritise access to parks, unlike other demographics.

\textbf{Mental health}: individuals with existing mental health issues
might seek out green spaces for therapeutic benefits or avoid them
because of social anxiety or other factors.

\textbf{Lifestyle choices}: individuals who prefer outdoor activities
might choose to live near green spaces. It is difficult to ascertain
whether proximity to green spaces directly causes improved well-being or
if it is merely a characteristic of individuals who already lead
healthier lifestyles.

\textbf{Personal values and social connections}: the decision to live
near green spaces might also be influenced by personal values, such as
environmentalism, or the desire to be part of a community that values
these spaces. Such values and connections can affect how individuals
interact with and benefit from living in proximity to green spaces.

These and other unmeasured factors can introduce biases, complicating
the interpretation of causal relationships in observational studies.

\subparagraph{The Causal Consistency Assumption and Treatment
Heterogeneity}\label{the-causal-consistency-assumption-and-treatment-heterogeneity}

Again we consider our interest in quantifying the causal effect of
living near green spaces. The definition of ``proximity to green
spaces'' itself varies significantly, leading to a diverse range of
experiences classified under the same ``treatment.'' Again, we set aside
the issue that proximity is a continuous variable and that the distance
to the nearest green space influences how often and easily individuals
can access these areas, affecting the impact of this ``treatment.''
However, we should note that causal inference always requires
contrasting conditions. When assessing causal contrasts we must specify
the points to be compared on a continuous scale. Arguably, such
comparisons will require artificiality and extrapolation. Focussing on
the variability of the green spaces themselves this includes:

\textbf{Diversity of green spaces}: the biodiversity and aesthetic value
of these spaces can vary widely. Some might have access to
well-maintained parks, while others to basic recreational areas with
limited natural appeal.

\textbf{Availability of amenities}: amenities such as walking paths,
benches, and recreational facilities can enhance the experience of green
spaces, encouraging more frequent and prolonged visits.

\textbf{Size and type of green space}: the type (e.g., urban park,
community garden) and size of the green space might affect the
psychological and physical benefits it offers. Thus, as we convert
observed outcomes under these heterogenous conditions, it might be
unclear which interventions it is that we are comparing.

\subparagraph{The Positivity Assumption in Observational
Settings}\label{the-positivity-assumption-in-observational-settings}

Positivity, the requirement that every individual has a chance of
receiving each level of treatment being compared, can be challenging to
ensure in observational studies. In some urban areas, it might be
practically impossible for certain demographics to have access to green
spaces owing to factors like housing prices or availability, limiting
treatment exposure variability within certain strata and complicating
valid causal inference.

In observational studies, understanding the context and carefully using
statistical methods helps us get close to the conditions of a randomised
experiment. The closer we get, the more we can trust our causal
conclusions. Yet, it is important to admit that sometimes our data will
not give us much confidence. By comparing our work to the ideal of a
randomised experiment, environmental psychologists can more clearly see
and explain what observational data can and cannot tell us about cause
and effect, especially for questions that practical or ethical
considerations are not amenable to experimental investigation.

We will set the question of positivity to the side. However, interested
readers may consult (\citeproc{ref-hernan2023}{Hernan and Robins 2023};
\citeproc{ref-westreich2010}{Westreich and Cole 2010}). The important
topic of causal consistency is briefly considered in Appendix B, see
also: (\citeproc{ref-hernan2023}{Hernan and Robins 2023};
\citeproc{ref-vanderweele2009}{VanderWeele 2009};
\citeproc{ref-vanderweele2013}{VanderWeele and Hernan 2013}).

The primary application of causal diagrams is to evaluate the
conditional exchangeability assumption of ``no unmeasured confounding.''
The next section considers how causal diagrams powerfully assist with
evaluating the demands of this assumption, guide data collection, and
allow researchers to develop statistical modelling strategies that
consistently estimate average magnitudes of causal effects within levels
of the interventions they seek to contrast.

\subsection{Part 2. Causal Diagrams}\label{part-2.-causal-diagrams}

This section introduces causal diagrams, starting with essential
terminology. Understanding this terminology is not straightforward for
newcomers, but it is necessary for applying causal diagrams effectively.
After explaining the terms, we will examine practical examples. We will
see that various forms of confounding arise from four key causal
structures. Recognising these structures is crucial for applying them in
practical scenarios. \textbf{Appendix A} provides a comprehensive
glossary.

Causal diagrams use specific symbols to represent elements important in
causal inference, as established in seminal works
(\citeproc{ref-greenland1999}{Greenland \emph{et al.} 1999};
\citeproc{ref-pearl1995}{Pearl 1995}, \citeproc{ref-pearl2009}{2009a}).
The symbols and their meanings are listed in Table Table~\ref{tbl-01}:

\begin{itemize}
\tightlist
\item
  \textbf{\(A\)} is the treatment or exposure variable, the intervention
  or condition whose effect on an outcome is under investigation.
  \textbf{This symbol represents the cause}.
\item
  \textbf{\(Y\)} is the outcome variable, the effect or result that is
  being studied. \textbf{This symbol represents the effect}.
\item
  \textbf{\(L\)} includes all measured confounders, variables that may
  affect both the treatment and the outcome.
\item
  \textbf{\(U\)} includes unmeasured confounders, variables not included
  in the analysis that could influence both the treatment and the
  outcome, potentially leading to biased conclusions.
\item
  \textbf{\(M\)} is a mediator variable, a factor through which the
  treatment affects the outcome. The focus here is on identifying the
  total effect of treatment \(A\) on an outcome \(Y\), but it is also
  important to understand how controlling for mediators can affect
  estimates of this total effect.
\end{itemize}

\begin{table}

\caption{\label{tbl-01}Terminology that is used in this article for
causal diagrams. (This table is adapted from
(\citeproc{ref-bulbulia2023}{Bulbulia 2023}))}

\centering{

\terminologylocalconventionssimple

}

\end{table}%

\begin{table}

\caption{\label{tbl-02}Basic conventions for causal diagrams. (This
table is adapted from (\citeproc{ref-bulbulia2023}{Bulbulia 2023}))}

\centering{

\terminologygeneralbasic

}

\end{table}%

\subsubsection{Elements of Causal
Diagrams}\label{elements-of-causal-diagrams}

Having established the meanings of our symbols, we now turn to the
fundamental components of causal diagrams themselves. Table~\ref{tbl-02}
describes the basic elements of causal diagrams themselves. Key features
are:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Nodes}: representations of variables or events within a causal
  system. Each node stands for a distinct element that can influence or
  be influenced within the system.
\item
  \textbf{Edges}: signify the relationships between the variables
  represented by nodes. In causal diagrams, edges are directed that
  define pathways of causal influence. Importantly, causal diagrams are
  non-parametric; thus, the representation of a relationship does not
  change with its nature---be it linear or non-linear, an arrow is used
  in both cases.
\item
  \textbf{Parent and child relationships}: a variable is termed a
  ``child'' if it receives an arrow from another variable, which is then
  referred to as its ``parent.'' This terminology helps to describe the
  direction of causal influence within the diagram.
\item
  \textbf{Acyclic}: causal diagrams must not contain cycles; that is,
  they cannot have feedback loops where a variable can be both a cause
  and effect of itself, directly or indirectly. This requirement ensures
  clarity in the direction of causal influence. In scenarios involving
  repeated measurements, nodes should be indexed by time to maintain
  acyclicity.
\item
  \textbf{Conditioning}: a central aspect of causal analysis is
  determining how controlling for certain variables affects the unbiased
  estimation of the relationship between the treatment and the outcome.
  This process of ``conditioning'' or ``adjustment'' is visually
  represented by enclosing the variable in a box within the diagram.
\end{enumerate}

\paragraph{The Rules of D-separation}\label{the-rules-of-d-separation}

Pearl proved how rules of d-separation may allow us to evaluate
relationships between nodes in a causal diagram
(\citeproc{ref-pearl1995}{Pearl 1995}).

When we write \(X_1 \cancel\coprod X_2\), it means the probability
distributions of \(X_1\) and \(X_2\) are intertwined. In simpler terms,
what happens with \(X_1\) gives us clues about what is happening with
\(X_2\), indicating a connection or dependence between them.

Conversely, \(X_1 \coprod X_2\) means \(X_1\) and \(X_1\) means the
probability distributions of \(X_1\) and \(X_2\) are independent. Their
probability distributions do not mix; knowing about \(X_1\) does not
help guess anything about \(X_2\).

We call a path ``blocked'' or ``d-separated'' if a node along it
prevents the transmission of influence. Two variables are deemed
d-separated, denoted as \(X_1 \coprod X_2\), if every pathway connecting
them is obstructed, indicating no influence or statistical association
passes through. Conversely, if at least one path remains unblocked,
allowing for the transmission of influence, the variables are considered
d-connected, denoted as \(X_1 \cancel\coprod X_2\)
(\citeproc{ref-pearl1995}{Pearl 1995}).

Next, we define three basic rules that link causal relationships to
statistical associations. All confounding results from these
rules.\footnote{Recall that to ensure balance in confounders across
  treatments, we must ensure that \(A\coprod Y(a)|L\) -- that the
  potential outcomes are independent within levels of the treatments to
  be compared, conditional on measured covariates \(L\). We do not need
  to ensure that the treatment has no effect such that
  \(A\coprod Y |L\).}

\subparagraph{Two variables with no arrows: absence of
causatoin}\label{two-variables-with-no-arrows-absence-of-causatoin}

\[\xorxA\] If \(X_0\) and \(X_1\) have no arrows between them, we assert
there is no causal effect. Without a causal link, they are independent,
denoted mathematically as \(X_0 \coprod X_1\). This means knowing one
variable tells you nothing about the other.

\paragraph{Fundamental causal association: two variables with a causal
arrow}\label{fundamental-causal-association-two-variables-with-a-causal-arrow}

\[\xtoxA\]

Adding a causal arrow from \(X_0 \to X_1\) indicates \(X_0\) causes a
change in \(X_1\), or that \(X_1\) ``listens to'' \(X_0\). Causality
creates a \emph{statistical} dependency, denoted mathematically as
\(X_0 \cancel\coprod X_1\). This means knowing \(X_0\) gives information
about \(X_1\)

\paragraph{Three Variables: Fork, Chain, and Collider
Structures}\label{three-variables-fork-chain-and-collider-structures}

There are only three fundamental ways in which we may add another causal
relationship to the basic two-node structure: the fork structure, the
chain structure, and the collider structure. Each structure has a
corresponding rule for how the probability distributions of the three
nodes become related or unrelated when we condition on them.

\subparagraph{The Fork Structure}\label{the-fork-structure}

\[\fork\]

The fork structure (\(X_0 \rightarrow X_1\), \(X_0 \rightarrow X_2\))
shows \(X_0\) causes both \(X_1\) and \(X_2\). Given \(X_0\), \(X_1\)
and \(X_2\) are independent, written as \(X_1 \coprod X_2 | X_0\). This
means knowing \(X_0\) removes any link between \(X_1\) and \(X_2\).
Thus, if \(X_1\) and \(X_2\) are statistically related after
conditioning on their common cause this statistical association reflects
causation. Here, association is causation.

\textbf{Fork Rule}: *if interested in the causal effect of
\(X_1 \to X_2\), condition on \(\boxed{X_1}\)**.

\subparagraph{The Chain Structure}\label{the-chain-structure}

\[\chain\]

The chain structure (\(X_0 \rightarrow X_1 \rightarrow X_2\)) describes
a setting in which \(X_0\) affects \(X_1\) and \(X_1\) affects \(X_2\).
In this structure, \(X_0\) and \(X_2\) are conditionally independent
given \(X_1\). That is, if \(\boxed{X_1}\), then
\(X_0 \coprod X_2 | X_1\)). Hence, conditioning on \(X_1\) \emph{blocks}
the association of \(X_0\) and \(X_2\). This implies that if there is a
causal association between \(X_0\) and \(X_2\) we might not detect it
after conditioning on \(\boxed{X_1}\).

Chain Rule: \textbf{if interested in the (total) causal effect of
\(X_0 \to X_2\), do not condition on \(X_1\)}.

\subparagraph{The Collider Structure}\label{the-collider-structure}

\[\immorality\]

The collider structure describes a setting in which both \(X_0\to X_2\)
and \(X_1 \to X_2\). In a collider structure, \(X_0\) and \(X_1\) are
independent. However, conditioning on \(X_2\) -- the common descendant
of \(X_2\), \(X_3\) -- introduces a stastisical association between
\(X_0\) and \(X_1\), such that \(X_0 \cancel\coprod X_1 | X_2\). That is
conditioning on the common effect (or its descendant) creates a pathway
through which \(X_0\) and \(X_1\) share information about each other.
Again, this statistical association between \(X_0\) and \(X_1\) can
arise in the absence of any causal association.

Collider Rule: \textbf{if interested in the causal effect of
\(X_0 \to X_1\), do not condition on \(X_2\)}.

\subparagraph{Extensions: all confounding bias results from combinations
of basic causal associations and the three elementary structures (forks,
chains,
colliders)}\label{extensions-all-confounding-bias-results-from-combinations-of-basic-causal-associations-and-the-three-elementary-structures-forks-chains-colliders}

For example, consider the implications of conditioning on the descendant
of a collider:

\[\immoralityChild\]

Because every descendant is statistically associated with its parent
(basic causality), a descendant acts as a proxy for its parent. In this
scenario, conditioning on \(X_3\) (the descendant of \(X_2\), which is a
collider) inadvertently opens a path between \(X_0\) and \(X_1\) that
would otherwise be blocked by the unconditioned collider \(X_2\). This
occurs because \(X_3\) inherits all statistical associations of \(X_2\),
here, its statistical associations with \(X_0\) and \(X_1\).
Consequently, when we condition on \(X_3\), we indirectly condition on
\(X_2\), thereby introducing a statistical association between \(X_0\)
and \(X_1\) through \(X_3\).

This principle underscores the nuanced nature of causal inference,
demonstrating how conditioning on certain variables, here a descendant
of colliders, can inadvertently create statistical dependencies such
that association is \emph{not} causation. The implication: do not
condition on a descendent of a collider.

For those new to the causal diagrams, the fundamental structures of
causation presented in Pearl's rules of d-separation might seem
abstract. Similarly, individuals not versed in probability theory might
find the notation of conditional dependencies and independencies
challenging. Yet, with the groundwork of rules and conventions laid,
applying causal diagrams to real-world problems is need not be
intimidating. In fact, based on our experience, many find constructing
causal diagrams to be enjoyable conversation starters. Once we know how
conditioning affects statistical associations within causal structures,
we can work backwards through statistics to make causal inferences. Put
another way, causal diagrams reveal how and when association is
causation. This is good news for psychological questions that are
inaccessible to experiments. However, we must temper our enthusiasm. We
must remember that \textbf{all structure relationships in a graph except
the focal treatment/outcome relationship must be assumed.} This because
most observational datasets are typically compatible with many causal
diagrams. Statistically ``significant'' associations do not in
themselves reveal anything about causation. Causal diagrams are powerful
aids because they help investigators to understand what follows from
their assumptions the data. However, assumptions about the structural
relationships in the data cannot be avoided. Because every path except
the \(A\to Y\) path is assumed, causal diagrams should be created in
collaboration with area experts. Where there is expert disagreement,
multiple causal diagrams should be proposed to reflect the implications
of disagreements for causal inference.

\subsubsection{How to Create Causal
Diagrams}\label{how-to-create-causal-diagrams}

\paragraph{Causal diagrams must address a clearly stated identification
problem}\label{causal-diagrams-must-address-a-clearly-stated-identification-problem}

The \emph{identification problem} is the task of determining if the
effect of a treatment (\(A\)) on an outcome (\(Y\)) can be accurately
inferred from statistical accociations in the data. Causal diagrams,
which incorporate \emph{structural assumptions}, serve as tools for
assessing the potential to identify causal effects from the patterns
observed in data (\citeproc{ref-hernuxe1n2004a}{Hernán \emph{et al.}
2004}). These assumptions, fundamental to causal inference, are
generally not empirically testable through data alone. The
identification challenge encompasses two principal considerations:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{In the absence of a causality}, when all known confounders are
  controlled for, is there statistical independence between the
  treatment and outcome? This involves identifying all potential
  confounders, implementing strategies to adjust for these confounders,
  and then checking if there are any indirect connections, known as
  \emph{backdoor paths}, between \(A\) and \(Y\). A \emph{backdoor path}
  refers to a sequence of links in the causal diagram that could
  introduce spurious associations between \(A\) and \(Y\) if not
  properly adjusted for. (Below we work through seven examples). The
  goal here is to evaluate whether \(A\) and \(Y\) are d-separated,
  indicating that any observed association between \(A\) and \(Y\) does
  not stem from these indirect paths.
\item
  \textbf{With an existing causal link}, once all known confounders have
  been adjusted for, does a direct and unbiased relationship between the
  treatment and outcome remain observable? This verifies whether the
  statistical relationship between \(A\) and \(Y\), as analysed under
  the specified model, can be interpreted as reflecting a true causal
  effect, mindful of potential \emph{over-conditioning biases}.
  \emph{Over-conditioning bias} occurs when the adjustment process
  inadvertently introduces or magnifies associations that misrepresent
  the actual causal relationship.
\end{enumerate}

The process breaks down into two tasks:

\begin{itemize}
\item
  \textbf{First task}: verify that \(A\) and \(Y\) are not linked after
  adjusting for known common causes. This step involves ensuring that no
  \emph{backdoor paths} -- indirect paths that could falsely suggest a
  relationship between \(A\) and \(Y\) -- remain open after an
  adjustment strategy is applied.
\item
  \textbf{Second task}: confirm that an assocation between \(A\) and
  \(Y\) remains unbiased after the conditioning strategy from the first
  task. This step is crucial for validating that the observed
  statistical association accurately reflects the causal relationship,
  free from distortions caused by the adjustment strategy.
\end{itemize}

\paragraph{Steps in creating causal diagrams that aaddress
identification
problems}\label{steps-in-creating-causal-diagrams-that-aaddress-identification-problems}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Include all common causes of the exposure and outcome}
\end{enumerate}

There are two main types of common causes: measured and unmeasured.
Functionally similar common causes should be grouped under a single
variable, where possible. For instance, instead of listing every
demographic variable, consider using a single label, such as \(L_0\).

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  \textbf{Include all ancestors of measured confounders linked with the
  treatment, the outcome, or both}
\end{enumerate}

Include in your causal diagram any measured variables that are ancestors
of unmeasured confounders if this variable is associated with the
exposure or outcome or both. This inclusion facilitates identifying
strategies where conditioning on a proxy of an unmeasured cause reduces
bias from the unmeasured common cause and helps prevent M-bias, a form
of over-conditioning bias described below. Again, group functionally
similar variables under a common label to simplify the diagram and
highlight critical variables affecting the relationship between \(A_1\)
and \(Y_0\).

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  \textbf{Explicitly state assumptions about the relative timing of
  events}
\end{enumerate}

Denote the assumed timing of events with time subscripts (e.g., \(L_0\),
\(A_1\), \(Y_2\)), where subscripts indicate their relative order in
time. This clarifies acyclicity and highlights the temporal nature of
causal relationships we assume to hold in our data.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{3}
\tightlist
\item
  \textbf{Arrange the temporal order of causality visually}
\end{enumerate}

Organising the assumed temporal order of causality from left to right or
top to bottom aids in understanding the causal assertions within the
diagram (\citeproc{ref-bulbulia2023}{Bulbulia 2023}). As we shall see in
Part 3, ensuring appropriate temporal measurement of variables can
effectively address common identification problems.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{4}
\tightlist
\item
  \textbf{Box variables that are conditioned on}
\end{enumerate}

Generally, the exposure and outcome are not boxed unless measured with
error, as they do not act as confounders.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{5}
\tightlist
\item
  \textbf{Use conventions to clarify sources of bias and clearly state
  these conventions}
\end{enumerate}

The creation of a causal diagram lacks a universal approach; therefore
we must be clear about our conventions. For instance, drawing open back
door paths in red, as we do here, may help highlight and identify bias
sources. However, not everyone has colour vision, and indeed not
everyone has vision. Clarity demands verbally describing the contents of
our causal diagrams.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{6}
\tightlist
\item
  \textbf{Represent paths structurally, not parametrically}
\end{enumerate}

Avoid any attempt to graphically describe non-linear relationships
between nodes. The purpose of a causal diagram is to evaluate
confounding, regardless of the linearity of the paths. Attempting to
denote non-linearities can clutter the diagram with assumptions that are
not relevant to the task at hand, evaluating sources of bias.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{7}
\tightlist
\item
  \textbf{Minimise paths to those necessary for the identification
  problem}
\end{enumerate}

Avoid unnecessary clutter in causal diagrams. Include only paths
essential for addressing a stated identification problem, such as
evaluating open back door paths or mediator bias.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{8}
\tightlist
\item
  \textbf{Clarify the research question evaluated by the causal diagram}
\end{enumerate}

State the specific question your causal diagram addresses. For example,
in causal mediation analysis, distinguishing between the indirect and
direct effects of \(A\) on \(Y\) requires specific paths to be drawn and
conditioned on. A strategy that would induce confounding for one
question might not be relevant to for a different question.

Next, we shall consider how applying the the rules of forks, chains, and
colliders, causal diagrams offer a powerful tool for researchers to
obtain consistent causal estimates tailored to their specific questions.

\subsection{Part 3. Applying causal diagrams to causal inference: worked
examples}\label{part-3.-applying-causal-diagrams-to-causal-inference-worked-examples}

\begin{table}

\caption{\label{tbl-04}Worked examples: This table is adapted from
(\citeproc{ref-bulbulia2023}{Bulbulia 2023}).}

\centering{

\terminologyelconfoundersLONG

}

\end{table}%

\subsubsection{1. The problem of confounding by a common
cause}\label{the-problem-of-confounding-by-a-common-cause}

Table~\ref{tbl-04} 1 describes the problem of confounding by common
cause and its solution. We encountered this problem in Part 1. Such
confounding arises when there is a variable or set of variables, denoted
by \(L\), that influence both the exposure, denoted by \(A\), and the
outcome, denoted by \(Y.\) Because \(L\) is a common cause of both \(A\)
and \(Y\), \(L\) may create a statistical association between \(A\) and
\(Y\) that does not reflect a causal association.

For instance, in the context of green spaces, consider that people who
choose to live closer to green spaces (exposure \(A\)) and their
experience of improved happiness (outcome \(Y\)). A common cause might
be socioeconomic status \(L\). Individuals with higher socioeconomic
status might have the financial capacity to afford housing near green
spaces and simultaneously afford better healthcare and lifestyle
choices, contributing to greater happiness. Thus, although the data may
show a statistical association between living closer to green spaces
\(A\) and greater happiness \(Y\), this association might not reflect a
direct causal relationship owing to confounding by socioeconomic status
\(L\).

How might we obtain balance in this confounder for the treatments to be
compared? Addressing confounding by a common cause involves adjusting
for the confounder in one's statistical model. This may be done through
regression, or more complicated methods, such as inverse probability of
treatment weighting, marginal structural models, and others see Hernán
and Monge (\citeproc{ref-hernuxe1n2023}{2023}). Such adjustment
effectively closes the backdoor path from the exposure to the outcome.
Equivalently, conditioning on \(L\) d-separates \(A\) and \(Y\).

Table~\ref{tbl-04} Row 1, Column 3, emphasises that a confounder by
common cause must precede both the exposure and the outcome. While it is
often clear that a confounder precedes the exposure (e.g., a person's
country of birth), in other cases, the timing might be uncertain. By
positioning the confounder before the exposure in our causal diagrams,
we assert its temporal precedence. However, when relying on
cross-sectional data, such a timing assumption might be strong. In such
cases, exploring causal scenarios where the confounder follows the
treatment or outcome can be insightful. Causal diagrams are instrumental
in examining possible timings and their implications for causal
inference.

Next, we examine the effects of conditioning on a variable that is an
effect of the treatment.

\subsubsection{2. Mediator Bias}\label{mediator-bias}

Consider again the question of whether proximity to green spaces, \(A\),
affects happiness, \(Y\). Suppose that physical activity is a mediator,
\(L\).

To fill out the example, imagine that living close to green spaces \(A\)
influences physical activity \(L\), which subsequently affects happiness
\(Y\). Notice that if we were to condition on physical activity \(L\),
assuming it to be a confounder, we would then bias our estimates of the
total effect of proximity to green spaces \(A\) on happiness \(Y\). Such
a bias arises as a consequence of the chain rule. Conditioning on \(L\)
``d-separates'' the total effect of \(A\) on \(Y\). This phenomenon is
known as mediator bias. Notably, Montgomery \emph{et al.}
(\citeproc{ref-montgomery2018}{2018}) finds dozens of examples of
mediator bias in \emph{experiments} in which control is made for
variables that occur after the treatment. For example, the practice of
obtaining demographic and other information from participants
\emph{after} a study is an invitation to mediator bias. If the treatment
affects these variables, and the variables affect the outcome (as we
assume by controlling for them), then researchers may induce mediator
bias.

To avoid mediator bias when estimating a total causal effect we should,
of course, avoid conditioning on a mediator! The surest way to avoid
this problem is to ensure that \(L\) occurs before the treatment \(A\)
as well as before the outcome \(Y\). This solution is presented
Table~\ref{tbl-04} Row 2 Col 3.

\subsubsection{3. Confounding by Collider Stratification (Conditioning
on a Common
Effect)}\label{confounding-by-collider-stratification-conditioning-on-a-common-effect}

Conditioning on a common effect, also known as collider stratification,
occurs when a variable, denoted by \(L\), is influenced by both the
exposure, denoted by \(A\), and the outcome, denoted by \(Y\).

Imagine, again the context of an access to green space question, that an
individual's choice to live closer to green spaces (exposure \(A\)) and
their happiness (outcome \(Y\)) both affect the individual's overall
sense of physical health (common effect \(L\)). Initially, \(A\) and
\(Y\) could be independent, that is \(A \coprod Y(a)\), suggesting that
the decision to live near green spaces is not directly a cause of
happiness.

However, if we were to condition on physical health \(L\) (the common
effect of \(A\) and \(Y\)), a backdoor path between \(A\) and \(Y\)
would be opened. That is, by ``controlling for'' physical health we
might \emph{induce} a non-causal association between proximity to green
spaces and happiness.

The reason that conditioning on physical health \(L\) leads to
confounding is that this variable provides information about both the
proximity to green spaces \(A\) and one's happiness \(Y\). Once we learn
something about one's physical health, for example, that it is poor, it
becomes more probable that a person is happy if they have low access to
green spaces (assuming the relationship between green space access and
happiness is positive.)

Causal diagrams point a way to respond to the problem of collider
stratification bias: we should generally ensure that:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  All confounders \(L\) that are common causes of the exposure \(A\) and
  the outcome \(Y\) are measured before \(A\) has occurred, and
\item
  \(A\) is measured before \(Y\) has occurred.
\end{enumerate}

If such temporal order is preserved, \(L\) cannot be an effect of \(A\),
and thus neither of \(Y\).

\subsubsection{4. Confounding by Conditioning on a Descendant of a
Confounder}\label{confounding-by-conditioning-on-a-descendant-of-a-confounder}

The rules of d-separation also apply to conditioning on descendants of a
confounder. As shown in Table~\ref{tbl-04} Row 4, when conditioning on a
measured descendant of an unmeasured collider we may unwittingly evoke
confounding by proxy. For example, if doctor visits were encoded in our
data and doctor visits were an effect of poor health, then conditioning
on doctor visits would function in a similar way to conditioning on poor
health, introducing collider confounding.

There are only four elementary forms of confounding. Any confounding
scenario we might imagine can be developed from these elementary forms.
We next consider how we may combine these elementary causal
relationships in causal diagrams to develop effective strategies for
confounding control.

\subsubsection{5. M-bias: Conditioning on Pre-Exposure
Collider}\label{m-bias-conditioning-on-pre-exposure-collider}

Table~\ref{tbl-04} Row 5 presents a form of pre-exposure
over-conditioning confounding known as ``M-bias''. This bias combines
the collider structure and the fork structure revealing what might not
otherwise be obvious: it is possible to induce confounding even if we
ensure that all variables have been measured \textbf{before} the
treatment. The collider structure is evident in the path \(U_Y \to L_0\)
and \(U_A \to L_0\). We know from the collider rule that conditioning on
\(L_0\) opens a path between \(U_Y\) and \(U_A\). What is the result? We
find that \(U_Y\) is associated with the outcome \(Y\) and \(U_A\) is
associated with treatment \(A\). This is a fork (common cause)
structure. The association between treatment and outcome that is opened
by conditioning on \(L\) arises from an open back-door path that occurs
from the collider structure. We thus have confounding. How might such
confounding play out in a real-world setting?

In the context of green spaces, consider the scenario where an
individual's level of physical activity \(L\) is influenced by an
unmeasured factor related to their propensity to live near green spaces
\(A\) -- say childhood upbringing. Suppose further that another
unmeasured factor -- say a genetic factor -- increases both physical
activity \(L\) and happiness \(Y\). Here, physical activity \(L\) does
not affect the decision to live near green spaces \(A\) or happiness
\(Y\) but is a descendent of unmeasured variables that do. If we were to
condition on physical activity \(L\) in this scenario, we would create
the bias just described -- ``M-bias.''

How shall we respond to this problem? The solution is straightforward.
If \(L\) is neither a common cause of \(A\) and \(Y\) nor the effect of
a shared common cause, then \(L\) should not be included in a causal
model. In terms of the conditional exchangeability principle, we find
\(A \coprod Y(a)\) yet \(A \cancel{\coprod} Y(a)| L\). So we should not
condition on \(L\): do not control for exercise
(\citeproc{ref-cole2010}{Cole \emph{et al.} 2010}).\footnote{Note that
  when we draw a chronologically ordered path from left to right the M
  shape for which ``M-bias'' takes its name changes to an E shape We
  shall avoid proliferating jargon and retain the term ``M bias.''}

\subsubsection{6. Conditioning on a Descendent May Sometimes Reduce
Confounding}\label{conditioning-on-a-descendent-may-sometimes-reduce-confounding}

Consider how we may use the rules of d-separation to obtain unexpected
strategies for confounding control. In Table~\ref{tbl-04} Row 6, we
encounter a causal diagram in which an unmeasured confounder opens a
back-door path that links the treatment and outcome. We have what
appears to be intractable confounding. Return to our green space
example. Suppose an unmeasured genetic factor \(U\) affects one's desire
to seek out isolation in green spaces \(A\) and also independently
affects one's happiness \(Y\). Were such an unmeasured confounder to
exist, we could not obtain an unbiased estimate for the causal effect of
green space access on happiness. However, imagine a variable
\(L^\prime\) that is a trait that is expressed later in life, which
arises from this genetic factor. If such a trait could be measured, even
though the trait \(L'\) is expressed after the treatment and outcome
have occurred, controlling for \(L'\) would enable investigators to
close the backdoor path between the treatment and the outcome. The
reason this strategy works is that a measured effect is a \emph{proxy}
for its cause \(U\), the unmeasured confounder. By conditioning on the
late-adulthood trait, \(L'\), we partially condition on its cause,
\(U\), the confounder of \(A \to Y\). Thus, not all effective
confounding control strategies need to rely on measuring pre-exposure
variables.

\subsubsection{7. Confounding Control with Three Waves of Data is
Powerful and Reveals Possibilities for Estimating an ``Incident
Exposure''
Effect}\label{confounding-control-with-three-waves-of-data-is-powerful-and-reveals-possibilities-for-estimating-an-incident-exposure-effect}

Table~\ref{tbl-04} row 7 presents another setting in which there is
unmeasured confounding. In response to this problem, we use the rules of
d-separation to develop a strategy for data collection and modelling
that may greatly reduce the influence of unmeasured confounding.
Table~\ref{tbl-04} row 7 col 3, by collecting data for both the
treatment and the outcome at baseline and controlling for baseline
values of the treatment and outcome, any unmeasured association between
the treatment \(A_1\) and the outcome \(Y_2\) would need to be
\emph{independent} of their baseline measurements. As such, including
the baseline treatment and outcome, along with other measured covariates
that might be measured descendants of unmeasured confounders, is a
strategy that exerts considerable confounding control
(\citeproc{ref-vanderweele2020}{VanderWeele \emph{et al.} 2020}).

Furthermore, the graph makes evident a second benefit of this strategy.
Consider that an ordinary regression would estimate what is called a
``prevalence exposure effect.'' The prevalence exposure effect evaluates
the association between the exposure or treatment status at time \(t1\)
and the outcome observed at a later time \(t2\). It is defined by the
pathway \(A_{1} \to Y_{2}\). It is expressed as:

\[
\text{Prevalence exposure effect:} \quad A_{1} \to Y_{2}
\]

Note that a prevalence exposure effect estimate does not consider the
initial status of the exposure. As such, a prevalence exposure effect
estimate describes the effect of current or ongoing exposures on
outcomes. This is often \emph{not} the effect of theoretical interest.

For example, imagine that living far from green spaces makes people so
depressed that they never respond to surveys. When we observe the
association between green space and happiness in the data, we are left
with only those people who are so incurably happy that even living far
from green space cannot bring them down. As such , it might appear in
our data that \(A_{1} \to Y_{2}\) is helpful when, in fact, the
treatment is initially harmful; see: Hernán \emph{et al.}
(\citeproc{ref-hernuxe1n2016}{2016}); Danaei \emph{et al.}
(\citeproc{ref-danaei2012}{2012}); VanderWeele \emph{et al.}
(\citeproc{ref-vanderweele2020}{2020}); Bulbulia
(\citeproc{ref-bulbulia2022}{2022}).

By contrast, adjusting for the baseline exposure and outcome enables us
to recover an incident exposure effect. The incident exposure effect
evaluates the causal association between the exposure or treatment
status at time \(t1\) and the outcome observed at a later time \(t2\)
conditional on the baseline exposure: \(A_{0} \to A_{1} \to Y_{2}\).

By including the baseline exposure, then, we consider the
\emph{transition} in treatment or exposure status from \(A_0\) to
\(A_1\). The initiation of a treatment provides a clearer intervention
from which to estimate a causal effect at \(Y_2\), and we more closely
emulate an experiment. It is expressed:

\[
\text{Incident exposure effect:} \quad \boxed{A_{0}} \to A_{1} \to Y_{2}
\]

Returning to our example, a model that controls for baseline exposure
would require that people initiate a change from the level of \(A_0\)
observed baseline. Put differently, by controlling for the baseline
value of the treatment, we may learn about the causal effect of shifting
one's access to green space status. The incident exposure effect better
emulates a ``target trial'' or the the organisation of observational
data into a hypothetical experiment in which there is a ``time-zero''
initiation of treatment in the data; see Hernán \emph{et al.}
(\citeproc{ref-hernuxe1n2016}{2016}); Danaei \emph{et al.}
(\citeproc{ref-danaei2012}{2012}); VanderWeele \emph{et al.}
(\citeproc{ref-vanderweele2020}{2020}); Bulbulia
(\citeproc{ref-bulbulia2022}{2022}).

Finally, we obtain still further control for unmeasured confounding by
including, in addition to the baseline exposure \(A_0\), the baseline
outcome, \(Y_0\), such that:

\[
\boxed{
\begin{aligned}
L_{0} \\
A_{0} \\
Y_{0}
\end{aligned}
}
\to A_{1} \to Y_{2}
\]

In this example, we discover that causal diagrams may novel insights
both in data collection and data modelling. To obtain the incident
exposure effect, we generally require that events in the data can be
accurately classified into at least three relative time intervals and we
must model the treatments and outcomes as separate elements in our
statistical model.

\subsection{Part 4. Putting Causal Diagrams to Use in Cross-Sectional
and Longitudinal
Designs}\label{part-4.-putting-causal-diagrams-to-use-in-cross-sectional-and-longitudinal-designs}

\subsubsection{Cross-sectional designs}\label{cross-sectional-designs}

In environmental psychology, researchers often grapple with whether
causal inferences can be drawn from cross-sectional data, especially
when longitudinal data are not available. The challenge is not unique to
cross-sectional designs; even longitudinal studies require careful
assumption-management. We next discuss how causal diagrams can guide
inference in both data types, with examples relevant to environmental
psychologists.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Assumption mmnagement}: the crux of causal inference,
  regardless of data type, is the management of assumptions. Although
  cross-sectional analyses typically demand stronger assumptions owing
  to the snapshot nature of data, these assumptions, when transparently
  articulated, do not always bar causal analysis. For instance, if
  investigating the effect of green spaces on mental health,
  longitudinal data might allow discovery of effects owing to changes in
  access to green spaces, however cross-sectional data might still offer
  insights if we control for known confounders such as age,
  socioeconomic status, and urban vs.~rural living environments.
\item
  \textbf{Stable confounders}: in cross-sectional studies, some
  confounders are inherently stable over time, such as ethnicity, year
  and place of birth, and biological gender. For environmental
  psychologists examining the relationship between access to natural
  environments and psychological well-being, these stable confounders
  can be adjusted for without concern for introducing bias from
  mediators or colliders. For example, conditioning on year of birth can
  help to isolate the effect of recent urban development on mental
  health, independent of generational differences in attitudes toward
  green spaces.
\item
  \textbf{Invariable confounders}: other confounders, while not
  immutable, are less likely to be influenced by the treatment.
  Variables such as sexual orientation, educational attainment, and
  often income level fall into this category. For instance, the effect
  of exposure to polluted environments on cognitive outcomes can be
  analysed by conditioning on education level, assuming that recent
  exposure to pollution is unlikely to retroactively change someone's
  educational history.
\item
  \textbf{Timing and reverse causation}: the sequence of treatment and
  outcome is crucial. In some cases, the temporal order is clear,
  reducing concerns about reverse causation. Mortality is a definitive
  outcome where the timing issue is unambiguous. If researching the
  effects of air quality on mortality, the causal direction (poor air
  quality leading to higher mortality rates) is straightforward.
\item
  \textbf{Multiple causal diagrams}: given the complexity of
  environmental influences on psychological outcomes, it's prudent to
  construct multiple causal diagrams to cover various hypothetical
  scenarios. For example, when studying the effect of community green
  space on stress reduction, one diagram might assume direct benefits of
  green space on stress, while another might include potential mediators
  like physical activity. By analysing and reporting findings based on
  multiple diagrams, researchers can explore the robustness of their
  conclusions across different theoretical frameworks.
\end{enumerate}

Table~\ref{tbl-cs} describes ambigious confounding control setting
arising from cross-sectional data. Suppose again we are interested in
the causal effect of access to greenspace denoted by \(A\) on
`happiness', denoted by \(Y\). We are uncertain whether excercise,
denoted by \(L\), is a common cause of \(A\) and \(Y\) and thus a
confounder, or whether excercise is a mediator along the path from \(A\)
to \(Y\). We may use causal diagrams to investigate the consequences of
such ambiguity.

\textbf{Assumption 1: Exercise is a common cause of \(A\) and \(Y\)},
this scenario is presented in Table~\ref{tbl-cs} row 1. Here, our
strategy for confounding control is to estimate the effect of \(A\) on
\(Y\) conditioning on \(L\).

\textbf{Assumption 2: Exercise is a mediator of \(A\) and \(Y\)}, this
scenario is presented in Table~\ref{tbl-cs} row 2. Here, our strategy
for confounding control is to simply estimate the effect of \(A\) on
\(Y\) without including \(L\) (assuming there are no other common causes
of the treatment and outcome).

\begin{table}

\caption{\label{tbl-cs}This table is adapted from
(\citeproc{ref-bulbulia2023}{Bulbulia 2023})}

\centering{

\examplecrosssection

}

\end{table}%

To clarify how answer may differ we can simulate data and run separate
regressions, reflecting the different conditioning strategies embedded
in the different assumptions. The following simulation generates data
from a process in which exercise is a mediator (Scenario 2). (See
Appendix C)

\begin{table}
\caption{Code for a simulation of a data generating process in which the effect
of excercise (L) fully mediates the effect of greenspace (A) on
happiness (Y).}\tabularnewline

\centering
\begin{tabular}{lcccccc}
\toprule
\multicolumn{1}{c}{ } & \multicolumn{3}{c}{Model: Exercise assumed confounder} & \multicolumn{3}{c}{Model: Exercise assumed to be a mediator} \\
\cmidrule(l{3pt}r{3pt}){2-4} \cmidrule(l{3pt}r{3pt}){5-7}
\textbf{Characteristic} & \textbf{Beta} & \textbf{95\% CI} & \textbf{p-value} & \textbf{Beta} & \textbf{95\% CI} & \textbf{p-value}\\
\midrule
A & -0.27 & -0.53, -0.01 & 0.043 & 2.9 & 2.6, 3.2 & <0.001\\
L & 1.6 & 1.5, 1.7 & <0.001 &  &  & \\
\bottomrule
\multicolumn{7}{l}{\rule{0pt}{1em}\textsuperscript{1} CI = Confidence Interval}\\
\end{tabular}
\end{table}

This table presents us the conditional treatment effect. Where an
outcome is continuous and there are no interactions, the coeficient for
the treatment (\(A_1\)) reflect the average treatment effect. With
covariates and interactions, to estimate an average treatment effects
requires additional steps. We present code for obtaining marginal
treatment effects in Appendix C

On the assumptions outlined in , in which we \emph{assert} that exercise
is a confounder, the average treatment effect of access to green space
on happiness is ATE = -0.27, CI = {[}-0.52, -0.01{]}.

However, on the assumptions outline in DAG , in which we \emph{assert}
that exercise is a mediator, the average treatment effect of access to
green space on happiness is ATE = 2.92, CI = {[}2.66, 3.21{]}.

These findings illustrate the role that assumptions about the relative
timing of exercise as a confounder or as a mediator plays.

\subsubsection{Recommendations for conducting and reporting causal
analyses with cross-sectional
data.}\label{recommendations-for-conducting-and-reporting-causal-analyses-with-cross-sectional-data.}

We offer the following recommendations for analysing and reporting
analyses with cross-sectional data

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Draw multiple causal diagrams}: as the previous example
  illustrates, we may wish to draw a number of causal diagrams that
  represent different theoretical assumptions about the both the
  relationships and timing in the occurrance of variables relevant to an
  identification problem. This practice allows for a comprehensive
  exploration of potential causal pathways and the roles variables may
  play ---- be they confounders, mediators, or colliders. For instance,
  when examining the effect of urban green spaces on mental health,
  consider diagrams that account for direct effects, as well as those
  that include mediators like physical activity or social interaction.
\item
  \textbf{Perform and report analyses for each assumption}: conduct
  separate analyses based on the different scenarios outlined by your
  causal diagrams. This approach ensures that the analytical strategy
  aligns with the theoretical underpinnings of each model. Transparently
  reporting the results from each analysis, including the assumptions
  and statistical methods employed, enhances balance in a study. Editors
  and reviewers will not be accustomed to this practice of conditioning
  inferences on different assumptions. However, for meaningful progress,
  it essential to persevere. The problem inherent to cross-sectional
  analyses must be addressed by broadining the scope of our imaginations
  beyond testing one or another favoured hypothesis. Rather, we should
  graph the structural assumptions encoded by different theories, and
  condition on specific combinations in the occurance of events.
\item
  \textbf{Interpret findings with attention to ambiguities}: Ccrefully
  interpret the results, paying close attention to any ambiguities or
  inconsistencies that emerge across the different analyses. Describe
  how different assumptions about structural relationships and timing
  can lead to different conclusions. For example, if the effect of green
  space access on mental health appears positive when treating exercise
  as a mediator but negative when treated as a confounder -- as in the
  present simulation -- discuss the theoretical and practical
  implications of these findings.
\item
  \textbf{Recommend caution when findings diverge}: wherever findings
  lead to different practical conclusions, embrace caution about drawing
  firm conclusions.
\item
  \textbf{Suggest specific avenues for future research}: identify gaps
  in the current understanding that arise from ambiguities in your
  findings. Recommend specific, targeted data collection that might
  clarify the nature of the relationships among variables. For instance,
  longitudinal studies or experiments.
\item
  \textbf{Supplement observational data with simulated data}: data
  simulation is a powerful tool for understanding the complexities of
  causal inference in environmental psychology. By simulating data based
  on various theoretical models, researchers can explore how different
  assumptions about confounders, mediators, and the structure of causal
  relationships might influence their findings. Simulation studies allow
  for the testing of analytical strategies under controlled conditions,
  providing insights into the robustness of methods against violations
  of assumptions or the presence of unobserved confounders. Appendices C
  and D provides example code.
\item
  \textbf{Conduct sensitivity analyses to assess robustness}:
  sensitivity analyses are essential for evaluating the extent to which
  conclusions are dependent on specific assumptions or parameters within
  the causal model. Data simulation can be a powerful tool for
  evaluating the sensitivity of results to assumptions. In the next
  section, we consider a simple sensitivity analysis for assessing
  robustness to unmeasured confounding.
\end{enumerate}

\subsubsection{Longitudinal Designs}\label{longitudinal-designs}

Causation occurs in time. Longitudinal designs offer a substantial
advantage over cross-sectional designs for causal inference because
sequential measurements allow us to capture causation, and quantify its
magnitude. We typically do not need to assert timing as we do in
cross-sectional data settings. Despite this advantage, longitudinal
studies neverhteless rely on assumptions. These must be stated, and
carefully managed.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Temporal sequencing}: the core strength of longitudinal
  designs is their ability to capture temporal sequencing, reducing
  ambiguity about the directionality of causal relationships. For
  instance, tracking changes in ``happiness'' following changes in
  access to green spaces over time can more definitively suggest
  causation than cross-sectional snapshots.
\item
  \textbf{Managing assumptions}: although longitudinal data reduces the
  need to assert timing , researchers still face assumptions regarding
  the absence of unmeasured confounders and the stability of over time.
  These assumptions must be explicitly stated. As with cross sectional
  designs, wherever assumptions differ, researchers should draw
  different causal diagrams that reflect these assumptions, and
  subsequently conduct and report separate analyses.
\item
  \textbf{Conditioning on baselines}: In longitudinal designs,
  conditioning on baseline measures of both the treatment and outcome
  can significantly address problems of unmeasured confounding. As
  mentioned before, by conditioning on baseline levels of access to
  green spaces and baseline mental health, researchers can more
  accurately estimate the \emph{incident effect} of changes in green
  space access on changes in mental health.
\end{enumerate}

\begin{table}

\caption{\label{tbl-lg}This table is adapted from
(\citeproc{ref-bulbulia2023}{Bulbulia 2023})}

\centering{

\examplelongitudinal

}

\end{table}%

\subsubsection{Recommendations for conducting and reporting causal
analyses with cross-sectional
data.}\label{recommendations-for-conducting-and-reporting-causal-analyses-with-cross-sectional-data.-1}

We offer the following recommendations for analysing and reporting
analyses with cross-sectional data

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Draw two causal diagrams: one that describes the
  identification problem and the other for its solutions}: not only is
  it beneficial to draw separate causal diagrams to represent distinct
  structural assumptions, it is often helpful to draw a distinct causal
  diagram -- or several -- for describing the identification problem and
  a distinct causal diagram for describing its proposed solutions. As
  illustrated in Table~\ref{tbl-lg}, we recommend constructing at least
  two causal diagrams: first, draft an initial causal diagram that
  depicts assumed relationships among variables, including potential
  confounders and mediators. Second, draft a subsequent causal diagram
  that depicts a strategy for addressing these relationships, for
  example by highlighting variables on which to condition to reveal the
  causal effect of interest.
\item
  \emph{Use at least three-wave of data}: a three-wave longitudinal
  design offers a refined approach by incorporating data from three time
  points. This design enables the examination of temporal precedence and
  lagged effects, providing stronger evidence of causality. For
  instance, if increased access to green spaces (time 0) is followed by
  increased exercise (time 1), which then precedes improvements in
  happiness (time 2), the temporal pattern can clarify a causal pathway
  from green space access through exercise to mental health.
  Table~\ref{tbl-lg} describe the core of a three-wave longitudinal
  problem together with a strategy for addressing it.
\item
  \emph{Simulate data}
\item
  \emph{Conduct sensitivity analyses such as the E-value}
\item
  \emph{Calculate average treatment effects for the entire population}
\item
  \emph{Effect-modification}
\end{enumerate}

\subsection{Summary}\label{summary}

This chapter has offered an introduction to the potential outcomes
framework for causal inference and directed acylic graphs to
environmental psychology. Part 1 introduced three crucial assumptions
for estimating magnitudes average treatment effects from data:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Conditional Exchangeability}: the allocation of treatment is
  randomised and independent of the potential outcomes, conditional on
  measured covariates.
\item
  \textbf{Causal Consistency}: the outcome observed under the treatment
  condition matches the outcome that would have been observed if the
  unit had received the treatment, and vice versa for the control
  condition.
\item
  \textbf{Positivity}: every unit has a non-zero probability of
  receiving the treatments under comparison.
\end{enumerate}

Although (perfectly) randomised controlled experiments naturally satisfy
these assumptions by design-- randomisation ensures exchangeability,
control ensures consistency and positivity -- observational studies
generally do not. To obtain consistent causal estimates from
observational data we must evaluate whether and how these assumptions
may be satisfied.

Part 2 discussed causal diagrams and their role in addressing the
conditional exchangeability assumption, or the assumption of ``no
unmeasured confounders.'' We discovered four elementary structures from
which all causal relationships are build. And within these structures we
discovered elementary rules for evaluating the implications that
conditioning on elements within these structures entail about
statistical associations that are observable in data. As such, causal
diagrams offer a simplified visual vocabulary for mapping complex causal
relationships onto observations in data. However, the relationships
encoded in graphs are assertions that are generally not themselves
verifiable from data. The only causal relationships that are not
asserted are those between treatments and outcomes. We use causal
diagrams to evaluate structural sources of bias in the statistical
associations of treatments and outcomes that causal diagrams that may
arise from causal relationships in the world that we assume might
statistically associate treatments with outcomes irrespective of their
causal associations.

In Part 3, we applied causal diagrams to various confounding scenarios.
We observed that a causal diagram need highlight only those aspects of a
causal setting relevant to assessing structural sources of bias that
link the treatment and outcome in the absence of causality. Throughout
we omitted nodes and paths that were not strictly necessary for
evaluating our stated identification problem. Again we demonstrated that
causal diagrams are not merely related to context-dependent questions
but also to the assumptions we make about the causal structure of the
world -- structural assumptions.

Although here we have only discussed seven specific applications of
causal diagrams, their utility spans far beyond. The simple rules by
which variables become associated and disassociated by conditioning on
nodes within four elementary structures allow researchers to apply
causal diagrams to quantitatively investigate causality across
innumerably many questions. We discovered that such applications are not
limited to the production of effective modelling strategies but may also
inform strategies for data collection, such as repeated measures data
collection.

We hope the material we present here will encourage environmental
psychologists to learn more about causal inference and to integrate
causal diagrams into their workflow. The tools for assessing causation
from correlation have been developed. There is no longer any excuse to
ignore them.

\newpage{}

\subsection{Funding}\label{funding}

This work is supported by a grant from the Templeton Religion Trust
(TRT0418). JB received support from the Max Planck Institute for the
Science of Human History. The funders had no role in preparing the
manuscript or the decision to publish it.

\subsection{Contributions}\label{contributions}

DH proposed the chapter. JB developed the approach and wrote the first
draft. Both authors contributed substantially to the final
work.@appendix\_a

\subsection{References}\label{references}

\phantomsection\label{refs}
\begin{CSLReferences}{1}{0}
\bibitem[\citeproctext]{ref-angrist2009mostly}
Angrist, JD, and Pischke, J-S (2009) \emph{Mostly harmless econometrics:
An empiricist's companion}, Princeton university press.

\bibitem[\citeproctext]{ref-bulbulia2022}
Bulbulia, JA (2022) A workflow for causal inference in cross-cultural
psychology. \emph{Religion, Brain \& Behavior}, \textbf{0}(0), 1--16.
doi:\href{https://doi.org/10.1080/2153599X.2022.2070245}{10.1080/2153599X.2022.2070245}.

\bibitem[\citeproctext]{ref-bulbulia2023}
Bulbulia, JA (2023) Causal diagrams (directed acyclic graphs): A
practical guide.

\bibitem[\citeproctext]{ref-bulbulia2023a}
Bulbulia, JA, Afzali, MU, Yogeeswaran, K, and Sibley, CG (2023)
Long-term causal effects of far-right terrorism in {N}ew {Z}ealand.
\emph{PNAS Nexus}, \textbf{2}(8), pgad242.

\bibitem[\citeproctext]{ref-cinelli2022}
Cinelli, C, Forney, A, and Pearl, J (2022) A Crash Course in Good and
Bad Controls. \emph{Sociological Methods \& Research},
00491241221099552.
doi:\href{https://doi.org/10.1177/00491241221099552}{10.1177/00491241221099552}.

\bibitem[\citeproctext]{ref-cole2010}
Cole, SR, Platt, RW, Schisterman, EF, \ldots{} Poole, C (2010)
Illustrating bias due to conditioning on a collider. \emph{International
Journal of Epidemiology}, \textbf{39}(2), 417--420.
doi:\href{https://doi.org/10.1093/ije/dyp334}{10.1093/ije/dyp334}.

\bibitem[\citeproctext]{ref-cook2002experimental}
Cook, TD, Campbell, DT, and Shadish, W (2002) \emph{Experimental and
quasi-experimental designs for generalized causal inference}, Vol. 1195,
Houghton Mifflin Boston, MA.

\bibitem[\citeproctext]{ref-danaei2012}
Danaei, G, Tavakkoli, M, and Hernán, MA (2012) Bias in observational
studies of prevalent users: lessons for comparative effectiveness
research from a meta-analysis of statins. \emph{American Journal of
Epidemiology}, \textbf{175}(4), 250--262.
doi:\href{https://doi.org/10.1093/aje/kwr301}{10.1093/aje/kwr301}.

\bibitem[\citeproctext]{ref-edwards2015}
Edwards, JK, Cole, SR, and Westreich, D (2015) All your data are always
missing: Incorporating bias due to measurement error into the potential
outcomes framework. \emph{International Journal of Epidemiology},
\textbf{44}(4), 14521459.

\bibitem[\citeproctext]{ref-greenland1999}
Greenland, S, Pearl, J, and Robins, JM (1999) Causal diagrams for
epidemiologic research. \emph{Epidemiology (Cambridge, Mass.)},
\textbf{10}(1), 37--48.

\bibitem[\citeproctext]{ref-hernan2023}
Hernan, MA, and Robins, JM (2023) \emph{Causal inference}, Taylor \&
Francis. Retrieved from
\url{https://books.google.co.nz/books?id=/_KnHIAAACAAJ}

\bibitem[\citeproctext]{ref-hernuxe1n2004a}
Hernán, MA, Hernández-Díaz, S, and Robins, JM (2004) A structural
approach to selection bias. \emph{Epidemiology}, \textbf{15}(5),
615--625. Retrieved from \url{https://www.jstor.org/stable/20485961}

\bibitem[\citeproctext]{ref-hernuxe1n2023}
Hernán, MA, and Monge, S (2023) Selection bias due to conditioning on a
collider. \emph{BMJ}, \textbf{381}, p1135.
doi:\href{https://doi.org/10.1136/bmj.p1135}{10.1136/bmj.p1135}.

\bibitem[\citeproctext]{ref-hernan2017per}
Hernán, MA, Robins, JM, et al. (2017) Per-protocol analyses of pragmatic
trials. \emph{N Engl J Med}, \textbf{377}(14), 1391--1398.

\bibitem[\citeproctext]{ref-hernuxe1n2016}
Hernán, MA, Sauer, BC, Hernández-Díaz, S, Platt, R, and Shrier, I (2016)
Specifying a target trial prevents immortal time bias and other
self-inflicted injuries in observational analyses. \emph{Journal of
Clinical Epidemiology}, \textbf{79}, 7075.

\bibitem[\citeproctext]{ref-holland1986}
Holland, PW (1986) Statistics and causal inference. \emph{Journal of the
American Statistical Association}, \textbf{81}(396), 945960.

\bibitem[\citeproctext]{ref-hume1902}
Hume, D (1902) \emph{Enquiries Concerning the Human Understanding: And
Concerning the Principles of Morals}, Clarendon Press.

\bibitem[\citeproctext]{ref-lash2020}
Lash, TL, Rothman, KJ, VanderWeele, TJ, and Haneuse, S (2020)
\emph{Modern epidemiology}, Wolters Kluwer. Retrieved from
\url{https://books.google.co.nz/books?id=SiTSnQEACAAJ}

\bibitem[\citeproctext]{ref-mancuso2018revolutionary}
Mancuso, S (2018) \emph{The revolutionary genius of plants: A new
understanding of plant intelligence and behavior}, Simon; Schuster.

\bibitem[\citeproctext]{ref-montgomery2018}
Montgomery, JM, Nyhan, B, and Torres, M (2018) How conditioning on
posttreatment variables can ruin your experiment and what to do about
It. \emph{American Journal of Political Science}, \textbf{62}(3),
760--775.
doi:\href{https://doi.org/10.1111/ajps.12357}{10.1111/ajps.12357}.

\bibitem[\citeproctext]{ref-morgan2014}
Morgan, SL, and Winship, C (2014) \emph{Counterfactuals and causal
inference: Methods and principles for social research}, 2nd edn,
Cambridge: Cambridge University Press.
doi:\href{https://doi.org/10.1017/CBO9781107587991}{10.1017/CBO9781107587991}.

\bibitem[\citeproctext]{ref-neyman1923}
Neyman, JS (1923) On the application of probability theory to
agricultural experiments. Essay on principles. Section 9.(tlanslated and
edited by dm dabrowska and tp speed, statistical science (1990), 5,
465-480). \emph{Annals of Agricultural Sciences}, \textbf{10}, 151.

\bibitem[\citeproctext]{ref-pearl1995}
Pearl, J (1995) Causal diagrams for empirical research.
\emph{Biometrika}, \textbf{82}(4), 669--688.

\bibitem[\citeproctext]{ref-pearl2009}
Pearl, J (2009a) \emph{\href{https://doi.org/10.1214/09-SS057}{Causal
inference in statistics: An overview}}.

\bibitem[\citeproctext]{ref-pearl2009a}
Pearl, J (2009b) \emph{Causality}, Cambridge University Press.

\bibitem[\citeproctext]{ref-pearl2018}
Pearl, J, and Mackenzie, D (2018) \emph{The book of why: The new science
of cause and effect}, Basic books.

\bibitem[\citeproctext]{ref-robins1986}
Robins, J (1986) A new approach to causal inference in mortality studies
with a sustained exposure period---application to control of the healthy
worker survivor effect. \emph{Mathematical Modelling}, \textbf{7}(9-12),
1393--1512.

\bibitem[\citeproctext]{ref-robins2008estimation}
Robins, J, and Hernan, M (2008) Estimation of the causal effects of
time-varying exposures. \emph{Chapman \& Hall/CRC Handbooks of Modern
Statistical Methods}, 553--599.

\bibitem[\citeproctext]{ref-rubin1976}
Rubin, DB (1976) Inference and missing data. \emph{Biometrika},
\textbf{63}(3), 581--592.
doi:\href{https://doi.org/10.1093/biomet/63.3.581}{10.1093/biomet/63.3.581}.

\bibitem[\citeproctext]{ref-rubin2005}
Rubin, DB (2005) Causal inference using potential outcomes: Design,
modeling, decisions. \emph{Journal of the American Statistical
Association}, \textbf{100}(469), 322--331. Retrieved from
\url{https://www.jstor.org/stable/27590541}

\bibitem[\citeproctext]{ref-vanderweele2009}
VanderWeele, TJ (2009) Concerning the consistency assumption in causal
inference. \emph{Epidemiology}, \textbf{20}(6), 880.
doi:\href{https://doi.org/10.1097/EDE.0b013e3181bd5638}{10.1097/EDE.0b013e3181bd5638}.

\bibitem[\citeproctext]{ref-vanderweele2015}
VanderWeele, TJ (2015) \emph{Explanation in causal inference: Methods
for mediation and interaction}, Oxford University Press.

\bibitem[\citeproctext]{ref-vanderweele2018}
VanderWeele, TJ (2018) On well-defined hypothetical interventions in the
potential outcomes framework. \emph{Epidemiology}, \textbf{29}(4), e24.
doi:\href{https://doi.org/10.1097/EDE.0000000000000823}{10.1097/EDE.0000000000000823}.

\bibitem[\citeproctext]{ref-vanderweele2019}
VanderWeele, TJ (2019) Principles of confounder selection.
\emph{European Journal of Epidemiology}, \textbf{34}(3), 211219.

\bibitem[\citeproctext]{ref-vanderweele2013}
VanderWeele, TJ, and Hernan, MA (2013) Causal inference under multiple
versions of treatment. \emph{Journal of Causal Inference},
\textbf{1}(1), 120.

\bibitem[\citeproctext]{ref-vanderweele2020}
VanderWeele, TJ, Mathur, MB, and Chen, Y (2020) Outcome-wide
longitudinal designs for causal inference: A new template for empirical
studies. \emph{Statistical Science}, \textbf{35}(3), 437466.

\bibitem[\citeproctext]{ref-vanderweele2022b}
VanderWeele, TJ, and Vansteelandt, S (2022) A statistical test to reject
the structural interpretation of a latent factor model. \emph{Journal of
the Royal Statistical Society Series B: Statistical Methodology},
\textbf{84}(5), 20322054.

\bibitem[\citeproctext]{ref-westreich2012berkson}
Westreich, D (2012) Berkson's bias, selection bias, and missing data.
\emph{Epidemiology (Cambridge, Mass.)}, \textbf{23}(1), 159.

\bibitem[\citeproctext]{ref-westreich2010}
Westreich, D, and Cole, SR (2010) Invited commentary: positivity in
practice. \emph{American Journal of Epidemiology}, \textbf{171}(6).
doi:\href{https://doi.org/10.1093/aje/kwp436}{10.1093/aje/kwp436}.

\bibitem[\citeproctext]{ref-westreich2015}
Westreich, D, Edwards, JK, Cole, SR, Platt, RW, Mumford, SL, and
Schisterman, EF (2015) Imputation approaches for potential outcomes in
causal inference. \emph{International Journal of Epidemiology},
\textbf{44}(5), 17311737.

\bibitem[\citeproctext]{ref-westreich2013}
Westreich, D, and Greenland, S (2013) The table 2 fallacy: Presenting
and interpreting confounder and modifier coefficients. \emph{American
Journal of Epidemiology}, \textbf{177}(4), 292298.

\end{CSLReferences}

\subsection{Appendix A: Glossary}\label{appendix-a-glossary}

\textbf{Acyclic}: a causal diagram cannot contain feedback loops. More
precisely, no variable can be an ancestor or descendant of itself. If
variables are repeatedly measured here, it is especially important to
index nodes by the relative timing of the nodes.

\textbf{Adjustment set}: a collection of variables we must either
condition upon or deliberately avoid conditioning upon to obtain a
consistent causal estimate for the effect of interest
(\citeproc{ref-pearl2009}{Pearl 2009a}).

\textbf{Ancestor (parent)}: a node with a direct or indirect influence
on others, positioned upstream in the causal chain.

\textbf{Arrow}: denotes a causal relationship linking nodes.

\textbf{Backdoor path}: a ``backdoor path'' between a treatment
variable, \(A\), and an outcome variable, \(Y\), is a sequence of links
in a causal diagram that starts with an arrow into \(A\) and reaches
\(Y\) through common causes, introducing potential confounding bias such
that stastical association does not reflect causality. To estimate the
causal effect of \(A\) on \(Y\) without bias, these paths must be
blocked by adjusting for confounders. The backdoor criterion guides the
selection of variables for adjustment to ensure unbiased causal
inference.

\textbf{Conditioning}: the process of explicitly accounting for a
variable in our statistical analysis to address the identification
problem. In causal diagrams, we usually represent conditioning by
drawing a box around a node of the conditioned variable, for example,
\(\boxed{L_{0}}\to A_{1} \to L_{2}\). We do not box exposures and
outcomes, because we assume they are included in a model by default.
Depending on the setting, we may condition by regression stratification,
inverse-probability of treatment weighting, g-methods, doubly robust
machine learning algorithms, or other methods. We do not cover such
methods in this tutorial, however see Hernan and Robins
(\citeproc{ref-hernan2023}{2023}).

\textbf{Counterfactual}: a hypothetical outcome that would have occurred
for the same individuals under a different treatment condition than the
one they actually experienced.

\textbf{Direct effect}: the portion of the total effect of a treatment
on an outcome that is not mediated by other variables within the causal
pathway.

\textbf{Collider}: a variable in a causal diagram at which two incoming
paths meet head-to-head. For example if
\(A \rightarrowred \boxed{L} \leftarrowred Y\), then \(L\) is a
collider. If we do not condition on a collider (or its descendants), the
path between \(A\) and \(Y\) remains closed. Conditioning on a collider
(or its descendants) will induce an association between \(A\) and \(Y\).

\textbf{Confounder}: a member of an adjustment set. Notice a variable is
a `confounder' in relation to a specific adjustment set. `Confounder' is
a relative concept (\citeproc{ref-lash2020}{Lash \emph{et al.} 2020}).

\textbf{D-separation}: in a causal diagram, a path is `blocked' or
`d-separated' if a node along it interrupts causation. Two variables are
d-separated if all paths connecting them are blocked, making them
conditionally independent. Conversely, unblocked paths result in
`d-connected' variables, implying potential dependence
(\citeproc{ref-pearl1995}{Pearl 1995}).

\textbf{Descendant (child)}: a node influenced, directly or indirectly,
by upstream nodes (parents).

\textbf{Effect-modifier}: a variable is an effect-modifier, or
`effect-measure modifier' if its presence changes the magnitude or
direction of the effect of an exposure or treatment on an outcome across
the levels or values of this variable. In other words, the effect of the
exposure is different at different levels of the effect-modifier.

\textbf{External validity}: the extent to which causal inferences can be
generalizsd to other populations, settings, or times, also called
``Target Validity.''

\textbf{Identification problem}: the challenge of estimating the causal
effect of a variable using by adjusting for measured variables on units
in a study. Causal diagrams were developed to address the identification
problem by application of the rules of d-separation to a causal diagram.

\textbf{Indirect effect (Mediated effect)}: The portion of the total
effect that is transmitted through a mediator variable.

\textbf{Internal validity}: the degree to which the design and conduct
of a study are likely to have prevented bias, ensuring that the causal
relationship observed can be confidently attributed to the treatment and
not to other factors.

\textbf{Instrumental variable}: an ancestor of the exposure but not of
the outcome. An instrumental variable affects the outcome only through
its effect on the exposure and not otherwise. Whereas conditioning on a
variable causally associated with the outcome but not with the exposure
will generally increase modelling precision, we should avoid
conditioning on instrumental variables
(\citeproc{ref-cinelli2022}{Cinelli \emph{et al.} 2022}). Second, when
an instrumental variable is the descendant of an unmeasured confounder,
we should generally condition the instrumental variable to provide a
partial adjustment for a confounder.

\textbf{Mediator}: a variable that transmits the effect of the treatment
variable on the outcome variable, part of the causal pathway between
treatment and outcome.

\textbf{Modified Disjunctive Cause Criterion}: VanderWeele
(\citeproc{ref-vanderweele2019}{2019}) recommends obtaining a maximally
efficient adjustment which he calls a `confounder set' A member of this
set is any set of variables that can reduce or remove a structural
sources of bias. The strategy is as follows:

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\tightlist
\item
  Control for any variable that causes the exposure, the outcome, or
  both.
\item
  Control for any proxy for an unmeasured variable that is a shared
  cause of both the exposure and outcome.
\item
  Define an instrumental variable as a variable associated with the
  exposure but does not influence the outcome independently, except
  through the exposure. Exclude any instrumental variable that is not a
  proxy for an unmeasured confounder from the confounder set
  (\citeproc{ref-vanderweele2019}{VanderWeele 2019}).
\end{enumerate}

Note that the concept of a `confounder set' is broader than that of an
`adjustment set.' Every adjustment set is a member of a confounder set.
Hence, the Modified Disjunctive Cause Criterion will eliminate bias when
the data permit. However, a confounder set includes variables that will
reduce bias in cases where confounding cannot be eliminated.

\textbf{Node}: characteristic or features of units in a population
(`variable') represented on a causal diagram. In a causal diagram, nodes
are drawn with reference to variables defomed for the target population.

\textbf{Randomisation}: The process of randomly assigning subjects to
different treatments or control groups, aiming to eliminate selection
bias in experimental studies.

\textbf{Reverse Causation}: \(\atoyassert\), but in reality \(\ytoa\)

\textbf{Statistical model:} a mathematical representation of the
relationships between variables in which we quantify covariances and
their corresponding uncertainties in the data. Statistical models
typically correspond to multiple causal structures
(\citeproc{ref-hernan2023}{Hernan and Robins 2023};
\citeproc{ref-pearl2018}{Pearl and Mackenzie 2018};
\citeproc{ref-vanderweele2022b}{VanderWeele and Vansteelandt 2022}).
That is, the causes of such covariances cannot be identified without
assumptions.

\textbf{Structural model:} defines assumptions about causal
relationships. Causal diagrams graphically encode these assumptions
(\citeproc{ref-hernan2023}{Hernan and Robins 2023}), leaving out the
assumption about whether the exposure and outcome are causally
associated. Outside of randomised experiments, we cannot compute causal
effects in the absence of structural models. A structural model is
needed to interpret the statistical findings in causal terms. Structural
assumptions should be developed in consultation with experts. The role
of structural assumptions when interpreting statistical results remains
poorly understood across many human sciences and forms the motivation
for my work here.

\textbf{Time-varying confounding:} occurs when a confounder that changes
over time also acts as a mediator or collider in the causal pathway
between exposure and outcome. Controlling for such a confounder can
introduce bias. Not controlling for it can retain bias.

\subsection{Appendix B: Causal Consistency in observational
settings}\label{appendix-b-causal-consistency-in-observational-settings}

In observational research, there are typically multiple versions of
treatment. The theory of causal inference under multiple versions of
treatment proves we can consistently estimate causal effects where the
different versions of treatment are conditionally independent of the
outcomes VanderWeele (\citeproc{ref-vanderweele2009}{2009})

Let \(\coprod\) denote independence. Where there are \(K\) different
versions of treatment \(A\) and no confounding for \(K\)'s effect on
\(Y\) given measured confounders \(L\) such that

\[
Y(k) \coprod K | L
\]

Then it can be proved that causal consistency follows. According to the
theory of causal inference under multiple versions of treatment, the
measured variable \(A\) functions as a ``coarsened indicator'' for
estimating the causal effect of the multiple versions of treatment \(K\)
on \(Y(k)\) (\citeproc{ref-vanderweele2009}{VanderWeele 2009},
\citeproc{ref-vanderweele2018}{2018};
\citeproc{ref-vanderweele2013}{VanderWeele and Hernan 2013}).

In the context of green spaces, let \(A\) represent the general action
of moving closer to any green space and \(K\) represent the different
versions of this treatment. For instance, \(K\) could denote moving
closer to different types of green spaces such as parks, forests,
community gardens, or green spaces with varying amenities and features.

Here, the conditional independence implies that, given measured
confounders \(L\) (e.g.~socioeconomic status, age, personal values), the
type of green space one moves closer to (\(K\)) is independent of the
outcomes \(Y(k)\) (e.g.~mental well-being under the \(K\) conditions).
In other words, the version of green space one chooses to live near does
not affect the \(K\) potential outcomes, provided the confounders \(L\)
are properly controlled for in our statistical models.

Put simply, strategies for confounding control and for consistently
estimating causal effects when there are multiple versions of treatment
converge. However, the quantities we estimate under multiple versions of
treatment might lack any clear interpretations. For example, we cannot
readily determine which of the many versions of treatment is most causal
efficacious and which lack any causal effect, or are harmful.

\subsection{Appendix C: Computing the Average Treatment
Effect}\label{appendix-c-computing-the-average-treatment-effect}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# load libraries}
\FunctionTok{library}\NormalTok{(gtsummary) }\CommentTok{\# gtsummary: nice tables}
\FunctionTok{library}\NormalTok{(kableExtra) }\CommentTok{\#  tables in latex/markdown}
\FunctionTok{library}\NormalTok{(clarify) }\CommentTok{\# simulate ATE}

\CommentTok{\# simulation seed}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{) }\CommentTok{\#  reproducibility}

\CommentTok{\# define the parameters }
\NormalTok{n }\OtherTok{=} \DecValTok{1000} \CommentTok{\# Number of observations}
\NormalTok{p }\OtherTok{=} \FloatTok{0.5}  \CommentTok{\# Probability of A = 1 (access to greenspace)}
\NormalTok{alpha }\OtherTok{=} \DecValTok{0} \CommentTok{\# Intercept for L (excercise)}
\NormalTok{beta }\OtherTok{=} \DecValTok{2}  \CommentTok{\# Effect of A on L }
\NormalTok{gamma }\OtherTok{=} \DecValTok{1} \CommentTok{\# Intercept for Y }
\NormalTok{delta }\OtherTok{=} \FloatTok{1.5} \CommentTok{\# Effect of L on Y}
\NormalTok{sigma\_L }\OtherTok{=} \DecValTok{1} \CommentTok{\# Standard deviation of L}
\NormalTok{sigma\_Y }\OtherTok{=} \FloatTok{1.5} \CommentTok{\# Standard deviation of Y}

\CommentTok{\# simulate the data: fully mediated effect }
\NormalTok{A }\OtherTok{=} \FunctionTok{rbinom}\NormalTok{(n, }\DecValTok{1}\NormalTok{, p) }\CommentTok{\# binary exposure variable}
\NormalTok{L }\OtherTok{=}\NormalTok{ alpha }\SpecialCharTok{+}\NormalTok{ beta}\SpecialCharTok{*}\NormalTok{A }\SpecialCharTok{+} \FunctionTok{rnorm}\NormalTok{(n, }\DecValTok{0}\NormalTok{, sigma\_L) }\CommentTok{\# continuous mediator}
\NormalTok{Y }\OtherTok{=}\NormalTok{ gamma }\SpecialCharTok{+}\NormalTok{ delta}\SpecialCharTok{*}\NormalTok{L }\SpecialCharTok{+} \FunctionTok{rnorm}\NormalTok{(n, }\DecValTok{0}\NormalTok{, sigma\_Y) }\CommentTok{\# continuous outcome}

\CommentTok{\# make the data frame}
\NormalTok{data }\OtherTok{=} \FunctionTok{data.frame}\NormalTok{(}\AttributeTok{A =}\NormalTok{ A, }\AttributeTok{L =}\NormalTok{ L, }\AttributeTok{Y =}\NormalTok{ Y)}

\CommentTok{\# fit regression in which L is assume to be a mediator}
\NormalTok{fit\_1 }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{( Y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ A }\SpecialCharTok{+}\NormalTok{ L, }\AttributeTok{data =}\NormalTok{ data)}

\CommentTok{\# fit regression in which L is assume to be a mediator}
\NormalTok{fit\_2 }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{( Y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ A, }\AttributeTok{data =}\NormalTok{ data)}

\CommentTok{\# create gtsummary tables for each regression model}
\NormalTok{table1 }\OtherTok{\textless{}{-}} \FunctionTok{tbl\_regression}\NormalTok{(fit\_1)}
\NormalTok{table2 }\OtherTok{\textless{}{-}} \FunctionTok{tbl\_regression}\NormalTok{(fit\_2)}

\CommentTok{\# merge the tables for comparison}
\NormalTok{table\_comparison }\OtherTok{\textless{}{-}} \FunctionTok{tbl\_merge}\NormalTok{(}
  \FunctionTok{list}\NormalTok{(table1, table2),}
  \AttributeTok{tab\_spanner =} \FunctionTok{c}\NormalTok{(}\StringTok{"Model: Exercise assumed confounder"}\NormalTok{, }
                  \StringTok{"Model: Exercise assumed to be a mediator"}\NormalTok{)}
\NormalTok{)}
\CommentTok{\# make latex table}
\NormalTok{markdown\_table\_0 }\OtherTok{\textless{}{-}} \FunctionTok{as\_kable\_extra}\NormalTok{(table\_comparison, }
                                   \AttributeTok{format =} \StringTok{"latex"}\NormalTok{, }
                                   \AttributeTok{booktabs =} \ConstantTok{TRUE}\NormalTok{)}
\CommentTok{\# print table}
\NormalTok{markdown\_table\_0}
\end{Highlighting}
\end{Shaded}

Next, we present the code for calculating an average treatment effect.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# use \textasciigrave{}clarify\textasciigrave{} package to obtain ATE}
\FunctionTok{library}\NormalTok{(clarify)}
\CommentTok{\# simulate fit 1 ATE}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}
\NormalTok{sim\_coefs\_fit\_1 }\OtherTok{\textless{}{-}} \FunctionTok{sim}\NormalTok{(fit\_1)}
\NormalTok{sim\_coefs\_fit\_2 }\OtherTok{\textless{}{-}} \FunctionTok{sim}\NormalTok{(fit\_2)}

\CommentTok{\# marginal risk difference ATE, simulation{-}based: model 1 (L is a confounder)}
\NormalTok{sim\_est\_fit\_1 }\OtherTok{\textless{}{-}}
  \FunctionTok{sim\_ame}\NormalTok{(}
\NormalTok{    sim\_coefs\_fit\_1,}
    \AttributeTok{var =} \StringTok{"A"}\NormalTok{,}
    \AttributeTok{subset =}\NormalTok{ A }\SpecialCharTok{==} \DecValTok{1}\NormalTok{,}
    \AttributeTok{contrast =} \StringTok{"RD"}\NormalTok{,}
    \AttributeTok{verbose =} \ConstantTok{FALSE}
\NormalTok{  )}

\CommentTok{\# marginal risk difference ATE, simulation{-}based: model 2 (L is a mediator)}
\NormalTok{sim\_est\_fit\_2 }\OtherTok{\textless{}{-}}
  \FunctionTok{sim\_ame}\NormalTok{(}
\NormalTok{    sim\_coefs\_fit\_2,}
    \AttributeTok{var =} \StringTok{"A"}\NormalTok{,}
    \AttributeTok{subset =}\NormalTok{ A }\SpecialCharTok{==} \DecValTok{1}\NormalTok{,}
    \AttributeTok{contrast =} \StringTok{"RD"}\NormalTok{,}
    \AttributeTok{verbose =} \ConstantTok{FALSE}
\NormalTok{  )}

\CommentTok{\# obtain summaries}
\NormalTok{summary\_sim\_est\_fit\_1 }\OtherTok{\textless{}{-}} \FunctionTok{summary}\NormalTok{(sim\_est\_fit\_1, }\AttributeTok{null =} \FunctionTok{c}\NormalTok{(}\StringTok{\textasciigrave{}}\AttributeTok{RD}\StringTok{\textasciigrave{}} \OtherTok{=} \DecValTok{0}\NormalTok{))}
\NormalTok{summary\_sim\_est\_fit\_2 }\OtherTok{\textless{}{-}} \FunctionTok{summary}\NormalTok{(sim\_est\_fit\_2, }\AttributeTok{null =} \FunctionTok{c}\NormalTok{(}\StringTok{\textasciigrave{}}\AttributeTok{RD}\StringTok{\textasciigrave{}} \OtherTok{=} \DecValTok{0}\NormalTok{))}

\CommentTok{\# get coefficients for reporting}
\CommentTok{\# ate for fit 1, with 95\% CI}
\NormalTok{ATE\_fit\_1 }\OtherTok{\textless{}{-}}\NormalTok{ glue}\SpecialCharTok{::}\FunctionTok{glue}\NormalTok{(}
  \StringTok{"ATE =}
\StringTok{                        \{round(summary\_sim\_est\_fit\_1[3, 1], 2)\},}
\StringTok{                        CI = [\{round(summary\_sim\_est\_fit\_1[3, 2], 2)\},}
\StringTok{                        \{round(summary\_sim\_est\_fit\_1[3, 3], 2)\}]"}
\NormalTok{)}
\CommentTok{\# ate for fit 2, with 95\% CI}
\NormalTok{ATE\_fit\_2 }\OtherTok{\textless{}{-}}
\NormalTok{  glue}\SpecialCharTok{::}\FunctionTok{glue}\NormalTok{(}
    \StringTok{"ATE = \{round(summary\_sim\_est\_fit\_2[3, 1], 2)\},}
\StringTok{                        CI = [\{round(summary\_sim\_est\_fit\_2[3, 2], 2)\},}
\StringTok{                        \{round(summary\_sim\_est\_fit\_2[3, 3], 2)\}]"}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

\subsection{Appendix D: Simulation of Different Confounding Control
Strategies}\label{appendix-d-simulation-of-different-confounding-control-strategies}

Here are data illustrating how different confounding control strategies
perform against the ``ground truth'' of simulated data. This simulation
reveals that (1) standard control for confounding may not perform
optimally when there is unmeasured confounding and (2) including the
baseline treatment and outcome, if these variables are strongly
correlated with the unmeasured confounder, may substantially reduce the
effect of unmeasured confounding

In this simulation:

\begin{itemize}
\tightlist
\item
  The interaction between \(A_1\) (treatment) and \(L_0\) (baseline
  confounders) is specified by the coefficient \(\theta_{A1L0} = 0.5\),
  indicating that the effect of \(A_1\) on \(Y_2\) varies depending on
  the value of \(L_0\).
\item
  The inclusion of this interaction allows us to examine how the
  baseline level of a confounder (\(L_0\)) modifies the effect of the
  treatment (\(A_1\)) on the outcome (\(Y_2\)).
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(kableExtra)}
\FunctionTok{library}\NormalTok{(gtsummary)}
\ControlFlowTok{if}\NormalTok{(}\SpecialCharTok{!}\FunctionTok{require}\NormalTok{(grf))\{}\FunctionTok{install.packages}\NormalTok{(}\StringTok{"grf"}\NormalTok{)\} }\CommentTok{\# causal forest}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{) }\CommentTok{\# Ensure reproducibility}

\NormalTok{n }\OtherTok{\textless{}{-}} \DecValTok{10000} \CommentTok{\# Number of observations}

\CommentTok{\# Baseline covariates}
\NormalTok{U }\OtherTok{\textless{}{-}} \FunctionTok{rnorm}\NormalTok{(n) }\CommentTok{\# Unmeasured confounder}
\NormalTok{A\_0 }\OtherTok{\textless{}{-}} \FunctionTok{rbinom}\NormalTok{(n, }\DecValTok{1}\NormalTok{, }\AttributeTok{prob =} \FunctionTok{plogis}\NormalTok{(U)) }\CommentTok{\# Baseline exposure}
\NormalTok{Y\_0 }\OtherTok{\textless{}{-}} \FunctionTok{rnorm}\NormalTok{(n, }\AttributeTok{mean =}\NormalTok{ U, }\AttributeTok{sd =} \DecValTok{1}\NormalTok{) }\CommentTok{\# Baseline outcome}
\NormalTok{L\_0 }\OtherTok{\textless{}{-}} \FunctionTok{rnorm}\NormalTok{(n, }\AttributeTok{mean =}\NormalTok{ U, }\AttributeTok{sd =} \DecValTok{1}\NormalTok{) }\CommentTok{\# Baseline confounders}

\CommentTok{\# Coefficients for treatment assignment}
\NormalTok{beta\_A0 }\OtherTok{=} \FloatTok{0.25}
\NormalTok{beta\_Y0 }\OtherTok{=} \FloatTok{0.3}
\NormalTok{beta\_L0 }\OtherTok{=} \FloatTok{0.2}
\NormalTok{beta\_U }\OtherTok{=} \FloatTok{0.1}

\CommentTok{\# Simulate treatment assignment}
\NormalTok{A\_1 }\OtherTok{\textless{}{-}} \FunctionTok{rbinom}\NormalTok{(n, }\DecValTok{1}\NormalTok{, }\AttributeTok{prob =} \FunctionTok{plogis}\NormalTok{(}\SpecialCharTok{{-}}\FloatTok{0.5} \SpecialCharTok{+} 
\NormalTok{                                    beta\_A0 }\SpecialCharTok{*}\NormalTok{ A\_0 }\SpecialCharTok{+}
\NormalTok{                                    beta\_Y0 }\SpecialCharTok{*}\NormalTok{ Y\_0 }\SpecialCharTok{+} 
\NormalTok{                                    beta\_L0 }\SpecialCharTok{*}\NormalTok{ L\_0 }\SpecialCharTok{+} 
\NormalTok{                                    beta\_U }\SpecialCharTok{*}\NormalTok{ U))}

\CommentTok{\# Coefficients for continuous outcome}
\NormalTok{delta\_A1 }\OtherTok{=} \FloatTok{0.3}
\NormalTok{delta\_Y0 }\OtherTok{=} \FloatTok{0.9}
\NormalTok{delta\_A0 }\OtherTok{=} \FloatTok{0.1}
\NormalTok{delta\_L0 }\OtherTok{=} \FloatTok{0.3}
\NormalTok{theta\_A0Y0L0 }\OtherTok{=} \FloatTok{0.5} \CommentTok{\# Interaction effect between A\_1 and L\_0}
\NormalTok{delta\_U }\OtherTok{=} \FloatTok{0.05}

\CommentTok{\# Simulate continuous outcome including interaction}
\NormalTok{Y\_2 }\OtherTok{\textless{}{-}} \FunctionTok{rnorm}\NormalTok{(n,}
             \AttributeTok{mean =} \DecValTok{0} \SpecialCharTok{+}
\NormalTok{               delta\_A1 }\SpecialCharTok{*}\NormalTok{ A\_1 }\SpecialCharTok{+} 
\NormalTok{               delta\_Y0 }\SpecialCharTok{*}\NormalTok{ Y\_0 }\SpecialCharTok{+} 
\NormalTok{               delta\_A0 }\SpecialCharTok{*}\NormalTok{ A\_0 }\SpecialCharTok{+} 
\NormalTok{               delta\_L0 }\SpecialCharTok{*}\NormalTok{ L\_0 }\SpecialCharTok{+} 
\NormalTok{               theta\_A0Y0L0 }\SpecialCharTok{*}\NormalTok{ Y\_0 }\SpecialCharTok{*} 
\NormalTok{               A\_0 }\SpecialCharTok{*}\NormalTok{ L\_0 }\SpecialCharTok{+} 
\NormalTok{               delta\_U }\SpecialCharTok{*}\NormalTok{ U,}
             \AttributeTok{sd =}\NormalTok{ .}\DecValTok{5}\NormalTok{)}

\CommentTok{\# Data frame}
\NormalTok{data }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(Y\_2, A\_0, A\_1, L\_0, Y\_0, U)}

\CommentTok{\# models}
\CommentTok{\# no control}
\NormalTok{fit\_no\_control }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(Y\_2 }\SpecialCharTok{\textasciitilde{}}\NormalTok{ A\_1, }\AttributeTok{data =}\NormalTok{ data)}
\CommentTok{\#summary(fit\_no\_control)}

\CommentTok{\# standard covariate control}
\NormalTok{fit\_standard }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(Y\_2 }\SpecialCharTok{\textasciitilde{}}\NormalTok{ A\_1 }\SpecialCharTok{+}\NormalTok{ L\_0, }\AttributeTok{data =}\NormalTok{ data)}
\CommentTok{\#summary(fit\_standard)}

\CommentTok{\# interaction}
\NormalTok{fit\_interaction  }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(Y\_2 }\SpecialCharTok{\textasciitilde{}}\NormalTok{ A\_1 }\SpecialCharTok{+}\NormalTok{ L\_0 }\SpecialCharTok{+}\NormalTok{ A\_0 }\SpecialCharTok{+}\NormalTok{ Y\_0 }\SpecialCharTok{+}\NormalTok{ A\_0}\SpecialCharTok{:}\NormalTok{L\_0}\SpecialCharTok{:}\NormalTok{Y\_0, }\AttributeTok{data =}\NormalTok{ data)}
\CommentTok{\#summary(fit\_interaction)}


\CommentTok{\# create gtsummary tables for each regression model}
\NormalTok{tbl\_fit\_no\_control}\OtherTok{\textless{}{-}} \FunctionTok{tbl\_regression}\NormalTok{(fit\_no\_control)  }
\NormalTok{tbl\_fit\_standard }\OtherTok{\textless{}{-}} \FunctionTok{tbl\_regression}\NormalTok{(fit\_standard)}
\NormalTok{tbl\_fit\_interaction }\OtherTok{\textless{}{-}}
  \FunctionTok{tbl\_regression}\NormalTok{(fit\_interaction)}


\CommentTok{\# get only the treatment variable}
\NormalTok{tbl\_list\_modified }\OtherTok{\textless{}{-}} \FunctionTok{lapply}\NormalTok{(}\FunctionTok{list}\NormalTok{(}
\NormalTok{  tbl\_fit\_no\_control,}
\NormalTok{  tbl\_fit\_standard,}
\NormalTok{  tbl\_fit\_interaction),}
\ControlFlowTok{function}\NormalTok{(tbl) \{}
\NormalTok{  tbl }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{modify\_table\_body}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\NormalTok{ .x }\SpecialCharTok{\%\textgreater{}\%}\NormalTok{ dplyr}\SpecialCharTok{::}\FunctionTok{filter}\NormalTok{(variable }\SpecialCharTok{==} \StringTok{"A\_1"}\NormalTok{))}
\NormalTok{\})}


\CommentTok{\# merge the tables}
\NormalTok{table\_comparison }\OtherTok{\textless{}{-}} \FunctionTok{tbl\_merge}\NormalTok{(}
  \AttributeTok{tbls =}\NormalTok{ tbl\_list\_modified,}
  \AttributeTok{tab\_spanner =} \FunctionTok{c}\NormalTok{(}
    \StringTok{"No Control"}\NormalTok{,}
    \StringTok{"Standard"}\NormalTok{,}
    \StringTok{"Interaction"}\NormalTok{)}
\NormalTok{) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{modify\_table\_styling}\NormalTok{(}
    \AttributeTok{column =} \FunctionTok{c}\NormalTok{(p.value\_1, p.value\_2, p.value\_3),}
    \AttributeTok{hide =} \ConstantTok{TRUE}
\NormalTok{  )}

\CommentTok{\#create latex table for publication}
\NormalTok{markdown\_table }\OtherTok{\textless{}{-}}
  \FunctionTok{as\_kable\_extra}\NormalTok{(table\_comparison, }\AttributeTok{format =} \StringTok{"latex"}\NormalTok{, }\AttributeTok{booktabs =} \ConstantTok{TRUE}\NormalTok{) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{kable\_styling}\NormalTok{(}\AttributeTok{latex\_options =} \StringTok{"scale\_down"}\NormalTok{)}
\CommentTok{\# print it}
\NormalTok{markdown\_table}
\end{Highlighting}
\end{Shaded}

\begin{table}
\centering
\resizebox{\linewidth}{!}{
\begin{tabular}{lcccccc}
\toprule
\multicolumn{1}{c}{ } & \multicolumn{2}{c}{No Control} & \multicolumn{2}{c}{Standard} & \multicolumn{2}{c}{Interaction} \\
\cmidrule(l{3pt}r{3pt}){2-3} \cmidrule(l{3pt}r{3pt}){4-5} \cmidrule(l{3pt}r{3pt}){6-7}
\textbf{Characteristic} & \textbf{Beta} & \textbf{95\% CI} & \textbf{Beta} & \textbf{95\% CI} & \textbf{Beta} & \textbf{95\% CI}\\
\midrule
A\_1 & 1.5 & 1.5, 1.6 & 0.86 & 0.80, 0.93 & 0.29 & 0.27, 0.31\\
\bottomrule
\multicolumn{7}{l}{\rule{0pt}{1em}\textsuperscript{1} CI = Confidence Interval}\\
\end{tabular}}
\end{table}

Next we simulate average treatment effect

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# use \textasciigrave{}clarify\textasciigrave{} package to obtain ATE}
\FunctionTok{library}\NormalTok{(clarify)}
\CommentTok{\# simulate fit 1 ATE}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}
\NormalTok{sim\_coefs\_fit\_std }\OtherTok{\textless{}{-}} \FunctionTok{sim}\NormalTok{(fit\_standard)}
\NormalTok{sim\_coefs\_fit\_int }\OtherTok{\textless{}{-}} \FunctionTok{sim}\NormalTok{(fit\_interaction)}

\CommentTok{\# marginal risk difference ATE, simulation{-}based: model 1 (L is a confounder)}
\NormalTok{sim\_est\_fit\_std }\OtherTok{\textless{}{-}}
  \FunctionTok{sim\_ame}\NormalTok{(}
\NormalTok{    sim\_coefs\_fit\_std,}
    \AttributeTok{var =} \StringTok{"A\_1"}\NormalTok{,}
    \AttributeTok{subset =}\NormalTok{ A\_1 }\SpecialCharTok{==} \DecValTok{1}\NormalTok{,}
    \AttributeTok{contrast =} \StringTok{"RD"}\NormalTok{,}
    \AttributeTok{verbose =} \ConstantTok{FALSE}
\NormalTok{  )}

\CommentTok{\# marginal risk difference ATE, simulation{-}based: model 2 (L is a mediator)}
\NormalTok{sim\_est\_fit\_int }\OtherTok{\textless{}{-}}
  \FunctionTok{sim\_ame}\NormalTok{(}
\NormalTok{    sim\_coefs\_fit\_int,}
    \AttributeTok{var =} \StringTok{"A\_1"}\NormalTok{,}
    \AttributeTok{subset =}\NormalTok{ A\_1 }\SpecialCharTok{==} \DecValTok{1}\NormalTok{,}
    \AttributeTok{contrast =} \StringTok{"RD"}\NormalTok{,}
    \AttributeTok{verbose =} \ConstantTok{FALSE}
\NormalTok{  )}

\CommentTok{\# obtain summaries}
\NormalTok{summary\_sim\_est\_fit\_std }\OtherTok{\textless{}{-}}
  \FunctionTok{summary}\NormalTok{(sim\_est\_fit\_std, }\AttributeTok{null =} \FunctionTok{c}\NormalTok{(}\StringTok{\textasciigrave{}}\AttributeTok{RD}\StringTok{\textasciigrave{}} \OtherTok{=} \DecValTok{0}\NormalTok{))}
\NormalTok{summary\_sim\_est\_fit\_int }\OtherTok{\textless{}{-}}
  \FunctionTok{summary}\NormalTok{(sim\_est\_fit\_int, }\AttributeTok{null =} \FunctionTok{c}\NormalTok{(}\StringTok{\textasciigrave{}}\AttributeTok{RD}\StringTok{\textasciigrave{}} \OtherTok{=} \DecValTok{0}\NormalTok{))}

\CommentTok{\# get coefficients for reporting}
\CommentTok{\# ate for fit 1, with 95\% CI}
\NormalTok{ATE\_fit\_std }\OtherTok{\textless{}{-}}\NormalTok{ glue}\SpecialCharTok{::}\FunctionTok{glue}\NormalTok{(}
  \StringTok{"ATE = \{round(summary\_sim\_est\_fit\_std[3, 1], 2)\}, }
\StringTok{  CI = [\{round(summary\_sim\_est\_fit\_std[3, 2], 2)\},}
\StringTok{  \{round(summary\_sim\_est\_fit\_std[3, 3], 2)\}]"}
\NormalTok{)}
\CommentTok{\# ate for fit 2, with 95\% CI}
\NormalTok{ATE\_fit\_int }\OtherTok{\textless{}{-}}
\NormalTok{  glue}\SpecialCharTok{::}\FunctionTok{glue}\NormalTok{(}
    \StringTok{"ATE = \{round(summary\_sim\_est\_fit\_int[3, 1], 2)\},}
\StringTok{    CI = [\{round(summary\_sim\_est\_fit\_int[3, 2], 2)\},}
\StringTok{    \{round(summary\_sim\_est\_fit\_int[3, 3], 2)\}]"}
\NormalTok{  )}
\CommentTok{\# ATE\_fit\_std}
\CommentTok{\# ATE\_fit\_int}
\end{Highlighting}
\end{Shaded}

Using the \texttt{clarify} package, we infer the ATE for the standard
model is ATE = 0.86, CI = {[}0.8, 0.93{]}.

Using the \texttt{clarify} package, we infer the ATE for the model that
conditions on the baseline exposure and baseline outcome to be: ATE =
0.29, CI = {[}0.27, 0.31{]}, which is close to the values supplied to
the data-generating mechanism.

Because the baseline exposure and baseline outcome are often the most
important variables to include when estimating an incident effect, we
should endevour to obtain at least three waves of data such that these
variables along with other baseline confounders are included in at time
0, the exposure is included at time 1, and the outcome is included at
time 2.

\subsection{Appendix E: Non-parametric Estimation of Average Treatment
Effects Using Causal
Forests}\label{appendix-e-non-parametric-estimation-of-average-treatment-effects-using-causal-forests}

Semi-parametric and non-parametric estimators have many advantages, most
especially the ability to fit non-restrictive statistical models. Here
we include an example of such a model using the causal forest package.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# load causal forest library }
\FunctionTok{library}\NormalTok{(grf) }\CommentTok{\# estimate conditional and average treatment effects}
\FunctionTok{library}\NormalTok{(glue) }\CommentTok{\# reporting }

\CommentTok{\#  \textquotesingle{}data\textquotesingle{} is our data frame with columns \textquotesingle{}A\_1\textquotesingle{} for treatment, \textquotesingle{}L\_0\textquotesingle{} for a covariate, and \textquotesingle{}Y\_2\textquotesingle{} for the outcome}
\CommentTok{\#  we also have the baseline exposure \textquotesingle{}A\_0\textquotesingle{} and \textquotesingle{}Y\_0\textquotesingle{}}
\CommentTok{\#  ensure W (treatment) and Y (outcome) are vectors}
\NormalTok{W }\OtherTok{\textless{}{-}} \FunctionTok{as.matrix}\NormalTok{(data}\SpecialCharTok{$}\NormalTok{A\_1)  }\CommentTok{\# Treatment}
\NormalTok{Y }\OtherTok{\textless{}{-}} \FunctionTok{as.matrix}\NormalTok{(data}\SpecialCharTok{$}\NormalTok{Y\_2)  }\CommentTok{\# Outcome}
\NormalTok{X }\OtherTok{\textless{}{-}} \FunctionTok{as.matrix}\NormalTok{(data[, }\FunctionTok{c}\NormalTok{(}\StringTok{"L\_0"}\NormalTok{, }\StringTok{"A\_0"}\NormalTok{, }\StringTok{"Y\_0"}\NormalTok{)])}

\CommentTok{\# fit causal forest model }
\NormalTok{fit\_causal\_forest }\OtherTok{\textless{}{-}} \FunctionTok{causal\_forest}\NormalTok{(X, Y, W)}

\CommentTok{\# estimate the average treatment effect (ATE)}
\NormalTok{ate }\OtherTok{\textless{}{-}} \FunctionTok{average\_treatment\_effect}\NormalTok{(fit\_causal\_forest)}

\CommentTok{\# make data frame for reporting using "glue\textquotesingle{} }
\NormalTok{ate}\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(ate)}

\CommentTok{\# obtain ate for report}
\NormalTok{ATE\_fit\_causal\_forest }\OtherTok{\textless{}{-}}
\NormalTok{  glue}\SpecialCharTok{::}\FunctionTok{glue}\NormalTok{(}
    \StringTok{"ATE = \{round(ate[1, 1], 2)\}, se = \{round(ate[2, 1], 2)\}"}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

Causal forest estimates the average treatment effect as ATE = 0.3, se =
0.01. This converges to to the true value supplied to the generating
mechanism of 0.3



\end{document}
