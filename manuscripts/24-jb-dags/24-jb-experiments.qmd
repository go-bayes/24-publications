---
title: 'Confounding in Experiments'
abstract: |
  Confounding bias arises when a treatment and outcome share a common cause. In randomised controlled trials (experiments), treatment assignment is random. It would appear there can be no confounding bias in experiments. Here, we use causal directed acyclic graphs to clarify seven structural sources of bias in randomised controlled trials and demonstrate the interest of causal inference methods for the design and analysis of experiments.
  
  **Keywords:** *Causal Inference*; *Experiments*; *DAGs*; *Evolution*; *Per Protocol Effect*; *Intention to Treat Effect*
author: 
  - name: Joseph A. Bulbulia
    affiliation: Victoria University of Wellington, New Zealand
    orcid: 0000-0002-5861-2056
    email: joseph.bulbulia@vuw.ac.nz
    corresponding: no
editor_options: 
  chunk_output_type: console
format:
  pdf:
    sanitise: true
    keep-tex: true
    link-citations: true
    colorlinks: true
    documentclass: article
    classoption: [single column]
    lof: false
    lot: false
    geometry:
      - top=30mm
      - left=25mm
      - heightrounded
      - headsep=22pt
      - headheight=11pt
      - footskip=33pt
      - ignorehead
      - ignorefoot
    template-partials: 
      - /Users/joseph/GIT/templates/quarto/title.tex
    header-includes:
      - \input{/Users/joseph/GIT/latex/latex-for-quarto.tex}
date: last-modified
execute:
  echo: false
  warning: false
  include: true
  eval: true
fontfamily: libertinus
bibliography: /Users/joseph/GIT/templates/bib/references.bib
csl: /Users/joseph/GIT/templates/csl/camb-a.csl
---


```{r}
#| label: load-libraries
#| echo: false
#| include: false
#| eval: true

## WARNING SET THIS PATH TO YOUR DATA ON YOUR SECURE MACHINE. 
# pull_path <-
#   fs::path_expand(
#     #'/Users/joseph/v-project\ Dropbox/Joseph\ Bulbulia/00Bulbulia\ Pubs/DATA/nzavs_refactor/nzavs_data_23'
#     '/Users/joseph/Library/CloudStorage/Dropbox-v-project/Joseph\ Bulbulia/00Bulbulia\ Pubs/DATA/nzavs-current/r-data/nzavs_data_qs'
#   )
# 


push_mods <-  fs::path_expand(
  '/Users/joseph/Library/CloudStorage/Dropbox-v-project/data/nzvs_mods/24/church-prosocial-v7'
)


#tinytext::tlmgr_update()

# WARNING:  COMMENT THIS OUT. JB DOES THIS FOR WORKING WITHOUT WIFI
#source('/Users/joseph/GIT/templates/functions/libs2.R')
# # WARNING:  COMMENT THIS OUT. JB DOES THIS FOR WORKING WITHOUT WIFI
# source('/Users/joseph/GIT/templates/functions/funs.R')

#ALERT: UNCOMMENT THIS AND DOWNLOAD THE FUNCTIONS FROM JB's GITHUB

# source(
#   'https://raw.githubusercontent.com/go-bayes/templates/main/functions/experimental_funs.R'
# )
# 
# source(
#   'https://raw.githubusercontent.com/go-bayes/templates/main/functions/funs.R'
# )

# check path:is this correct?  check so you know you are not overwriting other directors
#push_mods

# for latex graphs
# for making graphs
library('tinytex')
library('extrafont')
library('tidyverse')
library('kableExtra')
#devtools::install_github('go-bayes/margot')
library(margot)
loadfonts(device = 'all')
```



## Introduction

Here, we consider seven structural sources of bias in randomised controlled trials, henceforth 'experiments.' The examples are not exhaustive. We do not consider biases from selection into the study, which may threaten generalisation, nor do we consider censoring biases from study attrition, where population characteristics of the restricted sample at the end of the study differ from those at baseline concerning treatment effects. Both types of sample/target population issues can invalidate experimental results. However, such biases present as failures in external validity. Our focus here will be on threats to internal validity from confounding that relate the treatment to the outcome in the absence of confounding. We will assume large samples such that random differences in the distribution of variables that may affect treatment do not come into play. We will also assume that the experimental designs are double-blind, that treatment conditions are the same across all arms, and that the investigators are careful and scrupulous. No chance event, other than randomisation, is left to chance.

'Doesn't randomisation, by its very nature, eliminate all systematic causes of treatment assignment and so of treatment assignment and outcome?'

We assume the answer is yes.

'Doesn't this mean that confounding is ruled out?'

The answer is no. Eight examples illustrate why. Understanding how confounding arises in experiments is important for experimental design, data analysis, and inference. However, the examples clarify problems of general interest about how causality operates over time, confounding where the treatment affects attrition, non-compliance, and non-response, and differences between the effects of randomisation (intention-to-treat effects) and the causal effects of treatments themselves (per-protocol effects).

We begin by defining our terms ([Appendix A](#id-app-a) provides a glossary of terms in causal inference.)

### Terminology

- **Confounding**: Treatment and outcome are associated independently of causality or are disassociated in the presence of causality relative to the causal question at hand.

- **Intention-to-Treat Effect (equivalent to 'intent-to-treat effect')**: The effect of treatment assignment, analysed based on initial treatment assignment, reflecting real-world effectiveness but possibly obscuring mechanisms.

- **Per-protocol effect**: The effect of adherence to a randomly assigned treatment if adherence were perfect [@hernan2017per]. We have no guarantee that the intention-to-treat effect will be the same as the per-protocol effect. A safe assumption is that:

$$
\widehat{ATE}_{\text{target}}^{\text{Per-Protocol}} \ne \widehat{ATE}_{\text{target}}^{\text{Intention-to-Treat}}
$$

When evaluating evidence for causality, in addition to specifying their causal contrast, effect measure, and target population, investigators should specify whether they are estimating an intention-to-treat or per-protocol effect [@hernÃ¡n2004; @tripepi2007].

### Meaning of Symbols

We use the following conventions in our directed acyclic graphs:

- **Node**: A node or vertex represents characteristics or features of units within a population on a causal diagram -- that is, a 'variable.' In causal directed acyclic graphs, we draw nodes with respect to the *target population*, which is the population for whom investigators seek causal inferences [@suzuki2020]. A time-indexed node $X_t$ denotes relative chronology; $X_{\phi t}$ indicates assumed timing, possibly erroneous. 

- **Edge without an Arrow** ($\association$): Path of association, causality not asserted.

- **Red Edge without an Arrow** ($\associationred$): Confounding path: ignores arrows to clarify statistical dependencies. 

- **Arrow** ($\rightarrowNEW$): Denotes a causal relationship from the node at the base of the arrow (a parent) to the node at the tip of the arrow (a child). We typically refrain from drawing an arrow from treatment to outcome to avoid asserting a causal path from $A$ to $Y$ because the function of a causal directed acyclic graph is to evaluate whether causality can be identified for this path.

- **Red Arrow** ($\rightarrowred$): Path of non-causal association between the treatment and outcome. The path is associational and may run against arrows.

- **Dashed Arrow** ($\rightarrowdotted$): Denotes a true association between the treatment and outcome that becomes partially obscured when conditioning on a mediator, assuming $A$ causes $Y$.

- **Dashed Red Arrow** ($\rightarrowdottedred$): Highlights over-conditioning bias from conditioning on a mediator.

- **Boxed Variable** $\boxed{X}$: Conditioning or adjustment for $X$. 

- **Red-Boxed Variable** $\boxedred{X}$: Highlights the source of confounding bias from adjustment.

- **Dashed Circle** $\circledotted{X}$: Indicates no adjustment is made for a variable (implied for unmeasured confounders).

- **$\mathcal{R} \rightarrow A$**: Randomisation into the treatment condition.


### Review of d-separation for Causal Identification on a Graph


::: {#tbl-fiveelementary}

```{=latex}
\terminologydirectedgraph
```
The five elementary structures of causality from which all causal directed acyclic graphs can be built.
:::

In the 1990s, Judea Pearl demonstrated that causal dependencies could be evaluated using observable probability distributions [@pearl1995; @pearl2009a]. He also showed that causal directed acyclic graphs (causal DAGs) could be employed to clarify the conditional dependencies among variables [@pearl1995]. This means that, based on assumptions about causal structure, investigators could investigate strategies for identifying causal effects from the joint distributions of observed data. 

The graphical rules that Pearl developed and proved are known as the rules of d-separation [@pearl1995]. 

\begin{enumerate}[a)]
     \item  {\bf Fork rule} ($B \leftarrowNEW \boxed{A} \rightarrowNEW C$): $B$ and $C$ are independent when conditioning on $A$: ($B \coprod C \mid A$).
     \item  {\bf Chain rule} ($A \rightarrowNEW \boxed{B} \rightarrowNEW C$): Conditioning on $B$ blocks the path between $A$ and $C$: ($A \coprod C \mid B$).
     \item  {\bf Collider rule} ($A \rightarrowNEW \boxed{C} \leftarrowNEW B$): $A$ and $B$ are independent until conditioning on $C$, which introduces dependence: ($A \cancel{\coprod} B \mid C$). 
 \end{enumerate}

The rules of d-separation give rise to the backdoor criterion and 'backdoor adjustment' theorem, which provide identification algorithms conditional on the structural assumptions encoded in a causal directed acyclic graph [@pearl1995]. Here, we use the symbol $\mathcal{G}$ to name a graph, which we will identify by referring to a row in a table.

::: {#tbl-terminologygeneral}
```{=latex}
\terminologydirectedgraph
```
Elements of Causal Graphs 
:::

Consider @tbl-terminologygeneral $\mathcal{G}_1$: If we assume that $A$ and $B$ are not causally related, and further that they do not share common causes, then $A$ and $B$ will not be statistically related.

Consider @tbl-terminologygeneral $\mathcal{G}_2$: If we assume that $A$ and $B$ are causally related, that they do not share common causes or that their common causes have been accounted for, then $A$ and $B$ will be statistically related.

Consider @tbl-terminologygeneral $\mathcal{G}_3$: If we assume that $A$ causes $B$ and that $A$ causes $C$, then the rules of d-separation imply that we may condition on or 'control for' $A$ to consistently estimate the effect of $B$ on $C$.

Consider @tbl-terminologygeneral $\mathcal{G}_4$: If we assume that $A$ causes $B$ and that $B$ causes $C$, then the rules of d-separation imply that if we condition on $B$, the true causal effect of $A$ on $C$ will be obscured such that $A$ will be independent of $C$ despite being causally associated with $C$.

Finally, consider @tbl-terminologygeneral $\mathcal{G}_5$: If we assume that $A$ causes $C$ and that $B$ causes $C$, then the rules of d-separation imply that if we condition on $C$, the variables $A$ and $B$ will be associated, despite having no causal effect on each other.

If we assume that the variables encoded in the graph correspond to 'Structural Causal Models' then all causal relationships can be defined by the elementary structures presented in @tbl-terminologygeneral.

Now that we have clarified how causal directed graphs work, we may use them to clarify the first concept we consider confounding bias in randomised experiments.


## Eight Examples of Confounding Bias in Experiments

::: {#tbl-terminologyelconfoundersexperiments}
```{=latex}
\terminologyelconfoundersexperiments
```
Eight confounding biases in Randomised Controlled Trials.
:::

### Example 1: Post-treatment Adjustment Blocks Treatment Effect

@tbl-terminologyelconfoundersexperiments $\mathcal{G}_{1.1}$ illustrates the threat of confounding bias by conditioning on a post-treatment mediator. Imagine investigators are interested in whether the framing of an authority as religious or secular -- 'source framing' -- affects subjective ratings of confidence in the authority -- 'source credibility.' There are two conditions. A claim is presented from an authority. The content of the claim does not vary by condition. Participants are asked to rate the claim on a credibility scale. Next, imagine that the investigators decide they should control for religiosity. Furthermore, imagine there is a true effect of source framing. Finally, assume that the source framing not only affects source credibility but also affects religiosity. Perhaps viewing a religious authority makes religious people more religious. In this scenario, measuring religiosity following the experimental intervention will partially block the effect of the treatment on the outcome. It might make it appear that the treatment does not work for religious people, when in reality it works because it amplifies religiosity. (Note that in this graph we assume that $L_1$ occurs before $Y_2$, however investigators may have measured $L_1$ after $Y_2$. Our time index pertains to the occurrence of the event, not of its measurement. This statement applies to all examples that follow.)

@tbl-terminologyelconfoundersexperiments $\mathcal{G}_{1.2}$ clarifies a response: do not control post-treatment variables, here the intermediary effects of 'religiosity'. If effect-modification by religiosity is scientifically interesting, measure religiosity before randomisation. Randomisation did not prevent confounding.

### Example 2: Post-treatment Adjustment Induces Collider Stratification Bias

@tbl-terminologyelconfoundersexperiments $\mathcal{G}_{2.1}$ illustrates the threat of confounding bias by conditioning on a post-treatment collider. Imagine the same experiment as in Example 1 and the same conditioning strategy, where religiosity is measured following the treatment. We assume the treatment affects religiosity. However, in this example, religiosity has no causal effect on the outcome, source credibility. Finally, imagine an unmeasured variable affects both the mediator, religiosity ($L_1$), and the outcome, source credibility ($Y_2$). This unmeasured confounder might be religious education in childhood. In this scenario, conditioning on the post-treatment variable religiosity will open a backdoor path between the treatment and outcome, leading to an association in the absence of causation. Randomisation did not prevent confounding.

@tbl-terminologyelconfoundersexperiments $\mathcal{G}_{2.2}$ clarifies a response: do not control post-treatment variables.

The point that investigators should not condition on post-treatment variables is worth developing using a common flaw in experimental designs: exclusion from 'attention checks.' Consider that if an experimental condition affects attention and an unmeasured variable is a common cause of attention and the outcome, then selection on attention will induce confounding bias in a randomised experiment. For example, imagine that people are more attentive in the scientific authority design because science is interesting -- whether or not one is religious, yet religion is not interesting whether or not one is religious. Suppose further that an unmeasured 'altruistic disposition' affects both attention and ratings of source credibility. By selecting on attention, investigators may unwittingly destroy randomisation. If attention is a scientifically interesting effect modifier, it should be measured before random assignment to treatment.


### Example 3: Demographic Measures at End of Study Induce Collider Stratification Bias

@tbl-terminologyelconfoundersexperiments $\mathcal{G}_{3.1}$ illustrates the threat of confounding bias from adjusting for post-treatment variables, here, one affected by the treatment and outcome absent any unmeasured confounder. In our example, imagine both the treatment, source framing, and the outcome, source credibility, affect religiosity measured at the end of the study. Investigators measure religiosity at the end of the study and include this measure as a covariate. However, doing so induces collider bias such that if both the treatment and outcome are positively associated with religiosity, the collider, they will be negatively associated with each other. Conditioning on the collider risks the illusion of a negative experimental effect in the absence of causality.

@tbl-terminologyelconfoundersexperiments $\mathcal{G}_{3.2}$ clarifies a response: again, do not control post-treatment variables! Here, 'religiosity' measured after the end of the study. If the scientific interest is in effect modification or obtaining statistical precision, measure covariates before randomisation.

### Example 4: Demographic Measures at End of Study Condition on a Collider That Opens a Backdoor Path

@tbl-terminologyelconfoundersexperiments $\mathcal{G}_{4.1}$ illustrates the threat of confounding bias by adjusting for post-treatment variables, here affected only by the treatment and an unmeasured cause of the outcome. Suppose source credibility affects religiosity (religious people are reminded of their faith), but there is no experimental effect of framing on credibility. Imagine further that there is an unmeasured common cause of the covariate religiosity and the outcome source credibility. This unmeasured confounder might be religious education in childhood. In this scenario, conditioning on the post-treatment variable religiosity will open a backdoor path between the treatment and outcome, leading to an association in the absence of causation. Again, we find that randomisation did not prevent confounding.

@tbl-terminologyelconfoundersexperiments $\mathcal{G}_{4.2}$ clarifies a response. Again, unless investigators can rule out an effect of treatment, they should not condition on a post-treatment covariate. The covariates of interest should be measured before randomisation.

### Example 5: Treatment Affects Attrition Biasing Measure of Outcome

@tbl-terminologyelconfoundersexperiments $\mathcal{G}_{5}$ Suppose that the experimental condition affects measurement error in self-reported source credibility $U_{\Delta Y}$. For example, suppose that source framing has no effect on credibility. However, those in the scientific authority condition are more likely to express credibility for science due to self-presentation bias. Likewise, perceiving the investigators to be irreligious, participants in the religious source framing condition might report less credibility for religious authorities than they secretly harbour. Directed measurement error from the treatment to the measurement error of the outcomes creates an association in the absence of true causality, which we denote by removing any arrow between the treatment $A$ and the true outcome $Y$.

@tbl-terminologyelconfoundersexperiments $\mathcal{G}_{5}$ reveals there is no easy solution to directed measurement error bias. If the magnitude of bias were known, investigators could apply adjustments. Additional experiments might be devised that are insensitive to directed measurement error bias. Investigators might compute sensitivity analyses to examine how much measurement error bias would be required to explain away a result (refer to @linden2020EVALUE for relatively easy-to-implement sensitivity analysis). The point we make here is that randomisation does not prevent confounding by directed measurement error bias. Investigators must be vigilant.


### Example 6: Per Protocol Effect Lost in Sustained Treatments Where Treatment Adherence Is Affected by a Measured Confounder

Setting aside self-inflicted injuries of post-treatment conditioning and directed measurement error, randomisation recovers unbiased causal effect estimates for randomisation into treatment. Under perfect adherence, these effect estimates correspond to the causal effects of the treatments themselves. However, adherence is seldom perfect. The following examples reveal challenges for recovering per-protocol effects in settings where there is imperfect adherence. @tbl-terminologyelconfoundersexperiments $\mathcal{G}_{6-8}$ are adapted from @hernan2017per.

@tbl-terminologyelconfoundersexperiments $\mathcal{G}_{6}$ illustrates the threat for identifying the per-protocol effect in sustained treatments where treatment adherence is affected by a measured confounder. Consider a sequential experiment that investigates the effects of sustained adherence to yoga on psychological distress, measured at the end of the study. Suppose that inflexible people are less likely to adhere to the protocols set out in the experiment and therefore do not. Suppose that flexibility is measured by indicator $L$. If we do not condition on $L$, there is an open path from $A_1 \associationred L_0 \associationred U \associationred Y_2$. Although investigators may recover the effect of randomisation into treatment, the per-protocol effect is confounded.

@tbl-terminologyelconfoundersexperiments $\mathcal{G}_{6}$ also clarifies a response. Conditioning on $L_0$ and $L_1$ will block the backdoor path, leading to an unbiased per-protocol effect estimate.

### Example 7: Per protocol effect lost in sustained treatments where past treatments affect measured confounder of future treatment adherence


@tbl-terminologyelconfoundersexperiments $\mathcal{G}_{7}$ illustrates the threat for identifying the per-protocol effect in sustained treatments where past treatments affect measured confounder of future treatment adherence.  Suppose that yoga affects flexibility. We should condition on pre-treatment measures of flexibility to identify the per-protocal effect. However, conditioning on the post-treatment measure of flexibilty, $\boxed{L_1}$ induces collider stratification bias. This path runs from $A_1 \associationred L_1 \associationred U \associationred Y_3$. However, if we do not condition on $L_1$ there is an open backdoor path from $A_1 \associationred U \associationred Y_3$.  We cannot estimate a per-protocal effect by conditioning strategies.  


@tbl-terminologyelconfoundersexperiments $\mathcal{G}_{7}$ does not clarify the response.  However, in a sequential treatment with fixed strategies, in which there is sequential exchangeability -- or no unmeasured confounding at each time point -- valid estimators for the sequential treatments may be constructed (refer to @hernan2024WHATIF; @diaz2021nonparametric; @hoffman2023). Although we may niavely obtain an intention-to-treat effect estimate withouth special methods, infering an effect of doing yoga on well-being -- the per-protocal effect, requires special methods. These methods are not routinely used in the human sciences. 


### Example 8: Per Protocol Effect Lost in Sustained Treatments Because Both Measured and Unmeasured Confounders Affect Treatment Adherence

@tbl-terminologyelconfoundersexperiments $\mathcal{G}_{8}$ illustrates the threat for identifying the per-protocol effect in sustained treatments where there are both measured and unmeasured confounders. Suppose flexibility affects adherence, yoga affects flexibility, and an unmeasured variable, such as prejudice toward eastern spiritual practices, affects adherence. We have no measures for this variable. There is unmeasured confounding.

If there were no effect of yoga on well-being except through flexibility, and if flexibility were not affected by the unmeasured antipathy toward eastern spiritual practices, and further, if the effect of flexibility on yoga at each time point were conditionally independent of all future counterfactual data, both for the treatments and the outcomes, then it might be possible to construct special estimators that identify the per-protocol effect of yoga on well-being in the presence of unmeasured confounding that affects adherence (refer to @hernan2017per). Clearly, we have come a long way from the ANOVAs routinely deployed in experimental studies. However, if we seek to understand the effect of yoga on well-being and not the effect of random assignment to yoga on well-being, we require special estimators.


## Conclusions

The examples we have considered here hardly exhaust threats to causal inference in experiments. Whenever the sample at the end of a study differs in the distribution of effect modifiers from the sample population at the start, results will not generalise as we hope. Such bias goes by different names, such as selection bias or target-restriction bias (refer to @yola). We have not considered these threats here. However, I hope the eight examples presented persuade investigators of the following:

First, confounding biases are possible in randomised experiments even when randomisation succeeds.

Second, causal directed acyclic graphs are useful for clarifying these biases.

Third, many such biases are self-inflicted, in the sense that it is easy to destroy the benefits of randomisation through poor research designs. These self-inflicted biases arise from conditioning on variables that may be affected by treatment assignment. By the same token, the remedy to self-inflicted injury is easily applied. Do not injury yourself. If an experiment consists of a single treatment, unless you are certain that treatments do not affect covariates, covariate data should be collected before randomisation.

Fourth, 'attention checks' should not be used to select participants after treatments have been randomised. If attention is a relevant covariate, measures should be taken before randomisation.

Fifth, investigators should not adopt naive practices of inferring per-protocol effects from the portion of the sample that has followed experimental protocols. Not only is such selection nearly guaranteed to result in differences between the study population at the start and end of the study, compromising external validity [@yola], but we have also considered that both measured and unmeasured confounders may invalidate per-protocol results for the retained sample.

Sixth, methods for identifying causal effects in observational settings may be useful for identifying causal effects in randomised experiments because, after randomisation, every experiment becomes an observational study.

Seventh, the points that we consider here for experiments apply to observational studies that have obtained pseudorandomisation through baseline adjustments. Standardly employed methods in observational science, such as structural equation models or multi-level models, will encounter the same problems that arise in experimental settings with sustained treatment strategies. Sustained treatment strategies require sequential randomisation (refer to @richardson2013; @young2014identification;  @yola). 

Here, we have demonstrated that satisfying the assumptions for valid causal inferences is typically much more challenging than many experimental human scientists presently understand. Methods for causal inference have wide scope for improving the design, analysis, and interpretation of experimental research.



{{< pagebreak >}}

## Funding

This work is supported by a grant from the Templeton Religion Trust (TRT0418) and RSNZ Marsden 3721245, 20-UOA-123; RSNZ 19-UOO-090. I also received support from the Max Planck Institute for the Science of Human History. The Funders had no role in preparing the manuscript or the decision to publish it.



{{< pagebreak >}}


## References

::: {#refs}
:::


{{< pagebreak >}}

## Appendix A: Glossary {#id-app-a}


::: {#tbl-experiments}
```{=latex}
\glossaryTerms
```
Glossary
:::



