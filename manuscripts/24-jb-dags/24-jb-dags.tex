% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames,x11names}{xcolor}
%
\documentclass[
  single column]{article}

\usepackage{amsmath,amssymb}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage[]{libertinus}
\ifPDFTeX\else  
    % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\usepackage[top=30mm,left=25mm,heightrounded,headsep=22pt,headheight=11pt,footskip=33pt,ignorehead,ignorefoot]{geometry}
\setlength{\emergencystretch}{3em} % prevent overfull lines
\setcounter{secnumdepth}{-\maxdimen} % remove section numbering
% Make \paragraph and \subparagraph free-standing
\makeatletter
\ifx\paragraph\undefined\else
  \let\oldparagraph\paragraph
  \renewcommand{\paragraph}{
    \@ifstar
      \xxxParagraphStar
      \xxxParagraphNoStar
  }
  \newcommand{\xxxParagraphStar}[1]{\oldparagraph*{#1}\mbox{}}
  \newcommand{\xxxParagraphNoStar}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
  \let\oldsubparagraph\subparagraph
  \renewcommand{\subparagraph}{
    \@ifstar
      \xxxSubParagraphStar
      \xxxSubParagraphNoStar
  }
  \newcommand{\xxxSubParagraphStar}[1]{\oldsubparagraph*{#1}\mbox{}}
  \newcommand{\xxxSubParagraphNoStar}[1]{\oldsubparagraph{#1}\mbox{}}
\fi
\makeatother


\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
% definitions for citeproc citations
\NewDocumentCommand\citeproctext{}{}
\NewDocumentCommand\citeproc{mm}{%
  \begingroup\def\citeproctext{#2}\cite{#1}\endgroup}
\makeatletter
 % allow citations to break across lines
 \let\@cite@ofmt\@firstofone
 % avoid brackets around text for \cite:
 \def\@biblabel#1{}
 \def\@cite#1#2{{#1\if@tempswa , #2\fi}}
\makeatother
\newlength{\cslhangindent}
\setlength{\cslhangindent}{1.5em}
\newlength{\csllabelwidth}
\setlength{\csllabelwidth}{3em}
\newenvironment{CSLReferences}[2] % #1 hanging-indent, #2 entry-spacing
 {\begin{list}{}{%
  \setlength{\itemindent}{0pt}
  \setlength{\leftmargin}{0pt}
  \setlength{\parsep}{0pt}
  % turn on hanging indent if param 1 is 1
  \ifodd #1
   \setlength{\leftmargin}{\cslhangindent}
   \setlength{\itemindent}{-1\cslhangindent}
  \fi
  % set entry spacing
  \setlength{\itemsep}{#2\baselineskip}}}
 {\end{list}}
\usepackage{calc}
\newcommand{\CSLBlock}[1]{\hfill\break\parbox[t]{\linewidth}{\strut\ignorespaces#1\strut}}
\newcommand{\CSLLeftMargin}[1]{\parbox[t]{\csllabelwidth}{\strut#1\strut}}
\newcommand{\CSLRightInline}[1]{\parbox[t]{\linewidth - \csllabelwidth}{\strut#1\strut}}
\newcommand{\CSLIndent}[1]{\hspace{\cslhangindent}#1}

\usepackage{booktabs}
\usepackage{longtable}
\usepackage{array}
\usepackage{multirow}
\usepackage{wrapfig}
\usepackage{float}
\usepackage{colortbl}
\usepackage{pdflscape}
\usepackage{tabu}
\usepackage{threeparttable}
\usepackage{threeparttablex}
\usepackage[normalem]{ulem}
\usepackage{makecell}
\usepackage{xcolor}
\input{/Users/joseph/GIT/latex/latex-for-quarto.tex}
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\AtBeginDocument{%
\ifdefined\contentsname
  \renewcommand*\contentsname{Table of contents}
\else
  \newcommand\contentsname{Table of contents}
\fi
\ifdefined\listfigurename
  \renewcommand*\listfigurename{List of Figures}
\else
  \newcommand\listfigurename{List of Figures}
\fi
\ifdefined\listtablename
  \renewcommand*\listtablename{List of Tables}
\else
  \newcommand\listtablename{List of Tables}
\fi
\ifdefined\figurename
  \renewcommand*\figurename{Figure}
\else
  \newcommand\figurename{Figure}
\fi
\ifdefined\tablename
  \renewcommand*\tablename{Table}
\else
  \newcommand\tablename{Table}
\fi
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Listing}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}
\makeatother
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\@ifpackageloaded{subcaption}{}{\usepackage{subcaption}}
\makeatother
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\usepackage{bookmark}

\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same} % disable monospaced font for URLs
\hypersetup{
  pdftitle={Causal Directed Acyclic Graphs (DAGs): A Practical Guide},
  pdfauthor={Joseph A. Bulbulia},
  colorlinks=true,
  linkcolor={blue},
  filecolor={Maroon},
  citecolor={Blue},
  urlcolor={Blue},
  pdfcreator={LaTeX via pandoc}}

\title{Causal Directed Acyclic Graphs (DAGs): A Practical Guide}

\usepackage{academicons}
\usepackage{xcolor}

  \author{Joseph A. Bulbulia}
            \affil{%
             \small{     Victoria University of Wellington, New Zealand
          ORCID \textcolor[HTML]{A6CE39}{\aiOrcid} ~0000-0002-5861-2056 }
              }
      


\date{2024-05-27}
\begin{document}
\maketitle
\begin{abstract}
Causal inference requires contrasting counterfactual states of the world
under pre-specified interventions. Obtaining counterfactual contrasts
from data relies on explicit assumptions and careful, multi-step
workflows. Causal diagrams are powerful tools for clarifying whether and
how the counterfactual contrasts we seek can be identified from data.
Here, I explain how to use causal directed acyclic graphs (causal DAGs)
to determine whether and how causal effects can be identified from
`real-world' non-experimental observational data. I offer practical tips
for reporting and suggest ways to avoid common pitfalls.

\textbf{KEYWORDS}: \emph{Causal Inference}; \emph{Culture}; \emph{DAGs};
\emph{Evolution}; \emph{Human Sciences}; \emph{Longitudinal}
\end{abstract}

\subsection{Introduction}\label{id-sec-introduction}

Human research begins with two fundamental questions:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  What do I want to know?
\item
  For which population does this knowledge generalise?
\end{enumerate}

In the human sciences, our questions are typically causal. We aim to
understand the effects of interventions on certain variables. However,
many researchers report non-causal associations, collecting data,
applying complex regressions, and reporting coefficients. We often speak
of covariates as `predicting' outcomes. Yet, even when our models
predict well, it remains unclear how these predictions relate to the
scientific questions that sparked our interest. We fail to recognise
that these `oracles' lack coherent interpretation.

Some say that association cannot imply causation. However, our
experimental traditions reveal that when interventions are controlled
and randomised, the coefficients we recover from statistical models can
permit causal interpretations.

Despite familiarity with experimental protocols, many researchers
struggle to emulate randomisation and control with non-experimental or
`real-world' data. Though many use terms like `control' and employ
sophisticated adjustment strategies---such as multilevel modelling and
structural equation models---these practices are not systematic. We
often overlook that what we take as control can undermine our ability to
consistently estimate causal effects. Although the term `crisis' is
overused, the state of causal inference across many human sciences,
including experimental sciences, has much headroom for improvement.
Moreover, poor experimental designs unintentionally weaken causal
claims; causal inferences in experiments also deserve greater attention.
Fortunately, recent decades have seen progress in the health sciences,
economics, and computer science that has markedly improved our ability
to obtain causal inferences in scientific research. This work
demonstrates that obtaining causal inferences from data is feasible but
requires careful, systematic workflows.

Within the workflows of causal inference, causal diagrams---or causal
graphs---are powerful tools for evaluating whether and how causal
effects can be identified from data. My purpose here is to explain where
these tools fit within causal inference workflows and to illustrate
their practical applications. I focus on causal directed acyclic graphs
because they are relatively easy to use and optimally clear for most
applications. However, causal directed acyclic graphs can also be
misused. I will also explain common pitfalls and how to avoid them.

In \hyperref[id-sec-1]{Part 1}, I review the conceptual foundations of
causal inference. The basis of all causal inference lies in
counterfactual contrasts. Although there are different philosophical
approaches to counterfactual reasoning, they are largely similar in
practice. The overview here builds on the Neyman-Rubin potential
outcomes framework, extended for longitudinal data by epidemiologist
James Robins. Although this is not the framework within which causal
directed acyclic graphs were developed, the potential outcomes framework
is easier to interpret. (I discuss Pearl's non-parametric structural
equation approach in \hyperref[id-app-d]{Appendix D}).

In \hyperref[id-sec-2]{Part 2}, I describe how causal directed acyclic
graphs help identify causal effects. I outline five elementary graphical
structures that encode all causal relations, forming the building blocks
of all causal directed acyclic graphs. I then examine five rules that
clarify whether and how investigators can identify causal effects from
data.

In \hyperref[id-sec-3]{Part 3}, I apply causal directed acyclic graphs
to practical problems, showing how repeated measures data collection can
solve seven common identification issues. Timing is critical but not
sufficient alone. I also use causal diagrams to highlight the
limitations of repeated-measures data collection for identifying causal
effects, tempering enthusiasm for easy solutions. Indeed, I will review
how many statistical structural equation models and sophisticated
multi-level models are not well-calibrated for identifying causal
effects.

In \hyperref[id-sec-4]{Part 4}, I offer practical suggestions for
creating and reporting causal directed acyclic graphs in scientific
research. These graphs represent investigator assumptions about causal
(or structural) relationships in nature. These relationships cannot
typically be derived from data alone and must be developed with
scientific specialists. Where ambiguity or debate exists, investigators
should report multiple causal diagrams and conduct distinct analyses. I
explain how to do this and walk through the steps.

By understanding and applying causal directed acyclic graphs,
investigators can more effectively identify and communicate causal
relationships, in the service of the causal questions that animate our
scientific interests.

\subsection{Part 1: Causal Inference as Counterfactual Data
Science}\label{id-sec-1}

The first step in answering a causal question is to ask it
(\citeproc{ref-hernuxe1n2016}{Hernán \emph{et al.} 2016a}).

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  What causal quantity do I want to consistently estimate?
\item
  For which population does this knowledge generalise?
\end{enumerate}

Causal diagrams come after we have stated a causal question and
identified the population of interest, the `target population'. We begin
by considering what is required to state these questions precisely.

\paragraph{The Fundamental Problem of Causal Inference: Missing
Counterfactual
Observations}\label{the-fundamental-problem-of-causal-inference-missing-counterfactual-observations}

To ask a causal question, we must consider the concept of causality
itself. Consider an intervention, \(A\), and its effect, \(Y\). We say
that \(A\) causes \(Y\) if altering \(A\) would lead to a change in
\(Y\) (\citeproc{ref-hume1902}{Hume 1902};
\citeproc{ref-lewis1973}{Lewis 1973}). If altering \(A\) would not
change \(Y\), we say that \(A\) has no causal effect on \(Y\).

In causal inference, we aim to quantitatively contrast the potential
outcomes of \(Y\) in response to different levels of a well-defined
intervention. Commonly, we refer to such interventions as `exposures' or
`treatments;' we refer to the possible effects of interventions as
`potential outcomes.'

Consider a binary treatment variable \(A \in \{0,1\}\). For each unit
\(i\) in the set \(\{1, 2, \ldots, n\}\), when \(A_i\) is set to 0, the
potential outcome under this condition is denoted \(Y_i(0)\).
Conversely, when \(A_i\) is set to 1, the potential outcome is denoted
\(Y_i(1)\). We refer to the terms \(Y_i(1)\) and \(Y_i(0)\) as
`potential outcomes' because, until realised, the effects of
interventions describe counterfactual states.

Suppose that each unit \(i\) receives either \(A_i = 1\) or \(A_i = 0\).
The corresponding outcomes are realised as \(Y_i|A_i = 1\) or
\(Y_i|A_i = 0\). For now, let us assume that each realised outcome under
that intervention is equivalent to one of the potential outcomes
required for a quantitative causal contrast, such that
\([(Y_i(a)|A_i = a)] = (Y_i|A_i = a)\). Thus, when \(A_i = 1\),
\(Y_i(1)|A_i = 1\) is observed. However, when \(A_i = 1\), it follows
that \(Y_i(0)|A_i = 1\) is not observed:

\[
Y_i|A_i = 1 \implies Y_i(0)|A_i = 1~ \text{is counterfactual}
\]

Conversely, if \(A_i = 0\), we may assume the potential outcome
\(Y_i(0)|A_i = 0\) is observed as \(Y_i|A_i = 0\). However, the
potential outcome \(Y_i(1)|A_i = 0\) is never realised and so not
observed:

\[
Y_i|A_i = 0 \implies Y_i(1)|A_i = 0~ \text{is counterfactual}
\]

We define \(\delta_i\) as the individual causal effect for unit \(i\)
and express the individual causal effect as:

\[
\delta_i = Y_i(1) - Y_i(0)
\]

Notice that each unit can only be exposed to one level of the treatment
\(A_i = a\) at a time. This implies that \(\delta_i\) is not merely
unobserved but inherently \emph{unobservable}.

That individual causal effects cannot be identified from observations is
known as `\emph{the fundamental problem of causal inference}'
(\citeproc{ref-holland1986}{Holland 1986};
\citeproc{ref-rubin1976}{Rubin 1976}).

\paragraph{Identifying Causal Effects Using Randomised
Experiments}\label{identifying-causal-effects-using-randomised-experiments}

Although it is not typically feasible to compute individual causal
effects, under certain assumptions, it may be possible to estimate
\emph{average} treatment effects, also called `marginal effects'. We
define an average treatment effect (ATE) as the difference between the
expected or average outcomes under treatment and contrast conditions for
a pre-specified population, typically the population from which an
observed sample is drawn.

Consider a binary treatment, \(A \in \{0,1\}\):

\[
\text{Average Treatment Effect} = \mathbb{E}[Y(1)] - \mathbb{E}[Y(0)]
\]

This is our pre-specified estimand for our target population. A
challenge remains in computing these treatment-group averages, given
that individual causal effects are unobservable. We can frame the
problem by referring to the \emph{full data} required to compute this
estimand --- that is, in terms of the complete counterfactual dataset
where the missing potential outcomes, inherent in observational data,
were somehow available. The text highlighted in red denotes inherently
missing responses over the joint distribution of the full counterfactual
dataset (also stated by underbraces). We find that for each treatment
condition, half the observations over the joint distribution of the
counterfactual data are inherently unobserved.

\[
\text{Average Treatment Effect} = \left(\underbrace{\underbrace{\mathbb{E}[Y(1)|A = 1]}_{\text{observed for } A = 1} + \underbrace{\textcolor{red}{\mathbb{E}[Y(1)|A = 0]}}_{\textcolor{red}{\text{unobserved for } A = 0}}}_{\text{effect among treated}}\right) - \left(\underbrace{\underbrace{\mathbb{E}[Y(0)|A = 0]}_{\text{observed for } A = 0} + \underbrace{\textcolor{red}{\mathbb{E}[Y(0)|A = 1]}}_{\textcolor{red}{\text{unobserved for } A = 1}}}_{\text{effect among untreated}}\right)
\]

Randomisation allows investigators to recover the treatment group
averages even though treatment groups contain inherently missing
observations. We do not require the joint distribution over the full
data (i.e., the counterfactual data) to obtain average treatment groups.
This is because when investigators randomise units into treatment
conditions, there is full adherence, and the sample is sufficiently
large to rule out chance differences in the composition of the treatment
groups. Randomisation under such conditions rules out explanations for
differences in treatment group averages except for the treatment. Put
differently, randomisation implies:

\[
\mathbb{E}[Y(0) | A = 1] = \mathbb{E}[Y(0) | A = 0]
\]

and

\[
\mathbb{E}[Y(1) | A = 1] = \mathbb{E}[Y(1) | A = 0]
\]

We assume (by causal consistency, explained in the next sub-section):

\[ 
\mathbb{E}[Y(1) | A = 1] = \mathbb{E}[Y | A = 1]
\]

and

\[
\mathbb{E}[Y(0) | A = 0] = \mathbb{E}[Y | A = 0]
\]

It follows that the average treatment effect of the randomised
experiment can be computed as:

\[
\text{Average Treatment Effect} = \widehat{\mathbb{E}}[Y | A = 1] - \widehat{\mathbb{E}}[Y | A = 0]
\]

There are four critical aspects of how ideally randomised experiments
enable the estimation of average treatment effects worth highlighting.

First, investigators must specify a population for whom they seek to
generalise their results. We refer to this population as the
\emph{target population}. If the study population differs from the
target population in the distribution of covariates that interact with
the treatment, the investigators will have no guarantees their results
will generalise (for discussions of sample/target population mismatch,
refer to: Imai \emph{et al.}
(\citeproc{ref-imai2008misunderstandings}{2008}); Westreich \emph{et
al.} (\citeproc{ref-westreich2019target}{2019}); Westreich \emph{et al.}
(\citeproc{ref-westreich2017}{2017}); Pearl and Bareinboim
(\citeproc{ref-pearl2022}{2022}); Bareinboim and Pearl
(\citeproc{ref-bareinboim2013general}{2013}); Stuart \emph{et al.}
(\citeproc{ref-stuart2018generalizability}{2018}); Webster-Clark and
Breskin (\citeproc{ref-webster2021directed}{2021})).

Second, because the units in the study sample at randomisation may
differ from the units in the study after randomisation, investigators
must be careful to avoid biases that arise from sample/population
mismatch over time. If there is sample attrition or non-response, the
treatment effect investigators obtain for the sample may differ from the
treatment effect in the target population.

Third, a randomised experiment recovers the causal effect of random
treatment assignment, not of the treatment itself, which may differ if
some participants do not adhere to their treatment (even if they remain
in the study). The effect of randomised assignment is called the
intent-to-treat effect. The effect of perfect adherence is called the
per-protocol effect (\citeproc{ref-hernan2017per}{Hernán \emph{et al.}
2017}; \citeproc{ref-lash2020}{Lash \emph{et al.} 2020}). To obtain the
per protocol effect for randomised experiments, methods for causal
inference in observational settings must be applied.

Fourth, I have presented the average treatment effect on the difference
scale, that is, as a difference in average potential outcomes for the
target population under two distinct levels of treatment. However,
depending on the scientific question at hand, investigators may wish to
estimate causal effects on the risk-ratio scale, the rate-ratio scale,
the hazard-ratio scale, or another scale. Where there are interactions
such that treatment effects vary across different strata of the
population, an estimate of the causal effect on the risk difference
scale will differ in at least one stratum to be compared from the
estimate on the risk ratio scale
(\citeproc{ref-greenland2003quantifying}{Greenland 2003}). The
sensitivity of treatment effects in the presence of interactions to the
scale of contrast underscores the importance of pre-specifying a scale
for the causal contrast investigators hope to obtain.

Fifth, investigators may unintentionally spoil randomisation by
adjusting for indicators that might be affected by the treatment,
outcome, or both, by excluding participants using attention checks, by
collecting covariate data that might be affected by the experimental
conditions, by failing to account for non-response and
loss-to-follow-up, and by committing any number of other self-inflicted
injuries. Unfortunately, such practices are widespread
(\citeproc{ref-montgomery2018}{Montgomery \emph{et al.} 2018}). Notably,
causal graphical methods are useful for describing causal identification
in experiments (refer to Hernán \emph{et al.}
(\citeproc{ref-hernan2017per}{2017})), a topic we consider elsewhere
(\citeproc{ref-bulbulia_2024_experiments}{Bulbulia 2024c}).

In observational studies, investigators might wish to describe the
target population of interest as a restriction of the study sample
population. For example, investigators might wish to estimate the
average treatment effect only in the population that received the
treatment. This treatment effect is sometimes called the average
treatment effect in the treated (ATT) and may be expressed as:

\[
\text{Average Treatment Effect in the Treated} = \mathbb{E}[Y(1) - Y(0) | A = 1]
\]

Consider that if investigators are interested in the average treatment
effect in the treated, counterfactual comparisons are deliberately
\emph{restricted} to the sample population that was treated. That is,
the investigators will seek to obtain the average of the missing
counterfactual outcomes for \emph{the treated population were they not
treated}, without necessarily obtaining the counterfactual outcomes for
the untreated population were they treated. This difference in focus may
imply different assumptions and analytic workflows.
\hyperref[id-app-b]{Appendix B} describes an example for which the
assumptions required to estimate the average treatment effect may be
preferred. In what follows, we will use the term ATE as a placeholder to
mean the average treatment effect, or equivalently the `marginal
effect', for a target population on a pre-specified scale of causal
contrast.

Setting aside the important detail that the `average treatment effect'
requires considerable care in its specification, it is worth pausing to
marvel at how an ideally conducted randomised controlled experiment
provides a means for identifying inherently unobservable
counterfactuals. It does so by using a Sherlock-Holmes-method of
inference by elimination of confounders, which randomisation balances
across treatments.

When experimenters observe a difference in average treatment effects,
and all else goes right, they may infer that the distribution of
potential outcomes differs by treatment because randomisation exhausts
every other explanation except that of the treatment. They are entitled
to this inference because randomisation balances the distribution of
potential confounders across the treatment groups to be compared.

Outside of randomised experiments, however, we lack guarantees of
balance in the confounders. For this reason, investigators should prefer
developing sound randomised experiments for addressing every causal
question that experiments can address. Unfortunately, randomised
experiments cannot address many scientifically important questions. This
bitter constraint is familiar to evolutionary human scientists. We
typically confront `What if?' questions that are rooted in the
unidirectional nature of human history.

Understanding how randomisation obtains the missing counterfactual
outcomes that we require to consistently estimate average treatment
effects clarifies the tasks of causal inference in non-experimental
settings (\citeproc{ref-hernuxe1n2008a}{Hernán \emph{et al.} 2008a};
\citeproc{ref-hernuxe1n2022}{Hernán \emph{et al.} 2022};
\citeproc{ref-hernuxe1n2006}{Hernán and Robins 2006a}).

Next, we examine these identification assumptions in greater detail
because using causal diagrams without understanding these assumptions is
unsafe.

\subsubsection{Fundamental Assumptions Required for Causal Inference in
the Potential Outcomes
Framework}\label{fundamental-assumptions-required-for-causal-inference-in-the-potential-outcomes-framework}

There are three fundamental identification assumptions that must be
satisfied to consistently estimate causal effects with data. These
assumptions are typically satisfied in randomised controlled trials but
not in real-world studies where randomised treatment assignment is
absent.

\paragraph{Assumption 1: Causal
Consistency}\label{assumption-1-causal-consistency}

We satisfy the causal consistency assumption when, for each unit \(i\)
in the set \(\{1, 2, \ldots, n\}\), the observed outcome corresponds to
one of the specific counterfactual outcomes to be compared such that:

\[
Y_i^{observed}|A_i = 
\begin{cases} 
Y_i(a^*) & \text{if } A_i = a^* \\
Y_i(a) & \text{if } A_i = a
\end{cases}
\]

The causal consistency assumption implies that the observed outcome at a
specific treatment level equates to the counterfactual outcome for that
individual at the observed treatment level. Although it seems
straightforward, treatment conditions vary, and treatment heterogeneity
poses considerable challenges for satisfying this assumption. Refer to
\hyperref[id-app-c]{Appendix C} for further discussion on how
investigators may satisfy causal consistency in real-world settings.

\paragraph{Assumption 2: Positivity}\label{assumption-2-positivity}

We satisfy the positivity assumption if there is a non-zero probability
of receiving each treatment level for every combination of covariates
that occurs in the population. Where \(A\) is the treatment and \(L\) is
a vector of covariates, positivity is achieved if:

\[
0 < Pr(A = a | L = l) < 1, \quad \text{for all } a, l \text{ with } Pr(L = l) > 0
\]

There are two types of positivity violation:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Random non-positivity}: When a treatment is theoretically
  possible, but specific treatment levels are not represented in the
  data. Notably, random non-positivity is the only identifiability
  assumption verifiable with data.
\item
  \textbf{Deterministic non-positivity}: When the treatment is
  implausible by nature, such as a hysterectomy in biological males.
\end{enumerate}

Satisfying the positivity assumption can present considerable data
challenges (\citeproc{ref-bulbulia2023a}{Bulbulia \emph{et al.} 2023};
\citeproc{ref-westreich2010}{Westreich and Cole 2010}). For instance, if
we wanted to estimate a one-year causal effect of weekly religious
service attendance on charitable donations, controlling for baseline
attendance, and the natural transition rate to weekly service attendance
is low, the effective sample size for the treatment condition may be
insufficient. Where the positivity assumption is violated, causal
diagrams will be of limited utility because observations in the data do
not support valid causal inferences. (\hyperref[id-app-b]{Appendix B}
presents a worked example illustrating this difficulty in a cultural
evolutionary context.)

\paragraph{Assumption 3: Conditional Exchangeability (also `No
Unmeasured Confounding', `Conditional Ignorability',
`d-separation')}\label{assumption-3-conditional-exchangeability-also-no-unmeasured-confounding-conditional-ignorability-d-separation}

We satisfy the conditional exchangeability assumption if the treatment
groups are conditionally balanced in the variables that could affect the
potential outcomes. In experimental designs, random assignment
facilitates this assumption. In observational studies, more effort is
required to control for any covariate that could account for observed
correlations between \(A\) and \(Y\) in the absence of a causal effect
of \(A\) on \(Y\).

Let \(\coprod\) denote independence, and let \(L\) denote the set of
covariates necessary to ensure this conditional independence.
Conditional exchangeability is satisfied when:

\[
Y(a) \coprod A | L \quad \text{or equivalently} \quad A \coprod Y(a) | L
\]

Assuming conditional exchangeability and the other assumptions required
for consistent causal inference hold, we may compute the average
treatment effect (ATE) on the difference scale:

\[
\text{Average Treatment Effect} = \mathbb{E}[Y(1) | L] - \mathbb{E}[Y(0) | L]
\]

In randomised controlled experiments, exchangeability is unconditional.
Although adjusting by interacting the treatment with pre-treatment
variables may improve efficiency and diminish threats to randomisation
from chance imbalances, it is a confusion to think of such adjustment as
`control.'

In real-world observational studies, where measured confounders are
sufficient to ensure conditional exchangeability, we obtain estimates
for the average treatment effect by conditioning on the densities of
measured confounders by treatment group. Where \(A = a\) and \(A = a^*\)
are the treatment levels we seek to contrast:

\[
\widehat{\text{ATE}} =  \sum_l \big( \mathbb{E}[Y(a^*) \mid L] - \mathbb{E}[Y(a) \mid L] \big) \times Pr(L)
\]

By causal consistency, we obtain:

\[
\widehat{\text{ATE}} =  \sum_l \big( \mathbb{E}[Y \mid A = a^*, L] - \mathbb{E}[Y \mid A = a, L] \big) \times Pr(L)
\]

For continuous covariates \(L\), we have:

\[
\widehat{\text{ATE}} = \int \big( \mathbb{E}[Y \mid A = a^*, L] - \mathbb{E}[Y \mid A = a, L] \big) dP(L)
\]

The primary function of a causal directed acyclic graph is to identify
sources of bias that may lead to an association between an exposure and
outcome in the absence of causation. These graphs visually encode
features of a causal order necessary to evaluate the assumptions of
conditional exchangeability---also called `no-unmeasured confounding',
`ignorability', and in the idiom of causal graphical models,
`d-separation'. Although directed acyclic graphs can also be useful for
addressing broader threats and opportunities for causal inferences, they
are designed to evaluate the assumptions of conditional exchangeability.

Finally, without randomisation, we cannot fully ensure the no-unmeasured
confounding assumption necessary to recover the missing counterfactuals
required to consistently estimate causal effects from data
(\citeproc{ref-greifer2023}{Greifer \emph{et al.} 2023};
\citeproc{ref-stuart2015}{Stuart \emph{et al.} 2015}). Because we must
nearly always assume unmeasured confounding, the workflows of causal
data science must ultimately rely on sensitivity analyses to clarify how
much unmeasured confounding would be required to compromise a study's
findings (\citeproc{ref-vanderweele2019}{VanderWeele 2019}).

\subsubsection{Summary of Part 1}\label{summary-of-part-1}

Causal data science is not ordinary data science. The initial step
involves formulating a precise causal question that clearly identifies
the exposure, outcome, and population of interest. We must then satisfy
the three fundamental assumptions required for causal inference, which
are implicit in the ideal of a randomised experiment:

\begin{itemize}
\tightlist
\item
  \textbf{Causal consistency}: Ensuring outcomes at a specific exposure
  level align with their counterfactual counterparts.
\item
  \textbf{Positivity}: The existence of a non-zero probability for each
  exposure level across all covariates.
\item
  \textbf{Conditional exchangeability}: The absence of unmeasured
  confounding, or equivalently the `ignorability' of treatment
  assignment, conditional on measured confounders.
\end{itemize}

\newpage{}

\subsection{Part 2: How Causal Directed Acyclic Graphs Clarify the
Conditional Exchangeability Assumption}\label{id-sec-2}

We introduce causal directed acyclic graphs by describing the meaning of
our symbols.

\subsubsection{Variable Naming
Conventions}\label{variable-naming-conventions}

\begin{table}

\caption{\label{tbl-terminology}Variable naming conventions}

\centering{

\terminologylocalconventionssimple

}

\end{table}%

\begin{itemize}
\item
  \textbf{\(X\)}: Denotes a random variable without reference to its
  role.
\item
  \textbf{\(A\)}: Denotes the `treatment' or `exposure'---a random
  variable. This is the variable for which we seek to understand the
  effect of intervening on it. It is the `cause.'
\item
  \textbf{\(A=a\)}: Denotes a fixed `treatment' or `exposure.' Random
  variable \(A\) is set to level \(A=a\).
\item
  \textbf{\(Y\)}: Denotes the outcome or response of an intervention. It
  is the `effect.'
\item
  \textbf{\(Y(a)\)}: Denotes the counterfactual or potential state of
  \(Y\) in response to setting the level of the treatment to a specific
  level, \(A=a\). The outcome \(Y\) as it would be observed when,
  perhaps contrary to fact, treatment \(A\) is set to level \(A=a\).
  There are different conventions for expressing a potential or
  counterfactual outcome, such as \(Y^a\), \(Y_a\).
\item
  \textbf{\(L\)}: Denotes a measured confounder or set of confounders.
  This set, if conditioned upon, ensures that any differences between
  the potential outcomes under different levels of the treatment are the
  result of the treatment and not the result of a common cause of the
  treatment and the outcome. Mathematically, we write this independence:
\end{itemize}

\[
Y(a) \coprod A \mid L
\]

\begin{itemize}
\tightlist
\item
  \textbf{\(U\)}: Denotes an unmeasured confounder or confounders. \(U\)
  is a variable or set of variables that may affect both the treatment
  and the outcome, leading to an association in the absence of
  causality, even after conditioning on measured covariates:
\end{itemize}

\[
Y(a) \cancel{\coprod} A \mid L \quad \text{[because of unmeasured } U]
\]

\begin{itemize}
\item
  \textbf{\(F\)}: Denotes a modifier of the treatment effect. \(F\)
  alters the magnitude or direction of the effect of a treatment \(A\)
  on an outcome \(Y\).
\item
  \textbf{\(M\)}: Denotes a mediator, a variable that transmits the
  effect of a treatment \(A\) on an outcome \(Y\).
\item
  \textbf{\(\bar{X}\)}: Denotes a sequence of variables, for example, a
  sequence of treatments.
\item
  \textbf{\(\mathcal{R}\)}: Denotes a randomisation to treatment
  condition.
\item
  \textbf{\(\mathcal{G}\)}: Denotes a graph, here, a causal directed
  acyclic graph.
\end{itemize}

Note that investigators use a variety of different symbols. There is no
unique right way to create a causal directed acyclic graph, except that
meaning must be clear and the graph must be capable of identifying
relationships of conditional and unconditional independence between the
treatment and outcome. Although directed acyclic graphs are accessible
tools, general graphical models such as `Single World Intervention
Graphs,' which allow for the explicit representation of counterfactual
dependencies, may be preferable when there are multiple interventions
(\citeproc{ref-richardson2013}{Richardson and Robins 2013a}).

\subsubsection{Convention We Use to Draw Causal Directed Acyclic
Graphs}\label{convention-we-use-to-draw-causal-directed-acyclic-graphs}

The conventions we use to describe components of our causal graphs are
given in Table~\ref{tbl-general}.

\begin{table}

\caption{\label{tbl-general}Nodes, Edges, Conditioning Conventions.}

\centering{

\terminologygeneraldags

}

\end{table}%

\begin{itemize}
\item
  \textbf{Node}: a node or vertex represents characteristics or features
  of units within a population on a causal diagram -- that is a
  `variable.' In causal directed acyclic graphs, we draw nodes with
  respect to the \emph{target population}, which is the population for
  whom investigators seek causal inferences
  (\citeproc{ref-suzuki2020}{Suzuki \emph{et al.} 2020}). Time-indexed
  node: \(X_t\) denotes relative chronology; \(X_{\customphi{t}}\) is
  our convention for indicating that timing is assumed, perhaps
  erroneously.
\item
  \textbf{Edge without an Arrow} (\(\association\)): path of
  association, causality not asserted.
\item
  \textbf{Red Edge without an Arrow} (\(\associationred\)): confounding
  path: ignores arrows to clarify statistical dependencies.
\item
  \textbf{Arrow} (\(\rightarrowNEW\)): denotes causal relationship from
  the node at the base of the arrow (a parent) to the node at the tip of
  the arrow (a child). We typically refrain from drawing an arrow from
  treatment to outcome to avoid asserting a causal path from \(A\) to
  \(Y\) because the function of a causal directed acyclic graph is to
  evaluate whether causality can be identified for this path.
\item
  \textbf{Red Arrow} (\(\rightarrowred\)): path of non-causal
  association between the treatment and outcome. Path is associational
  and may run against arrows.
\item
  \textbf{Dashed Arrow} (\(\rightarrowdotted\)): denotes a true
  association between the treatment and outcome that becomes partially
  obscured when conditioning on a mediator, assuming \(A\) causes \(Y\).
\item
  \textbf{Dashed Red Arrow} (\(\rightarrowdottedred\)): highlights
  over-conditioning bias from conditioning on a mediator.
\item
  \textbf{Open Blue Arrow} (\(\rightarrowblue\)): highlights effect
  modification, which occurs when the levels of the effect of treatment
  vary within levels of a covariate. We do not assess the causal effect
  of the effect modifier on the outcome, recognising that it may be
  incoherent to consider intervening on the effect modifier.
  \textbf{This is an `off-label' convention we use to clarify that (1)
  direct or indirect effect modification is assumed for the parent node
  (2) we do not attempt to identify causality in the relationship of
  effect-modification that we project on the graph.} It would be
  possible, however, to replace these open blue arrows with ordinary
  nodes together with an explanation that we draw the edges not for
  identification by for generalisation (see
  \citeproc{ref-bulbulia2024swigstime}{Bulbulia 2024b})
\item
  \textbf{Boxed Variable} \(\big(\boxed{X}\big)\): conditioning or
  adjustment for \(X\).
\item
  \textbf{Red-Boxed Variable} \(\big(\boxedred{X}\big)\): highlights the
  source of confounding bias from adjustment.
\item
  \textbf{Dashed Circle} \(\big( \circledotted{X}\big)\): no adjustment
  is made for a variable (implied for unmeasured confounders.)
\item
  \textbf{\(\mathbf{\mathcal{R}}\)}
  \(\big(\mathcal{R} \rightarrow A\big)\) randomisation into the
  treatment condition.
\item
  \textbf{Presenting Temporal Order}: Causal directed acyclic graphs
  must be --- as truth in advertising implies--- \emph{acyclic.}
  Directed edges or arrows define ancestral relations. No descendant
  node can cause an ancestor node. Therefore causal diagrams are, by
  default, sequentially ordered.
\end{itemize}

Nevertheless, to make our causal graphs more readable, we adopt the
following conventions:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  The layout of a causal diagram is structured from left to right to
  reflect the assumed sequence of causality as it unfolds.
\item
  We often index our nodes using \(X_t\) to indicate their relative
  timing and chronological order, where \(t\) represents the time point
  or sequence in the timeline of events.
\item
  Where temporal order is uncertain or unknown, we use the notation
  \(X_{\phi t}\) to propose a temporal order that is uncertain.
\end{enumerate}

Typically, the timing of unmeasured confounders is unknown, except that
they occur before the treatments of interest; hence, we place
confounders to the left of the treatments and outcomes they are assumed
to affect, but without any time indexing.

Again, both spatial and temporal organisation in a causal directed
acyclic graph are optional. Temporal order is implied by the
relationship of nodes and edges. However, explicitly representing the
order in the layout of one's causal graph often makes it easier to
evaluate, and the convention representing uncertainty is useful,
particularly when the data do not ensure the relative timing of the
occurrence of the variable in a causal graph.

More generally, investigators use various conventions to convey causal
structures on graphs. Whichever convention we adopt must be clear.

Finally, note that all nodes and paths on causal graphs -- including the
absence of nodes and paths -- are asserted. Constructing causal diagrams
requires expert judgment of the scientific system under investigation.
It is a great power that we bestow upon those who construct causal
graphs. With great power comes great responsibility to be transparent.
If there is debate about which graphical model fits reality,
investigators should present multiple causal graphs, and where
identification is possible, perform and report multiple analyses.

\subsubsection{How Causal Directed Acyclic Graphs Relate Observations to
Counterfactuals}\label{how-causal-directed-acyclic-graphs-relate-observations-to-counterfactuals}

\paragraph{Ancestral Relations in Directed Acyclic
Graphs}\label{ancestral-relations-in-directed-acyclic-graphs}

We define the relation of `parent' and `child' on a directed acyclic
graph as follows:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Node \(A\) is a \textbf{parent} of node \(B\) if there is a directed
  edge from \(A\) to \(B\), denoted \(A \rightarrow B\).
\item
  Node \(B\) is a \textbf{child} of node \(A\) if there is a directed
  edge from \(A\) to \(B\), denoted \(A \rightarrow B\).
\end{enumerate}

It follows that a parent and child are \textbf{adjacent nodes} connected
by a directed edge.

We denote the set of all parents of a node \(B\) as \(\text{Pa}(B)\).

In a directed acyclic graph, the directed edge \(A \rightarrow B\)
indicates a statistical dependency where \(A\) may provide information
about \(B\). In a causal directed acyclic graph, the directed edge
\(A \rightarrow B\) is interpreted as a causal relationship such that
\(A\) is a direct cause of \(B\).

We further define the relations of \textbf{ancestor} and
\textbf{descendant} on a directed acyclic graph as follows:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Node \(A\) is an \textbf{ancestor} of node \(C\) if there exists a
  directed path from \(A\) to \(C\). Formally, \(A\) is an ancestor of
  \(C\) if there exists a sequence of adjacent nodes
  \((A, B_1, B_2, \ldots, B_k, C)\) such that
  \(A \rightarrow B_1 \rightarrow B_2 \rightarrow \cdots \rightarrow B_k \rightarrow C\).
\item
  Node \(C\) is a \textbf{descendant} of node \(A\) if there exists a
  directed path from \(A\) to \(C\). Formally, \(C\) is a descendant of
  \(A\) if there exists a sequence of adjacent nodes
  \((A, B_1, B_2, \ldots, B_k, C)\) such that
  \(A \rightarrow B_1 \rightarrow B_2 \rightarrow \cdots \rightarrow B_k \rightarrow C\).
\end{enumerate}

It follows that a node can have multiple ancestors and descendants.

\paragraph{Markov Factorisation and the Local Markov
Assumption}\label{markov-factorisation-and-the-local-markov-assumption}

Pearl (\citeproc{ref-pearl2009a}{2009}) p 52 asks us to imagine the
following. Suppose we have a distribution \(\mathcal{P}\) defined on n
discrete variables, \(X_1, X_2, \dots, X_n\). By the chain rule, the
joint distribution for variables \(X_1, X_2, \dots, X_n\) on a graph can
be decomposed into the product of \(n\) conditional distributions such
that we may obtain the following factorisation:

\[
\Pr(x_1, \dots, x_n) = \prod_{j=1}^n \Pr(x_j \mid x_1, \dots, x_{j-1})
\]

We translate nodes and edges on a graph into a set of conditional
independences that a graph implies over statistical distributions.

According to \textbf{the local Markov assumption}, given its parents in
a directed acyclic graph, a node is said to be independent of all its
non-descendants. Under this assumption, we obtain what Pearl calls
Bayesian network factorisation, such that:

\[
\Pr(x_j \mid x_1, \dots, x_{j-1}) = \Pr(x_j \mid \text{pa}_j)
\]

This factorisation greatly simplifies the calculation of the joint
distributions encoded in the directed acyclic graph (causal or
non-causal) by reducing complex factorisations of the conditional
distributions in \(\mathcal{P}\) to simpler conditional distributions in
the set \(\text{PA}_j\), represented in the structural elements of a
directed acyclic graph (\citeproc{ref-lauritzen1990}{Lauritzen \emph{et
al.} 1990}; \citeproc{ref-pearl1988}{Pearl 1988},
\citeproc{ref-pearl1995}{1995}, \citeproc{ref-pearl2009a}{2009}).

\paragraph{Minimality Assumption}\label{minimality-assumption}

The minimality assumption combines (a) the local Markov assumption with
(b) the assumption that adjacent nodes on the graph are dependent. This
is needed for causal directed acyclic graphs because the local Markov
assumption permits that adjacent nodes may be independent
(\citeproc{ref-neal2020introduction}{Neal 2020}).

\paragraph{Causal Edges Assumption}\label{causal-edges-assumption}

The \textbf{causal edges assumption} states that every parent is a
direct cause of its children. Given minimalism, the causal edges
assumption allows us to read causal dependence in directed acyclic
graphs. In Pearl's formalism, we use non-parametric structural equations
to evaluate causal assumptions using statistical distributions (refer to
Appendix E; Neal (\citeproc{ref-neal2020introduction}{2020})).

\paragraph{Compatibility Assumption}\label{compatibility-assumption}

The compatibility assumption ensures that the joint distribution of
variables aligns with the conditional independencies implied by the
causal graph. This assumption requires that the probabilistic model
conform to the graph's structural assumptions. Demonstrating
compatibility directly from data is challenging, as it involves
verifying that all conditional independencies specified by the causal
directed acyclic graph (DAG) are present in the data. Therefore, we
typically assume compatibility rather than empirically proving it
(\citeproc{ref-pearl2009a}{Pearl 2009}).

\paragraph{Faithfulness}\label{faithfulness}

\textbf{Faithfulness} complements Markov factorisation in causal
diagrams. A causal diagram is considered faithful to a given set of data
if all the conditional independencies present in the data are accurately
depicted in the graph. Conversely, the graph is faithful if every
dependency implied by the graph's structure can be observed in the data
(\citeproc{ref-hernan2024WHATIF}{Hernan and Robins 2024}). This concept
ensures that the graphical representation of relationships between
variables aligns with empirical evidence
(\citeproc{ref-pearl2009a}{Pearl 2009}).

The distinction between \textbf{weak faithfulness} and \textbf{strong
faithfulness} addresses the nature of observed independencies:

\begin{itemize}
\item
  \textbf{Weak faithfulness} allows for the possibility that some
  observed independencies might occur by chance, such as through
  cancellation of effects among multiple causal paths.
\item
  \textbf{Strong faithfulness} assumes that all observed statistical
  relationships directly reflect the underlying causal structure, with
  no difference left to chance.
\end{itemize}

The faithfulness assumption, whether weak or strong, is not directly
testable from observed data (\citeproc{ref-pearl2009a}{Pearl 2009}).

\subsubsection{The Rules of
d-separation}\label{the-rules-of-d-separation}

\textbf{d-separation}: in a causal diagram, a path is `blocked' or
`d-separated' if a node along it interrupts causation. Two variables are
d-separated if all paths connecting them are blocked, making them
conditionally independent. Conversely, unblocked paths result in
`d-connected' variables, implying potential dependence
(\citeproc{ref-pearl1995}{Pearl 1995}, \citeproc{ref-pearl2009a}{2009}).
(Note that `d' stands for `directional'.)

The rules of d-separation are as follows:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Fork rule} (\(B \leftarrowNEW \boxed{A} \rightarrowNEW C\)):
  \(B\) and \(C\) are independent when conditioning on \(A\)
  (\(B \coprod C \mid A\)).
\item
  \textbf{Chain rule} (\(A \rightarrowNEW \boxed{B} \rightarrowNEW C\)):
  Conditioning on \(B\) blocks the path between \(A\) and \(C\)
  (\(A \coprod C \mid B\)).
\item
  \textbf{Collider rule}
  (\(A \rightarrowNEW \boxed{C} \leftarrowNEW B\)): \(A\) and \(B\) are
  independent until conditioning on \(C\), which introduces dependence
  (\(A \cancel{\coprod} B \mid C\)). Judea Pearl proved these theorems
  in the 1990s (\citeproc{ref-pearl1995}{Pearl 1995},
  \citeproc{ref-pearl2009a}{2009}).
\end{enumerate}

According to these rules:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  An open path (no variables conditioned on) is blocked only if two
  arrows point to the same node: \(A \rightarrowred C \leftarrowred B\).
  The node of common effect (here \(C\)) is called a \emph{collider}.
\item
  Conditioning on a collider does not block a path, such that
  \(A \rightarrowred \boxed{C} \leftarrowred B\) can lead to an
  association between \(A\) with \(B\) in the absence of causation.
\item
  Conditioning on a descendant of a collider does not block a path, such
  that if \(C \rightarrowNEW \boxed{C'}\), then
  \(A \rightarrowred \boxed{C'} \leftarrowred B\) is open.
\item
  If a path does not contain a collider, any variable conditioned along
  the path is blocked, such that
  \(A \rightarrowNEW \boxed{B} \rightarrowNEW C\) blocks the path from
  \(A\) to \(C\) (\citeproc{ref-hernan2023}{Hernan and Robins 2023 p.
  78}; \citeproc{ref-pearl2009a}{Pearl 2009}).
\end{enumerate}

\subsubsection{Backdoor Path Adjustment}\label{backdoor-path-adjustment}

To obtain an unbiased estimate for the causal effect of \(A\) on \(C\),
we need to block all backdoor paths. We do this using the rules of
d-separation by conditioning on a set of covariates \(B\) that is
sufficient to close all backdoor paths linking \(A\) and \(C\). A path
is effectively blocked by \(B\) if it includes at least one non-collider
that is a member of \(B\), or if it does not contain any collider or
descendants of a collider.

Pearl also proves a `front-door adjustment' criterion, which is rarely
used, refer to \hyperref[id-app-e]{Appendix E}.

\subsubsection{Comment on Pearl's Do-Calculus versus the Potential
Outcomes
Framework}\label{comment-on-pearls-do-calculus-versus-the-potential-outcomes-framework}

Here, we have developed counterfactual contrasts using the potential
outcomes framework. Pearl develops counterfactual contrasts using
operations on structural functionals, referred to as `do-calculus'. In
Pearl's framework, we obtain counterfactual inference by assuming that
the nodes in a causal directed acyclic graph correspond to a system of
structural equation models, refer to \hyperref[id-app-d]{Appendix D}.

Mathematically, potential outcomes and counterfactual interventions are
equivalent, such that:

\[
P(Y(a) = y) \equiv P(Y = y \mid do(A = a))
\]

where the left-hand side of the equivalence is the potential outcomes
framework formalisation of a potential outcome recovered by causal
consistency, and the right-hand side is given by Pearl's do-calculus,
which formalises interventional distributions on nodes of a graph that
correspond to structural causal models or, equivalently, to
non-parametric structural equation models with independent errors.

In practice, whether one uses the do-calculus or the potential outcomes
framework to interpret causal inferences is often irrelevant to
identification results. However, there are theoretically interesting
debates about edge cases. For example, Pearl's structural causal models
permit the identification of contrasts that cannot be falsified under
any experiment (\citeproc{ref-richardson2013}{Richardson and Robins
2013a}). Because advocates of non-parametric structural equation models
treat causality as primitive, they are less concerned with the
requirement for falsification (\citeproc{ref-diaz2021nonparametric}{Dı́az
\emph{et al.} 2021}, \citeproc{ref-Diaz2023}{2023};
\citeproc{ref-pearl2009a}{Pearl 2009};
\citeproc{ref-rudolph2024mediation}{Rudolph \emph{et al.} 2024}). On the
other hand, some advocates of the potential outcomes framework require
falsifiability (\citeproc{ref-richardson2023potential}{Richardson and
Robins 2023}; \citeproc{ref-robins2010alternative}{Robins and Richardson
2010}). Conversely, the potential outcomes framework can obtain
identification while the do-calculus does not because the potential
outcomes framework does not require the non-independence of all error
structures irrespective of a specific intervention level
(\citeproc{ref-richardson2013}{Richardson and Robins 2013a}).

I have presented the potential outcomes framework because it is easier
to interpret, more general, and---to my mind---clearer and more
intellectually compelling (moreover, one does not need to be a
verificationist to adopt it). However, for nearly every practical
purpose, the do-calculus and `po-calculus' (potential outcomes
framework, refer to Shpitser and Tchetgen
(\citeproc{ref-shpitser2016causal}{2016})) are both mathematically and
practically equivalent. Furthermore, readers should be aware that there
are causal diagrams called `Single World Intervention Graphs' which
enable investigators to represent conditional independencies of
potential outcomes on graphs
(\citeproc{ref-richardson2013swigsprimer}{Richardson and Robins 2013b}),
which can be useful, refer to Bulbulia
(\citeproc{ref-bulbulia2024swigstime}{2024b}).

\subsubsection{Five Elementary Structures of
Causality}\label{five-elementary-structures-of-causality}

\begin{table}

\caption{\label{tbl-fiveelementary}The five elementary structures of
causality from which all causal directed acyclic graphs can be built.}

\centering{

\terminologydirectedgraph

}

\end{table}%

Table~\ref{tbl-fiveelementary} presents five elementary structures of
causality from which all causal directed acyclic graphs are built.

\paragraph{Causal Relations with Two
Variables}\label{causal-relations-with-two-variables}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Causality Absent:} There is no causal effect between variables
  \(A\) and \(B\). They do not influence each other, denoted as
  \(A \coprod B\), indicating they are statistically independent.
\item
  \textbf{Causality:} Variable \(A\) causally affects variable \(B\).
  This relationship suggests an association between them, denoted as
  \(A \cancel{\coprod} B\), indicating they are statistically dependent.
\end{enumerate}

\paragraph{Causal Relations with Three
Variables}\label{causal-relations-with-three-variables}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  \textbf{Fork Relation:} Variable \(A\) causally affects both \(B\) and
  \(C\). Variables \(B\) and \(C\) are conditionally independent given
  \(A\), denoted as \(B \coprod C \mid A\). This structure implies that
  knowing \(A\) removes any association between \(B\) and \(C\) due to
  their common cause.
\item
  \textbf{Chain Relation:} A causal chain exists where \(C\) is affected
  by \(B\), which in turn is affected by \(A\). Variables \(A\) and
  \(C\) are conditionally independent given \(B\), denoted as
  \(A \coprod C \mid B\). This indicates that \(B\) mediates the effect
  of \(A\) on \(C\), and knowing \(B\) breaks the association between
  \(A\) and \(C\).
\item
  \textbf{Collider Relation:} Variable \(C\) is affected by both \(A\)
  and \(B\), which are independent. However, conditioning on \(C\)
  induces an association between \(A\) and \(B\), denoted as
  \(A \cancel{\coprod} B \mid C\). This structure is important because
  it suggests that \(A\) and \(B\), while initially independent, become
  associated when we account for their common effect \(C\).
\end{enumerate}

Understanding the basic relationships between two variables allows us to
build upon these to create more complex relationships. These elementary
structures can be assembled in different combinations to clarify the
causal relationships presented in a causal directed acyclic graph. Such
clarity is crucial for ensuring whether confounders may be balanced
across treatment groups to be compared, conditional on measured
covariates, so that \(Y(a) \coprod A \mid L\).

\newpage{}

\subsubsection{Five Elementary Rules for Causal
Identification}\label{five-elementary-rules-for-causal-identification}

Table~\ref{tbl-terminologyconfounders} describes five elementary rules
for identifying conditional independence using directed acyclic causal
diagrams.

\begin{table}

\caption{\label{tbl-terminologyconfounders}Five elementary rules for
causal identification.}

\centering{

\terminologyelconfounders

}

\end{table}%

There are no shortcuts to reasoning about causality. Each causal
question must be asked in the context of a specific scientific question,
and each causal graph must be built under the best lights of domain
expertise. However, the following five elementary rules for confounding
control are implied by the theorems that underpin causal directed
acyclic graphs. They may be a useful start for evaluating the prospects
for causal identification across a broad range of settings.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Ensure That Treatments Precede Outcomes}: This rule is a
  logical consequence of our assumption that causality follows the arrow
  of time and that a causal directed acyclic graph is faithful to this
  ordering. However, the assumption that treatments precede outcomes may
  be easily violated where investigators cannot ensure the relative
  timing of events from their data.

  Note that this assumption does not raise concerns in settings where
  past outcomes may affect future treatments. Indeed, an effective
  strategy for confounding control in such settings is to condition on
  past outcomes, and where relevant, on past treatments as well. For
  example, if we wish to identify the causal effect of \(A_1\) on
  \(Y_2\), and repeated-measures time series data are available, it may
  be useful to condition such that
  \(\boxed{A_{-1}} \to \boxed{Y_0} \to A_1 \rightarrow Y_2\).
  Critically, the relations of variables must be arranged sequentially
  without cycles.

  To estimate a causal effect of \(Y\) on \(A\), we would focus on:
  \(\boxed{Y_{-1}} \to \boxed{A_0} \to Y_1 \rightarrow A_2\). Departing
  from convention, here \(Y\) denotes the treatment and \(A\) denotes
  the outcome. Graphs must be acyclic. Most processes in nature include
  feedback loops. There is no contradiction as long as we represent
  these loops as sequential events.
\item
  \textbf{Condition on Common Causes or Their Proxies}: This rule
  applies to settings in which the treatment \(A\) and the outcome \(Y\)
  share common causes. By conditioning on these common causes, we block
  the open backdoor paths that could introduce bias into our causal
  estimates. Controlling for these common causes (or their proxies)
  helps to isolate the specific effect of \(A\) on \(Y\). Note that we
  do not draw a path from \(A \to Y\) in this context because it
  represents an interventional distribution. In a causal directed
  acyclic graph, conditioning does not occur on interventional
  distributions. We do not box \(A\) and \(Y\).
\item
  \textbf{Do Not Condition on a Mediator when Estimating Total Effects}:
  This rule applies to settings in which the variable \(L\) is a
  mediator of \(A \to Y\). Recall Pearl's backdoor path criterion
  requires that we do not condition on a descendant of the treatment.
  Here, conditioning on \(L\) violates the backdoor path criterion,
  risking bias for a total causal effect estimate. We must not condition
  on a mediator if we are interested in total effect estimates. Note we
  draw the path from \(A \to Y\) to underscore that this specific
  overconditioning threat occurs in the presence of a true treatment
  effect. Over-conditioning bias can operate in the absence of a true
  treatment effect. This is important because conditioning on a mediator
  might create associations without causation. In many settings,
  ensuring accuracy in the relative timing of events in our data will
  prevent the self-inflicted injury of conditioning on a common effect
  of the treatment.
\item
  \textbf{Do Not Condition on a Collider}: This rule applies to settings
  in which \(L\) is a common effect of \(A\) and \(Y\). Conditioning on
  a collider may invoke a spurious association. Again, the backdoor path
  criterion requires that we do not condition on a descendant of the
  treatment. We would not be tempted to condition on \(L\) if we knew
  that it was an effect of \(A\). In many settings, ensuring accuracy in
  the relative timing of events in our data will prevent the
  self-inflicted injury of conditioning on a common effect of the
  treatment and outcome.
\item
  \textbf{Proxy Rule: Conditioning on a Descendant Is Akin to
  Conditioning on Its Parent}: This rule applies to settings where
  \(L'\) is an effect from another variable \(L\). The graph considers
  when \(L'\) is downstream of a collider. Here again, in many settings,
  ensuring accuracy in the relative timing of events in our data will
  prevent the self-inflicted injury of conditioning on a common effect
  of the treatment and outcome.
\end{enumerate}

\newpage{}

\subsubsection{Summary Part 2}\label{summary-part-2}

We use causal directed acyclic graphs to represent and evaluate
structural sources of bias. We do not use these causal graphs to
represent the entirety of the causal system in which we are interested,
but rather only those features necessary to evaluate conditional
exchangeability, or equivalently to evaluate d-separation. Moreover,
causal directed acyclic graphs should not be confused with the
structural equation models employed in the statistical structural
equation modelling traditions (see also Rohrer \emph{et al.}
(\citeproc{ref-rohrer2022PATH}{2022})). Although Pearl's formalism is
built upon `Non-Parametric Structural Equation Models,' the term
`Structural Equation Model' can be misleading. Causal graphs are
structural models, not statistical models. We use structural causal
models to evaluate identifiability. We create causal graphs before we
embark on statistical modelling. They aim to clarify how to write
statistical models by elucidating which variables we must include in our
statistical models and, equally important, which variables we must
exclude to avoid invalidating our causal inferences. All causal graphs
are grounded in our assumptions about the structures of causation.
Although it is sometimes possible to use causal diagrams for causal
discovery, their primary (and original) use is to evaluate the
implications of assumptions.

This distinction between structural and statistical models is
fundamental because, absent clearly defined causal contrasts and
carefully evaluated assumptions about structural sources of bias, the
statistical structural equation modelling tradition offers no guarantees
that the coefficients investigators recover are interpretable.
Misunderstanding this difference between structural and statistical
models has led to considerable confusion across the human sciences
(\citeproc{ref-bulbulia2022}{Bulbulia 2022};
\citeproc{ref-vanderweele2015}{VanderWeele 2015};
\citeproc{ref-vanderweele2022}{VanderWeele 2022};
\citeproc{ref-vanderweele2022b}{VanderWeele and Vansteelandt 2022}).

\subsection{Part 3. How Causal Directed Acyclic Graphs Clarify the
Importance of Timing of Events Recorded in Data}\label{id-sec-3}

\begin{table}

\caption{\label{tbl-elementary-chronological-hyg}}

\centering{

\captionsetup{labelsep=none}

\terminologychronologicalhygeine

}

\end{table}%

As hinted at in the previous section, the five elementary rules of
confounding control reveal the importance of ensuring accurate timing in
the occurrence of the variables whose structural features a causal
directed acyclic graph encodes. We begin by considering seven examples
of confounding problems resolved when such accuracy is ensured.

The first seven case studies illustrate the focus that causal directed
acyclic graphs bring to the fundamental imperative to ensure accurate
timing in the chronology of events recorded in data. These illustrations
refer to causal graphs in Table~\ref{tbl-elementary-chronological-hyg}.
We use the symbol \(\mathcal{G}\) to denote a causal directed acyclic
graph in the table. The first digit in the graph subscript indexes the
example. The second digit in the graph subscript indexes the problem or
the response to the problem. Specifically, if the subscript `1' is used,
it refers to the graph associated with the problem; if `2' is used, it
refers to the graph associated with the response. We use this convention
throughout the remainder of this article.

\subsubsection{Example 1: Reverse
Causation}\label{example-1-reverse-causation}

Table~\ref{tbl-elementary-chronological-hyg} \(\mathcal{G}_{3.1}\)
illustrates bias from reverse causation. Suppose we are interested in
the causal effect of marriage on well-being. If we observe that married
people are happier than unmarried people, we might erroneously infer
that marriage causes happiness, or happiness causes marriage (refer to
McElreath (\citeproc{ref-mcelreath2020}{2020})).

Table~\ref{tbl-elementary-chronological-hyg} \(\mathcal{G}_{3.2}\)
clarifies a response. Ensure that the treatment is observed before the
outcome is observed. Note further that the treatment, in this case, is
not clearly specified because `marriage' is unclear. There are at least
four causal contrasts we might consider when thinking of `marriage',
namely:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \(Y(0, 0)\): The potential outcome when there is no marriage.
\item
  \(Y(0, 1)\): The potential outcome when there is marriage.
\item
  \(Y(1, 0)\): The potential outcome when there is divorce.
\item
  \(Y(1, 1)\): The potential outcome from marriage prevalence.
\end{enumerate}

Each of these outcomes allows for a specific contrast. There are
\(\binom{4}{2}\) unique contrasts. Which of the six unique contrasts do
we wish to consider? Not only is it important to order our data in time,
but the question,`What is the causal effect of marriage on happiness?'
is also ambiguous. We must state a causal contrast of interest, given
our substantive research interests. This statement requires a sequence
of treatments to be contrasted and a population over whom causal
inferences are meant to generalise. Refer to Bulbulia
(\citeproc{ref-bulbulia2024swigstime}{2024b}) for a worked example,
revealing the complexity of estimating causal effects when considering
interventions on marriage in the presence of time-varying confounding.

\subsubsection{Example 2: Confounding by Common
Cause}\label{example-2-confounding-by-common-cause}

Table~\ref{tbl-elementary-chronological-hyg} \(\mathcal{G}_{3.2}\)
illustrates confounding by common cause. Suppose there is a common
cause, \(L\), of the treatment, \(A\), and outcome, \(Y\). In this
setting, \(L\) may create a statistical association between \(A\) and
\(Y\), implying causation in its absence. Most human scientists will be
familiar with the threat to inference in this setting: a `third
variable' leads to association without causation.

Consider an example where smoking, \(L\), is a common cause of both
yellow fingers, \(A\), and cancer, \(Y\). Here, \(A\) and \(Y\) may show
an association without causation. If investigators were to scrub the
hands of smokers, this would not affect cancer rates.

Table~\ref{tbl-elementary-chronological-hyg} \(\mathcal{G}_{3.2}\)
clarifies a response. Condition on the common cause, smoking. Within
strata of smokers and non-smokers, there will be no association between
yellow fingers and cancer.

\subsubsection{Example 3: Mediator Bias}\label{example-3-mediator-bias}

Table~\ref{tbl-elementary-chronological-hyg} \(\mathcal{G}_{3.1}\)
illustrates mediator bias. Conditioning on the effect of a treatment in
this graph partially blocks the flow of information from treatment to
outcome, biasing the total effect estimate.

Suppose investigators are interested in whether cultural `beliefs in big
Gods' \(A\) affect social complexity \(Y\). Suppose that `economic
trade', \(L\), is both a common cause of the treatment and outcome. To
address confounding by a common cause, we must condition on economic
trade. However, timing matters. If we condition on measurements that
reflect economic trade after the emergence of beliefs in big Gods, we
may bias our total effect estimate.

Table~\ref{tbl-elementary-chronological-hyg} \(\mathcal{G}_{3.2}\)
clarifies a response. Ensure that measurements of economic trade are
obtained for cultural histories before big-Gods arise. Do not condition
on post-treatment instances of economic trade.

\subsubsection{Example 4: Collider Bias}\label{example-4-collider-bias}

Table~\ref{tbl-elementary-chronological-hyg} \(\mathcal{G}_{4.1}\)
illustrates collider bias. Imagine a randomised experiment investigating
the effects of different settings on individuals' self-rated health. In
this study, participants are assigned to either civic settings (e.g.,
community centres) or religious settings (e.g., places of worship). The
treatment of interest, \(A\), is the type of setting, and the outcome,
\(Y\), is self-rated health. Suppose there is no effect of setting on
self-rated health. However, suppose both setting and rated health
independently influence a third variable: cooperativeness. Specifically,
imagine religious settings encourage cooperative behaviour, and at the
same time, individuals with better self-rated health are more likely to
engage cooperatively. Now suppose the investigators decide to condition
on cooperativeness, which in reality is the common effect of \(A\) and
the outcome \(Y\). Their rationale might be to study the effects of
setting on health among those who are more cooperative or perhaps to
`control for' cooperation in the health effects of religious settings.
By introducing such `control', the investigators would inadvertently
introduce collider bias, because the control variable is a common effect
of the treatment and the outcome.

Table~\ref{tbl-elementary-chronological-hyg} \(\mathcal{G}_{4.2}\)
clarifies a response. If the worry is that cooperativeness is a
confounder, ensure that cooperativeness is measured before the
initiation of exposure to religious settings. Note that in experimental
settings, investigators do not have this worry, assuming randomisation
succeeds and samples are large. However, conditioning on a variable that
is associated with the outcome may improve estimation efficiency and
safeguard against random imbalance arising from sampling variability.

\subsubsection{Example 5: Collider Proxy
Bias}\label{example-5-collider-proxy-bias}

Table~\ref{tbl-elementary-chronological-hyg} \(\mathcal{G}_{5.1}\)
illustrates bias from conditioning on the proxy of a collider. Consider
again the scenario described in \(\sec 3.4\), but instead of controlling
for cooperativeness, investigators control for charitable donations, a
proxy for cooperativeness. Here, because the control variable is a
descendant of a collider, conditioning on the proxy is akin to
conditioning on the collider.

Table~\ref{tbl-elementary-chronological-hyg} \(\mathcal{G}_{5.2}\)
clarifies a response. Do not condition on charitable donations, an
effect of treatment.

\subsubsection{Example 6: Post-Treatment Collider Stratification
Bias}\label{example-6-post-treatment-collider-stratification-bias}

Table~\ref{tbl-elementary-chronological-hyg} \(\mathcal{G}_{6.1}\)
illustrates post-treatment collider stratification bias. Consider again
an experiment investigating the effect of religious service on
self-rated health. Suppose we measure `religiosity' after the
experiment, along with other demographic data. Suppose further that
religious setting affects religiosity, as does an unmeasured confounder,
such as childhood deprivation. Suppose that childhood deprivation
affects self-reported health. Although our experiment ensured
randomisation of the treatment and thus ensured no unmeasured common
causes of the treatment and outcome, conditioning on the post-treatment
variable `religiosity' opens a back-door path from the treatment to the
outcome. This path is
\(A_0 \associationred L_1 \associationred U \associationred Y_2\). We
introduced confounding into our randomised experiment.

Table~\ref{tbl-elementary-chronological-hyg} \(\mathcal{G}_{6.2}\)
clarifies a response. Do not condition on a variable that the treatment
may affect (refer to Cole \emph{et al.} (\citeproc{ref-cole2010}{2010})
for theoretical examples; refer to Montgomery \emph{et al.}
(\citeproc{ref-montgomery2018}{2018}) for evidence of the widespread
prevalence of post-treatment adjustment in political science
experiments).

\subsubsection{Example 7: Conditioning on Past Treatments and Past
Outcomes to Control for Unmeasured
Confounders}\label{example-7-conditioning-on-past-treatments-and-past-outcomes-to-control-for-unmeasured-confounders}

Table~\ref{tbl-elementary-chronological-hyg} \(\mathcal{G}_{7.1}\)
illustrates the threat of unmeasured confounding. In `real world'
studies, this threat is ubiquitous.

Table~\ref{tbl-elementary-chronological-hyg} \(\mathcal{G}_{7.2}\)
clarifies a response. With at least three repeated measurements,
investigators may greatly reduce unmeasured confounding by controlling
for past measurements of the treatment as well as past measurements of
the outcome. With such control, any unmeasured confounder must be
orthogonal to its effects at baseline (refer to VanderWeele \emph{et
al.} (\citeproc{ref-vanderweele2020}{2020})). Moreover, controlling for
past treatments allows investigators to estimate an \textbf{incident
exposure} effect over a \textbf{prevalence exposure effect}. The
prevalence exposure effect describes the effect of current or ongoing
exposures (treatments) on outcomes. This effect risks pointing to
erroneous conclusions. The incident exposure targets initiation into
treatment, which is typically the effect we obtain from experiments. To
obtain the incident exposure effect, we generally require that events in
the data can be accurately classified into at least three relative time
intervals (refer to Hernán \emph{et al.}
(\citeproc{ref-hernuxe1n2016}{2016a}); Danaei \emph{et al.}
(\citeproc{ref-danaei2012}{2012}); VanderWeele \emph{et al.}
(\citeproc{ref-vanderweele2020}{2020}); Bulbulia
(\citeproc{ref-bulbulia2022}{2022})).

\subsection{Summary Part 3}\label{summary-part-3}

The examples in Part 3 reveal that the ability to order treatments,
outcomes, and their common causes on a timeline is necessary for
obtaining valid inferences. When timing is ensured, we can use Pearl's
backdoor path adjustment algorithm to evaluate identification, subject
to the assumptions encoded in a causal directed acyclic graph.

\newpage{}

\subsection{Part 4 How Causal Directed Acyclic Graphs Clarify The
Insufficiency of the Timing of Events Recorded in Data}\label{id-sec-4}

We next present a series of illustrations that clarify ordering
variables in time is insufficient insurance against confounding biases.
Time is on your side but time is not enough. Again we read graphs by
rows from left to right, such that \(\mathcal{G}_{1.1}\) is the
left-most graph in a row, followed by \(\mathcal{G}_{1.2} \ldots\). All
graphs in Part 4 refer to Table~\ref{tbl-chronology-notenough}.

\begin{table}

\caption{\label{tbl-chronology-notenough}Common confounding scenarios in
which chronology is not enough.}

\centering{

\terminologychronologicalhygeineNOTENOUGH

}

\end{table}%

\subsubsection{Example 1: M-bias}\label{example-1-m-bias}

Table~\ref{tbl-chronology-notenough} \(\mathcal{G}_{1.1}\) illustrates
the threat of over-conditioning on pre-treatment variables. Suppose we
hope to estimate the effect of religious service attendance on
charitable donations. We obtain time-series data and include a rich set
of covariates, including baseline measures of religious service and
charity. Suppose there is no treatment effect. Suppose further that we
condition on loyalty measures, yet loyalty neither affects religious
service attendance nor charitable giving. Measures of loyalty are
included in the confounder set \(L\). However, imagine that loyalty is
affected by two unmeasured confounders. Imagine that one's childhood
upbringing (an unmeasured variable) affects both loyalty and
inclinations to religious service but not charitable giving. \(U_A\)
denotes this unmeasured confounder. Furthermore, suppose wealth affects
loyalty and charitable giving but not religious service. \(U_Y\) denotes
this unmeasured confounder. In this setting, because loyalty is a
collider of the unmeasured confounders, conditioning on loyalty opens a
path between treatment and outcome. This path is
\(A \associationred U_A \associationred U_Y \associationred Y\).

Table~\ref{tbl-chronology-notenough} \(\mathcal{G}_{1.2}\) clarifies a
response. If we are confident that \(\mathcal{G}_{1.1}\) describes the
structural features of confounding, we should not condition on loyalty.

\subsubsection{Example 2: M-bias Where the Pre-treatment Collider is a
Confounder}\label{example-2-m-bias-where-the-pre-treatment-collider-is-a-confounder}

Table~\ref{tbl-chronology-notenough} \(\mathcal{G}_{2.1}\) illustrates
the threat of incorrigible confounding. Imagine the scenario in
\(\mathcal{G}_{1.1}\) and \(\mathcal{G}_{1.2}\) but with one change.
Loyalty is indeed a common cause of religious service attendance (the
treatment) and charitable giving (the outcome). If we do not condition
on loyalty, we have unmeasured confounding. This is bad. If we condition
on loyalty, as we have just considered, we also have unmeasured
confounding. This is also bad.

Table~\ref{tbl-chronology-notenough} \(\mathcal{G}_{2.2}\) clarifies a
response. Suppose that although we have not measured wealth, we have
measured a surrogate of wealth, say neighbourhood deprivation.
Conditioning on this surrogate is akin to conditioning on the unmeasured
confounder; we should adjust for neighbourhood deprivation.

\subsubsection{Example 3: Opportunities for Post-treatment Conditioning
for Confounder
Control}\label{example-3-opportunities-for-post-treatment-conditioning-for-confounder-control}

Table~\ref{tbl-chronology-notenough} \(\mathcal{G}_{3.1}\) illustrates
the threat of unmeasured confounding. Suppose we are interested in
whether curiosity affects educational attainment. The effect might be
unclear. Curiosity might increase attention but it might also increase
distraction. Consider an unmeasured genetic factor \(U\) that influences
both curiosity and educational attainment, say anxiety. Suppose we do
not have early childhood measures of anxiety in our dataset. We have
unmeasured confounding. This is bad.

Table~\ref{tbl-chronology-notenough} \(\mathcal{G}_{3.2}\) clarifies a
response. Suppose \(U\) also affects melanin production in hair
follicles. If grey hair is an effect of curiosity along the path to
educational attainment and cannot be an effect of educational
attainment, we could diminish unmeasured confounding by adjusting for
grey hair in adulthood. This example illustrates how conditioning on a
variable that occurs after the treatment has occurred, or even after the
outcome has been observed, may prove useful for confounding control.
When considering adjustment strategies, it is sometimes useful to
consider adjustment on post-treatment confounders.

\subsubsection{Example 4: Residual Confounding After Conditioning on
Past Treatments and Past
Outcomes}\label{example-4-residual-confounding-after-conditioning-on-past-treatments-and-past-outcomes}

Table~\ref{tbl-chronology-notenough} \(\mathcal{G}_{4.1}\) illustrates
the threat of confounding even after adjusting for baseline measures of
the treatment and the outcome. Imagine that childhood deprivation, an
unmeasured variable, affects both religious service attendance and
charitable giving. Despite adjusting for religious status and charitable
giving at baseline, childhood deprivation might affect the incidence of
change in one or both variables, inducing a longitudinal association
between religious service and charitable giving without a causal
association.

Table~\ref{tbl-chronology-notenough} \(\mathcal{G}_{4.2}\) clarifies a
response. This response pertains to all `real-world' observational
studies. Perform sensitivity analyses (refer to Linden \emph{et al.}
(\citeproc{ref-linden2020EVALUE}{2020})); seek negative controls (refer
to Hernan and Robins (\citeproc{ref-hernan2024WHATIF}{2024})).

\subsubsection{Example 5: Intermediary Confounding in Causal
Mediation}\label{example-5-intermediary-confounding-in-causal-mediation}

Table~\ref{tbl-chronology-notenough} \(\mathcal{G}_5\) illustrates the
threat of treatment confounding in causal mediation. Imagine that the
treatment is randomised; there is no treatment-outcome confounding. Nor
is there treatment-mediator confounding. \(\mathcal{R} \to A\) ensures
that backdoor paths from the treatment to the outcome are closed. We may
obtain biased results despite randomisation because the mediator is not
randomised. Suppose we are interested in whether the effects of COVID-19
lockdowns on psychological distress were mediated by levels of
satisfaction with the government. Suppose that assignment to COVID-19
lockdowns was random, and that time series data taken before COVID-19
provides comparable population-level contrasts. Despite random
assignment to treatment, assume that there are variables that may affect
both satisfaction with the government and psychological distress. For
example, job security or relationship satisfaction might plausibly
function as common causes of the mediator (government satisfaction) and
the outcome (psychological distress). To obtain valid inference for the
mediator-outcome path, we must control for these common causes.

Table~\ref{tbl-chronology-notenough} \(\mathcal{G}_5\) reveals the
difficulty in decomposing the total effect of COVID-19 on psychological
distress into the direct effect of COVID-19 that is not mediated by
satisfaction with the government and the indirect effect that is
mediated. Let us assume that confounders of the mediator-outcome path
are themselves potentially affected by the treatment. In this example,
imagine that COVID-19 lockdowns affect relationship satisfaction because
couples are trapped in ``captivity.'' Imagine further that COVID-19
lockdowns affect job security, which is reasonable if one owns a
street-facing business. If we adjust for these intermediary variables
along the path between the treatment and outcome, we will partially
block the treatment-mediator path. This means that we will not be able
to obtain a natural indirect effect estimate that decomposes the effect
of the treatment into that part that goes through the intermediary path
\(A \associationred V \associationred M \associationred Y\) and that
part that goes through the mediated path independently of \(V\), namely
\(A \associationred V \associationred M \associationred Y\), at least
not without additional assumptions and methods to disentangle separable
effects or that permute the estimand (refer to Dı́az \emph{et al.}
(\citeproc{ref-Diaz2023}{2023}); Shpitser \emph{et al.}
(\citeproc{ref-shpitser2022multivariate}{2022}); Stensrud \emph{et al.}
(\citeproc{ref-stensrud2023conditional}{2023})). However, it may be
possible to estimate controlled direct effects---that is, direct effects
when the mediator is fixed to different levels (refer to Greenland
\emph{et al.} (\citeproc{ref-greenland1999}{1999}); VanderWeele
(\citeproc{ref-vanderweele2015}{2015})). We consider intermediate
confounding in more detail in Bulbulia
(\citeproc{ref-bulbulia2024swigstime}{2024b}).

\subsubsection{Example 6: Treatment Confounder Feedback in Sequential
Treatments}\label{example-6-treatment-confounder-feedback-in-sequential-treatments}

Table~\ref{tbl-chronology-notenough} \(\mathcal{G}_6\) illustrates the
threat of treatment confounder feedback in sequential treatment regimes.
Suppose we are interested in whether beliefs in big gods affect social
complexity. Suppose that beliefs in big gods affect economic trade and
that economic trade may affect beliefs in big gods and social
complexity. Suppose the historical record is fragmented such that there
are unmeasured variables that affect both trade and social complexity.
Even if these unmeasured variables do not affect the treatment,
conditioning on the \(L\) (a confounder) and sequential treatment opens
a backdoor path
\(A \associationred L \associationred U \associationred Y\). We have
confounding.

Table~\ref{tbl-chronology-notenough} \(\mathcal{G}_6\) reveals the
difficulty of sequentially estimating causal effects. To estimate an
effect requires special estimators under the assumption of sequential
randomisation for fixed treatments and the assumption of strong
sequential randomisation for time-varying treatments---that is, for
treatments whose levels depend on past treatments and confounders (refer
to Robins (\citeproc{ref-robins1986}{1986}); Hernán \emph{et al.}
(\citeproc{ref-hernan2004STRUCTURAL}{2004}); Van Der Laan and Rose
(\citeproc{ref-vanderlaan2011}{2011}); Van Der Laan and Rose
(\citeproc{ref-vanderlaan2018}{2018}); Haneuse and Rotnitzky
(\citeproc{ref-haneuse2013estimation}{2013}); Young \emph{et al.}
(\citeproc{ref-young2014identification}{2014}); Rotnitzky \emph{et al.}
(\citeproc{ref-rotnitzky2017multiply}{2017}); Richardson and Robins
(\citeproc{ref-richardson2013}{2013a}); Dı́az \emph{et al.}
(\citeproc{ref-diaz2021nonparametric}{2021}); Williams and Díaz
(\citeproc{ref-williams2021}{2021}); Hoffman \emph{et al.}
(\citeproc{ref-hoffman2023}{2023})).

Importantly, we have six potential contrasts for the two sequential
treatments: beliefs in big gods at both time points vs.~beliefs in big
gods at neither time point; beliefs in big gods first, then lost
vs.~never believing in big gods at both:

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.1333}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.5467}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3200}}@{}}
\caption{Table outlines four fixed treatment regimens and six causal
contrasts in time-series data where treatments vary over
time.}\label{tbl-regimens}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Type
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Description
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Counterfactual Outcome
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Type
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Description
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Counterfactual Outcome
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Regime & Always believe in big gods & \(Y(1,1)\) \\
Regime & Never believe in big gods & \(Y(0,0)\) \\
Regime & Believe once first, then scepticism & \(Y(1,0)\) \\
Regime & Start with scepticism, then believe & \(Y(0,1)\) \\
Contrast & Always believe vs.~Never believe & \(E[Y(1,1) - Y(0,0)]\) \\
Contrast & Always believe vs.~Treat once first &
\(E[Y(1,1) - Y(1,0)]\) \\
Contrast & Always believe vs.~Treat once second &
\(E[Y(1,1) - Y(0,1)]\) \\
Contrast & Never believe vs.~Treat once first &
\(E[Y(0,0) - Y(1,0)]\) \\
Contrast & Never believe vs.~Treat once second &
\(E[Y(0,0) - Y(0,1)]\) \\
Contrast & Believe once first vs.~Believe once second &
\(E[Y(1,0) - Y(0,1)]\) \\
\end{longtable}

We can compute six causal contrasts for these four fixed regimens, as
shown in Table~\ref{tbl-regimens}.

A limitation of directed acyclic causal diagrams is that we do not
project factorisations of the counterfactual contrasts onto the graphs
themselves. To evaluate counterfactual identification, using Single
World Intervention Graphs (refer to Robins and Richardson
(\citeproc{ref-robins2010alternative}{2010}); Richardson and Robins
(\citeproc{ref-richardson2013swigsprimer}{2013b}); Richardson and Robins
(\citeproc{ref-richardson2023potential}{2023})) can be helpful. We
consider intermediate confounding in more detail in Bulbulia
(\citeproc{ref-bulbulia2024swigstime}{2024b}).

\subsubsection{Example 7: Collider Stratification Bias in Sequential
Treatments}\label{example-7-collider-stratification-bias-in-sequential-treatments}

Table~\ref{tbl-chronology-notenough} \(\mathcal{G}_7\) illustrates the
threat of confounding bias in sequential treatments even without
treatment-confounder feedback. Assume the setting is \(\mathcal{G}_6\)
with two differences. First, assume that the treatment, beliefs in big
gods, does not affect trade networks. However, assume that an unmeasured
confounder affects both the beliefs in big gods and the confounder,
trade networks. Such a confounder might be openness to outsiders, a
feature of ancient cultures for which no clear measures are available.
We need not imagine that treatment affects future states of confounders
for time-varying confounding. It would be sufficient to induce bias for
an unmeasured confounder to affect the treatment and the confounder, in
the presence of another confounder that affects both the confounder and
the outcome.

Table~\ref{tbl-chronology-notenough} \(\mathcal{G}_7\) reveals the
challenges of sequentially estimating causal effects. Yet again, to
estimate causal effects here requires special estimators, under the
assumption of sequential randomisation for fixed treatments, and the
assumption of strong sequential randomisation for time-varying
treatments (refer to Robins (\citeproc{ref-robins1986}{1986}); Hernán
\emph{et al.} (\citeproc{ref-hernan2004STRUCTURAL}{2004}); Van Der Laan
and Rose (\citeproc{ref-vanderlaan2011}{2011}); Van Der Laan and Rose
(\citeproc{ref-vanderlaan2018}{2018}); Haneuse and Rotnitzky
(\citeproc{ref-haneuse2013estimation}{2013}); Young \emph{et al.}
(\citeproc{ref-young2014identification}{2014}); Rotnitzky \emph{et al.}
(\citeproc{ref-rotnitzky2017multiply}{2017}); Richardson and Robins
(\citeproc{ref-richardson2013}{2013a}); Dı́az \emph{et al.}
(\citeproc{ref-diaz2021nonparametric}{2021}); Williams and Díaz
(\citeproc{ref-williams2021}{2021}); Hoffman \emph{et al.}
(\citeproc{ref-hoffman2023}{2023})). We note again that a specific
causal contrast must be stated, and we must ask, for which cultures do
causal effects generalise.

Readers should be aware that merely applying currently popular tools of
time-series data analysis---multi-level models and structural equation
models---will not overcome the threats of confounding in sequential
treatments. Applying models to data will not recover consistent causal
effect estimates. Again, space constraints prevent us from discussing
statistical estimands and estimation here (refer to Bulbulia
(\citeproc{ref-bulbulia2024PRACTICAL}{2024a})).

\subsubsection{Summary Part 4}\label{summary-part-4}

Directed acyclic graphs reveal that ensuring the timing of events in
one's data does not ensure identification. In some cases, certain
mediated effects cannot be identified by any data, as we discussed in
the context of mediation. However, across the human sciences, we often
apply statistical models to data and interpret the outputs as
meaningful. The key takeaway from this article is that causal diagrams
demonstrate that standard approaches, no matter how sophisticated, are
often hazardous and can lead to misleading conclusions. (Refer to
Bulbulia (\citeproc{ref-bulbulia2024swigstime}{2024b}))

\subsection{Part 5. Creating Causal Diagrams: Pitfalls and
Tips}\label{id-sec-5}

The primary interest of causal diagrams is to address
\textbf{identification problems}. Pearl's backdoor adjustment theorem
proves that if we adopt an adjustment set such that \(A\) and \(Y\) are
d-separated, and furthermore do not condition on a variable along the
path from \(A\) to \(Y\), association is causation.

Here is how investigators may construct safe and effective directed
acyclic graphs.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Clarify the causal question and target population}

  An identification strategy is relative to the question at hand. The
  adjustment criteria for estimating an effect of \(A\) on \(Y\) will
  generally differ from the adjustment criteria for estimating an effect
  of \(Y\) on \(A\). Before attempting to draw any causal diagram, state
  the problem your diagram addresses and the population to whom it
  applies. Consider further that in adopting a specific identification
  strategy for a treatment or set of treatments, the coefficients we
  obtain for the other variables in the model will often be biased
  causal effect estimates for those variables. Moreover, the
  coefficients we obtain from statistical models we develop to estimate
  causal effects will typically not have a marginal interpretation
  unless we employ marginal structural models (refer to Cole and Hernán
  (\citeproc{ref-cole2008}{2008}); VanderWeele
  (\citeproc{ref-vanderweele2009a}{2009b})). This structural implication
  has wide-ranging implications for scientific reporting. For example,
  if regression coefficients are reported at all, they should be
  reported with clearly labelled warnings against interpreting them as
  having any meaning or interpretation (refer to Westreich and Greenland
  (\citeproc{ref-westreich2013}{2013}); McElreath
  (\citeproc{ref-mcelreath2020}{2020}); Bulbulia
  (\citeproc{ref-bulbulia2023}{2023})). Powerful machine learning
  algorithms treat these parameters as nuisance, and in many cases,
  coefficients cannot be obtained. Referees of human science journals
  will need to be alerted to this fact and retrained.
\item
  \textbf{Consider whether the three fundamental assumptions for causal
  inference may be satisfied}

  Merely possessing data, even if the data are richly detailed
  time-series data, does not mean our causal questions will find
  answers. Along with identification, we must also consider the causal
  consistency and positivity assumptions (refer to
  \hyperref[id-sec-1]{Part 1}.
\item
  \textbf{Clarify the meanings of symbols and conventions}

  It is fair to say that the state of terminology in causal inference is
  a dog's breakfast. Meanings and conventions vary not only for
  terminology but also for causal graphs. For example, whereas we have
  denoted unmeasured confounders using the variable \(U\), those who
  follow Pearl will often draw a bi-directional arrow. Although
  investigators will have their preferences, there is generally little
  substantive interest in one's conventions, only that they are made
  clear, frequently repeated (as I have done for each graph table), and
  applied correctly.
\item
  \textbf{Include all common causes of the treatment and outcome}

  Once we have stated our causal question, we are ready to create a
  draft of our causal graph. This graph should incorporate the most
  recent common causes (parents) of both the treatment and the outcome,
  or, where measures are not available, measures for available proxies.

  Where possible, aggregate functionally similar common causes into a
  single variable notation. For example, include all functionally
  similar demographic variables in \(L_0\).

  Recall that a causal directed acyclic graph \emph{asserts} structural
  assumptions. Merely because one has become expert in crafting causal
  diagrams does not ensure that one will be an expert in encoding
  plausible structural assumptions. The processes of creating and
  revising them should be detailed in published research, typically in
  supplements.
\item
  \textbf{Consider potential unmeasured confounders}

  We leverage domain expertise not merely to clarify measured sources of
  confounding but also---and perhaps most importantly---to clarify
  possible unmeasured confounding. We should include these in our causal
  diagrams.

  Note that because we cannot guard against all unmeasured confounding,
  we will always need to perform sensitivity analyses and consider
  negative control treatments, negative control outcomes, instrumental
  variables, and other strategies for improving the clarity of our
  causal inferences and their limitations.
\item
  \textbf{Ensure the causal directed acyclic graph is acyclic}

  Although not strictly necessary, it may be useful to annotate the
  temporal sequence of events using subscripts (e.g., \(L_0\), \(A_1\),
  \(Y_2\)), as we have done here. Moreover, spatially ordering your
  directed acyclic graph to reflect the progression of causality in
  time---either left-to-right or top-to-bottom---will often enhance the
  graph's comprehensibility. Again, although establishing temporal
  ordering is not sufficient for addressing identification problems, it
  is necessary.
\item
  \textbf{Represent paths structurally, not parametrically}

  Whether a path is linear is unimportant for causal
  identification---and remember causal diagrams are tools for causal
  identification. Focus on whether paths exist, not their functional
  form (linear, non-linear, etc.).

  Consider a subway map of Paris. We do not include all the streets on
  this map, all noteworthy sites, or a detailed overview of the holdings
  by room in the Louvre. We use other maps for these purposes. Almost
  every detail about assumed reality must be left out of a causal
  diagram if it is to be useful for causal identification.
\item
  \textbf{Minimise paths to those necessary for addressing an
  identification problem}

  Reduce clutter; only include paths critical for a specific question
  (e.g., backdoor paths, mediators). For example, in
  Table~\ref{tbl-chronology-notenough} \(\mathcal{G} 6\) and
  Table~\ref{tbl-chronology-notenough} \(\mathcal{G} 7\), I did not draw
  arrows from the first treatment to the second treatment. Although I
  assume that such arrows exist, drawing them was not, in these
  examples, relevant to evaluating the identification problem at hand.
\item
  \textbf{When temporal order is unknown, explicitly represent this
  uncertainty on your causal diagram}

  In many settings, the relevant timing of events cannot be ascertained
  with confidence. Here we have adopted a convention of indexing nodes
  with uncertain timing using \(X_{\phi t}\) notation. There is no
  widely adopted convention for representing uncertainty in timing. We
  are only bound to be clear.
\item
  \textbf{Create, report, and deploy multiple graphs}

  Causal inference turns on assumptions. Experts might disagree. Where
  the structure of reality encoded in a causal graph is uncertain or
  debated, investigators should produce multiple causal diagrams that
  reflect uncertainties and debates.

  By stating different assumptions and adopting multiple modelling
  strategies that accord with these different assumptions, we might
  discover that our causal conclusions are robust to differences in
  structural assumptions. Even where the implications of different
  structural assumptions lead to opposing causal inferences, such
  knowledge might better direct future data collection to eventually
  settle such differences. The overarching requirement of causal
  inference, as with other areas of science, is to truthfully advance
  empirical understanding. Assertions are poor substitutes for honesty.
  Rather than asserting one causal directed graph, investigators might
  want to follow the implications of several.
\item
  \textbf{Use automated identification algorithms with care}

  Automated software can assist with identification tasks, which amount
  to factorising typically complex conditional independencies. However,
  automated software may not converge on identifying the optimal set of
  confounders in the presence of intractable confounding.

  Consider Tyler Vanderweele's \textbf{modified disjunctive cause
  criterion}. VanderWeele (\citeproc{ref-vanderweele2019}{2019})
  recommends obtaining a maximally efficient adjustment, which he calls
  a `confounder set.' A member of this set is any set of variables that
  can reduce or remove structural sources of bias. The strategy is as
  follows:

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \item
    Control for any variable that causes the treatment, the outcome, or
    both.
  \item
    Control for any proxy for an unmeasured variable that is a shared
    cause of both the treatment and outcome.
  \item
    Define an instrumental variable as a variable associated with the
    treatment but does not influence the outcome independently, except
    through the treatment. Exclude any instrumental variable that is not
    a proxy for an unmeasured confounder from the confounder set
    (\citeproc{ref-vanderweele2019}{VanderWeele 2019}).
  \end{enumerate}

  Vanderweele's modified disjunctive cause criterion is an excellent
  strategy for selecting an optimal confounder set. However, this set
  might not, and likely will not, remove all structural sources of
  confounding bias in most observational settings. As such, an automated
  algorithm might reject it. However, we might think that such rejection
  is unwise because where treatment assignment has not been randomised,
  we should almost always draw relations of unmeasured confounding on
  our causal graphs. Rejecting causal inferences in observational
  settings entirely would be unwise because there are many examples in
  which they closely approximate randomised control trials (refer to
  Hernán \emph{et al.} (\citeproc{ref-hernan2016}{2016b}); Hernán and
  Robins (\citeproc{ref-hernan2006estimating}{2006b}); Hernán \emph{et
  al.}
  (\citeproc{ref-hernan2008aObservationalStudiesAnalysedLike}{2008b})).

  For example, return to Table~\ref{tbl-chronology-notenough}
  \(\mathcal{G} 2.1\). We encountered intractable confounding. What if
  there were no proxy for an unmeasured confounder? Should we condition
  on the measured confounder and induce M-bias or leave the backdoor
  path from the measured confounder open? Or should we simply not
  attempt causal inferences? The answer will depend on assumptions about
  the relative strength of confounding in the graph. In place of a
  generic strategy, we require subject-specialist expertise.
\item
  \textbf{Clarify assumptions about structural bias from measurement
  error and restriction bias}

  Space constraints prevented us from examining how causal directed
  acyclic graphs may be used to clarify structural biases from
  measurement error and restrictions of the target population in the
  sample population at the start and end of the study. We can (and
  should) examine structural features of bias in these settings. Refer
  to Bulbulia (\citeproc{ref-bulbulia2024wierd}{2024d}) for an overview.
\end{enumerate}

\subsection{Conclusions}\label{id-sec-6}

\paragraph{Limitations}\label{limitations}

\textbf{First, I have focussed here on the application of causal
diagrams to confounding bias, not bias generally}: Causal directed
acyclic graphs can also be extended to evaluate measurement-error biases
and some features of target population restriction bias (also called
`selection restriction bias') (refer to Hernán
(\citeproc{ref-hernan2017SELECTIONWITHOUTCOLLIDER}{2017}); Liu \emph{et
al.} (\citeproc{ref-liu2023application}{2023}); Hernan and Robins
(\citeproc{ref-hernan2024WHATIF}{2024}); Hernán and Cole
(\citeproc{ref-hernan2009MEASUREMENT}{2009}); VanderWeele and Hernán
(\citeproc{ref-vanderweele2012MEASUREMENT}{2012})). Valid causal
inferences require addressing all structural sources of bias. This work
does not aim for complete coverage but hopes to stimulate curiosity.
(Refer to Bulbulia (\citeproc{ref-bulbulia2024wierd}{2024d}) for an
overview.)

\textbf{Second, I have not reviewed other graphical tools for
identification, such as Single World Intervention Graphs}: Although
causal directed acyclic graphs are powerful tools for addressing
identification problems, they are not the only graphical tools
researchers use to investigate causality. For example, Robins
(\citeproc{ref-robins1986}{1986}) developed the `finest fully randomized
causally interpreted structured tree graph (FFRCISTG),' which has been
more recently revived and simplified in Single World Intervention Graphs
(refer to Richardson and Robins
(\citeproc{ref-richardson2013swigsprimer}{2013b})). Such graphs
explicitly factorise counterfactual states, which can be helpful for
identification in complex longitudinal settings. Moreover, for some, the
representation of counterfactual states on a graph is more satisfying,
as it allows inspection of the conditional independence of expectations
over \(Y(a^*)\) and \(Y(a)\) separately on a graph. Refer to Bulbulia
(\citeproc{ref-bulbulia2024swigstime}{2024b}) for use cases.

\textbf{Third, I have not reviewed workflows downstream of causal
identification}: I have not discussed statistical estimands, statistical
estimation, and the interpretations and reporting of causal inferences,
all of which come downstream of causal graphs in the workflows of causal
inference. Rapid developments in machine learning present applied
researchers with new tools for handling model misspecification (refer to
Van Der Laan and Rose (\citeproc{ref-vanderlaan2018}{2018}); Laan and
Gruber (\citeproc{ref-van2012targeted}{2012}); Dı́az \emph{et al.}
(\citeproc{ref-diaz2021nonparametric}{2021}); Williams and Díaz
(\citeproc{ref-williams2021}{2021}); Hoffman \emph{et al.}
(\citeproc{ref-hoffman2023}{2023})) and for assessing treatment effect
heterogeneity (refer to Athey \emph{et al.}
(\citeproc{ref-athey2019}{2019}); Athey and Wager
(\citeproc{ref-athey2021}{2021}); Wager and Athey
(\citeproc{ref-wager2018}{2018}); Vansteelandt and Dukes
(\citeproc{ref-vansteelandt2022a}{2022})). Those interested in workflows
within the human sciences might consider VanderWeele \emph{et al.}
(\citeproc{ref-vanderweele2020}{2020}); the workflows in my research
group may be found here: Bulbulia
(\citeproc{ref-bulbulia2024PRACTICAL}{2024a}). However, I expect all
such workflows will evolve considerably in the near future.

Nevertheless, after precisely stating our causal question, the most
difficult and important challenge is considering whether and how it
might be identified in the data. I believe the `statistical models
first' approach routinely applied in most human sciences is soon ending.
This approach has been attractive because it is relatively easy to
implement---the methods do not require extensive training---and because
the application of statistical models to data appears rigorous. However,
if the coefficients we recover from these methods have meaning, this is
typically accidental. It is not merely that the coefficients are
uninformative about what works and why; absent a causal framework, they
do not have any meaning (\citeproc{ref-ogburn2021}{Ogburn and Shpitser
2021}).

There are many good resources available for learning causal directed
acyclic graphs (\citeproc{ref-barrett2021}{Barrett 2021};
\citeproc{ref-cinelli2022}{Cinelli \emph{et al.} 2022};
\citeproc{ref-greenland1999}{Greenland \emph{et al.} 1999},
\citeproc{ref-greenland1999}{1999}; \citeproc{ref-hernan2023}{Hernan and
Robins 2023}, \citeproc{ref-hernan2024WHATIF}{2024};
\citeproc{ref-major2023exploring}{Major-Smith 2023};
\citeproc{ref-mcelreath2020}{McElreath 2020};
\citeproc{ref-morgan2014}{Morgan and Winship 2014};
\citeproc{ref-pearl2009a}{Pearl 2009}; \citeproc{ref-rohrer2018}{Rohrer
2018}; \citeproc{ref-suzuki2020}{Suzuki \emph{et al.} 2020}). This work
aims to add to these resources by:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Providing additional conceptual orientation to the frameworks and
  workflows of causal data science, highlighting the risks of applying
  causal graphs without this understanding.
\item
  Using causal diagrams to emphasise the importance of ensuring relative
  timing for the variables whose causal relationships are represented on
  the graph.
\item
  Employing causal diagrams to clarify the limitations of longitudinal
  data for certain questions in causal mediation and time-varying
  confounding under time-varying treatments, which remain areas of
  confusion in many human sciences (see Bulbulia
  (\citeproc{ref-bulbulia2024swigstime}{2024b}) for a detailed
  explanation).
\end{enumerate}

\subsubsection{Neurath's Boat: On the Priority of Assumptions in
Science}\label{neuraths-boat-on-the-priority-of-assumptions-in-science}

We might wonder, ``If not from the data, where do our assumptions about
causality come from?'' We have said that they must come from expert
knowledge. This reliance on expert knowledge might seem counterintuitive
for building scientific knowledge---shouldn't we use data to build
knowledge, not the other way around? Isn't scientific history a record
of expert opinions being undone?

The Austrian philosopher Otto Neurath famously described scientific
progress using the metaphor of a ship that must be rebuilt at sea:

\begin{quote}
\ldots{} every statement about any happening is saturated with
hypotheses of all sorts and these in the end are derived from our whole
world-view. We are like sailors who on the open sea must reconstruct
their ship but are never able to start afresh from the bottom. Where a
beam is taken away a new one must at once be put there, and for this the
rest of the ship is used as support. In this way, by using the old beams
and driftwood, the ship can be shaped entirely anew, but only by gradual
reconstruction. (\citeproc{ref-neurath1973}{Neurath 1973 p. 199})
\end{quote}

This quotation emphasises the iterative process of accumulating
scientific knowledge; new insights are formed from the foundation of
existing knowledge.

Causal diagrams are at home in Neurath's boat. The tradition of science
that believes knowledge develops solely from the results of statistical
tests applied to data should be resisted. The data have never fully
contained the answers we seek. When reconstructing knowledge, we have
always relied on assumptions. Causal graphs enable us to make these
assumptions explicit and to understand what we obtain based on them.

\newpage{}

\subsection{Funding}\label{funding}

This work is supported by a grant from the Templeton Religion Trust
(TRT0418) and RSNZ Marsden 3721245, 20-UOA-123; RSNZ 19-UOO-090. I also
received support from the Max Planck Institute for the Science of Human
History. The Funders had no role in preparing the manuscript or deciding
to publish it.

\subsection{Acknowledgements}\label{acknowledgements}

I am grateful to Dr Inkuk Kim for checking previous versions of this
manuscript and offering feedback.

I am also grateful to two anonymous reviewers and the editor, Charles
Efferson, of \emph{Evolutionary Human Sciences} for their constructive
feedback that improved this manuscript.

Any remaining errors are my own.

\newpage{}

\subsection{Appendix A: Glossary}\label{id-app-a}

\begin{table}

\caption{\label{tbl-experiments}Glossary}

\centering{

\glossaryTerms

}

\end{table}%

\newpage{}

\subsection{Appendix B: Causal Inference in History: The Difficulty in
Satisfying the Three Fundamental Assumptions}\label{id-app-b}

Consider the Protestant Reformation of the 16th century, which initiated
religious change throughout much of Europe. Historians have argued that
Protestantism caused social, cultural, and economic changes in those
societies where it took hold (see: Weber
(\citeproc{ref-weber1905}{1905}); Weber
(\citeproc{ref-weber1993}{1993}); Swanson
(\citeproc{ref-swanson1967}{1967}); Swanson
(\citeproc{ref-swanson1971}{1971}); Basten and Betz
(\citeproc{ref-basten2013}{2013}), and for an overview, see: Becker
\emph{et al.} (\citeproc{ref-becker2016}{2016})).

Suppose we are interested in estimating the Protestant Reformation's
`Average Treatment Effect'. Let \(A = a^*\) denote the adoption of
Protestantism. We compare this effect with that of remaining Catholic,
represented as \(A = a\). We assume that both the concepts of `adopting
Protestantism' and `economic development' are well-defined (e.g., GDP +1
century after a country has a Protestant majority contrasted with
remaining Catholic). The causal effect for any individual country is
\(Y_i(a^*) - Y_i(a)\). Although we cannot identify this effect, if the
basic assumptions of causal inference are met, we can estimate the
average or marginal effect by conditioning on the confounding effects of
\(L\):

\[
ATE_{\textnormal{economic~development}} = \mathbb{E}[Y(\textnormal{Became~Protestant}|L) - Y(\textnormal{Remained~Catholic}|L)]
\]

When asking causal questions about the economic effect of adopting
Protestantism versus remaining Catholic, several challenges arise
regarding the three fundamental assumptions required for causal
inference.

\textbf{Causal Consistency}: This requires that the outcome under each
level of treatment to be compared is well-defined. In this context,
defining what `adopting Protestantism' and `remaining Catholic' mean may
present challenges. The practices and beliefs of each religion might
vary significantly across countries and time periods, making it
difficult to create a consistent, well-defined treatment. Furthermore,
the outcome---economic development---may also be challenging to measure
consistently across different countries and time periods.

There is undoubtedly considerable heterogeneity in the `Protestant
treatment.' In England, Protestantism was closely tied to the monarchy
(\citeproc{ref-collinson2003}{Collinson 2003}). In Germany, Martin
Luther's teachings emphasized individual faith in scripture, which, it
has been claimed, supported economic development by promoting literacy
(\citeproc{ref-gawthrop1984}{Gawthrop and Strauss 1984}). In England,
King Henry VIII abolished Catholicism
(\citeproc{ref-collinson2003}{Collinson 2003}). The Reformation, then,
occurred differently in different places. The treatment needs to be
better defined.

There is also ample scope for interference: 16th-century societies were
interconnected through trade, diplomacy, and warfare. Thus, the
religious decisions of one society were unlikely to have been
independent from those of other societies.

\textbf{Exchangeability}: This requires that given the confounders, the
potential outcomes are independent of the treatment assignment. It might
be difficult to account for all possible confounders in this context.
For example, historical, political, social, and geographical factors
could influence both a country's religious affiliations and its economic
development.

\textbf{Positivity}: This requires that there is a non-zero probability
of every level of treatment for every strata of confounders. If we
consider various confounding factors such as geographical location,
historical events, or political circumstances, some countries might only
ever have the possibility of either remaining Catholic or becoming
Protestant, but not both. For example, it is unclear under which
conditions 16th-century Spain could have been randomly assigned to
Protestantism (\citeproc{ref-nalle1987}{Nalle 1987};
\citeproc{ref-westreich2010}{Westreich and Cole 2010}).

Perhaps a more credible measure of effect in the region of our interests
is the Average Treatment Effect in the Treated (ATT) expressed:

\[
ATT_{\textnormal{economic~development}} = \mathbb{E}[(Y(a^*) - Y(a))|A = a^*,L]
\]

Where \(Y(a^*)\) represents the potential outcome if treated, and
\(Y(a)\) represents the potential outcome if not treated. The
expectation is taken over the distribution of the treated units (i.e.,
those for whom \(A = a^*\)). \(L\) is a set of covariates on which we
condition to ensure that the potential outcomes \(Y(a^*)\) and \(Y(a)\)
are independent of the treatment assignment \(A\), given \(L\). This
accounts for any confounding factors that might bias the estimate of the
treatment effect.

Here, the ATT defines the expected difference in economic success for
cultures that became Protestant compared with the expected economic
success if those cultures had not become Protestant, conditional on
measured confounders \(L\), among the exposed (\(A = a^*\)). To estimate
this contrast, our models would need to match Protestant cultures with
comparable Catholic cultures effectively. By estimating the ATT, we
avoid the assumption of non-deterministic positivity for the untreated.
However, whether matching is conceptually plausible remains debatable.
Ostensibly, it would seem that assigning a religion to a culture is not
as easy as administering a pill (\citeproc{ref-watts2018}{Watts \emph{et
al.} 2018}).

\newpage{}

\subsubsection{Appendix C: Causal Consistency Under Multiple Versions of
Treatment}\label{id-app-c}

To better understand how the causal consistency assumption might fail,
consider a question discussed in the evolutionary human science
literature about whether a society's beliefs in big gods affect its
development of social complexity (\citeproc{ref-beheim2021}{Beheim
\emph{et al.} 2021}; \citeproc{ref-johnson2015}{Johnson 2015};
\citeproc{ref-norenzayan2016}{Norenzayan \emph{et al.} 2016};
\citeproc{ref-sheehan2022}{Sheehan \emph{et al.} 2022};
\citeproc{ref-slingerland2020coding}{Slingerland \emph{et al.} 2020};
\citeproc{ref-watts2015}{Watts \emph{et al.} 2015};
\citeproc{ref-whitehouse2023}{Whitehouse \emph{et al.} 2023}).
Historians and anthropologists report that such beliefs vary over time
and across cultures in intensity, interpretations, institutional
management, and rituals (\citeproc{ref-bulbuliaj.2013}{Bulbulia, J.
\emph{et al.} 2013}; \citeproc{ref-decoulanges1903}{De Coulanges 1903};
\citeproc{ref-geertz2013}{Geertz \emph{et al.} 2013};
\citeproc{ref-wheatley1971}{Wheatley 1971}). This variation in content
and settings could influence social complexity. Moreover, the treatments
realised in one society might affect those realised in other societies,
resulting in \emph{spill-over} effects in the exposures (`treatments')
to be compared (\citeproc{ref-murray2021a}{Murray \emph{et al.} 2021};
\citeproc{ref-shiba2023uncovering}{Shiba \emph{et al.} 2023}).

The theory of causal inference under multiple versions of treatment,
developed by VanderWeele and Hernán, formally addresses this challenge
of treatment-effect heterogeneity
(\citeproc{ref-vanderweele2009}{VanderWeele 2009a},
\citeproc{ref-vanderweele2018}{2018};
\citeproc{ref-vanderweele2013}{VanderWeele and Hernan 2013}). The
authors proved that if the treatment variations, \(K\), are
conditionally independent of the potential outcomes, \(Y(k)\), given
covariates \(L\), then conditioning on \(L\) allows us to consistently
estimate causal effects over the heterogeneous treatments
(\citeproc{ref-vanderweele2009}{VanderWeele 2009a}).

Where \(\coprod\) denotes independence, we may assume causal consistency
where the interventions to be compared are independent of their
potential outcomes, conditional on covariates, \(L\):

\[
K \coprod Y(k) | L
\]

According to the theory of causal inference under multiple versions of
treatment, we may think of \(K\) as a `coarsened indicator' for \(A\).

The theory also clarifies what we require to estimate causal effects in
the presence of interference or `spill-over', a special case of
treatment-effect heterogeneity. To handle interference, the potential
outcome of each unit \(i \neq j\) must be independent of its own
treatment received as well as the treatment that all other units
received on \(j \neq i\), conditional on measured covariates \(L\):

\[
Y_i(k) \coprod K_i, K_j | L, \quad \forall i, \forall j \neq i
\]

Although the theory of causal inference under multiple versions of
treatment provides a formal solution to the problem of treatment-effect
heterogeneity, computing and interpreting causal effect estimates under
this theory can be challenging.

Consider the question of whether a reduction in Body Mass Index (BMI)
affects health (\citeproc{ref-hernuxe1n2008}{Hernán and Taubman 2008}).
Weight loss can occur through various methods, each with different
health implications. Specific methods, such as regular exercise or a
calorie-reduced diet, benefit health. However, weight loss might result
from adverse conditions such as infectious diseases, cancers,
depression, famine, or accidental amputations, which are generally not
beneficial to health. Hence, even if causal effects of `weight loss'
could be consistently estimated when adjusting for covariates \(L\), we
might be uncertain about how to interpret the effect we are consistently
estimating. This uncertainty highlights the need for precise and
well-defined causal questions. For example, rather than stating the
intervention vaguely as `weight loss', we could state the intervention
clearly and specifically as `weight loss achieved through aerobic
exercise over at least five years, compared with no weight loss.' This
specificity in the definition of the treatment, along with comparable
specificity in the statement of the outcomes, helps ensure that the
causal estimates we obtain are not merely unbiased but also
interpretable; for discussion, see Hernán \emph{et al.}
(\citeproc{ref-hernuxe1n2022}{2022}); Murray \emph{et al.}
(\citeproc{ref-murray2021a}{2021}); Hernán and Taubman
(\citeproc{ref-hernuxe1n2008}{2008}).

Beyond uncertainties for the interpretation of heterogeneous treatment
effect estimates, there is the additional consideration that we cannot
fully verify from data whether the measured covariates \(L\) suffice to
render the multiple versions of treatment independent of the
counterfactual outcomes. This problem is acute when there is
\emph{interference}, which occurs when treatment effects are relative to
the density and distribution of treatment effects in a population. Scope
for interference will often make it difficult to warrant the assumption
that the potential outcomes are independent of the many versions of
treatment that have been realised, dependently, on the administration of
previous versions of treatments across the population
(\citeproc{ref-bulbulia2023a}{Bulbulia \emph{et al.} 2023};
\citeproc{ref-ogburn2022}{Ogburn \emph{et al.} 2022};
\citeproc{ref-vanderweele2013}{VanderWeele and Hernan 2013}).

In short, although the theory of causal inference under multiple
versions of treatment provides a formal solution for consistent causal
effect estimation in observational settings, \emph{treatment
heterogeneity} remains a practical threat. Generally, we should assume
that causal consistency is unrealistic unless proven innocent.

For now, we note that the causal consistency assumption provides a
theoretical starting point for recovering the missing counterfactuals
required for computing causal contrasts. It identifies half of these
missing counterfactuals directly from observed data. The concept of
conditional exchangeability, which we examine next, offers a means for
recovering the remaining half.

\newpage{}

\subsection{Appendix D Pearl's Do-Calculus and Structural Causal
Models}\label{id-app-d}

Below is Díaz \emph{et al.} (\citeproc{ref-duxedaz2021}{2021})'s
formulation of Pearl (\citeproc{ref-pearl2009a}{2009})'s mathematical
representation of a structural causal model. I use Díaz \emph{et al.}
(\citeproc{ref-duxedaz2021}{2021})'s formulation because it allows us to
present a structural causal model for dynamic treatment strategies
Richardson and Robins (\citeproc{ref-richardson2013}{2013a}), also known
as longitudinal modified treatment policies Hoffman \emph{et al.}
(\citeproc{ref-hoffman2023}{2023}).

We begin by defining the sequence of variables in our model:

\[
S_i= (W, Y_0, L_1, A_1, L_2, A_2, ..., L_\tau, A_\tau, Y_{\tau}) \sim \mathbf{P}
\]

where \(S_i\) is a sample from the distribution \(\mathbf{P}\) and
includes baseline covariates \(W\), intermediate outcomes \(L_t\),
treatments \(A_t\), and final outcomes \(Y_{\tau}\) over time periods
\(t = 1, 2, \ldots, \tau\).

We define the final outcome:

\[
Y = A_{\tau + 1}
\]

We define the history of all variables up to treatment \(A_t\) as:

\[
H_t = (\bar{A}_{t-1}, \bar{L}_t)
\]

Here, \(\bar{A}_{t-1}\) represents the history of treatments up to time
\(t-1\), and \(\bar{L}_t\) represents the history of intermediate
outcomes up to time \(t\).

We define the vector of exogenous variables (error terms). Note on
Pearl's structural causal model account, but not the potential outcomes
framework, the error terms must always be independent:

\[
U = (U_{L,t}, U_{A,t}, U_{Y}: t \in \{1 \dots \tau\})
\]

Where \(U\) describes the set of exogenous variables affecting \(L_t\),
\(A_t\), and \(Y\).

We assume the following deterministic functions for the intermediate
outcomes, treatments, and final outcome:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  For intermediate outcomes:
\end{enumerate}

\[
L_t = f_{L_t}(A_{t-1}, H_{t-1}, U_{L,t})
\]

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  For treatments:
\end{enumerate}

\[
A_t = f_{A_t}(H_t, U_{A,t})
\]

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  For the final outcome:
\end{enumerate}

\[
Y = f_{Y}(A_{\tau}, H_{\tau}, U_{Y})
\]

Longitudinal modified treatment policies (LMTPs) are defined as
functions that assign treatments flexibly based on individual co-variate
histories. Note that where there are multiple treatments, these
histories will be partially counterfactual.

We replace the deterministic function for treatments:

\[
A_t = f_{A_t}(H_t, U_{A,t})
\]

With the intervention function:

\[
A^\mathbf{g}_t
\]

On the structural causal model account, this intervention produces
counterfactual histories given:

\[
L_t(\bar{A}^\mathbf{g}_{t-1}) = f_{L_t}(A^\mathbf{g}_{t-1}, H_{t-1}(\bar{A}^\mathbf{g}_{t-2}), U_{L,t})
\]

For treatments, the counterfactual variable
\(A_t(\bar{A}^\mathbf{g}_{t-1})\) is defined as the natural value of the
treatment, i.e., the value of the treatment that would have been
observed at time \(t\) under the intervention history leading up to it
at \(t-1\), and then discontinued:

\[
A_t(\bar{A}^\mathbf{g}_{t-1}) = f_{A_t}(H_t(\bar{A}^\mathbf{g}_{t-1}), H_{t-1}(\bar{A}^\mathbf{g}_{t-2}), U_{L,t})
\]

When all variables are intervened on, the counterfactual final outcome
is:

\[
Y(\bar{A}^\mathbf{g}) = f_Y(A^\mathbf{g}_\tau, H_\tau(\bar{A}^\mathbf{g}_{\tau-1}), U_{Y})
\]

Again, mathematically the potential outcomes framework is mathematically
equivalent to the structural causal model framework when it is assumed
the the exogenous error structures are independent. However, the
potential outcomes framework is more general and clarifies how
identification is possible even when the error structures are not
independent. See Robins (\citeproc{ref-robins1986}{1986}), Richardson
and Robins (\citeproc{ref-richardson2013}{2013a}).

\newpage{}

\paragraph{Appendix E: Front Door Path Criterion}\label{id-app-e}

To obtain an unbiased estimate for the causal effect of \(A\) on \(C\)
using the front door criterion, we need to identify a set of variables
\(M\) that mediates the effect of \(A\) on \(C\).

Pearl defines the front door criterion more generally as follows: a set
of variables \(B\) satisfies the front door criterion relative to
variables \(A\) and \(C\) in a causal directed acyclic graph
\(\mathcal{G}\) if:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \(B\) is affected by \(A\).
\item
  \(B\) affects \(C\).
\item
  There are no backdoor paths from \(A\) to \(B\).
\item
  All backdoor paths from \(B\) to \(C\) are blocked by conditioning on
  \(A\).
\end{enumerate}

In other words, \(B\) must be an intermediate variable that captures the
entire causal effect of \(A\) on \(C\), with no confounding paths
remaining between \(A\) and \(B\), and any confounding between \(B\) and
\(C\) must be blocked by \(A\).

The frontdoor criterion is less widely used compared to the backdoor
criterion because it requires the identification of an appropriate
mediator that fully captures the causal effect
(\citeproc{ref-pearl2009a}{Pearl 2009}). Here, we state the frontdoor
path criterion for completeness.

\newpage{}

\subsection{References}\label{references}

\phantomsection\label{refs}
\begin{CSLReferences}{1}{0}
\bibitem[\citeproctext]{ref-athey2019}
Athey, S, Tibshirani, J, and Wager, S (2019) Generalized random forests.
\emph{The Annals of Statistics}, \textbf{47}(2), 1148--1178.
doi:\href{https://doi.org/10.1214/18-AOS1709}{10.1214/18-AOS1709}.

\bibitem[\citeproctext]{ref-athey2021}
Athey, S, and Wager, S (2021) Policy Learning With Observational Data.
\emph{Econometrica}, \textbf{89}(1), 133--161.
doi:\href{https://doi.org/10.3982/ECTA15732}{10.3982/ECTA15732}.

\bibitem[\citeproctext]{ref-bareinboim2013general}
Bareinboim, E, and Pearl, J (2013) A general algorithm for deciding
transportability of experimental results. \emph{Journal of Causal
Inference}, \textbf{1}(1), 107--134.

\bibitem[\citeproctext]{ref-barrett2021}
Barrett, M (2021) \emph{Ggdag: Analyze and create elegant directed
acyclic graphs}. Retrieved from
\url{https://CRAN.R-project.org/package=ggdag}

\bibitem[\citeproctext]{ref-basten2013}
Basten, C, and Betz, F (2013) Beyond work ethic: Religion, individual,
and political preferences. \emph{American Economic Journal: Economic
Policy}, \textbf{5}(3), 67--91.
doi:\href{https://doi.org/10.1257/pol.5.3.67}{10.1257/pol.5.3.67}.

\bibitem[\citeproctext]{ref-becker2016}
Becker, SO, Pfaff, S, and Rubin, J (2016) Causes and consequences of the
protestant reformation. \emph{Explorations in Economic History},
\textbf{62}, 1--25.

\bibitem[\citeproctext]{ref-beheim2021}
Beheim, B, Atkinson, QD, Bulbulia, J, \ldots{} Willard, AK (2021)
Treatment of missing data determined conclusions regarding moralizing
gods. \emph{Nature}, \textbf{595}(7866), E29--E34.
doi:\href{https://doi.org/10.1038/s41586-021-03655-4}{10.1038/s41586-021-03655-4}.

\bibitem[\citeproctext]{ref-bulbulia2024PRACTICAL}
Bulbulia, J (2024a) A practical guide to causal inference in three-wave
panel studies. \emph{PsyArXiv Preprints}.
doi:\href{https://doi.org/10.31234/osf.io/uyg3d}{10.31234/osf.io/uyg3d}.

\bibitem[\citeproctext]{ref-bulbulia2024swigstime}
Bulbulia, J (2024b) Causal inference for interaction, mediation, and
time-varying treatments. \emph{PsyArXiv}.
doi:\href{https://doi.org/10.31234/osf.io/vr268}{10.31234/osf.io/vr268}.

\bibitem[\citeproctext]{ref-bulbulia_2024_experiments}
Bulbulia, J (2024c) Confounding in experiments. \emph{PsyArXiv}.
doi:\href{https://doi.org/10.31234/osf.io/6rnj5}{10.31234/osf.io/6rnj5}.

\bibitem[\citeproctext]{ref-bulbulia2024wierd}
Bulbulia, J (2024d) The weirdest causal inferences in the world.
\emph{PsyArXiv}.
doi:\href{https://doi.org/10.31234/osf.io/kj7rv}{10.31234/osf.io/kj7rv}.

\bibitem[\citeproctext]{ref-bulbulia2022}
Bulbulia, JA (2022) A workflow for causal inference in cross-cultural
psychology. \emph{Religion, Brain \& Behavior}, \textbf{0}(0), 1--16.
doi:\href{https://doi.org/10.1080/2153599X.2022.2070245}{10.1080/2153599X.2022.2070245}.

\bibitem[\citeproctext]{ref-bulbulia2023}
Bulbulia, JA (2023) Causal diagrams (directed acyclic graphs): A
practical guide.

\bibitem[\citeproctext]{ref-bulbulia2023a}
Bulbulia, JA, Afzali, MU, Yogeeswaran, K, and Sibley, CG (2023)
Long-term causal effects of far-right terrorism in {N}ew {Z}ealand.
\emph{PNAS Nexus}, \textbf{2}(8), pgad242.

\bibitem[\citeproctext]{ref-bulbuliaj.2013}
Bulbulia, J., Geertz, AW, Atkinson, QD, \ldots{} Wilson, DS (2013) The
cultural evolution of religion. In P. J. Richerson and M. Christiansen,
eds., Cambridge, MA: MIT press, 381--404.

\bibitem[\citeproctext]{ref-cinelli2022}
Cinelli, C, Forney, A, and Pearl, J (2022) A Crash Course in Good and
Bad Controls. \emph{Sociological Methods \&Research}, 00491241221099552.
doi:\href{https://doi.org/10.1177/00491241221099552}{10.1177/00491241221099552}.

\bibitem[\citeproctext]{ref-cole2008}
Cole, SR, and Hernán, MA (2008) Constructing inverse probability weights
for marginal structural models. \emph{American Journal of Epidemiology},
\textbf{168}(6), 656--664.

\bibitem[\citeproctext]{ref-cole2010}
Cole, SR, Platt, RW, Schisterman, EF, \ldots{} Poole, C (2010)
Illustrating bias due to conditioning on a collider. \emph{International
Journal of Epidemiology}, \textbf{39}(2), 417--420.
doi:\href{https://doi.org/10.1093/ije/dyp334}{10.1093/ije/dyp334}.

\bibitem[\citeproctext]{ref-collinson2003}
Collinson, P (2003) \emph{The reformation: A history}, Weidenfeld;
Nicholson; London, England.

\bibitem[\citeproctext]{ref-danaei2012}
Danaei, G, Tavakkoli, M, and Hernán, MA (2012) Bias in observational
studies of prevalent users: lessons for comparative effectiveness
research from a meta-analysis of statins. \emph{American Journal of
Epidemiology}, \textbf{175}(4), 250--262.
doi:\href{https://doi.org/10.1093/aje/kwr301}{10.1093/aje/kwr301}.

\bibitem[\citeproctext]{ref-decoulanges1903}
De Coulanges, F (1903) \emph{La cité antique: Étude sur le culte, le
droit, les institutions de la grèce et de rome}, Hachette.

\bibitem[\citeproctext]{ref-duxedaz2021}
Díaz, I, Williams, N, Hoffman, KL, and Schenck, EJ (2021) Non-parametric
causal effects based on longitudinal modified treatment policies.
\emph{Journal of the American Statistical Association}.
doi:\href{https://doi.org/10.1080/01621459.2021.1955691}{10.1080/01621459.2021.1955691}.

\bibitem[\citeproctext]{ref-diaz2021nonparametric}
Dı́az, I, Hejazi, NS, Rudolph, KE, and Der Laan, MJ van (2021)
Nonparametric efficient causal mediation with intermediate confounders.
\emph{Biometrika}, \textbf{108}(3), 627--641.

\bibitem[\citeproctext]{ref-Diaz2023}
Dı́az, I, Williams, N, and Rudolph, KE (2023) \emph{Journal of Causal
Inference}, \textbf{11}(1), 20220077.
doi:\href{https://doi.org/doi:10.1515/jci-2022-0077}{doi:10.1515/jci-2022-0077}.

\bibitem[\citeproctext]{ref-gawthrop1984}
Gawthrop, R, and Strauss, G (1984) Protestantism and literacy in early
modern germany. \emph{Past \& Present}, (104), 31--55.

\bibitem[\citeproctext]{ref-geertz2013}
Geertz, AW, Atkinson, QD, Cohen, E, \ldots{} Wilson, DS (2013) The
cultural evolution of religion. In P. J. Richerson and M. Christiansen,
eds., Cambridge, MA: MIT press, 381--404.

\bibitem[\citeproctext]{ref-greenland2003quantifying}
Greenland, S (2003) Quantifying biases in causal models: Classical
confounding vs collider-stratification bias. \emph{Epidemiology},
300--306.

\bibitem[\citeproctext]{ref-greenland1999}
Greenland, S, Pearl, J, and Robins, JM (1999) Causal diagrams for
epidemiologic research. \emph{Epidemiology (Cambridge, Mass.)},
\textbf{10}(1), 37--48.

\bibitem[\citeproctext]{ref-greifer2023}
Greifer, N, Worthington, S, Iacus, S, and King, G (2023) \emph{Clarify:
Simulation-based inference for regression models}. Retrieved from
\url{https://iqss.github.io/clarify/}

\bibitem[\citeproctext]{ref-haneuse2013estimation}
Haneuse, S, and Rotnitzky, A (2013) Estimation of the effect of
interventions that modify the received treatment. \emph{Statistics in
Medicine}, \textbf{32}(30), 5260--5277.

\bibitem[\citeproctext]{ref-hernan2023}
Hernan, MA, and Robins, JM (2023) \emph{Causal inference}, Taylor \&
Francis. Retrieved from
\url{https://books.google.co.nz/books?id=/_KnHIAAACAAJ}

\bibitem[\citeproctext]{ref-hernan2024WHATIF}
Hernan, MA, and Robins, JM (2024) \emph{Causal inference: What if?},
Taylor \& Francis. Retrieved from
\url{https://www.hsph.harvard.edu/miguel-hernan/causal-inference-book/}

\bibitem[\citeproctext]{ref-hernan2017SELECTIONWITHOUTCOLLIDER}
Hernán, MA (2017) Invited commentary: Selection bias without colliders
\textbar{} american journal of epidemiology \textbar{} oxford academic.
\emph{American Journal of Epidemiology}, \textbf{185}(11), 1048--1050.
Retrieved from \url{https://doi.org/10.1093/aje/kwx077}

\bibitem[\citeproctext]{ref-hernan2008aObservationalStudiesAnalysedLike}
Hernán, MA, Alonso, A, Logan, R, \ldots{} Robins, JM (2008b)
Observational studies analyzed like randomized experiments: An
application to postmenopausal hormone therapy and coronary heart
disease. \emph{Epidemiology}, \textbf{19}(6), 766.
doi:\href{https://doi.org/10.1097/EDE.0b013e3181875e61}{10.1097/EDE.0b013e3181875e61}.

\bibitem[\citeproctext]{ref-hernuxe1n2008a}
Hernán, MA, Alonso, A, Logan, R, \ldots{} Robins, JM (2008a)
Observational studies analyzed like randomized experiments: An
application to postmenopausal hormone therapy and coronary heart
disease. \emph{Epidemiology}, \textbf{19}(6), 766.
doi:\href{https://doi.org/10.1097/EDE.0b013e3181875e61}{10.1097/EDE.0b013e3181875e61}.

\bibitem[\citeproctext]{ref-hernan2009MEASUREMENT}
Hernán, MA, and Cole, SR (2009) Invited commentary: Causal diagrams and
measurement bias. \emph{American Journal of Epidemiology},
\textbf{170}(8), 959--962.
doi:\href{https://doi.org/10.1093/aje/kwp293}{10.1093/aje/kwp293}.

\bibitem[\citeproctext]{ref-hernan2004STRUCTURAL}
Hernán, MA, Hernández-Díaz, S, and Robins, JM (2004) A structural
approach to selection bias. \emph{Epidemiology}, \textbf{15}(5),
615--625. Retrieved from \url{https://www.jstor.org/stable/20485961}

\bibitem[\citeproctext]{ref-hernuxe1n2006}
Hernán, MA, and Robins, JM (2006a) Estimating causal effects from
epidemiological data. \emph{Journal of Epidemiology \& Community
Health}, \textbf{60}(7), 578--586.
doi:\href{https://doi.org/10.1136/jech.2004.029496}{10.1136/jech.2004.029496}.

\bibitem[\citeproctext]{ref-hernan2006estimating}
Hernán, MA, and Robins, JM (2006b) Estimating causal effects from
epidemiological data. \emph{Journal of Epidemiology \& Community
Health}, \textbf{60}(7), 578--586.
doi:\href{https://doi.org/10.1136/jech.2004.029496}{10.1136/jech.2004.029496}.

\bibitem[\citeproctext]{ref-hernan2017per}
Hernán, MA, Robins, JM, et al. (2017) Per-protocol analyses of pragmatic
trials. \emph{N Engl J Med}, \textbf{377}(14), 1391--1398.

\bibitem[\citeproctext]{ref-hernuxe1n2016}
Hernán, MA, Sauer, BC, Hernández-Díaz, S, Platt, R, and Shrier, I
(2016a) Specifying a target trial prevents immortal time bias and other
self-inflicted injuries in observational analyses. \emph{Journal of
Clinical Epidemiology}, \textbf{79}, 70--75.

\bibitem[\citeproctext]{ref-hernan2016}
Hernán, MA, Sauer, BC, Hernández-Díaz, S, Platt, R, and Shrier, I
(2016b) Specifying a target trial prevents immortal time bias and other
self-inflicted injuries in observational analyses. \emph{Journal of
Clinical Epidemiology}, \textbf{79}, 70--75.

\bibitem[\citeproctext]{ref-hernuxe1n2008}
Hernán, MA, and Taubman, SL (2008) Does obesity shorten life? The
importance of well-defined interventions to answer causal questions.
\emph{International Journal of Obesity (2005)}, \textbf{32 Suppl 3},
S8--14.
doi:\href{https://doi.org/10.1038/ijo.2008.82}{10.1038/ijo.2008.82}.

\bibitem[\citeproctext]{ref-hernuxe1n2022}
Hernán, MA, Wang, W, and Leaf, DE (2022) Target trial emulation: A
framework for causal inference from observational data. \emph{JAMA},
\textbf{328}(24), 2446--2447.
doi:\href{https://doi.org/10.1001/jama.2022.21383}{10.1001/jama.2022.21383}.

\bibitem[\citeproctext]{ref-hoffman2023}
Hoffman, KL, Salazar-Barreto, D, Rudolph, KE, and Díaz, I (2023)
Introducing longitudinal modified treatment policies: A unified
framework for studying complex exposures.
doi:\href{https://doi.org/10.48550/arXiv.2304.09460}{10.48550/arXiv.2304.09460}.

\bibitem[\citeproctext]{ref-holland1986}
Holland, PW (1986) Statistics and causal inference. \emph{Journal of the
American Statistical Association}, \textbf{81}(396), 945--960.

\bibitem[\citeproctext]{ref-hume1902}
Hume, D (1902) \emph{Enquiries Concerning the Human Understanding: And
Concerning the Principles of Morals}, Clarendon Press.

\bibitem[\citeproctext]{ref-imai2008misunderstandings}
Imai, K, King, G, and Stuart, EA (2008) Misunderstandings between
experimentalists and observationalists about causal inference.
\emph{Journal of the Royal Statistical Society Series A: Statistics in
Society}, \textbf{171}(2), 481--502.

\bibitem[\citeproctext]{ref-johnson2015}
Johnson, DD (2015) Big gods, small wonder: Supernatural punishment
strikes back. \emph{Religion, Brain \& Behavior}, \textbf{5}(4),
290--298.

\bibitem[\citeproctext]{ref-van2012targeted}
Laan, MJ van der, and Gruber, S (2012) Targeted minimum loss based
estimation of causal effects of multiple time point interventions.
\emph{The International Journal of Biostatistics}, \textbf{8}(1).

\bibitem[\citeproctext]{ref-lash2020}
Lash, TL, Rothman, KJ, VanderWeele, TJ, and Haneuse, S (2020)
\emph{Modern epidemiology}, Wolters Kluwer. Retrieved from
\url{https://books.google.co.nz/books?id=SiTSnQEACAAJ}

\bibitem[\citeproctext]{ref-lauritzen1990}
Lauritzen, SL, Dawid, AP, Larsen, BN, and Leimer, H-G (1990)
Independence properties of directed {M}arkov fields. \emph{Networks},
\textbf{20}(5), 491--505.

\bibitem[\citeproctext]{ref-lewis1973}
Lewis, D (1973) Causation. \emph{The Journal of Philosophy},
\textbf{70}(17), 556--567.
doi:\href{https://doi.org/10.2307/2025310}{10.2307/2025310}.

\bibitem[\citeproctext]{ref-linden2020EVALUE}
Linden, A, Mathur, MB, and VanderWeele, TJ (2020) Conducting sensitivity
analysis for unmeasured confounding in observational studies using
e-values: The evalue package. \emph{The Stata Journal}, \textbf{20}(1),
162--175.

\bibitem[\citeproctext]{ref-liu2023application}
Liu, Y, Schnitzer, ME, Herrera, R, Dı́az, I, O'Loughlin, J, and
Sylvestre, M-P (2023) The application of target trials with longitudinal
targeted maximum likelihood estimation to assess the effect of alcohol
consumption in adolescence on depressive symptoms in adulthood.
\emph{American Journal of Epidemiology}, kwad241.

\bibitem[\citeproctext]{ref-major2023exploring}
Major-Smith, D (2023) Exploring causality from observational data: An
example assessing whether religiosity promotes cooperation.
\emph{Evolutionary Human Sciences}, \textbf{5}, e22.

\bibitem[\citeproctext]{ref-mcelreath2020}
McElreath, R (2020) \emph{Statistical rethinking: A {B}ayesian course
with examples in r and stan}, CRC press.

\bibitem[\citeproctext]{ref-montgomery2018}
Montgomery, JM, Nyhan, B, and Torres, M (2018) How conditioning on
posttreatment variables can ruin your experiment and what to do about
It. \emph{American Journal of Political Science}, \textbf{62}(3),
760--775.
doi:\href{https://doi.org/10.1111/ajps.12357}{10.1111/ajps.12357}.

\bibitem[\citeproctext]{ref-morgan2014}
Morgan, SL, and Winship, C (2014) \emph{Counterfactuals and causal
inference: Methods and principles for social research}, 2nd edn,
Cambridge: Cambridge University Press.
doi:\href{https://doi.org/10.1017/CBO9781107587991}{10.1017/CBO9781107587991}.

\bibitem[\citeproctext]{ref-murray2021a}
Murray, EJ, Marshall, BDL, and Buchanan, AL (2021) Emulating target
trials to improve causal inference from agent-based models.
\emph{American Journal of Epidemiology}, \textbf{190}(8), 1652--1658.
doi:\href{https://doi.org/10.1093/aje/kwab040}{10.1093/aje/kwab040}.

\bibitem[\citeproctext]{ref-nalle1987}
Nalle, ST (1987) Inquisitors, priests, and the people during the
catholic reformation in spain. \emph{The Sixteenth Century Journal},
557--587.

\bibitem[\citeproctext]{ref-neal2020introduction}
Neal, B (2020) Introduction to causal inference from a machine learning
perspective. \emph{Course Lecture Notes (Draft)}. Retrieved from
\url{https://www.bradyneal.com/Introduction_to_Causal_Inference-Dec17_2020-Neal.pdf}

\bibitem[\citeproctext]{ref-neurath1973}
Neurath, O (1973) Anti-spengler. In M. Neurath and R. S. Cohen, eds.,
\emph{Empiricism and sociology}, Dordrecht: Springer Netherlands,
158--213.
doi:\href{https://doi.org/10.1007/978-94-010-2525-6_6}{10.1007/978-94-010-2525-6\_6}.

\bibitem[\citeproctext]{ref-norenzayan2016}
Norenzayan, A, Shariff, AF, Gervais, WM, \ldots{} Henrich, J (2016) The
cultural evolution of prosocial religions. \emph{Behavioral and Brain
Sciences}, \textbf{39}, e1.
doi:\href{https://doi.org/10.1017/S0140525X14001356}{10.1017/S0140525X14001356}.

\bibitem[\citeproctext]{ref-ogburn2021}
Ogburn, EL, and Shpitser, I (2021) Causal modelling: The two cultures.
\emph{Observational Studies}, \textbf{7}(1), 179--183.
doi:\href{https://doi.org/10.1353/obs.2021.0006}{10.1353/obs.2021.0006}.

\bibitem[\citeproctext]{ref-ogburn2022}
Ogburn, EL, Sofrygin, O, Díaz, I, and Laan, MJ van der (2022) Causal
inference for social network data. \emph{Journal of the American
Statistical Association}, \textbf{0}(0), 1--15.
doi:\href{https://doi.org/10.1080/01621459.2022.2131557}{10.1080/01621459.2022.2131557}.

\bibitem[\citeproctext]{ref-pearl1988}
Pearl, J (1988) \emph{Probabilistic reasoning in intelligent systems:
Networks of plausible inference}, Morgan kaufmann.

\bibitem[\citeproctext]{ref-pearl1995}
Pearl, J (1995) Causal diagrams for empirical research.
\emph{Biometrika}, \textbf{82}(4), 669--688.

\bibitem[\citeproctext]{ref-pearl2009a}
Pearl, J (2009) \emph{Causality}, Cambridge University Press.

\bibitem[\citeproctext]{ref-pearl2022}
Pearl, J, and Bareinboim, E (2022) External validity: From do-calculus
to transportability across populations. In, 1st edn, Vol. 36, New York,
NY, USA: Association for Computing Machinery, 451--482. Retrieved from
\url{https://doi.org/10.1145/3501714.3501741}

\bibitem[\citeproctext]{ref-richardson2013}
Richardson, TS, and Robins, JM (2013a) Single world intervention graphs:
A primer. In, Citeseer. Retrieved from
\url{https://core.ac.uk/display/102673558}

\bibitem[\citeproctext]{ref-richardson2013swigsprimer}
Richardson, TS, and Robins, JM (2013b) Single world intervention graphs:
A primer. In \emph{Second UAI workshop on causal structure learning,
{B}ellevue, {W}ashington}, Citeseer. Retrieved from
\url{https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=07bbcb458109d2663acc0d098e8913892389a2a7}

\bibitem[\citeproctext]{ref-richardson2023potential}
Richardson, TS, and Robins, JM (2023) Potential outcome and decision
theoretic foundations for statistical causality. \emph{Journal of Causal
Inference}, \textbf{11}(1), 20220012.

\bibitem[\citeproctext]{ref-robins1986}
Robins, J (1986) A new approach to causal inference in mortality studies
with a sustained exposure period---application to control of the healthy
worker survivor effect. \emph{Mathematical Modelling}, \textbf{7}(9-12),
1393--1512.

\bibitem[\citeproctext]{ref-robins2010alternative}
Robins, JM, and Richardson, TS (2010) Alternative graphical causal
models and the identification of direct effects. \emph{Causality and
Psychopathology: Finding the Determinants of Disorders and Their Cures},
\textbf{84}, 103--158.

\bibitem[\citeproctext]{ref-rohrer2018}
Rohrer, JM (2018) Thinking clearly about correlations and causation:
Graphical causal models for observational data. \emph{Advances in
Methods and Practices in Psychological Science}, \textbf{1}(1), 27--42.

\bibitem[\citeproctext]{ref-rohrer2022PATH}
Rohrer, JM, Hünermund, P, Arslan, RC, and Elson, M (2022) That's a lot
to process! Pitfalls of popular path models. \emph{Advances in Methods
and Practices in Psychological Science}, \textbf{5}(2).
doi:\href{https://doi.org/10.1177/25152459221095827}{10.1177/25152459221095827}.

\bibitem[\citeproctext]{ref-rotnitzky2017multiply}
Rotnitzky, A, Robins, J, and Babino, L (2017) On the multiply robust
estimation of the mean of the g-functional. Retrieved from
\url{https://arxiv.org/abs/1705.08582}

\bibitem[\citeproctext]{ref-rubin1976}
Rubin, DB (1976) Inference and missing data. \emph{Biometrika},
\textbf{63}(3), 581--592.
doi:\href{https://doi.org/10.1093/biomet/63.3.581}{10.1093/biomet/63.3.581}.

\bibitem[\citeproctext]{ref-rudolph2024mediation}
Rudolph, KE, Williams, NT, and Diaz, I (2024) {Practical causal
mediation analysis: extending nonparametric estimators to accommodate
multiple mediators and multiple intermediate confounders}.
\emph{Biostatistics}, kxae012.
doi:\href{https://doi.org/10.1093/biostatistics/kxae012}{10.1093/biostatistics/kxae012}.

\bibitem[\citeproctext]{ref-sheehan2022}
Sheehan, O, Watts, J, Gray, RD, \ldots{} Atkinson, QD (2022) Coevolution
of religious and political authority in austronesian societies.
\emph{Nature Human Behaviour}.
doi:\href{https://doi.org/10.1038/s41562-022-01471-y}{10.1038/s41562-022-01471-y}.

\bibitem[\citeproctext]{ref-shiba2023uncovering}
Shiba, K, Daoud, A, Hikichi, H, \ldots{} Kawachi, I (2023) Uncovering
heterogeneous associations between disaster-related trauma and
subsequent functional limitations: A machine-learning approach.
\emph{American Journal of Epidemiology}, \textbf{192}(2), 217--229.

\bibitem[\citeproctext]{ref-shpitser2022multivariate}
Shpitser, I, Richardson, TS, and Robins, JM (2022) Multivariate
counterfactual systems and causal graphical models. In
\emph{Probabilistic and causal inference: The works of {J}udea {P}earl},
813--852.

\bibitem[\citeproctext]{ref-shpitser2016causal}
Shpitser, I, and Tchetgen, ET (2016) Causal inference with a graphical
hierarchy of interventions. \emph{Annals of Statistics}, \textbf{44}(6),
2433.

\bibitem[\citeproctext]{ref-slingerland2020coding}
Slingerland, E, Atkinson, QD, Ember, CR, \ldots{} Gray, RD (2020) Coding
culture: Challenges and recommendations for comparative cultural
databases. \emph{Evolutionary Human Sciences}, \textbf{2}, e29.

\bibitem[\citeproctext]{ref-stensrud2023conditional}
Stensrud, MJ, Robins, JM, Sarvet, A, Tchetgen Tchetgen, EJ, and Young,
JG (2023) Conditional separable effects. \emph{Journal of the American
Statistical Association}, \textbf{118}(544), 2671--2683.

\bibitem[\citeproctext]{ref-stuart2018generalizability}
Stuart, EA, Ackerman, B, and Westreich, D (2018) Generalizability of
randomized trial results to target populations: Design and analysis
possibilities. \emph{Research on Social Work Practice}, \textbf{28}(5),
532--537.

\bibitem[\citeproctext]{ref-stuart2015}
Stuart, EA, Bradshaw, CP, and Leaf, PJ (2015) Assessing the
Generalizability of Randomized Trial Results to Target Populations.
\emph{Prevention Science}, \textbf{16}(3), 475--485.
doi:\href{https://doi.org/10.1007/s11121-014-0513-z}{10.1007/s11121-014-0513-z}.

\bibitem[\citeproctext]{ref-suzuki2020}
Suzuki, E, Shinozaki, T, and Yamamoto, E (2020) Causal Diagrams:
Pitfalls and Tips. \emph{Journal of Epidemiology}, \textbf{30}(4),
153--162.
doi:\href{https://doi.org/10.2188/jea.JE20190192}{10.2188/jea.JE20190192}.

\bibitem[\citeproctext]{ref-swanson1967}
Swanson, GE (1967) Religion and regime: A sociological account of the
{R}eformation.

\bibitem[\citeproctext]{ref-swanson1971}
Swanson, GE (1971) Interpreting the reformation. \emph{The Journal of
Interdisciplinary History}, \textbf{1}(3), 419--446. Retrieved from
\url{http://www.jstor.org/stable/202620}

\bibitem[\citeproctext]{ref-vanderlaan2011}
Van Der Laan, MJ, and Rose, S (2011) \emph{Targeted Learning: Causal
Inference for Observational and Experimental Data}, New York, NY:
Springer. Retrieved from
\url{https://link.springer.com/10.1007/978-1-4419-9782-1}

\bibitem[\citeproctext]{ref-vanderlaan2018}
Van Der Laan, MJ, and Rose, S (2018) \emph{Targeted Learning in Data
Science: Causal Inference for Complex Longitudinal Studies}, Cham:
Springer International Publishing. Retrieved from
\url{http://link.springer.com/10.1007/978-3-319-65304-4}

\bibitem[\citeproctext]{ref-vanderweele2009}
VanderWeele, TJ (2009a) Concerning the consistency assumption in causal
inference. \emph{Epidemiology}, \textbf{20}(6), 880.
doi:\href{https://doi.org/10.1097/EDE.0b013e3181bd5638}{10.1097/EDE.0b013e3181bd5638}.

\bibitem[\citeproctext]{ref-vanderweele2009a}
VanderWeele, TJ (2009b) Marginal structural models for the estimation of
direct and indirect effects. \emph{Epidemiology}, 18--26.

\bibitem[\citeproctext]{ref-vanderweele2015}
VanderWeele, TJ (2015) \emph{Explanation in causal inference: Methods
for mediation and interaction}, Oxford University Press.

\bibitem[\citeproctext]{ref-vanderweele2018}
VanderWeele, TJ (2018) On well-defined hypothetical interventions in the
potential outcomes framework. \emph{Epidemiology}, \textbf{29}(4), e24.
doi:\href{https://doi.org/10.1097/EDE.0000000000000823}{10.1097/EDE.0000000000000823}.

\bibitem[\citeproctext]{ref-vanderweele2019}
VanderWeele, TJ (2019) Principles of confounder selection.
\emph{European Journal of Epidemiology}, \textbf{34}(3), 211--219.

\bibitem[\citeproctext]{ref-vanderweele2022}
VanderWeele, TJ (2022) Constructed measures and causal inference:
Towards a new model of measurement for psychosocial constructs.
\emph{Epidemiology}, \textbf{33}(1), 141.
doi:\href{https://doi.org/10.1097/EDE.0000000000001434}{10.1097/EDE.0000000000001434}.

\bibitem[\citeproctext]{ref-vanderweele2013}
VanderWeele, TJ, and Hernan, MA (2013) Causal inference under multiple
versions of treatment. \emph{Journal of Causal Inference},
\textbf{1}(1), 1--20.

\bibitem[\citeproctext]{ref-vanderweele2012MEASUREMENT}
VanderWeele, TJ, and Hernán, MA (2012) Results on differential and
dependent measurement error of the exposure and the outcome using signed
directed acyclic graphs. \emph{American Journal of Epidemiology},
\textbf{175}(12), 1303--1310.
doi:\href{https://doi.org/10.1093/aje/kwr458}{10.1093/aje/kwr458}.

\bibitem[\citeproctext]{ref-vanderweele2020}
VanderWeele, TJ, Mathur, MB, and Chen, Y (2020) Outcome-wide
longitudinal designs for causal inference: A new template for empirical
studies. \emph{Statistical Science}, \textbf{35}(3), 437--466.

\bibitem[\citeproctext]{ref-vanderweele2022b}
VanderWeele, TJ, and Vansteelandt, S (2022) A statistical test to reject
the structural interpretation of a latent factor model. \emph{Journal of
the Royal Statistical Society Series B: Statistical Methodology},
\textbf{84}(5), 2032--2054.

\bibitem[\citeproctext]{ref-vansteelandt2022a}
Vansteelandt, S, and Dukes, O (2022) Assumption-lean inference for
generalised linear model parameters. \emph{Journal of the Royal
Statistical Society Series B: Statistical Methodology}, \textbf{84}(3),
657--685.

\bibitem[\citeproctext]{ref-wager2018}
Wager, S, and Athey, S (2018) Estimation and inference of heterogeneous
treatment effects using random forests. \emph{Journal of the American
Statistical Association}, \textbf{113}(523), 1228--1242.
doi:\href{https://doi.org/10.1080/01621459.2017.1319839}{10.1080/01621459.2017.1319839}.

\bibitem[\citeproctext]{ref-watts2015}
Watts, J, Greenhill, SJ, Atkinson, QD, Currie, TE, Bulbulia, J, and
Gray, RD (2015) \emph{Broad supernatural punishment but not moralizing
high gods precede the evolution of political complexity in
{A}ustronesia} \emph{Proceedings of the Royal Society B: Biological
Sciences}, Vol. 282, The Royal Society, 20142556.

\bibitem[\citeproctext]{ref-watts2018}
Watts, J, Sheehan, O, Bulbulia, Joseph A, Gray, RD, and Atkinson, QD
(2018) Christianity spread faster in small, politically structured
societies. \emph{Nature Human Behaviour}, \textbf{2}(8), 559--564.
doi:\href{https://doi.org/gdvnjn}{gdvnjn}.

\bibitem[\citeproctext]{ref-weber1905}
Weber, M (1905) \emph{The protestant ethic and the spirit of capitalism:
And other writings}, Penguin.

\bibitem[\citeproctext]{ref-weber1993}
Weber, M (1993) \emph{The sociology of religion}, Beacon Press.

\bibitem[\citeproctext]{ref-webster2021directed}
Webster-Clark, M, and Breskin, A (2021) Directed acyclic graphs, effect
measure modification, and generalizability. \emph{American Journal of
Epidemiology}, \textbf{190}(2), 322--327.

\bibitem[\citeproctext]{ref-westreich2010}
Westreich, D, and Cole, SR (2010) Invited commentary: positivity in
practice. \emph{American Journal of Epidemiology}, \textbf{171}(6).
doi:\href{https://doi.org/10.1093/aje/kwp436}{10.1093/aje/kwp436}.

\bibitem[\citeproctext]{ref-westreich2019target}
Westreich, D, Edwards, JK, Lesko, CR, Cole, SR, and Stuart, EA (2019)
Target validity and the hierarchy of study designs. \emph{American
Journal of Epidemiology}, \textbf{188}(2), 438--443.

\bibitem[\citeproctext]{ref-westreich2017}
Westreich, D, Edwards, JK, Lesko, CR, Stuart, E, and Cole, SR (2017)
Transportability of trial results using inverse odds of sampling
weights. \emph{American Journal of Epidemiology}, \textbf{186}(8),
1010--1014.
doi:\href{https://doi.org/10.1093/aje/kwx164}{10.1093/aje/kwx164}.

\bibitem[\citeproctext]{ref-westreich2013}
Westreich, D, and Greenland, S (2013) The table 2 fallacy: Presenting
and interpreting confounder and modifier coefficients. \emph{American
Journal of Epidemiology}, \textbf{177}(4), 292--298.

\bibitem[\citeproctext]{ref-wheatley1971}
Wheatley, P (1971) \emph{The pivot of the four quarters : A preliminary
enquiry into the origins and character of the ancient chinese city},
Edinburgh University Press. Retrieved from
\url{https://cir.nii.ac.jp/crid/1130000795717727104}

\bibitem[\citeproctext]{ref-whitehouse2023}
Whitehouse, H, Francois, P, Savage, PE, \ldots{} Turchin, P (2023)
Testing the big gods hypothesis with global historical data: A review
and retake. \emph{Religion, Brain \& Behavior}, \textbf{13}(2),
124--166.

\bibitem[\citeproctext]{ref-williams2021}
Williams, NT, and Díaz, I (2021) \emph{{l}mtp: Non-parametric causal
effects of feasible interventions based on modified treatment policies}.
doi:\href{https://doi.org/10.5281/zenodo.3874931}{10.5281/zenodo.3874931}.

\bibitem[\citeproctext]{ref-young2014identification}
Young, JG, Hernán, MA, and Robins, JM (2014) Identification, estimation
and approximation of risk under interventions that depend on the natural
value of treatment using observational data. \emph{Epidemiologic
Methods}, \textbf{3}(1), 1--19.

\end{CSLReferences}



\end{document}
