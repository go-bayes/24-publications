% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames,x11names}{xcolor}
%
\documentclass[
  singlecolumn]{article}

\usepackage{amsmath,amssymb}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage[]{libertinus}
\ifPDFTeX\else  
    % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\usepackage[top=30mm,left=20mm,heightrounded]{geometry}
\setlength{\emergencystretch}{3em} % prevent overfull lines
\setcounter{secnumdepth}{-\maxdimen} % remove section numbering
% Make \paragraph and \subparagraph free-standing
\makeatletter
\ifx\paragraph\undefined\else
  \let\oldparagraph\paragraph
  \renewcommand{\paragraph}{
    \@ifstar
      \xxxParagraphStar
      \xxxParagraphNoStar
  }
  \newcommand{\xxxParagraphStar}[1]{\oldparagraph*{#1}\mbox{}}
  \newcommand{\xxxParagraphNoStar}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
  \let\oldsubparagraph\subparagraph
  \renewcommand{\subparagraph}{
    \@ifstar
      \xxxSubParagraphStar
      \xxxSubParagraphNoStar
  }
  \newcommand{\xxxSubParagraphStar}[1]{\oldsubparagraph*{#1}\mbox{}}
  \newcommand{\xxxSubParagraphNoStar}[1]{\oldsubparagraph{#1}\mbox{}}
\fi
\makeatother


\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
% definitions for citeproc citations
\NewDocumentCommand\citeproctext{}{}
\NewDocumentCommand\citeproc{mm}{%
  \begingroup\def\citeproctext{#2}\cite{#1}\endgroup}
\makeatletter
 % allow citations to break across lines
 \let\@cite@ofmt\@firstofone
 % avoid brackets around text for \cite:
 \def\@biblabel#1{}
 \def\@cite#1#2{{#1\if@tempswa , #2\fi}}
\makeatother
\newlength{\cslhangindent}
\setlength{\cslhangindent}{1.5em}
\newlength{\csllabelwidth}
\setlength{\csllabelwidth}{3em}
\newenvironment{CSLReferences}[2] % #1 hanging-indent, #2 entry-spacing
 {\begin{list}{}{%
  \setlength{\itemindent}{0pt}
  \setlength{\leftmargin}{0pt}
  \setlength{\parsep}{0pt}
  % turn on hanging indent if param 1 is 1
  \ifodd #1
   \setlength{\leftmargin}{\cslhangindent}
   \setlength{\itemindent}{-1\cslhangindent}
  \fi
  % set entry spacing
  \setlength{\itemsep}{#2\baselineskip}}}
 {\end{list}}
\usepackage{calc}
\newcommand{\CSLBlock}[1]{\hfill\break\parbox[t]{\linewidth}{\strut\ignorespaces#1\strut}}
\newcommand{\CSLLeftMargin}[1]{\parbox[t]{\csllabelwidth}{\strut#1\strut}}
\newcommand{\CSLRightInline}[1]{\parbox[t]{\linewidth - \csllabelwidth}{\strut#1\strut}}
\newcommand{\CSLIndent}[1]{\hspace{\cslhangindent}#1}

\input{/Users/joseph/GIT/latex/latex-for-quarto.tex}
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\AtBeginDocument{%
\ifdefined\contentsname
  \renewcommand*\contentsname{Table of contents}
\else
  \newcommand\contentsname{Table of contents}
\fi
\ifdefined\listfigurename
  \renewcommand*\listfigurename{List of Figures}
\else
  \newcommand\listfigurename{List of Figures}
\fi
\ifdefined\listtablename
  \renewcommand*\listtablename{List of Tables}
\else
  \newcommand\listtablename{List of Tables}
\fi
\ifdefined\figurename
  \renewcommand*\figurename{Figure}
\else
  \newcommand\figurename{Figure}
\fi
\ifdefined\tablename
  \renewcommand*\tablename{Table}
\else
  \newcommand\tablename{Table}
\fi
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Listing}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}
\makeatother
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\@ifpackageloaded{subcaption}{}{\usepackage{subcaption}}
\makeatother

\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\usepackage{bookmark}

\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same} % disable monospaced font for URLs
\hypersetup{
  pdftitle={An Invitation to Causal Inference in Environmental Psychology},
  pdfkeywords={DAGS, Causal
Inference, Confounding, Environmental, Longitudinal, Psychology},
  colorlinks=true,
  linkcolor={blue},
  filecolor={Maroon},
  citecolor={Blue},
  urlcolor={Blue},
  pdfcreator={LaTeX via pandoc}}


\title{An Invitation to Causal Inference in Environmental Psychology}

\usepackage{academicons}
\usepackage{xcolor}

  \author{Joseph A Bulbulia}
            \affil{%
             \small{     Victoria University of Wellington, New Zealand,
School of Psychology, Centre for Applied Cross-Cultural Research
          ORCID \textcolor[HTML]{A6CE39}{\aiOrcid} ~0000-0002-5861-2056 }
              }
      \usepackage{academicons}
\usepackage{xcolor}

  \author{Donald W Hine}
            \affil{%
             \small{     University of Canterbury, School of Psychology,
Speech and Hearing
          ORCID \textcolor[HTML]{A6CE39}{\aiOrcid} ~0000-0002-3905-7026 }
              }
      


\date{2024-10-15}
\begin{document}
\maketitle
\begin{abstract}
This chapter introduces causal inference within environmental
psychology, underscoring its fundamental differences from traditional
statistical analysis. The content is organised into four main sections:
1. \textbf{Non-technical introduction}. Stating a causal question
requires defining pre-specified contrasts between interventions
experienced by an entire population. Because each individual can
experience only one intervention, causal effects must be estimated using
assumptions. We build intuitions for these assumptions by clarifying
their satisfaction in randomised controlled experiments. 2.
\textbf{Causal Directed Acyclic Graphs (DAGs) tutorial}. Causal DAGs are
potent tools for clarifying whether and how causal effects may be
identified from data. We explain how they work. 3. \textbf{Practical
examples}. We apply causal DAGs to common scenarios in observational
environmental psychology. 4. \textbf{Guidelines}. The main aim of this
chapter is to motivate broader adoption of causal workflows.
\end{abstract}


Psychological scientists are taught that ``correlation does not imply
causation.'' By ``correlation,'' we refer to statistical measures of
association between variables. Most statistical techniques ---from
t-tests to structural equation models- estimate associations from data.
Although we know that measuring associations does not imply causation,
in observational settings, we often persist in reporting statistical
associations as if they are tentative evidence for causation. The
purpose of this chapter is to clarify why such reporting is confused and
misleading, and to guide researchers toward better practices.

What do we mean by ``causation?'' Causation has been a topic of
extensive interest and debate in philosophy
(\citeproc{ref-lewis1973}{Lewis 1973}). Here, we narrow our focus. We
consider the assumptions under which it is possible to estimate causal
effects from data and how to estimate them. In causal-effect estimation,
or ``causal inference,'' investigators seek to quantify the average
differences across a specified population or sub-population (the
``target population'') that interventions would produce on well-defined
outcomes. This requires comparing at least two states of the world: one
where the population experiences a treatment and another where they do
not. While the concept of causation in causal inference is more narrowly
defined than causation itself, it draws intellectual inspiration from
David Hume, who, in his \emph{Enquiries Concerning Human Understanding}
(1751), characterises the cause-effect relationship as follows:

\begin{quote}
``If the first object had not been, \emph{the second never would have
existed}.'' (\citeproc{ref-hume1902}{Hume 1902}) (emphasis added).
\end{quote}

This conceptualisation aligns closely with the counterfactual approach
in causal inference, which considers what would have happened to an
outcome in both the presence and the absence of a treatment.

Hume's definition relies on counterfactual thinking---specifically,
comparing two mutually exclusive states of the world: one where an event
occurs and one where it does not. For Hume, assessing causation requires
not just observing events as they happen but also considering how the
world might have differed had those events not occurred. Such
comparisons, where we consider scenarios where treatments did not take
place, are known as ``counterfactual contrasts;'' such contrasts are
fundamental to causal-effect estimation. In modern causal inference,
these contrasts are formalised within the potential outcomes framework,
which estimates the average difference in outcomes between treated and
control conditions had the entire population of interest been treated at
different levels of the intervention.

Importantly, although statistically evaluating associations from data is
essential for estimating average treatment effects, causation estimation
cannot be derived from the study of associations alone. Every
realisation in the data provides information about only one of the two
potential outcomes necessary for a causal contrast. We may only obtain
causal contrasts under assumptions that are not testable from the data.
Moreover, a careful and systematic workflow is required. By the end of
this chapter, you will understand why, without such a workflow, common
analytic techniques---such as linear regression, correlation, and
structural equation modelling---lack causal interpretations and may
mislead investigators.

\hyperref[section-part1]{\textbf{Part 1}} introduces the counterfactual
framework of causal inference, focusing on the three fundamental
assumptions necessary for estimating average causal effects. We build
intuition for these concepts by considering randomised controlled
trials, where these assumptions are met through enforced randomisation
(\citeproc{ref-hernan2017per}{Hernán and Robins 2017};
\citeproc{ref-robins2008estimation}{Robins and Hernan 2008};
\citeproc{ref-westreich2012berkson}{Westreich 2012};
\citeproc{ref-westreich2015}{Westreich \emph{et al.} 2015}).

\hyperref[section-part2]{\textbf{Part 2}} introduces causal Directed
Acyclic Graphs (DAGs), powerful tools for visualising and addressing the
assumption of conditional exchangeability (also known as the ``no
unmeasured confounders'' assumption). Here, we introduce Judea Pearl's
rules of d-separation (\citeproc{ref-pearl1995}{Pearl 1995}), which
allow investigators to identify appropriate variables to adjust for
confounding. Although most psychological scientists are aware that
regression adjustment is commonly used to control confounding, they may
not be fully aware of the formal criteria required to select appropriate
adjustment variables. Understanding the formal criteria is essential
because over-adjustment may introduce bias or reduce statistical power,
leading to misleading conclusions.

\hyperref[section-part3]{\textbf{Part 3}} examines seven practical
examples that demonstrate how causal Directed Acyclic Graphs (DAGs) can
be used to address causal questions. These examples serve as practical
guides for translating causal questions into causal identification
models that clarify whether and how causal effects can be estimated from
data. Only after stating and addressing these identification assumptions
should researchers develop statistical estimators and perform
statistical analysis (\citeproc{ref-vansteelandt2012}{Vansteelandt
\emph{et al.} 2012}; \citeproc{ref-wager2018}{Wager and Athey 2018}).

\hyperref[section-part4]{\textbf{Part 4}} offers practical guidelines
for environmental psychologists aiming to infer causal effects from
observational data. Given that assumptions about causal relationships
are often uncertain or subject to debate, we recommend developing
multiple causal diagrams and reporting their corresponding analyses to
evaluate different plausible causal pathways. This approach enhances
transparency in how causal relationships are inferred.

We conclude by suggesting further readings and resources for those
interested in learning more about causal inference.

\subsection{Part 1: An Overview of the Counterfactual Framework for
Causal Inference}\label{section-part1}

\subsubsection{The Origins of Causal
Inference}\label{the-origins-of-causal-inference}

Causal inference began in the early 20\(^{th}\) century with Jerzy
Neyman's invention of the potential outcomes framework, initially
developed for agricultural experiments. Neyman realised that
understanding the causal effect of an experimental treatment required
comparing potential outcomes under different treatment conditions, even
though only one outcome could be observed for each unit
(\citeproc{ref-neyman1923}{Splawa-Neyman 1990 (orig. 1923)}). This
framework laid the foundation for modern causal inference.

Donald Rubin extended this approach to observational settings into what
is now known as the Neyman-Rubin Causal Model
(\citeproc{ref-holland1986}{Holland 1986};
\citeproc{ref-rubin1976}{Rubin 1976}). James Robins advanced causal
inference by introducing the mathematical and conceptual framework for
understanding causal effects in settings where there are two or more
sequential treatments over time (\citeproc{ref-robins1986}{Robins
1986}). Robins and his colleagues also developed statistical tools such
as marginal structural models and structural nested models to enable
quantitative assessment of time-varying treatments and time-varying
confounding (\citeproc{ref-hernan2024WHATIF}{Hernan and Robins 2024}).
Computer scientist Judea Pearl significantly advanced the use of causal
Directed Acyclic Graphs (DAGs) to evaluate the ``no unmeasured
confounding'' assumption in causal inference, which is also known as
``no unmeasured confounding'', which is also referred to as
``conditional exchangeability,'' ``selection on observables,'' or
``ignorability.'' In Pearl's framework, this is formalised through
d-separation, a criterion used to check for ``no open backdoor paths''
in causal DAGs, ensuring that causal estimates are not biased by
confounding (\citeproc{ref-pearl2009}{Pearl 2009a};
\citeproc{ref-pearl1995a}{Pearl and Robins 1995}). In
\hyperref[section-part2]{\textbf{Part 2}} we introduce the concept of
causal identification, which occurs when, conditional on measured
covariates, there is no unmeasured confounding. This concept is distinct
from statistical identification. The rules of d-separation will be
explained later, but for now, it is important to recognise that causal
inference involves considerable specialised terminology.

Although terminology differs, the mathematical basis of causal inference
evolved almost independently in the fields of biostatistics, economics,
and computer science. This shared foundation, anchored in proofs, has
enabled a remarkable agreement across disciplinary lines despite
terminological differences. In every approach to causal inference, a
causal effect is conceptualised as a contrast between two states of the
world, only one of which may be observed on any individual -- a
``counterfactual contrast'' also known as a ``causal estimand''
(\citeproc{ref-hernan2024WHATIF}{Hernan and Robins 2024}). Notably,
prior to intervention, these scenarios are purely hypothetical.
Post-intervention, only one scenario is actualised for each realised
treatment, leaving the alternative as a non-observed counterfactual. For
any individual unit to be treated, that only one of the two possible
outcomes is realised underscores a critical property of causality:
causality is \textbf{not directly observable}
(\citeproc{ref-hume1902}{Hume 1902}). Causal inference, therefore, can
only quantify causal effects by combining data with counterfactual
simulation (\citeproc{ref-bulbulia2023a}{Bulbulia \emph{et al.} 2023};
\citeproc{ref-edwards2015}{Edwards \emph{et al.} 2015}). The concept of
a counterfactual data science -- may sound strange. However, anyone who
has encountered a randomised experiment has encountered counterfactual
data science. Before building intuitions for causal inference from the
familiar example of experiments, let's first build intuitions for the
idea that causal quantities are never directly observed.

\subsection{The Fundamental Problem of Causal Inference: Counterfactual
Comparisons in Environmental
Psychology}\label{the-fundamental-problem-of-causal-inference-counterfactual-comparisons-in-environmental-psychology}

Imagine you are faced with a significant life decision: enrolling in a
graduate programme in environmental psychology in New Zealand or
accepting a job offer from a leading renewable energy company. This
choice will shape your future, influencing your lifestyle, income, and
social network. Which option is best for you?

The challenge is that, once you choose one path, you cannot observe how
your life would have unfolded on the other. If you go to graduate
school, you will experience that outcome, but the outcome of taking the
job remains unknown---and vice versa. This is \textbf{the fundamental
problem of causal inference}: we can never observe both potential
outcomes for the same individual, so the path not taken remains an
unobservable ``what if?'' --- a counterfactual that cannot be measured
(\citeproc{ref-holland1986}{Holland 1986}).

Again, counterfactual comparisons lie at the heart of causal inference,
as they involve contrasting what actually happened with what would have
happened under a different scenario. In environmental psychology,
computing counterfactual contrasts is essential for understanding the
effects of psychological and behavioural interventions on well-defined
outcomes. However, the full data required to make such contrasts are
inevitably partially missing and can only be recovered by assumptions
(\citeproc{ref-edwards2015}{Edwards \emph{et al.} 2015};
\citeproc{ref-westreich2015}{Westreich \emph{et al.} 2015}).

\subsubsection{Causal Inference in Experiments: The Problem of Missing
Counterfactuals}\label{causal-inference-in-experiments-the-problem-of-missing-counterfactuals}

Consider a question relevant to environmental psychologists: What is the
causal effect of access to green spaces on subjective happiness? Denote
happiness by \(Y\), where \(Y_i\) represents the happiness of individual
\(i\). Assume that ``subjective happiness'' is a coherent concept. For
now, assume that errors in its measurement are not systematically linked
to access to green spaces. Suppose further that ``ample access to green
space'' is represented as a binary variable: \(A = 1\) for ``ample
access'' and \(A = 0\) for ``lack of ample access.'' These conditions
are mutually exclusive. While we simplify the treatment to a binary
variable, the concepts apply to more complex or continuous treatments.
Estimating causal effects always requires a contrast between
well-defined treatment conditions. Importantly, defining clear causal
questions is essential but often neglected in psychological science
outside experimental work.

Imagine our aim is to compare potential outcomes under different
treatment conditions. Specifically, we contrast the happiness of
individuals with access to green space (\(A = 1\)) against those without
(\(A = 0\)). The target population for this contrast should be
explicit---for example, all New Zealand residents in 2024.

A clear causal question, framed as a counterfactual contrast---also
known as a ``causal estimand''---might be:

\begin{quote}
\emph{``Among New Zealand residents, does access to abundant green space
increase self-perceived happiness compared to environments without such
spaces?''}
\end{quote}

Now, imagine---hypothetically and ethically---that we could randomise
individuals to high or low green space access. Notice that even if such
an experimental setup were possible, causal inference would face missing
data in the potential outcomes. For each person assigned to treatment,
only one potential outcome is observed, depending on the treatment they
receive. The outcome they would have experienced under the alternative
treatment is unobserved -- it remains counterfactual. Such missingness
is the fundamental problem of causal inference, raised to the level of
treatment groups. We only observe \(Y_i(1)\) for individuals with
\(A_i = 1\) and \(Y_i(0)\) for individuals with \(A_i = 0\). The other
potential outcome for each individual remains unobserved. However,
although this missingness in the ``full data'' poses a challenge at the
individual level, we can estimate the average treatment effect within
the sampled population without needing to obtain individual-level causal
effects.

Although individual causal effects are generally not recoverable from
data, we can recover causal effect estimates by changing our causal
question. For example, although randomised controlled experiments do not
solve the fundamental problem of causal inference at the level of
individuals, they may obtain consistent causal effect estimates for
average treatment effects at the level of populations. Randomised
experiments solve the missing data problem at the heart of causal
inference by satisfying three fundamental assumptions required for
obtaining average treatment effects. These are the (1) Conditional
Exchangeability Assumption, (2) The Causal Consistency Assumption, and
(3) The Positivity Assumption.

Before proceeding to explain these assumptions, we offer the following
clarification: the concept of within-subject effects as understood in
traditional statistical analyses does not directly apply in causal
inference and may even appear incoherent within this framework. In
conventional analyses, within-subject effects refer to changes observed
within the same individual across different conditions or over time,
using repeated measures to control for individual-specific variability.
However, causal inference is fundamentally concerned with estimating
causal effects based on hypothetical interventions and potential
outcomes---scenarios that cannot be simultaneously realized for a single
individual. Since an individual cannot both receive and not receive a
treatment at the same time, we cannot observe both potential outcomes
required to define the causal effect at the individual level. Instead,
causal inference relies on comparisons of population averages under
distinct treatments or treatment regimes. Therefore, what is often
termed a 'within-subject effect'' is more accurately understood as a
within-subject research design in the context of causal inference. In
causal inference, there is no coherent distinction for a
within-person/between-person causal effect
(\citeproc{ref-rohrer2023withinbetween}{Rohrer and Murayama 2023})

\paragraph{Assumption 1: Conditional
Exchangeability}\label{assumption-1-conditional-exchangeability}

First, we define the expected value of a treatment \(A=a\) as the sum of
individual counterfactual (or equivalently) ``potential'' outcomes for
individuals within a specified population:

\[
\mathbb{E}[Y(a)] \equiv \frac{1}{n} \sum_{i=1}^n Y_i(a)
\]

Note that in causal inference, we assume these potential outcomes to be
real, even if determined stochastically.

We next define the average treatment effect (ATE) as a contrast between
the averages of the potential outcomes in each treatment condition:

\[
\text{ATE} = \mathbb{E}[Y(1)] - \mathbb{E}[Y(0)]
\]

Suppose that the individuals in a sample are representative of the
population of interest---the target population. In this setting,
randomisation ensures there is no common cause of the treatment and the
potential outcomes that would be observed under treatment. That is, if
treatment assignment is determined by chance, any difference in the
average response within treatment groups is best explained by the
treatment itself.

Mathematically, we express the absence of a common cause of treatment
assignment and the potential outcomes under treatment:

\[
Y(a) \coprod A
\]

This notation means that the potential outcomes \(Y(a)\) are independent
of (\(\coprod\)) the treatment assignment \(A\). Importantly, the
observed outcomes need not be independent of treatment. Note that we do
not require that exposure is independent of the actual outcomes under
exposure (\(Y|A\)), which would be equivalent to requiring there can be
no causal effects for treatment! Randomisation achieves unconditional
independence, allowing us to estimate average causal effects because
treatments remain independent of potential outcomes under treatment. Put
differently, randomisation ensures that average treatment differences
(or equivalently, differences in treatment averages) are the result of
the treatments themselves rather than the effects of another variable.

Importantly, we can relax the requirement for unconditional independence
and allow randomisation to occur conditional on certain measurable
features of the sampled population. For example, suppose we randomise
treatment within different age cohorts such that older individuals have
a greater probability of receiving the treatment than younger
individuals. Assume that the treatment effect is constant across all
ages. In this scenario, we would expect to see higher average treatment
effects in older cohorts simply because a larger proportion of older
individuals receive the treatment.

When randomisation occurs conditional on a measurable feature such as
age, and the treatment effect is constant, differences in treatment
probabilities across groups can lead to variations in observed average
treatment effects. However, if we adjust for this difference in the
probability of receiving treatment, or simply compare treatment effects
within the different age strata, the bias from differential treatment
group assignment disappears.

We use the symbol \(L\) to denote measured variables that might be
common causes of the treatment (\(A\)) and the treatment effect
(\(Y(a)\)). We write that the potential or counterfactual outcome
\(Y(a)\) is independent of \(A\), conditional on \(L\):

\[
Y(a) \coprod A \mid L
\]

This assumption means that, within levels of \(L\), the treatment
assignment is as good as random. In experiments, randomisation ensures
unconditional exchangeability (\(Y(a) \coprod A\)). However, to compute
causal effects from data in which the treatments are not randomised, we
must believe that within levels of measured covariates \(L\), the
treatment is as good as random.

Practically, this involves measuring all common causes of \(A\) and
\(Y\), and appropriately adjusting for \(L\) in our statistical model.
In observational studies, achieving conditional exchangeability is
challenging because treatment assignment is not controlled, and there is
no statistical test that will tell investigators whether treatment
assignment is as good as random conditional on measured covariates.
However, as we will explain in the next section, causal Directed Acyclic
Graphs (causal DAGs) help investigators evaluate whether there exists a
set of measured covariates \(L\) such that conditioning on \(L\) ensures
that the potential outcomes are independent of the treatment -- that is,
to evaluate whether the Conditional Exchangeability Assumption may be
satisfied.

\paragraph{Assumption 2: Causal
Consistency}\label{assumption-2-causal-consistency}

Causal consistency requires that the potential outcome under the
treatment actually received equals the observed outcome:

\[
Y_i = Y_i(a) \quad \text{if } A_i = a
\]

Note that in causal inference, we compare potential outcomes under at
least two different treatments, say, \(Y(a = 1)\) and \(Y(a = 0)\) (in
causal inference, we use a lowercase variable to note that the random
variable \(A\) is fixed to a certain level (\(A = a\)). To compute
causal contrasts, the counterfactual or potential outcomes under
treatment must be observable. The causal consistency assumption allows
investigators to link counterfactual or potential outcomes to observed
outcomes. It might seem obvious that if one receives a treatment, we can
say that the observation of the outcome following treatment is no longer
counterfactual---it is actual. However, for the causal consistency
assumption to hold across a population, we must assume that the
treatments are well-defined and consistently administered. Thus, the
causal consistency assumption is a two-edged sword. On the one hand, we
may use it---if other assumptions are satisfied---to compute causal
contrasts. Simply put, causal consistency puts the factual into
counterfactual.

In controlled experiments, we may typically take causal consistency for
granted. The investigators administer consistent treatments. However, in
observational settings, no such control is ensured. For example, one
reason it has been difficult to investigate the causal effects of weight
loss from massive observational medical datasets is that there are many
ways to lose weight---eating less and healthfully and exercising causes
people to lose weight. However, one may also lose weight from smoking,
psychological distress, stomach stapling, amputation, cancer, and
famine. The mechanisms of the latter forms of weight loss are unhealthy.
Thus, stating a causal contrast, for example, as the expected difference
in all-cause mortality after five successive years of weight loss, is an
invitation for confusion. The treatments that lead to weight loss in
medical data are not comparable across all cases (refer to VanderWeele
and Hernan (\citeproc{ref-vanderweele2013}{2013}), Bulbulia
(\citeproc{ref-bulbulia2023}{2024b})). Even if we consistently estimate
a causal effect, it might be unclear which effect we are estimating
because our measures to not correspond to well-defined interventions.

\paragraph{Assumption 3: Positivity}\label{assumption-3-positivity}

Positivity requires that every individual has a non-zero probability of
receiving each level of the treatment, given their covariates \(L\):

\[
P(A = a \mid L = l) > 0 \quad \text{for all } a, l
\]

This assumption ensures that we have data to compare treatment effects
across all levels of \(L\). For example, suppose we are interested in
the causal effects of vasectomy on happiness. It would not make sense to
include biological females in this study because biological females
cannot have vasectomies (\citeproc{ref-hernan2023}{Hernan and Robins
2023}; \citeproc{ref-westreich2010}{Westreich and Cole 2010}).

\subsubsection{Summary}\label{summary}

Causal inference fundamentally relies on statistical inference; however,
statistics is only a part of a larger workflow that begins with defining
a causal quantity. This involves specifying contrasts that would be
observed if we had complete data for the population of interest, treated
at different levels of the intervention
(\citeproc{ref-ogburn2021}{Ogburn and Shpitser 2021}). The next step is
to determine how these contrasts can be estimated from the available
data.

Standard statistical practices, such as regression adjustment and
structural equation modelling, often fail to accurately estimate causal
effects. This is largely because these models typically do not
explicitly define the treatments being compared, nor do they properly
account for the causal structures that can bias treatment-outcome
associations.

In the following section, we introduce causal directed acyclic graphs
(causal DAGs) -- intuitive graphical tools that help researchers
evaluate the complex conditional dependencies needed to satisfy the ``no
unmeasured confounders'' assumption. This is done by inspecting
relationships in a graph using a straightforward algorithm known as the
rules of d-separation (\citeproc{ref-pearl1995}{Pearl 1995},
\citeproc{ref-pearl2009a}{2009b}). Causal DAGs also reveal how standard
practices in statistical modelling, such as over-adjustment in
regression or mediation analysis in structural equation modelling, may
inadvertently introduce bias through inappropriate variable adjustment.
Such over-adjustment can obscure or distort true causal relationships.

In causal inference, non-causal associations are termed ``spurious''
because they lack coherent interpretation. It is often unclear how to
interpret such associations, as they might imply no causal effect, a
causal effect of a different magnitude, or even a causal effect in the
opposite direction. By explicitly representing the causal structure and
carefully selecting which variables to adjust for, causal DAGs can help
environmental psychologists more effectively estimate causal effects
within their research.

\subsection{Part 2: An Introduction to Causal
Diagrams}\label{section-part2}

A causal directed acyclic graph (DAG) is a diagram that clarifies
whether and how the assumption of conditional exchangeability (no
unmeasured confounders) may be satisfied from data. A causal DAG is
constructed with the aim of evaluating this assumption for a specific
treatment variable (or set of treatment variables) and a specific
outcome (or set of outcomes). When there are multiple treatments, the
assumptions required for causal identification become increasingly
complex. Here, we restrict our focus to the setting in which there is
one treatment variable. The difficulty of ensuring identification for
multiple treatments will become evident from this restricted setting.

It is important to note at the outset that in a causal DAG, we do not
attempt to represent all causal relationships within a system, but only
those that are relevant to the causal question of interest. This
requires a conceptual shift from the use of graphs in the structural
equation modelling (SEM) tradition, where the goal is to represent a
system of statistical relationships obtained from data. Instead, causal
DAGs focus on the minimal set of relationships necessary to address the
causal question and assess whether pre-specified causal contrasts can be
identified from data.

We begin with definitions.

\paragraph{\texorpdfstring{1. \textbf{Node}}{1. Node}}\label{node}

A node represents a variable---a state of the world relevant to our
research question. Again, it is helpful to keep the example of a
randomised controlled experiment in mind when thinking about causality.
In a simple randomised experiment, there are two states of the world
that interest investigators. The experimental intervention, also called
a ``treatment'' or ``exposure''. This is the condition into which
participants are randomised. Here, we have defined interventions using
the symbol \(A\). In a causal DAG, we would simply draw the letter
\(A\). Note that the choice of symbol is arbitrary. It is only necessary
that we define our terminology. The second variable of interest in a
randomised experiment is the outcome. Here we have used the symbol \(Y\)
to denote an outcome. Again, the symbol itself is arbitrary. Not only
are our conventions arbitrary, so too are our interests. We might be
interested in how \(Y\) affects \(A\). In this case, \(Y\) would be the
intervention and \(A\) would be the outcome. There is nothing to prevent
us taking such an interest. However, as we shall see, the conditions in
which we hope to identify the causal effects of variables will typically
vary depending on which effect is of interest. For now, it is sufficient
to understand that we represent variables as nodes on a graph, typically
by using symbols to refer to interventions and outcomes that we must
define.

\paragraph{\texorpdfstring{2. \textbf{Arrows (also known as
Edges)}}{2. Arrows (also known as Edges)}}\label{arrows-also-known-as-edges}

Arrows indicate the direction and presence of causal relationships
between nodes. They represent the assumed flow of causal influence from
a ``parent'' (originating variable) to a ``child'' (receiving variable).
For example, an arrow from \(A \rightarrow Y\) suggests that \(A\)
causally influences \(Y\).

Note, however, that when drawing a causal DAG, we will typically not
draw an arrow linking \(A \rightarrow Y\). This is because the purpose
of a causal DAG is to evaluate whether there are paths linking \(A\) and
\(Y\) absent any assumption of a causal path. We evaluate whether there
are such paths by drawing all common causes of \(A\) and \(Y\), both
measured and unmeasured. For example, if \(L\) is a measured common
cause of \(A\) and \(Y\), we would draw the graph
\(A \leftarrow L \rightarrow Y\). Such a graph would reflect our
assumption that \(A\) and \(Y\) share a common cause \(L\). Likewise, we
might be interested in unmeasured common causes of the treatment and
outcome. We might define such unmeasured common causes using the symbol
\(U\) and draw the graph \(A \leftarrow U \rightarrow Y\).

\paragraph{\texorpdfstring{3.
\textbf{Conditioning}}{3. Conditioning}}\label{conditioning}

Recall that \(Y(a) \coprod A \mid L\) is the conditional exchangeability
assumption---or the assumption of ``no unmeasured confounders.'' To
infer causal effects, we must ensure that, conditional on a set of
measured covariates \(L\), the treatment assignment is essentially as
good as random. We graphically denote that we ``control for,'' or
equivalently ``condition on,'' or ``adjust for,'' or equivalently
``stratify by'' a variable by enclosing it in a box. For example,
\(A \leftarrow \boxed{L} \rightarrow Y\) means that \(L\) is included in
a statistical model that aims to evaluate the causal effect of \(A\) on
\(Y\). Again, we would not typically draw an arrow from \(A\) to \(Y\)
when seeking to evaluate the conditional exchangeability assumption.
This is because we want to understand whether there might be an
association between \(A\) and \(Y\) in the absence of causation.

Before proceeding, there are two basic considerations to keep in mind
when drawing a causal DAG.

First, causal DAGs are qualitative tools whose primary purpose is to
evaluate the conditions under which causal effects may be estimated from
data. We only include as much information in a causal DAG as is
necessary to evaluate the assumption of conditional exchangeability.
This means that we should not attempt to represent non-linear
relationships, for example by drawing arrows intersecting arrows---a
common misconception. Nor should we attempt to represent more than is
necessary for evaluating the assumption of conditional exchangeability.
That means omitting any variables from the graph that are irrelevant to
evaluating this assumption. Put simply, a causal DAG is not meant to be
a map of reality. It is a simple tool for investigating causal
assumptions. Although it might be tempting to compare causal DAGs to
Structural Equation Models, the purposes and interests have little in
common.

Second, causal DAGs must be \emph{acyclic}. This means that we draw
nodes and arrows in a manner that respects the flow of time.
Occasionally, students will worry that there are ``reciprocal effects''
between a treatment and an outcome. For example, access to greenspace
might cause one to be happier, and being happier one might seek out
greater access to greenspace. It might be tempting, then, to construct a
causal graph with double arrows such that \(A\leftarrow \rightarrow Y\).
However, on reflection it should be clear that there is only one arrow
to time, and as such, our graphs must respect that direction. To
represent a cycle, we would expand the graph: Greenspace (Time 0)
\(\to\) Happiness (Time 2) \(\to\) Greenspace (Time 3) \(\to\) Happiness
(Time 4). In this setting, which we assume to be nearly universal for
most questions in environmental psychology, investigators should state
the exposure and time point of interest, say Greenspace at Time 3, state
the time point and outcome of interest, say Happiness at Time 4, and
where data are available, control for past states of greenspace access
and of happiness. That is, Greenspace at Time 0 (and before) and
Happiness at Time 2 (and before) should be included in the confounder
set \(L\) when estimating the effect of \(A\) (Greenspace at Time 3) on
\(Y\) (Happiness at Time 4) (refer to VanderWeele \emph{et al.}
(\citeproc{ref-vanderweele2020}{2020}), Bulbulia
(\citeproc{ref-bulbulia2023}{2024b})).

Next, we consider how to use causal DAGs to evaluate whether and how
conditional exchangeability may be satisfied.

\subsubsection{d-Separation}\label{d-separation}

Having defined the essential components of a causal Directed Acyclic
Graph, consider the rules by which we may evaluate a causal graph to
ascertain whether the causal effect of one variable on another may be
consistently estimated from data without confounding bias.

In causal diagrams, d-separation is a graphical criterion used to
determine whether, and how, we may collect data and develop a
statistical model in which the association we estimate between a
treatment variable and an outcome variable reflects a causal
association. In the mid-1990s, Judea Pearl proved that there is a set of
simple rules by which we can inspect a causal diagram to evaluate
whether the causal effect of a treatment on an outcome is ``as good as
random.'' These are called the rules of ``d-separation''
(\citeproc{ref-pearl1995}{Pearl 1995},
\citeproc{ref-pearl2009a}{2009b}).

When stating the rules of d-separation, we will depart from our
convention of using \(A\) to denote the treatment and \(Y\) to denote
the outcome. The reason for doing so is that we want to flexibly adjust
our orientation depending on the specific causal effect in which we are
interested. So we will think of three variables, \(A\), \(B\), and
\(C\), which we will imagine to be ordered in time, such that \(A\)
precedes \(B\) and \(B\) precedes \(C\). We will describe the rules of
d-separation by referring to three structural relationships, where
``structural'' means causal, and where the causal relationships are
presented in a graph with nodes and arrows, such that arrows denote
causal relationships.

\paragraph{\texorpdfstring{1. \textbf{Fork (Common
Cause)}}{1. Fork (Common Cause)}}\label{fork-common-cause}

Diagram: \(B \leftarrow \boxed{A} \rightarrow C\)

Suppose that \(A\) is a common cause of \(B\) and \(C\), and that there
are no additional common causes or direct causal paths between \(B\) and
\(C\). Pearl proved that conditioning on \(A\), which we denote
\(\boxed{A}\), removes the association between \(B\) and \(C\) due to
the common cause \(A\). Therefore, \(B\) and \(C\) are statistically
independent given \(A\), which in the potential outcomes framework we
write:

\[
B \coprod C \mid A
\]

In Pearl's vocabulary, we say that \(B\) and \(C\) are ``d-separated''
conditional on \(A\). This means that any statistical association
between \(B\) and \(C\) observed in the data is due to the common cause
\(A\), and after conditioning on \(A\), there is no remaining
association between \(B\) and \(C\).

\paragraph{\texorpdfstring{2. \textbf{Chain
(Mediator)}}{2. Chain (Mediator)}}\label{chain-mediator}

Diagram: \(A \rightarrow \boxed{B} \rightarrow C\)

Suppose that we are interested in the causal effect of \(A\) on \(C\).
Suppose furthermore that \(B\) is an effect of \(A\) as well as a cause
of \(C\)---in other words, \(B\) is a mediator of \(A\)'s effect on
\(C\). Pearl proved that conditioning on \(B\), which we denote
\(\boxed{B}\), blocks the association between \(A\) and \(C\) along the
path \(A \rightarrow B \rightarrow C\). Thus, \(A\) and \(C\) are
independent given \(B\):

\[
A \coprod C \mid B
\]

Recall that earlier we said that we will typically not draw an arrow
linking the treatment and the outcome of interest. The exception to this
rule is when we are interested in whether our conditioning strategy will
introduce bias such that a true causal relationship becomes distorted.
Suppose that \(A\) is randomised and that \(C\) is the outcome of
interest. To estimate the total effect of \(A\) on \(C\), Pearl's rules
of d-separation imply that we must not condition on \(B\). Doing so will
introduce what is known as ``mediator bias.'' Mediator bias is a species
of overconditioning bias in which, by conditioning on a variable, we
unwittingly distort a causal effect estimate. If
\(A \rightarrow B \rightarrow C\) were an accurate representation of
reality, then we should not condition on \(B\). This should be easy,
right? In a recent study, Montgomery \emph{et al.}
(\citeproc{ref-montgomery2018}{2018}) found that 46.7\% of the
experimental studies published in leading experimental political science
journals conditioned on a post-treatment variable. These and other
self-inflicted injuries would be easily avoided were investigators
simply to graph the relationships in their data (for discussion refer to
Bulbulia (\citeproc{ref-bulbulia_2024_experiments}{2024e})). Note that
when data are cross-sectional---that is, collected at one time
point---we often cannot evaluate whether a variable is a cause or an
effect of another. We will return to the perils of cross-sectional data
below.

\paragraph{\texorpdfstring{3. \textbf{Collider (Common
Effect)}}{3. Collider (Common Effect)}}\label{collider-common-effect}

Diagram: \(A \rightarrow \boxed{C} \leftarrow B\)

Suppose now that we are interested in estimating the causal effect of
\(A\) on \(B\). Suppose there is no direct causal relationship between
\(A\) and \(B\) (we do not draw an arrow from \(A\) to \(B\)). Suppose
further that \(C\) is a common effect of \(A\) and \(B\), and that we
condition on \(C\). If we do not condition on \(C\), then \(A\) and
\(B\) are independent, or in Pearl's vocabulary, \(A\) and \(B\) are
d-separated. However, Pearl proved that conditioning on a common effect
of \(A\) and \(B\) may introduce a statistical dependence between \(A\)
and \(B\), such that:

\[
A \coprod B
\]

Yet

\[
A \cancel{\coprod} B \mid C
\]

When a variable is the child of two parents, we call this variable a
``collider.'' Here, \(C\) is a collider of \(A\) and \(B\). According to
Pearl's rules of d-separation, \(A\) and \(B\) are independent if we do
not condition on \(C\), yet dependent if we do. If we wish to estimate
the causal effect of \(A\) on \(B\), the advice would be simple: do not
condition on \(C\). However, with only cross-sectional data, we often
cannot be certain whether a variable that is a common effect of the
treatment and the outcome is a common effect. Experimental studies that
collect data after the outcomes have been measured and include these
data in a statistical model---or use the data to select
participants---may easily fall prey to collider biases (refer to
Bulbulia (\citeproc{ref-bulbulia_2024_experiments}{2024e})). However,
Pearl's graphical rules make it clear that such practices must be
avoided. A nice feature of Pearl's rules of d-separation is that they
are supported by proofs.

We can restate the rules of d-separation as follows:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Colliders block paths when unconditioned:} an open path (with
  no variables conditioned on) is blocked if it contains a collider---a
  node where two arrows converge. If
  \(A \rightarrow \boxed{C} \leftarrow B\), then \(A\) and \(B\) are
  blocked from being associated with each other when \(C\) is not
  conditioned upon.
\item
  \textbf{Conditioning on a collider opens a path:} conditioning on a
  collider opens a previously blocked path, potentially introducing an
  association between variables that are not causally related. If
  \(A \rightarrow \boxed{C} \leftarrow B\), conditioning on \(C\) allows
  information to flow between \(A\) and \(B\), leading to a possible
  association in the absence of a direct causal relationship.
\item
  \textbf{Conditioning on a descendant of a collider also opens the
  path:} conditioning on a descendant of a collider has a similar
  effect. If \(A \rightarrow C \rightarrow \boxed{C'} \leftarrow B\),
  and \(C \to C'\), then conditioning on \(C'\) is akin to conditioning
  on \(C\). The path
  \(A \rightarrow C \rightarrow \boxed{C'} \leftarrow B\) becomes
  unblocked. Thus, conditioning on \(C'\) opens an association between
  \(A\) and \(B\).
\item
  \textbf{Conditioning on non-collider nodes blocks the path:} if a path
  does not contain a collider, conditioning on any variable along that
  path blocks it. For instance, in the diagram
  \(A \rightarrow \boxed{B} \rightarrow C\), conditioning on \(B\)
  blocks the path from \(A\) to \(C\), rendering \(A\) and \(C\)
  conditionally independent given \(B\)
  (\citeproc{ref-hernan2024WHATIF}{Hernan and Robins 2024 p. 78};
  \citeproc{ref-pearl2009a}{Pearl 2009b}).
\end{enumerate}

Pearl proved these rules of d-separation in the 1990s, demonstrating
that we may use causal directed acyclic graphs to evaluate the
conditions under which consistent causal effect estimates are possible
from data (\citeproc{ref-pearl1995}{Pearl 1995},
\citeproc{ref-pearl2009a}{2009b}). Note again that the structures in a
graph must be assumed and cannot generally be verified by the data
alone.

\subsubsection{The Concept of a ``Backdoor
Path''}\label{the-concept-of-a-backdoor-path}

In causal diagrams, the concept of a \emph{backdoor path} is central to
understanding whether an association between treatment and outcome
variables might be confounded by other variables. A \emph{backdoor path}
is any path between the treatment and the outcome that flows through a
common cause of both variables, rather than through a direct causal
link. In general, backdoor paths introduce confounding, which means that
an association between the treatment and outcome variables is not purely
the result of the causal effect of the treatment.

To illustrate how blocking a backdoor path works we return to our
convention of using \(A_1\) to represent the treatment and \(Y_2\) to
represent the outcome. We index the nodes with numbers to clarify the
temporal order of causality. The numbers denote relative timing. A
backdoor path is any non-causal path between \(A_1\) and \(Y_2\) that
originates from \(A_1\), passes through one or more other variables, and
ends at \(Y_2\). These paths include arrows that flow both into and out
of the treatment or the outcome, such that the flow of influence
``sneaks'' in through the backdoor. For example in this causal DAG:

\[
A_1 \leftarrow L_0 \rightarrow Y_2
\]

\(L_0\) is a common cause of both \(A_1\) and \(Y_2\). Therefore, the
association between \(A_1\) and \(Y_2\) is confounded by \(L_0\), as
part of their association arises from the influence that \(L_0\) exerts
on both. The open backdoor path runs from \(A_1\) to \(L_0\) to \(Y_2\).
Because this path is open we say that \(A_1\) and \(Y_2\) are
``d-connected.''

\paragraph{Blocking Backdoor Paths}\label{blocking-backdoor-paths}

To estimate the causal effect of \(A_1\) on \(Y_2\), we need to block
all backdoor paths between them. This can be done by conditioning on, or
``adjusting for,'' a set of covariates that block all such paths. In our
example, adjusting for \(L_0\)---which we graphically represent as
\(\boxed{L_0}\)---blocks the backdoor path
\(A_1 \leftarrow \boxed{L_0} \rightarrow Y_2\) and removes the
confounding introduced by \(L_0\). Once this backdoor path is blocked,
the association between \(A\) and \(Y\) can be attributed to the causal
effect of \(A\) on \(Y\).

Formally, if all backdoor paths between \(A\) and \(Y\) are blocked by
conditioning on a set of covariates \(L\), then:

\[
Y_2(a) \coprod A_1 \mid L_0
\]

This is the conditional exchangeability assumption, which, as we
observed in Part 1, must be satisfied for us to identify the causal
effect of \(A_1\) on \(Y_2\). If all backdoor paths are closed between
\(A_1\) and \(Y_2\) we say that \(A_1\) and \(Y_2\) are ``d-separated.''

\paragraph{Rules for Blocking Backdoor
Paths}\label{rules-for-blocking-backdoor-paths}

To block a backdoor path, we can condition on a variable that lies on
the path, but the choice of variable must follow the rules of
d-separation:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Conditioning on a common cause:} if a variable is a common
  cause of both \(A\) and \(Y\), conditioning on this variable blocks
  the backdoor path. In our previous example, conditioning on \(L_1\)
  blocks the path \(A \leftarrow L_1 \rightarrow Y\).
\item
  \textbf{Avoid conditioning on a mediator:} conditioning on a mediator
  (a variable that lies on the causal path from \(A\) to \(Y\))
  introduces mediator bias. For example, if we condition on a variable
  \(M\) in the path \(A \rightarrow M \rightarrow Y\), we block the
  causal effect of \(A\) on \(Y\) and distort our estimate.
\item
  \textbf{Avoid conditioning on a collider:} conditioning on a collider
  can introduce spurious associations. As discussed earlier, if we
  condition on \(C\) in the diagram \(A \rightarrow C \leftarrow B\), we
  may induce an association between \(A\) and \(B\), even though no
  direct causal link exists between them.
\end{enumerate}

\subsubsection{How to apply d-Separation to Causal
DAGs}\label{how-to-apply-d-separation-to-causal-dags}

Now that we understand the essential components of a causal DAG, let us
apply the concept of d-separation to a series of practical example.

\subsection{Part 3: How to Use Causal Diagrams for Causal Identification
Tasks---Worked Examples}\label{section-part3}

\subsubsection{Notation}\label{notation}

In the previous section, we introduced causal Directed Acyclic Graphs
(causal DAGs), which use specific symbols to represent key elements in
causal inference (\citeproc{ref-greenland1999}{Greenland \emph{et al.}
1999}; \citeproc{ref-pearl1995}{Pearl 1995},
\citeproc{ref-pearl2009}{2009a}). Here, we adopt the following notation
to illustrate how to use causal DAGs:

\begin{itemize}
\tightlist
\item
  \textbf{Treatment (\(A\))}: This represents an intervention or
  exposure we are interested in studying, such as giving people access
  to green space.
\item
  \textbf{Outcome (\(Y\))}: This is the variable of interest that may be
  affected by the treatment, such as subjective well-being. Recall that
  \(Y(a)\) denotes the outcome \(Y\) when the treatment \(A\) is set to
  a specific value \(a\).
\item
  \textbf{Measured Confounders (\(L\))}: These are variables that we
  have measured and that may influence both the treatment and the
  outcome, such as age, gender, or income.
\item
  \textbf{Measured Descendants of Confounders Not Affected by Treatment
  or Outcome (\(L^\prime\))}: These are variables that are effects
  (descendants) of confounders, which we have measured and are not
  affected by the treatment or outcome. By adjusting for such variables,
  we can account for the effects of confounders; in other words, these
  descendants can serve as proxies for the confounders.
\item
  \textbf{Unmeasured Confounders (\(U\))}: These are variables that we
  have not directly observed but that may influence both the treatment
  and the outcome, potentially leading to a non-causal or ``spurious''
  association between them.
\end{itemize}

Table Table~\ref{tbl-04} provides seven worked examples that demonstrate
how to use causal diagrams in practice. In these examples, we focus on
the question of whether access to green space affects happiness. We
explore how different assumptions about (i) the structure of the world
and (ii) the observational data collected can influence strategies for
controlling confounding and our confidence in the results. Each example
corresponds to a row in the table.

\begin{table}

\caption{\label{tbl-04}}

\centering{

\terminologyelconfoundersLONG

Putting causal directed acyclic graphs (DAGs) to work.

}

\end{table}%

\subsubsection{1. Confounding by a Common
Cause}\label{confounding-by-a-common-cause}

Table Table~\ref{tbl-04}, Row 1, describes the problem of confounding by
a common cause. We encountered this issue in
\hyperref[section-part2]{Part 2}. Confounding arises when there is a
variable or set of variables, denoted by \(L_0\), that influences both
the treatment (\(A_1\)) and the outcome (\(Y_2\)). Because \(L_0\) is a
common cause of both \(A_1\) and \(Y_2\), it can create a statistical
association between them that does not reflect a causal relationship.

For example, in the context of green spaces, consider people who live
closer to green spaces (treatment \(A_1\)) and their experience of
improved happiness (outcome \(Y_2\)). A common cause might be
socioeconomic status (\(L_0\)). Individuals with higher socioeconomic
status might have the financial means to afford housing near green
spaces and simultaneously have better access to healthcare and lifestyle
choices that contribute to greater happiness. Thus, although data may
show a statistical association between living closer to green spaces
(\(A_1\)) and greater happiness (\(Y_2\)), this association might not
reflect a direct causal relationship due to confounding by socioeconomic
status (\(L_0\)).

To address confounding by a common cause, we adjust for the confounder
in our statistical model. This adjustment can be done through methods
such as regression analysis or more advanced techniques like inverse
probability of treatment weighting and marginal structural models (see
Hernan and Robins (\citeproc{ref-hernan2024WHATIF}{2024})). Adjusting
for \(L_0\) effectively \textbf{blocks the backdoor path} from the
treatment to the outcome. In other words, conditioning on \(L_0\) leads
to d-separation of \(A_1\) and \(Y_2\) because \(\boxed{L_0}\) closes
the backdoor path from \(A_1\) to \(Y_2\).

Table Table~\ref{tbl-04}, Row 1, Column 3, emphasises that a confounder
must precede both the treatment and the outcome in time. Although in
cross-sectional data (data collected at a single point in time) it is
sometimes clear whether a confounder precedes the treatment (e.g., a
person's country of birth), often the relative timing of events is
unclear. Therefore, we must draw several causal diagrams to consider the
implications of different assumptions. Below, we return to the idea that
investigators should draw multiple causal DAGs and perform multiple
analyses.

\subsubsection{2. Mediator Bias}\label{mediator-bias}

Table Table~\ref{tbl-04}, Row 2, presents a problem of mediator bias.
Let's reconsider whether proximity to green spaces (\(A_0\)) affects
happiness (\(Y_2\)), and suppose that physical activity is a mediator,
denoted by \(L_1\).

Imagine that living close to green spaces (\(A_0\)) increases physical
activity (\(L_1\)), which in turn improves happiness (\(Y_2\)). If we
mistakenly treat physical activity (\(L_1\)) as a confounder and adjust
for it in our analysis, we will bias our estimate of the total effect of
green space proximity (\(A_0\)) on happiness (\(Y_2\)). This bias occurs
because adjusting for \(L_1\) blocks part of the causal pathway from
\(A_0\) to \(Y_2\), effectively breaking the link between them.

Again, this bias is known as mediator bias, and as mentioned in
\hyperref[section-part2]{Part 2}, experiments are not immune to such
over-adjustment biases. (Recall that Montgomery \emph{et al.}
(\citeproc{ref-montgomery2018}{2018}) found that over half of all
published research in political science experiments were guilty of
conditioning on a post-treatment variable.)

To avoid mediator bias when estimating the total causal effect, we
should never condition on a mediator. The surest way to prevent this
problem is to ensure that data collection for covariates we assume to be
confounders (\(L_0\)) occurs before data collection of the treatment
(\(A_1\)), which in turn occurs before data collection of the outcome
(\(Y_2\)) (\citeproc{ref-vanderweele2020}{VanderWeele \emph{et al.}
2020}). We present this solution in Table Table~\ref{tbl-04}, Row 2,
Column 3.

\subsubsection{3. Confounding by Collider Stratification (Conditioning
on a Common
Effect)}\label{confounding-by-collider-stratification-conditioning-on-a-common-effect}

Table Table~\ref{tbl-04}, Row 3, illustrates a problem known as collider
bias. Recall that this type of bias occurs when we condition on a
variable that is a common effect (a ``collider'') of both the treatment
(\(A_1\)) and the outcome (\(Y_2\)), denoted by \(L_3\).

Suppose that access to green spaces (\(A_1\)) and happiness (\(Y_2\))
are actually independent; that is, there is no causal relationship
between them. Furthermore, assume that physical health (\(L_3\)) is
affected by both access to green space and happiness. In other words,
\(L_3\) is a common effect of \(A_1\) and \(Y_2\).

If we condition on \(L_3\)---for example, by including it as a covariate
in our analysis---we may inadvertently introduce an association between
\(A_1\) and \(Y_2\) where none exists. This happens because knowing the
value of \(L_3\) gives us information about both \(A_1\) and \(Y_2\),
creating a spurious association between them. This phenomenon is known
as collider stratification bias.

To avoid collider bias, it is important to carefully consider which
variables we adjust for in our analysis. Specifically, we should avoid
conditioning on variables that are common effects of the treatment and
outcome. Instead, we should focus on measuring and adjusting for
variables that are common causes (confounders) of both the treatment and
the outcome.

In practice, we can mitigate the risk of collider bias by ensuring the
following:

\begin{itemize}
\tightlist
\item
  Measure all common causes (\(L_0\)) of the treatment (\(A_1\)) and
  outcome (\(Y_2\)) before the treatment occurs.
\item
  Ensure that the outcome (\(Y_2\)) is measured after the treatment
  (\(A_1\)) occurs.
\end{itemize}

\subsubsection{4. Confounding by Conditioning on a Descendant of a
Confounder}\label{confounding-by-conditioning-on-a-descendant-of-a-confounder}

Table Table~\ref{tbl-04}, Row 4, presents a situation where conditioning
on a descendant (an effect) of a confounder can introduce bias. This
happens when we adjust for a variable that is influenced by an
unmeasured confounder.

For instance, imagine that poor health is an unmeasured confounder that
affects both access to green spaces (\(A_1\)) and happiness (\(Y_2\)).
Suppose further that poor health is an outcome of both the treatment and
outcome. To avoid confounding, we must measure poor health before the
\(A_1\) and \(Y_2\). Suppose further we have data on doctor visits
(\(L_4\)), which are a consequence of poor health, but the count of
doctors visits is compiled after the outcome has occurred. If we
condition on doctor visits (\(L_4\)), we are effectively conditioning on
a descendant of the unmeasured confounder (poor health at Time 3). This
can introduce bias into our analysis because it can open a backdoor path
between the treatment and outcome, leading to confounding. The solution
here is clear. Do not condition on \(L_4\). Rather, ensure that the the
confounders are not effects of the treatment. If the data are not
available then draw several causal graphs to clarify how estimation is
compromised (see: \hyperref[section-part4]{Part 4}).

\subsubsection{5. M-Bias: Conditioning on a Pre-Exposure
Collider}\label{section-mbias}

Table Table~\ref{tbl-04}, Row 5, presents a form of bias known as
M-bias, which arises from conditioning on a pre-exposure collider. This
bias combines the collider structure and the fork (common cause)
structure, revealing that it is possible to induce confounding even if
all variables have been measured before the treatment.

In the causal diagram, the collider structure is evident in the path
\(U_Y \to L_0 \leftarrow U_A\). Conditioning on \(L_0\) opens a path
between \(U_Y\) and \(U_A\). As a result, \(U_Y\) becomes associated
with \(U_A\), creating a backdoor path from the treatment (\(A_1\)) to
the outcome (\(Y_2\)) through \(U_A\) and \(U_Y\). This leads to
confounding, even though we have adjusted for \(L_0\).

How might this confounding play out in a real-world setting?

In the context of green spaces, consider that an individual's level of
physical activity (\(L_0\)) is influenced by an unmeasured factor
related to their propensity to live near green spaces (\(A_1\))---say,
their childhood upbringing (\(U_A\)). Suppose further that another
unmeasured factor---say, a genetic predisposition (\(U_Y\))---increases
both physical activity (\(L_0\)) and happiness (\(Y_2\)). Here, physical
activity (\(L_0\)) does not affect the decision to live near green
spaces (\(A_1\)) or happiness (\(Y_2\)) directly but is a descendant of
unmeasured variables that do.

If we condition on physical activity (\(L_0\)) in this scenario, we
create the bias just described---M-bias. By adjusting for \(L_0\), we
open a path between \(U_A\) and \(U_Y\), which introduces confounding
between \(A_1\) and \(Y_2\).

How can we respond to this problem? The solution is straightforward: if
\(L_0\) is not a common cause of \(A_0\) and \(Y_0\), or a descendent of
a common cause, \(L-0\) should not be included in our causal model. In
terms of the conditional exchangeability principle, here we find that
\(A_1\) and \(Y_2(a)\) are independent (\(A_1 \coprod Y_2(a)\)), but
they become dependent when we condition on \(L_0\)
(\(A_1 \cancel{\coprod} Y_2(a) \mid L_0\)). Therefore, again, we should
not condition on \(L_0\)---that is, do not control for physical activity
in this case (\citeproc{ref-cole2010}{Cole \emph{et al.} 2010}).

\subsubsection{6. Conditioning on a Descendant May Sometimes Reduce
Confounding}\label{section-conditioning-on-descendents}

In Table Table~\ref{tbl-04}, Row 6, we encounter a causal diagram where
an unmeasured confounder creates a backdoor path between the treatment
and the outcome. Here, we explore how we can use the rules of
d-separation to find unexpected strategies for controlling confounding.

Returning to our green space example, suppose there is an unmeasured
genetic factor (\(U\)) that influences both an individual's preference
for living near green spaces (\(A_1\)) and their happiness (\(Y_2\)). If
such an unmeasured confounder exists, it seems we cannot obtain an
unbiased estimate of the causal effect of green space access on
happiness because we cannot adjust for \(U\).

However, imagine there is a variable \(L_3'\), a trait that emerges
later in life as a result of the genetic factor \(U\). Even though
\(L_3'\) is expressed after the treatment and outcome have occurred, we
can measure it. By controlling for \(L_3'\), we can help close the
backdoor path between the treatment and outcome. This works because
\(L_3'\) serves as a proxy for the unmeasured confounder \(U\). By
adjusting for \(L_3'\), we partially account for the influence of \(U\).

Therefore, effective confounding control does not always require
measuring pre-exposure variables. This example shows that sometimes we
can control for confounding by conditioning on a post-outcome variable,
which is not intuitive. Although the common cause (\(U\)) must occur
before the treatment and outcome, its proxy (\(L_3'\)) does not have to.
If we can measure the proxy but not the confounder itself, we should
consider conditioning on the post-treatment proxy of a pre-treatment
confounder.

\subsubsection{7. Confounding Control with Three Waves of Data:
Estimating an ``Incident Exposure''
Effect}\label{confounding-control-with-three-waves-of-data-estimating-an-incident-exposure-effect}

Table Table~\ref{tbl-04}, Row 7, addresses another scenario involving
unmeasured confounding. Here, we use the rules of d-separation to
develop a data collection and modeling strategy that can significantly
reduce the impact of unmeasured confounders.

By collecting data on both the treatment and the outcome at an initial
baseline time point (e.g., time 0) and controlling for their baseline
values, we can mitigate confounding. When we include the baseline
measurements of the treatment (\(A_0\)) and the outcome (\(Y_0\)) in our
analysis, any unmeasured association between the treatment at a later
time (\(A_1\)) and the outcome at a subsequent time (\(Y_2\)) would need
to be independent of these baseline measurements. Including these
baseline variables, along with other measured covariates, provides
considerable control over confounding
(\citeproc{ref-vanderweele2020}{VanderWeele \emph{et al.} 2020}).

Moreover, this approach offers an additional benefit. By controlling for
the baseline exposure (\(A_0\)), we focus on individuals who have
changed their level of exposure between the baseline and the follow-up
period. This allows us to estimate the causal effect of initiating or
changing access to green space, known as the incident exposure effect.
This effect is more informative for understanding the impact of changes
in exposure over time.

This strategy better emulates a ``target trial''---an approach where
observational data are organized to mimic a hypothetical randomized
experiment starting at a defined ``time zero'' (see Hernán \emph{et al.}
(\citeproc{ref-hernuxe1n2016}{2016}); Danaei \emph{et al.}
(\citeproc{ref-danaei2012}{2012}); VanderWeele \emph{et al.}
(\citeproc{ref-vanderweele2020}{2020}); Bulbulia
(\citeproc{ref-bulbulia2022}{2023})). Without controlling for the
baseline treatment, we could only estimate a prevalent exposure effect,
which reflects the association with existing levels of exposure rather
than changes in exposure. If initial exposure to green space had caused
some people to become less happy, we would not detect this effect
without considering changes over time.

Finally, by controlling for both the baseline treatment (\(A_0\)) and
the baseline outcome (\(Y_0\)), we further reduce the potential for
unmeasured confounding. For an unmeasured confounder to bias our
estimates, it would need to affect both the treatment and the outcome
independently of their baseline values. Collecting repeated measures
over time allows us to use past states of the treatment and outcome to
exert more robust control over unmeasured confounding.

\subsubsection{Summary}\label{summary-1}

All forms of confounding bias stem from combinations of the basic causal
structures outlined above---absence of cause, causality, forks (common
causes), chains (mediators), and colliders (common effects).
Understanding these elements allows us to identify potential confounders
based on the assumptions encoded in a causal diagram.

There are three essential observations we wish to reiterate:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Causal DAGs Structure Assumptions but Do Not Prove Causation}.
  Causal directed acyclic graphs (DAGs) provide a framework for
  structuring assumptions about causal relationships so that
  investigators may clarify whether causal effects can be identified
  from data, and how. The data do not typically determine which Causal
  DAG investigators should chose, as multiple causal diagrams are
  typically consistent with the data. The strength of causal DAGs lies
  in helping researchers understand the implications of their
  assumptions. Therefore, causal DAGs should be created in collaboration
  with domain experts, because experts will typically have a better
  understanding of which assumptions are important and credible. When
  experts disagree, multiple causal diagrams should be proposed to
  reflect differing assumptions.
\item
  \textbf{Temporal Order is Crucial in Causal Inference}. Causal
  inference relies on the assumption that the treatment precedes the
  outcome in time. Cross-sectional data---that is, data collected at a
  single point in time---typically lack the temporal resolution
  investigators require to establish the order of events. Without
  knowing whether the exposure occurred before the outcome, it's
  difficult to make causal inferences.
\item
  \textbf{Longitudinal Data are Valuable but Not Sufficient}:
  Longitudinal data, which follow the same subjects over multiple time
  points, allow for a clearer temporal ordering of events. However,
  longitudinal data alone are insufficient to establish causality.
  Although we should seek longitudinal data to confirm that confounders
  precede the treatment and that the treatment precedes the outcome,
  longitudinal data are not a cure-all. As we have seen, obtaining a
  causal effect estimate requires a workflow that begins by stating a
  clearly defined causal question, with reference to a counterfactual
  contrast between specified treatments applied to a target population.
  We must then ensure that all assumptions required for causal inference
  are satisfied, particularly that treatment effects can be identified
  from the data. Only after we have stated our causal question and
  checked that our data support the necessary causal assumptions can we
  proceed to statistical analysis.
\end{enumerate}

\subsection{Part 4. Practical Guide For Constructing Causal Diagrams and
Reporting Results When Causal Structure is Unclear}\label{section-part4}

\paragraph{1. Clarify the Research Question and Target
Population}\label{clarify-the-research-question-and-target-population}

Before drawing a causal diagram, we must state the problem it addresses
and the population to whom it applies. Causal identification strategies
may vary by question. We have seen that if
\(A\to B\to C; A\to B; A\to C\) the confounding control strategy for
evaluating \(A \to C\) differs from that for \(B \to C\). Again, if our
interest is in estimating \(B \to C\) then reporting a coefficient for
\(A\) would be ill-advised because the path \(A\to C\) contains the
mediator \(B\); see Westreich and Greenland
(\citeproc{ref-westreich2013}{2013}); McElreath
(\citeproc{ref-mcelreath2020}{2020}); Bulbulia
(\citeproc{ref-bulbulia2023}{2024b}).

\subsubsection{2 Evaluate Bias in the Absence of a Treatment
Effect}\label{evaluate-bias-in-the-absence-of-a-treatment-effect}

Before attributing any statistical association to causality, we must
eliminate non-causal sources of correlation. We do this by, first, by
drawing the treatment (\(A\)) and outcome (\(Y\)) on our causal diagram
with no arrow linking them. As mentioned in
\hyperref[section-part2]{Part 2}, do not draw an arrow between \(A\) and
\(Y\) because we are evaluating bias in the absence of a treatment
effect. Second, we identify all common causes of \(A\) and \(Y\), and
consider whether they are measured or unmeasured. Third, we use the
rules of d-separation to evaluate whether conditioning on measured
common causes of \(A\) and \(Y\) will block all backdoor paths that
create indirect, non-causal associations between \(A\) and \(Y\). That
is, we consider whether \(A\) and \(Y\) are d-separated by the measured
common causes of \(A\) and \(Y\). If they are, then we have successfully
blocked all backdoor paths and can proceed to evaluate bias in the
presence of a treatment effect. If they are not, then we have failed to
block all backdoor paths and our estimate of the causal effect of \(A\)
on \(Y\) will be biased.

\paragraph{3. Draw the Most Recent Common Causes of Exposure and
Outcome}\label{draw-the-most-recent-common-causes-of-exposure-and-outcome}

Include all common causes (confounders) of both the exposure and the
outcome in your diagram, whether measured or unmeasured. Where possible,
group functionally similar common causes into a single variable (e.g.,
\(L_0\) for demographic variables).

\paragraph{4. Include All Ancestors of Measured
Confounders}\label{include-all-ancestors-of-measured-confounders}

Add any ancestors (precursors) of measured confounders associated with
the treatment, the outcome, or both. Simplify the causal diagram by
grouping similar variables. For example, suppose we believe that both
age and income are common causes of both the treatment and the outcome.
We may represent this belief by grouping age and income into a single
variable, \(L_0 = \{age,~income\}\).

\paragraph{5. Explicitly State Assumptions About Relative
Timing}\label{explicitly-state-assumptions-about-relative-timing}

Annotate the temporal sequence of events using subscripts (e.g.,
\(L_0\), \(A_1\), \(Y_2\)).

Note that it is imperative that causal DAGs are acyclic. The graph:
\(A \to Y \to A \to Y\) is not acyclic. \textbf{However, the demand for
an acyclic graph does not imply that dynamic causal relationships are
precluded from causal analysis.} For example
\(L_0 \to Y_1 \to  A_1 \to L_2 \to A_3 \to Y_5\) is an acyclic graph.
Here, past states of the confounders, treatment, and outcome are
depicted as affecting their future states. By indexing the relative
timing of states we ensure that the causal diagram is acyclic.

Note this example further illustrates the potential difficulties when
investigating multiple treatments. Suppose we are interested in causal
contrasts for multiple treatments over time, for example the contrast
between \(A_1 = 0, A_3 = 0\) and \(A_1 = 1, A_3 = 1\), with the outcome
\(Y_5\) measured at the end of study. Notice that \(L_2\) is a common
cause of \(A_3\) and \(Y_5\). We must condition on \(L_2\) to block the
back-door path from \(A_3\) to \(Y_5\). However, were we to condition on
\(L_2\), we would induce mediator bias because \(L_2\) blocks the path
from \(A_1\) to \(Y_5\). We cannot use standard regression, multi-level
regression, or structural equation modelling to estimate the
time-varying treatment regime of interest. We instead require special
methods (refer to Hernan and Robins
(\citeproc{ref-hernan2024WHATIF}{2024}); Bulbulia
(\citeproc{ref-bulbulia2024swigstime}{2024c})).

\paragraph{6. Arrange Temporal Order
Visually}\label{arrange-temporal-order-visually}

Arrange your diagram to reflect the temporal progression of causality,
either left-to-right or top-to-bottom. This enhances comprehension of
causal relations. Establishing temporal ordering is vital for evaluating
identification problems, as discussed in
\hyperref[sec-part3]{\textbf{Part 3}}.

\paragraph{7. Box Variables Adjusted for
Confounding}\label{box-variables-adjusted-for-confounding}

Mark variables for adjustment (e.g., confounders) with boxes or another
easy to understand convention. Be clear about this and other
conventions.

\paragraph{8. Present Paths Structurally, Not
Parametrically}\label{present-paths-structurally-not-parametrically}

Focus on whether paths exist, not their functional form (linear,
non-linear, etc.). Parametric descriptions are not relevant for bias
evaluation in a causal diagram. For an explanation of causal interaction
and diagrams, see Bulbulia (\citeproc{ref-bulbulia2023}{2024b}).

\paragraph{9. Minimise Paths to Those Necessary for the Causal
Identification
Problem}\label{minimise-paths-to-those-necessary-for-the-causal-identification-problem}

Reduce clutter by including only paths critical for the specific
question (e.g., back-door paths, mediators).

\paragraph{10. Consider Potential Unmeasured
Confounders}\label{consider-potential-unmeasured-confounders}

Use domain expertise to identify potential unmeasured confounders and
represent them in your diagram. This proactive step helps anticipate and
address \emph{all} possible sources of confounding bias.

\paragraph{11. State Your Graphical
Conventions}\label{state-your-graphical-conventions}

Establish and explain the graphical conventions used in your diagram in
your manuscript (e.g., using red to highlight open back-door paths).
Consistency in symbol use enhances interpretability, while explicit
descriptions improve accessibility and understanding.

\paragraph{12. Prepare Sensitivity
Analyses.}\label{prepare-sensitivity-analyses.}

Because unmeasured confounding cannot be ruled out, investigators should
implement sensitivity analyses to determine how dependent conclusions
are on specific assumptions or parameters within your causal model. A
relatively simple sensitivity analysis is VanderWeele's E-value
(\citeproc{ref-vanderweele2017}{VanderWeele and Ding 2017})

\subsubsection{Specific Advice for Causal Analysis and Reporting in
Cross-Sectional
Designs}\label{specific-advice-for-causal-analysis-and-reporting-in-cross-sectional-designs}

\begin{table}

\caption{\label{tbl-cs}Cross-sectional designs typically require
multiple causal DAGS where the temporal order of variables cannot be
ensured.}

\centering{

\examplecrosssection

}

\end{table}%

\subsubsection{Recommendations for Conducting and Reporting Causal
Analyses with Cross-Sectional
Data}\label{recommendations-for-conducting-and-reporting-causal-analyses-with-cross-sectional-data}

When analysing and reporting analyses with cross-sectional data,
researchers face the challenge of making causal inferences without the
benefit of temporal information.

The following recommendations aim to guide researchers in navigating
these challenges effectively:

\paragraph{\texorpdfstring{1. \textbf{Draw multiple causal
diagrams}}{1. Draw multiple causal diagrams}}\label{draw-multiple-causal-diagrams}

As shown in Table~\ref{tbl-cs}, draw multiple causal diagrams to
represent different theoretical assumptions about the relationships and
timing of variables relevant to an identification problem. If some
causal pathways cannot be ruled out, clarify the implications of
assigning variables the roles for which consensus or which the time
ordering of the data do not resolve.

\paragraph{\texorpdfstring{2. \textbf{Perform and report analyses for
each
assumption}}{2. Perform and report analyses for each assumption}}\label{perform-and-report-analyses-for-each-assumption}

Conduct and transparently report separate analyses for each scenario
your causal diagrams depict. This practice ensures that your study is
theoretically grounded for each model. Presenting results from each
analytical approach and the underlying assumptions and statistical
methods promotes a balanced interpretation of findings.

\paragraph{\texorpdfstring{3. \textbf{Report divergent
findings}}{3. Report divergent findings}}\label{report-divergent-findings}

Approach conclusions with caution, especially when findings suggest
differing practical implications. Acknowledge the limitations of
cross-sectional data in establishing causality and the potential for
alternative explanations. Do not over-sell.

\paragraph{\texorpdfstring{4. \textbf{Identify avenues for future
research}}{4. Identify avenues for future research}}\label{identify-avenues-for-future-research}

Target future research that might clarify ambiguities. Consider the
design of longitudinal studies or experiments capable of clarifying
lingering uncertainties.

\paragraph{\texorpdfstring{5. \textbf{Supplement observational data with
simulated
data}}{5. Supplement observational data with simulated data}}\label{supplement-observational-data-with-simulated-data}

Leverage data simulation to understand the complexities of causal
inference. Simulating data based on various theoretical models allows
researchers to examine the effect of different assumptions on their
findings. This method tests analytical strategies under controlled
conditions, assessing the robustness of conclusions against assumption
violations or unobserved confounders.

\subsubsection{Specific Advice for Causal Analysis and Reporting in
Longitudinal
Designs}\label{specific-advice-for-causal-analysis-and-reporting-in-longitudinal-designs}

Longitudinal designs offer a substantial advantage over cross-sectional
designs for causal inference because sequential measurements allow us to
capture causation and quantify its magnitude. We typically do not need
to assert timing as in cross-sectional data settings. Because we know
when variables have been measured, we can reduce ambiguity about the
directionality of causal relationships. For instance, tracking changes
in ``happiness'' following changes in access to green spaces over time
can more definitively suggest causation than cross-sectional snapshots.

Despite this advantage, longitudinal researchers nevertheless face
assumptions regarding the absence of unmeasured confounders or the
stability of measured confounders over time. These assumptions must be
explicitly stated. As with cross-sectional designs, wherever assumptions
differ, researchers should draw different causal diagrams that reflect
these assumptions and subsequently conduct and report separate analyses.
Supplementary materials D and E provide examples of how to conduct and
report analyses for multiple causal diagrams. The following summarises
our advice.

\paragraph{1. Draw multiple causal
diagrams}\label{draw-multiple-causal-diagrams-1}

\begin{itemize}
\item
  \textbf{Causal Identification problem diagram}: begin as usual by
  stating a causal question and population of interest. Then
  constructing a causal diagram that outlines your assumptions about the
  relationships among variables relevant for ensuring conditional
  exchangeability between the treatment and outcome.
\item
  \textbf{Solution diagram}: next, create a separate causal diagram that
  proposes solutions to the identified problems. Having distinct
  diagrams for the problem and its proposed solutions clarifies your
  study's analytic strategy and theoretical underpinning.
\end{itemize}

Table~\ref{tbl-lg} provides an example of a table with multiple causal
diagrams clarifying potential sources of confounding threats and reports
strategies for addressing them.

\begin{table}

\caption{\label{tbl-lg}Use causal DAGs to report both the causal
identification problem and its solution.}

\centering{

\examplelongitudinal

}

\end{table}%

\paragraph{2. Attempt longitudinal designs with at least three waves of
data}\label{attempt-longitudinal-designs-with-at-least-three-waves-of-data}

Incorporating repeated measures data from at least three time intervals
considerably enhances your ability to infer causal relationships. For
example, by adjusting for physical activity measured before the
treatment, we can ensure that physical activity does not result from a
new initiation to green spaces, which we establish by measuring green
space access at baseline. Establishing chronological order allows us to
avoid confounding problems 1-4 in Table~\ref{tbl-04}.

\paragraph{3. Where causality is unclear, report results for multiple
causal
graphs}\label{where-causality-is-unclear-report-results-for-multiple-causal-graphs}

Given that the true causal structure may be complex and partially
unknown, analysing and reporting results under each plausible causal
diagram is prudent.

\paragraph{4. Address missing data at baseline and study
attrition}\label{address-missing-data-at-baseline-and-study-attrition}

Longitudinal studies often need help with missing data and attrition,
which can introduce bias and affect the validity of causal inferences.
Implement and report strategies for handling missing data, such as
multiple imputation or sensitivity analyses that assess the bias arising
from missing responses at the study's conclusion. (For more about
addressing missing data, see:
(\citeproc{ref-bulbulia2024PRACTICAL}{Bulbulia 2024a})).

By following these recommendations, you will more effectively navigate
the inherent limitations of observational longitudinal data, improving
the quality of your causal inferences.

\subsubsection{On the Differences Between Methods for Causal Inference
And The Statistical Modelling
Tradition}\label{on-the-differences-between-methods-for-causal-inference-and-the-statistical-modelling-tradition}

In traditional statistical methods such as regression analysis and
structural equation modelling (SEM), the focus often lies in estimating
associations among variables, sometimes treating all variables
symmetrically within the model. However, causal inference shifts the
focus to estimating pre-specified causal contrasts, defined explicitly
in terms of interventions and outcomes measured on a target population.
This approach takes us beyond measuring associations in the data to
understanding the effects of hypothetical and actual interventions.

Here, we have focussed the place of counterfactual contrasts between
well-defined outcomes experienced by a target population under different
levels of treatment. It is important to emphasise that the other
variables in a model are typically treated as nuisance
variables---variables that are not of direct interest, but must be
accounted for to obtain valid causal effect estimates. In causal
inference, nuisance parameters often include confounders that need to be
adjusted to block non-causal associations between a treatment of
interest and the outcomes of interest. However, the coefficients
associated with these nuisance variables generally lack a causal
interpretation. For example, consider a causal pathway where
\(L \to A \to Y\), and we are interested in estimating the effect of
\(A\) on \(Y\) while controlling for \(L\). Although controlling for
\(L\) is necessary to obtain an unbiased estimate of the causal effect
of \(A\) on \(Y\), the coefficient of \(L\) in the model does not have a
straightforward causal interpretation and reporting it may be
misleading, especially if \(A\) mediates the effect of \(L\) on \(Y\).

By concentrating on pre-specified causal contrasts and appropriately
handling nuisance parameters, causal inference methods enable clarity
for pre-specified causal contrasts of interest. This approach departs
from traditional statistical methods by emphasising the estimation of
specific causal effects rather than modelling the entire system of
associations. It acknowledges that not all parameters in a model can
simultaneously be of equal interest; some parameters primarily serve to
adjust for confounding or other biases and should not be interpreted
causally.

Understanding and properly addressing nuisance parameters are critical
steps in a causal inference workflow. The observation that nuisance
parameters typically lack causal interpretation suggests that the
standard practice of reporting all model coefficients may be unwise or
even misleading (see Westreich and Greenland
(\citeproc{ref-westreich2013}{2013}); McElreath
(\citeproc{ref-mcelreath2020}{2020}); Bulbulia
(\citeproc{ref-bulbulia2023}{2024b}); Lonati \emph{et al.}
(\citeproc{ref-Lonati_Lalive_Efferson_2024}{2024})). When multiple
treatments are of interest, it is essential to ensure that the
identification assumptions required for each causal effect are
satisfied---a task that can be more challenging than it appears (see
VanderWeele (\citeproc{ref-vanderweele2015}{2015}); Hernán \emph{et al.}
(\citeproc{ref-hernan2008aObservationalStudiesAnalysedLike}{2008});
Robins (\citeproc{ref-robins1986}{1986}); Robins and Greenland
(\citeproc{ref-robins1992}{1992}); Robins and Hernan
(\citeproc{ref-robins2008estimation}{2008});
(\citeproc{ref-bulbulia2024eigstime}{\textbf{bulbulia2024eigstime?}})).

Our questions are nearly always causal. We want to know what would
happen, on average, if we were to intervene in the world. In the
observational psychological sciences, obtaining such understanding
requires a paradigm shift from traditional statistical modelling. Such a
paradigm shift has become established in economics, computer science,
and epidemiology, and they are making strong inroads in political
science. However, in observational psychological sciences, including
environmental psychology, methods for causal inference remain rare. We
hope this chapter inspires readers to begin developing robust causal
workflows capable of addressing their causal questions and those that
have long animated the field's scientific interests but which have yet
to be coherently addressed.

\subsection{Where to Go Next?}\label{where-to-go-next}

There are many good resources available for learning causal directed
acyclic graphs (\citeproc{ref-barrett2021}{Barrett 2021};
\citeproc{ref-cinelli2022}{Cinelli \emph{et al.} 2022};
\citeproc{ref-glymour2008causal}{Glymour and Greenland 2008};
\citeproc{ref-greenland1999}{Greenland \emph{et al.} 1999},
\citeproc{ref-greenland1999}{1999}, \citeproc{ref-greenland1999}{1999};
\citeproc{ref-hernan2024WHATIF}{Hernan and Robins 2024};
\citeproc{ref-major2023exploring}{Major-Smith 2023};
\citeproc{ref-mcelreath2020}{McElreath 2020};
\citeproc{ref-morgan2014}{Morgan and Winship 2014};
\citeproc{ref-pearl2009a}{Pearl 2009b};
\citeproc{ref-pearl_greenland_2007}{Pearl and Greenland 2007};
\citeproc{ref-rohrer2018}{Rohrer 2018}; \citeproc{ref-suzuki2020}{Suzuki
\emph{et al.} 2020}). For those just getting started on causal diagrams,
we recommend Miguel Hernan's free course here:
\href{https://www.edx.org/learn/data-analysis/harvard-university-causal-diagrams-draw-your-assumptions-before-your-conclusions}{https://www.edx.org/learn/data-analysis/harvard-university-causal-diagrams-draw-your-assumptions-before-your-conclusions,
accessed 10 June 2024}. For those seeking a slightly more technical but
still accessible introduction to causal inference and causal DAGs, we
recommend Brady Neal's introduction to causal inference course and
textbook, both freely available here
\href{https://www.bradyneal.com/causal-inference-course}{https://www.bradyneal.com/causal-inference-course,
accessed 10 June 2024}. For those interested in causal inference in
experiments refer to Hernán and Robins
(\citeproc{ref-hernan2017per}{2017}), Montgomery \emph{et al.}
(\citeproc{ref-montgomery2018}{2018}); Bulbulia
(\citeproc{ref-bulbulia_2024_experiments}{2024e}). For those interested
in causal mediation analysis and time-varying treatments refer to Robins
(\citeproc{ref-robins1986}{1986}); Robins and Greenland
(\citeproc{ref-robins1992}{1992}); Robins and Hernan
(\citeproc{ref-robins2008estimation}{2008}); Laan and Robins
(\citeproc{ref-vanderlaanRobins2003CensoringLongitudinal}{2003}); Dı́az
\emph{et al.} (\citeproc{ref-diaz2021nonparametric}{2021}); Williams and
Díaz (\citeproc{ref-williams2021}{2021}); Hoffman \emph{et al.}
(\citeproc{ref-hoffman2023}{2023});Bulbulia
(\citeproc{ref-bulbulia2024swigstime}{2024c}). Moreover, the fifth
segment of Miguel Hernan's free online course covers time-varying
treatments:
\href{https://www.edx.org/learn/data-analysis/harvard-university-causal-diagrams-draw-your-assumptions-before-your-conclusions}{https://www.edx.org/learn/data-analysis/harvard-university-causal-diagrams-draw-your-assumptions-before-your-conclusions,
accessed 10 June 2024}. On the pitfalls of traditional path modelling
refer to Rohrer \emph{et al.} (\citeproc{ref-rohrer2022PATH}{2022}). We
have set aside complications arising from measurement error. For more on
this topic see Hernán and Cole
(\citeproc{ref-hernan2009MEASUREMENT}{2009}); Hernan and Robins
(\citeproc{ref-hernan2024WHATIF}{2024}); Bulbulia
(\citeproc{ref-bulbulia2024wierd}{2024d}); VanderWeele
(\citeproc{ref-vanderweele2022}{2022}); VanderWeele and Vansteelandt
(\citeproc{ref-vanderweele2022a}{2022}).

\newpage{}

\subsection{Funding}\label{funding}

This work is supported by a grant from the Templeton Religion Trust
(TRT0418). JB received support from the Max Planck Institute for the
Science of Human History. The funders had no role in preparing the
manuscript or deciding to publish it.

\subsection{Contributions}\label{contributions}

DH proposed the chapter. JB developed the approach and wrote the first
draft. Both authors contributed substantially to the final work.

\newpage{}

\subsection{References}\label{references}

\phantomsection\label{refs}
\begin{CSLReferences}{1}{0}
\bibitem[\citeproctext]{ref-barrett2021}
Barrett, M (2021) \emph{Ggdag: Analyze and create elegant directed
acyclic graphs}. Retrieved from
\url{https://CRAN.R-project.org/package=ggdag}

\bibitem[\citeproctext]{ref-bulbulia2022}
Bulbulia, JA (2023) A workflow for causal inference in cross-cultural
psychology. \emph{Religion, Brain \& Behavior}, \textbf{13}(3),
291--306.
doi:\href{https://doi.org/10.1080/2153599X.2022.2070245}{10.1080/2153599X.2022.2070245}.

\bibitem[\citeproctext]{ref-bulbulia2024PRACTICAL}
Bulbulia, JA (2024a) A practical guide to causal inference in three-wave
panel studies. \emph{PsyArXiv Preprints}.
doi:\href{https://doi.org/10.31234/osf.io/uyg3d}{10.31234/osf.io/uyg3d}.

\bibitem[\citeproctext]{ref-bulbulia2023}
Bulbulia, JA (2024b) Methods in causal inference part 1: Causal diagrams
and confounding. \emph{Evolutionary Human Sciences}, \textbf{6}.
Retrieved from \url{https://osf.io/b23k7}

\bibitem[\citeproctext]{ref-bulbulia2024swigstime}
Bulbulia, JA (2024c) Methods in causal inference part 2: Interaction,
mediation, and time-varying treatments. \emph{Evolutionary Human
Sciences}, \textbf{6}. Retrieved from
\url{https://osf.io/preprints/psyarxiv/vr268}

\bibitem[\citeproctext]{ref-bulbulia2024wierd}
Bulbulia, JA (2024d) Methods in causal inference part 3: Measurement
error and external validity threats. \emph{Evolutionary Human Sciences},
\textbf{6}. Retrieved from \url{https://osf.io/preprints/psyarxiv/kj7rv}

\bibitem[\citeproctext]{ref-bulbulia_2024_experiments}
Bulbulia, JA (2024e) Methods in causal inference part 4: Confounding in
experiments. \emph{Evolutionary Human Sciences}, \textbf{6}. Retrieved
from \url{https://osf.io/preprints/psyarxiv/6rnj5}

\bibitem[\citeproctext]{ref-bulbulia2023a}
Bulbulia, JA, Afzali, MU, Yogeeswaran, K, and Sibley, CG (2023)
Long-term causal effects of far-right terrorism in {N}ew {Z}ealand.
\emph{PNAS Nexus}, \textbf{2}(8), pgad242.

\bibitem[\citeproctext]{ref-cinelli2022}
Cinelli, C, Forney, A, and Pearl, J (2022) A Crash Course in Good and
Bad Controls. \emph{Sociological Methods \&Research}, 00491241221099552.
doi:\href{https://doi.org/10.1177/00491241221099552}{10.1177/00491241221099552}.

\bibitem[\citeproctext]{ref-cole2010}
Cole, SR, Platt, RW, Schisterman, EF, \ldots{} Poole, C (2010)
Illustrating bias due to conditioning on a collider. \emph{International
Journal of Epidemiology}, \textbf{39}(2), 417--420.
doi:\href{https://doi.org/10.1093/ije/dyp334}{10.1093/ije/dyp334}.

\bibitem[\citeproctext]{ref-danaei2012}
Danaei, G, Tavakkoli, M, and Hernán, MA (2012) Bias in observational
studies of prevalent users: lessons for comparative effectiveness
research from a meta-analysis of statins. \emph{American Journal of
Epidemiology}, \textbf{175}(4), 250--262.
doi:\href{https://doi.org/10.1093/aje/kwr301}{10.1093/aje/kwr301}.

\bibitem[\citeproctext]{ref-diaz2021nonparametric}
Dı́az, I, Hejazi, NS, Rudolph, KE, and Der Laan, MJ van (2021)
Nonparametric efficient causal mediation with intermediate confounders.
\emph{Biometrika}, \textbf{108}(3), 627--641.

\bibitem[\citeproctext]{ref-edwards2015}
Edwards, JK, Cole, SR, and Westreich, D (2015) All your data are always
missing: Incorporating bias due to measurement error into the potential
outcomes framework. \emph{International Journal of Epidemiology},
\textbf{44}(4), 1452--1459.

\bibitem[\citeproctext]{ref-glymour2008causal}
Glymour, MM, and Greenland, S (2008) Causal diagrams. \emph{Modern
Epidemiology}, \textbf{3}, 183--209.

\bibitem[\citeproctext]{ref-greenland1999}
Greenland, S, Pearl, J, and Robins, JM (1999) Causal diagrams for
epidemiologic research. \emph{Epidemiology (Cambridge, Mass.)},
\textbf{10}(1), 37--48.

\bibitem[\citeproctext]{ref-hernan2023}
Hernan, MA, and Robins, JM (2023) \emph{Causal inference}, Taylor \&
Francis. Retrieved from
\url{https://books.google.co.nz/books?id=/_KnHIAAACAAJ}

\bibitem[\citeproctext]{ref-hernan2024WHATIF}
Hernan, MA, and Robins, JM (2024) \emph{Causal inference: What if?},
Taylor \& Francis. Retrieved from
\url{https://www.hsph.harvard.edu/miguel-hernan/causal-inference-book/}

\bibitem[\citeproctext]{ref-hernan2008aObservationalStudiesAnalysedLike}
Hernán, MA, Alonso, A, Logan, R, \ldots{} Robins, JM (2008)
Observational studies analyzed like randomized experiments: An
application to postmenopausal hormone therapy and coronary heart
disease. \emph{Epidemiology}, \textbf{19}(6), 766.
doi:\href{https://doi.org/10.1097/EDE.0b013e3181875e61}{10.1097/EDE.0b013e3181875e61}.

\bibitem[\citeproctext]{ref-hernan2009MEASUREMENT}
Hernán, MA, and Cole, SR (2009) Invited commentary: Causal diagrams and
measurement bias. \emph{American Journal of Epidemiology},
\textbf{170}(8), 959--962.
doi:\href{https://doi.org/10.1093/aje/kwp293}{10.1093/aje/kwp293}.

\bibitem[\citeproctext]{ref-hernan2017per}
Hernán, MA, and Robins, JM (2017) Per-protocol analyses of pragmatic
trials. \emph{N Engl J Med}, \textbf{377}(14), 1391--1398.

\bibitem[\citeproctext]{ref-hernuxe1n2016}
Hernán, MA, Sauer, BC, Hernández-Díaz, S, Platt, R, and Shrier, I (2016)
Specifying a target trial prevents immortal time bias and other
self-inflicted injuries in observational analyses. \emph{Journal of
Clinical Epidemiology}, \textbf{79}, 70--75.

\bibitem[\citeproctext]{ref-hoffman2023}
Hoffman, KL, Salazar-Barreto, D, Rudolph, KE, and Díaz, I (2023)
Introducing longitudinal modified treatment policies: A unified
framework for studying complex exposures.
doi:\href{https://doi.org/10.48550/arXiv.2304.09460}{10.48550/arXiv.2304.09460}.

\bibitem[\citeproctext]{ref-holland1986}
Holland, PW (1986) Statistics and causal inference. \emph{Journal of the
American Statistical Association}, \textbf{81}(396), 945--960.

\bibitem[\citeproctext]{ref-hume1902}
Hume, D (1902) \emph{Enquiries Concerning the Human Understanding: And
Concerning the Principles of Morals}, Clarendon Press.

\bibitem[\citeproctext]{ref-vanderlaanRobins2003CensoringLongitudinal}
Laan, MJ, and Robins, JM (2003) \emph{Unified methods for censored
longitudinal data and causality}, Springer.

\bibitem[\citeproctext]{ref-lewis1973}
Lewis, D (1973) Causation. \emph{The Journal of Philosophy},
\textbf{70}(17), 556--567.
doi:\href{https://doi.org/10.2307/2025310}{10.2307/2025310}.

\bibitem[\citeproctext]{ref-Lonati_Lalive_Efferson_2024}
Lonati, S, Lalive, R, and Efferson, C (2024) Identifying culture as
cause: Challenges and opportunities. \emph{Evolutionary Human Sciences},
\textbf{6}, e9.
doi:\href{https://doi.org/10.1017/ehs.2023.35}{10.1017/ehs.2023.35}.

\bibitem[\citeproctext]{ref-major2023exploring}
Major-Smith, D (2023) Exploring causality from observational data: An
example assessing whether religiosity promotes cooperation.
\emph{Evolutionary Human Sciences}, \textbf{5}, e22.

\bibitem[\citeproctext]{ref-mcelreath2020}
McElreath, R (2020) \emph{Statistical rethinking: A {B}ayesian course
with examples in {R} and {S}tan}, CRC press.

\bibitem[\citeproctext]{ref-montgomery2018}
Montgomery, JM, Nyhan, B, and Torres, M (2018) How conditioning on
posttreatment variables can ruin your experiment and what to do about
It. \emph{American Journal of Political Science}, \textbf{62}(3),
760--775.
doi:\href{https://doi.org/10.1111/ajps.12357}{10.1111/ajps.12357}.

\bibitem[\citeproctext]{ref-morgan2014}
Morgan, SL, and Winship, C (2014) \emph{Counterfactuals and causal
inference: Methods and principles for social research}, 2nd edn,
Cambridge: Cambridge University Press.
doi:\href{https://doi.org/10.1017/CBO9781107587991}{10.1017/CBO9781107587991}.

\bibitem[\citeproctext]{ref-ogburn2021}
Ogburn, EL, and Shpitser, I (2021) Causal modelling: The two cultures.
\emph{Observational Studies}, \textbf{7}(1), 179--183.
doi:\href{https://doi.org/10.1353/obs.2021.0006}{10.1353/obs.2021.0006}.

\bibitem[\citeproctext]{ref-pearl1995}
Pearl, J (1995) Causal diagrams for empirical research.
\emph{Biometrika}, \textbf{82}(4), 669--688.

\bibitem[\citeproctext]{ref-pearl2009}
Pearl, J (2009a) \emph{\href{https://doi.org/10.1214/09-SS057}{Causal
inference in statistics: An overview}}.

\bibitem[\citeproctext]{ref-pearl2009a}
Pearl, J (2009b) \emph{Causality}, Cambridge University Press.

\bibitem[\citeproctext]{ref-pearl_greenland_2007}
Pearl, J, and Greenland, S (2007) Causal diagrams. In S. Boslaugh, ed.,
\emph{Encyclopedia of epidemiology}, Thousand Oaks, CA: Sage
Publications, 149--156.

\bibitem[\citeproctext]{ref-pearl1995a}
Pearl, J, and Robins, JM (1995) Probabilistic evaluation of sequential
plans from causal models with hidden variables. In \emph{UAI}, Vol. 95,
Citeseer, 444--453.

\bibitem[\citeproctext]{ref-robins1986}
Robins, J (1986) A new approach to causal inference in mortality studies
with a sustained exposure period---application to control of the healthy
worker survivor effect. \emph{Mathematical Modelling}, \textbf{7}(9-12),
1393--1512.

\bibitem[\citeproctext]{ref-robins2008estimation}
Robins, J, and Hernan, M (2008) Estimation of the causal effects of
time-varying exposures. \emph{Chapman \& Hall/CRC Handbooks of Modern
Statistical Methods}, 553--599.

\bibitem[\citeproctext]{ref-robins1992}
Robins, JM, and Greenland, S (1992) Identifiability and exchangeability
for direct and indirect effects. \emph{Epidemiology}, \textbf{3}(2),
143--155.

\bibitem[\citeproctext]{ref-rohrer2018}
Rohrer, JM (2018) Thinking clearly about correlations and causation:
Graphical causal models for observational data. \emph{Advances in
Methods and Practices in Psychological Science}, \textbf{1}(1), 27--42.

\bibitem[\citeproctext]{ref-rohrer2022PATH}
Rohrer, JM, Hünermund, P, Arslan, RC, and Elson, M (2022) That's a lot
to process! Pitfalls of popular path models. \emph{Advances in Methods
and Practices in Psychological Science}, \textbf{5}(2).
doi:\href{https://doi.org/10.1177/25152459221095827}{10.1177/25152459221095827}.

\bibitem[\citeproctext]{ref-rohrer2023withinbetween}
Rohrer, JM, and Murayama, K (2023) These are not the effects you are
looking for: Causality and the within-/between-persons distinction in
longitudinal data analysis. \emph{Advances in Methods and Practices in
Psychological Science}, \textbf{6}(1), 25152459221140842.

\bibitem[\citeproctext]{ref-rubin1976}
Rubin, DB (1976) Inference and missing data. \emph{Biometrika},
\textbf{63}(3), 581--592.
doi:\href{https://doi.org/10.1093/biomet/63.3.581}{10.1093/biomet/63.3.581}.

\bibitem[\citeproctext]{ref-neyman1923}
Splawa-Neyman, J (1990 (orig. 1923)) On the application of probability
theory to agricultural experiments. Essay on principles. Section 9.
(1923). \emph{Statistical Science}, \textbf{5}(4), 465--472.

\bibitem[\citeproctext]{ref-suzuki2020}
Suzuki, E, Shinozaki, T, and Yamamoto, E (2020) Causal Diagrams:
Pitfalls and Tips. \emph{Journal of Epidemiology}, \textbf{30}(4),
153--162.
doi:\href{https://doi.org/10.2188/jea.JE20190192}{10.2188/jea.JE20190192}.

\bibitem[\citeproctext]{ref-vanderweele2015}
VanderWeele, TJ (2015) \emph{Explanation in causal inference: Methods
for mediation and interaction}, Oxford University Press.

\bibitem[\citeproctext]{ref-vanderweele2022}
VanderWeele, TJ (2022) Constructed measures and causal inference:
Towards a new model of measurement for psychosocial constructs.
\emph{Epidemiology}, \textbf{33}(1), 141.
doi:\href{https://doi.org/10.1097/EDE.0000000000001434}{10.1097/EDE.0000000000001434}.

\bibitem[\citeproctext]{ref-vanderweele2017}
VanderWeele, TJ, and Ding, P (2017) Sensitivity analysis in
observational research: Introducing the {E}-value. \emph{Annals of
Internal Medicine}, \textbf{167}(4), 268--274.
doi:\href{https://doi.org/10.7326/M16-2607}{10.7326/M16-2607}.

\bibitem[\citeproctext]{ref-vanderweele2013}
VanderWeele, TJ, and Hernan, MA (2013) Causal inference under multiple
versions of treatment. \emph{Journal of Causal Inference},
\textbf{1}(1), 1--20.

\bibitem[\citeproctext]{ref-vanderweele2020}
VanderWeele, TJ, Mathur, MB, and Chen, Y (2020) Outcome-wide
longitudinal designs for causal inference: A new template for empirical
studies. \emph{Statistical Science}, \textbf{35}(3), 437--466.

\bibitem[\citeproctext]{ref-vanderweele2022a}
VanderWeele, TJ, and Vansteelandt, S (2022) A statistical test to reject
the structural interpretation of a latent factor model. \emph{Journal of
the Royal Statistical Society Series B: Statistical Methodology},
\textbf{84}(5), 2032--2054.

\bibitem[\citeproctext]{ref-vansteelandt2012}
Vansteelandt, S, Bekaert, M, and Lange, T (2012) Imputation strategies
for the estimation of natural direct and indirect effects.
\emph{Epidemiologic Methods}, \textbf{1}(1), 131--158.

\bibitem[\citeproctext]{ref-wager2018}
Wager, S, and Athey, S (2018) Estimation and inference of heterogeneous
treatment effects using random forests. \emph{Journal of the American
Statistical Association}, \textbf{113}(523), 1228--1242.
doi:\href{https://doi.org/10.1080/01621459.2017.1319839}{10.1080/01621459.2017.1319839}.

\bibitem[\citeproctext]{ref-westreich2012berkson}
Westreich, D (2012) Berkson's bias, selection bias, and missing data.
\emph{Epidemiology (Cambridge, Mass.)}, \textbf{23}(1), 159.

\bibitem[\citeproctext]{ref-westreich2010}
Westreich, D, and Cole, SR (2010) Invited commentary: positivity in
practice. \emph{American Journal of Epidemiology}, \textbf{171}(6).
doi:\href{https://doi.org/10.1093/aje/kwp436}{10.1093/aje/kwp436}.

\bibitem[\citeproctext]{ref-westreich2015}
Westreich, D, Edwards, JK, Cole, SR, Platt, RW, Mumford, SL, and
Schisterman, EF (2015) Imputation approaches for potential outcomes in
causal inference. \emph{International Journal of Epidemiology},
\textbf{44}(5), 1731--1737.

\bibitem[\citeproctext]{ref-westreich2013}
Westreich, D, and Greenland, S (2013) The table 2 fallacy: Presenting
and interpreting confounder and modifier coefficients. \emph{American
Journal of Epidemiology}, \textbf{177}(4), 292--298.

\bibitem[\citeproctext]{ref-williams2021}
Williams, NT, and Díaz, I (2021) \emph{{l}mtp: Non-parametric causal
effects of feasible interventions based on modified treatment policies}.
doi:\href{https://doi.org/10.5281/zenodo.3874931}{10.5281/zenodo.3874931}.

\end{CSLReferences}




\end{document}
