% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames,x11names}{xcolor}
%
\documentclass[
  single column]{article}

\usepackage{amsmath,amssymb}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage[]{libertinus}
\ifPDFTeX\else  
    % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\usepackage[top=30mm,left=25mm,heightrounded,headsep=22pt,headheight=11pt,footskip=33pt,ignorehead,ignorefoot]{geometry}
\setlength{\emergencystretch}{3em} % prevent overfull lines
\setcounter{secnumdepth}{-\maxdimen} % remove section numbering
% Make \paragraph and \subparagraph free-standing
\makeatletter
\ifx\paragraph\undefined\else
  \let\oldparagraph\paragraph
  \renewcommand{\paragraph}{
    \@ifstar
      \xxxParagraphStar
      \xxxParagraphNoStar
  }
  \newcommand{\xxxParagraphStar}[1]{\oldparagraph*{#1}\mbox{}}
  \newcommand{\xxxParagraphNoStar}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
  \let\oldsubparagraph\subparagraph
  \renewcommand{\subparagraph}{
    \@ifstar
      \xxxSubParagraphStar
      \xxxSubParagraphNoStar
  }
  \newcommand{\xxxSubParagraphStar}[1]{\oldsubparagraph*{#1}\mbox{}}
  \newcommand{\xxxSubParagraphNoStar}[1]{\oldsubparagraph{#1}\mbox{}}
\fi
\makeatother


\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
% definitions for citeproc citations
\NewDocumentCommand\citeproctext{}{}
\NewDocumentCommand\citeproc{mm}{%
  \begingroup\def\citeproctext{#2}\cite{#1}\endgroup}
\makeatletter
 % allow citations to break across lines
 \let\@cite@ofmt\@firstofone
 % avoid brackets around text for \cite:
 \def\@biblabel#1{}
 \def\@cite#1#2{{#1\if@tempswa , #2\fi}}
\makeatother
\newlength{\cslhangindent}
\setlength{\cslhangindent}{1.5em}
\newlength{\csllabelwidth}
\setlength{\csllabelwidth}{3em}
\newenvironment{CSLReferences}[2] % #1 hanging-indent, #2 entry-spacing
 {\begin{list}{}{%
  \setlength{\itemindent}{0pt}
  \setlength{\leftmargin}{0pt}
  \setlength{\parsep}{0pt}
  % turn on hanging indent if param 1 is 1
  \ifodd #1
   \setlength{\leftmargin}{\cslhangindent}
   \setlength{\itemindent}{-1\cslhangindent}
  \fi
  % set entry spacing
  \setlength{\itemsep}{#2\baselineskip}}}
 {\end{list}}
\usepackage{calc}
\newcommand{\CSLBlock}[1]{\hfill\break\parbox[t]{\linewidth}{\strut\ignorespaces#1\strut}}
\newcommand{\CSLLeftMargin}[1]{\parbox[t]{\csllabelwidth}{\strut#1\strut}}
\newcommand{\CSLRightInline}[1]{\parbox[t]{\linewidth - \csllabelwidth}{\strut#1\strut}}
\newcommand{\CSLIndent}[1]{\hspace{\cslhangindent}#1}

\usepackage{booktabs}
\usepackage{longtable}
\usepackage{array}
\usepackage{multirow}
\usepackage{wrapfig}
\usepackage{float}
\usepackage{colortbl}
\usepackage{pdflscape}
\usepackage{tabu}
\usepackage{threeparttable}
\usepackage{threeparttablex}
\usepackage[normalem]{ulem}
\usepackage{makecell}
\usepackage{xcolor}
\input{/Users/joseph/GIT/latex/latex-for-quarto.tex}
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\AtBeginDocument{%
\ifdefined\contentsname
  \renewcommand*\contentsname{Table of contents}
\else
  \newcommand\contentsname{Table of contents}
\fi
\ifdefined\listfigurename
  \renewcommand*\listfigurename{List of Figures}
\else
  \newcommand\listfigurename{List of Figures}
\fi
\ifdefined\listtablename
  \renewcommand*\listtablename{List of Tables}
\else
  \newcommand\listtablename{List of Tables}
\fi
\ifdefined\figurename
  \renewcommand*\figurename{Figure}
\else
  \newcommand\figurename{Figure}
\fi
\ifdefined\tablename
  \renewcommand*\tablename{Table}
\else
  \newcommand\tablename{Table}
\fi
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Listing}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}
\makeatother
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\@ifpackageloaded{subcaption}{}{\usepackage{subcaption}}
\makeatother
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\usepackage{bookmark}

\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same} % disable monospaced font for URLs
\hypersetup{
  pdftitle={Confounding in Experiments},
  pdfauthor={Joseph A. Bulbulia},
  colorlinks=true,
  linkcolor={blue},
  filecolor={Maroon},
  citecolor={Blue},
  urlcolor={Blue},
  pdfcreator={LaTeX via pandoc}}

\title{Confounding in Experiments}

\usepackage{academicons}
\usepackage{xcolor}

  \author{Joseph A. Bulbulia}
            \affil{%
             \small{     Victoria University of Wellington, New Zealand
          ORCID \textcolor[HTML]{A6CE39}{\aiOrcid} ~0000-0002-5861-2056 }
              }
      


\date{2024-05-26}
\begin{document}
\maketitle
\begin{abstract}
Confounding bias arises when a treatment and outcome share a common
cause. In randomised controlled trials (experiments), treatment
assignment is random. It would appear there can be no confounding bias
in experiments. Here, we use causal directed acyclic graphs to clarify
seven structural sources of bias in randomised controlled trials and
demonstrate the interest of causal inference methods for the design and
analysis of experiments.

\textbf{Keywords:} \emph{Causal Inference}; \emph{Experiments};
\emph{DAGs}; \emph{Evolution}; \emph{Per Protocol Effect};
\emph{Intention to Treat Effect}
\end{abstract}

\subsection{Introduction}\label{introduction}

Here, we consider seven structural sources of bias in randomised
controlled trials, henceforth `experiments.' The examples are not
exhaustive. We do not consider biases from selection into the study,
which may threaten generalisation, nor do we consider censoring biases
from study attrition, where population characteristics of the restricted
sample at the end of the study differ from those at baseline concerning
treatment effects. Both types of sample/target population issues can
invalidate experimental results. However, such biases present as
failures in external validity. Our focus here will be on threats to
internal validity from confounding that relate the treatment to the
outcome in the absence of confounding. We will assume large samples such
that random differences in the distribution of variables that may affect
treatment do not come into play. We will also assume that the
experimental designs are double-blind, that treatment conditions are the
same across all arms, and that the investigators are careful and
scrupulous. No chance event, other than randomisation, is left to
chance.

`Doesn't randomisation, by its very nature, eliminate all systematic
causes of treatment assignment and so of treatment assignment and
outcome?'

We assume the answer is yes.

`Doesn't this mean that confounding is ruled out?'

The answer is no. Eight examples illustrate why. Understanding how
confounding arises in experiments is important for experimental design,
data analysis, and inference. However, the examples clarify problems of
general interest about how causality operates over time, confounding
where the treatment affects attrition, non-compliance, and non-response,
and differences between the effects of randomisation (intention-to-treat
effects) and the causal effects of treatments themselves (per-protocol
effects).

We begin by defining our terms (\hyperref[id-app-a]{Appendix A} provides
a glossary of terms in causal inference.)

\subsubsection{Terminology}\label{terminology}

\begin{itemize}
\item
  \textbf{Confounding}: Treatment and outcome are associated
  independently of causality or are disassociated in the presence of
  causality relative to the causal question at hand.
\item
  \textbf{Intention-to-Treat Effect (equivalent to `intent-to-treat
  effect')}: The effect of treatment assignment, analysed based on
  initial treatment assignment, reflecting real-world effectiveness but
  possibly obscuring mechanisms.
\item
  \textbf{Per-protocol effect}: The effect of adherence to a randomly
  assigned treatment if adherence were perfect
  (\citeproc{ref-hernan2017per}{Hern√°n \emph{et al.} 2017}). We have no
  guarantee that the intention-to-treat effect will be the same as the
  per-protocol effect. A safe assumption is that:
\end{itemize}

\[
\widehat{ATE}_{\text{target}}^{\text{Per-Protocol}} \ne \widehat{ATE}_{\text{target}}^{\text{Intention-to-Treat}}
\]

When evaluating evidence for causality, in addition to specifying their
causal contrast, effect measure, and target population, investigators
should specify whether they are estimating an intention-to-treat or
per-protocol effect (\citeproc{ref-hernuxe1n2004}{Hern√°n 2004};
\citeproc{ref-tripepi2007}{Tripepi \emph{et al.} 2007}).

\subsubsection{Meaning of Symbols}\label{meaning-of-symbols}

We use the following conventions in our directed acyclic graphs:

\begin{itemize}
\item
  \textbf{Node}: A node or vertex represents characteristics or features
  of units within a population on a causal diagram -- that is, a
  `variable.' In causal directed acyclic graphs, we draw nodes with
  respect to the \emph{target population}, which is the population for
  whom investigators seek causal inferences
  (\citeproc{ref-suzuki2020}{Suzuki \emph{et al.} 2020}). A time-indexed
  node \(X_t\) denotes relative chronology; \(X_{\phi t}\) indicates
  assumed timing, possibly erroneous.
\item
  \textbf{Edge without an Arrow} (\(\association\)): Path of
  association, causality not asserted.
\item
  \textbf{Red Edge without an Arrow} (\(\associationred\)): Confounding
  path: ignores arrows to clarify statistical dependencies.
\item
  \textbf{Arrow} (\(\rightarrowNEW\)): Denotes a causal relationship
  from the node at the base of the arrow (a parent) to the node at the
  tip of the arrow (a child). We typically refrain from drawing an arrow
  from treatment to outcome to avoid asserting a causal path from \(A\)
  to \(Y\) because the function of a causal directed acyclic graph is to
  evaluate whether causality can be identified for this path.
\item
  \textbf{Red Arrow} (\(\rightarrowred\)): Path of non-causal
  association between the treatment and outcome. The path is
  associational and may run against arrows.
\item
  \textbf{Dashed Arrow} (\(\rightarrowdotted\)): Denotes a true
  association between the treatment and outcome that becomes partially
  obscured when conditioning on a mediator, assuming \(A\) causes \(Y\).
\item
  \textbf{Dashed Red Arrow} (\(\rightarrowdottedred\)): Highlights
  over-conditioning bias from conditioning on a mediator.
\item
  \textbf{Boxed Variable} \(\boxed{X}\): Conditioning or adjustment for
  \(X\).
\item
  \textbf{Red-Boxed Variable} \(\boxedred{X}\): Highlights the source of
  confounding bias from adjustment.
\item
  \textbf{Dashed Circle} \(\circledotted{X}\): Indicates no adjustment
  is made for a variable (implied for unmeasured confounders).
\item
  \textbf{\(\mathcal{R} \rightarrow A\)}: Randomisation into the
  treatment condition.
\end{itemize}

\subsubsection{Review of d-separation for Causal Identification on a
Graph}\label{review-of-d-separation-for-causal-identification-on-a-graph}

\begin{table}

\caption{\label{tbl-fiveelementary}The five elementary structures of
causality from which all causal directed acyclic graphs can be built.}

\centering{

\terminologydirectedgraph

}

\end{table}%

In the 1990s, Judea Pearl demonstrated that causal dependencies could be
evaluated using observable probability distributions
(\citeproc{ref-pearl1995}{Pearl 1995}, \citeproc{ref-pearl2009a}{2009}).
He also showed that causal directed acyclic graphs (causal DAGs) could
be employed to clarify the conditional dependencies among variables
(\citeproc{ref-pearl1995}{Pearl 1995}). This means that, based on
assumptions about causal structure, investigators could investigate
strategies for identifying causal effects from the joint distributions
of observed data.

The graphical rules that Pearl developed and proved are known as the
rules of d-separation (\citeproc{ref-pearl1995}{Pearl 1995}).

\begin{enumerate}[a)]
     \item  {\bf Fork rule} ($B \leftarrowNEW \boxed{A} \rightarrowNEW C$): $B$ and $C$ are independent when conditioning on $A$: ($B \coprod C \mid A$).
     \item  {\bf Chain rule} ($A \rightarrowNEW \boxed{B} \rightarrowNEW C$): Conditioning on $B$ blocks the path between $A$ and $C$: ($A \coprod C \mid B$).
     \item  {\bf Collider rule} ($A \rightarrowNEW \boxed{C} \leftarrowNEW B$): $A$ and $B$ are independent until conditioning on $C$, which introduces dependence: ($A \cancel{\coprod} B \mid C$). 
 \end{enumerate}

The rules of d-separation give rise to the backdoor criterion and
`backdoor adjustment' theorem, which provide identification algorithms
conditional on the structural assumptions encoded in a causal directed
acyclic graph (\citeproc{ref-pearl1995}{Pearl 1995}). Here, we use the
symbol \(\mathcal{G}\) to name a graph, which we will identify by
referring to a row in a table.

\begin{table}

\caption{\label{tbl-terminologygeneral}Elements of Causal Graphs}

\centering{

\terminologydirectedgraph

}

\end{table}%

Consider Table~\ref{tbl-terminologygeneral} \(\mathcal{G}_1\): If we
assume that \(A\) and \(B\) are not causally related, and further that
they do not share common causes, then \(A\) and \(B\) will not be
statistically related.

Consider Table~\ref{tbl-terminologygeneral} \(\mathcal{G}_2\): If we
assume that \(A\) and \(B\) are causally related, that they do not share
common causes or that their common causes have been accounted for, then
\(A\) and \(B\) will be statistically related.

Consider Table~\ref{tbl-terminologygeneral} \(\mathcal{G}_3\): If we
assume that \(A\) causes \(B\) and that \(A\) causes \(C\), then the
rules of d-separation imply that we may condition on or `control for'
\(A\) to consistently estimate the effect of \(B\) on \(C\).

Consider Table~\ref{tbl-terminologygeneral} \(\mathcal{G}_4\): If we
assume that \(A\) causes \(B\) and that \(B\) causes \(C\), then the
rules of d-separation imply that if we condition on \(B\), the true
causal effect of \(A\) on \(C\) will be obscured such that \(A\) will be
independent of \(C\) despite being causally associated with \(C\).

Finally, consider Table~\ref{tbl-terminologygeneral} \(\mathcal{G}_5\):
If we assume that \(A\) causes \(C\) and that \(B\) causes \(C\), then
the rules of d-separation imply that if we condition on \(C\), the
variables \(A\) and \(B\) will be associated, despite having no causal
effect on each other.

If we assume that the variables encoded in the graph correspond to
`Structural Causal Models' then all causal relationships can be defined
by the elementary structures presented in
Table~\ref{tbl-terminologygeneral}.

Now that we have clarified how causal directed graphs work, we may use
them to clarify the first concept we consider confounding bias in
randomised experiments.

\subsection{Eight Examples of Confounding Bias in
Experiments}\label{eight-examples-of-confounding-bias-in-experiments}

\begin{table}

\caption{\label{tbl-terminologyelconfoundersexperiments}Eight
confounding biases in Randomised Controlled Trials.}

\centering{

\terminologyelconfoundersexperiments

}

\end{table}%

\subsubsection{Example 1: Post-treatment Adjustment Blocks Treatment
Effect}\label{example-1-post-treatment-adjustment-blocks-treatment-effect}

Table~\ref{tbl-terminologyelconfoundersexperiments}
\(\mathcal{G}_{1.1}\) illustrates the threat of confounding bias by
conditioning on a post-treatment mediator. Imagine investigators are
interested in whether the framing of an authority as religious or
secular -- `source framing' -- affects subjective ratings of confidence
in the authority -- `source credibility.' There are two conditions. A
claim is presented from an authority. The content of the claim does not
vary by condition. Participants are asked to rate the claim on a
credibility scale. Next, imagine that the investigators decide they
should control for religiosity. Furthermore, imagine there is a true
effect of source framing. Finally, assume that the source framing not
only affects source credibility but also affects religiosity. Perhaps
viewing a religious authority makes religious people more religious. In
this scenario, measuring religiosity following the experimental
intervention will partially block the effect of the treatment on the
outcome. It might make it appear that the treatment does not work for
religious people, when in reality it works because it amplifies
religiosity. (Note that in this graph we assume that \(L_1\) occurs
before \(Y_2\), however investigators may have measured \(L_1\) after
\(Y_2\). Our time index pertains to the occurrence of the event, not of
its measurement. This statement applies to all examples that follow.)

Table~\ref{tbl-terminologyelconfoundersexperiments}
\(\mathcal{G}_{1.2}\) clarifies a response: do not control
post-treatment variables, here the intermediary effects of
`religiosity'. If effect-modification by religiosity is scientifically
interesting, measure religiosity before randomisation. Randomisation did
not prevent confounding.

\subsubsection{Example 2: Post-treatment Adjustment Induces Collider
Stratification
Bias}\label{example-2-post-treatment-adjustment-induces-collider-stratification-bias}

Table~\ref{tbl-terminologyelconfoundersexperiments}
\(\mathcal{G}_{2.1}\) illustrates the threat of confounding bias by
conditioning on a post-treatment collider. Imagine the same experiment
as in Example 1 and the same conditioning strategy, where religiosity is
measured following the treatment. We assume the treatment affects
religiosity. However, in this example, religiosity has no causal effect
on the outcome, source credibility. Finally, imagine an unmeasured
variable affects both the mediator, religiosity (\(L_1\)), and the
outcome, source credibility (\(Y_2\)). This unmeasured confounder might
be religious education in childhood. In this scenario, conditioning on
the post-treatment variable religiosity will open a backdoor path
between the treatment and outcome, leading to an association in the
absence of causation. Randomisation did not prevent confounding.

Table~\ref{tbl-terminologyelconfoundersexperiments}
\(\mathcal{G}_{2.2}\) clarifies a response: do not control
post-treatment variables.

The point that investigators should not condition on post-treatment
variables is worth developing using a common flaw in experimental
designs: exclusion from `attention checks.' Consider that if an
experimental condition affects attention and an unmeasured variable is a
common cause of attention and the outcome, then selection on attention
will induce confounding bias in a randomised experiment. For example,
imagine that people are more attentive in the scientific authority
design because science is interesting -- whether or not one is
religious, yet religion is not interesting whether or not one is
religious. Suppose further that an unmeasured `altruistic disposition'
affects both attention and ratings of source credibility. By selecting
on attention, investigators may unwittingly destroy randomisation. If
attention is a scientifically interesting effect modifier, it should be
measured before random assignment to treatment.

\subsubsection{Example 3: Demographic Measures at End of Study Induce
Collider Stratification
Bias}\label{example-3-demographic-measures-at-end-of-study-induce-collider-stratification-bias}

Table~\ref{tbl-terminologyelconfoundersexperiments}
\(\mathcal{G}_{3.1}\) illustrates the threat of confounding bias from
adjusting for post-treatment variables, here, one affected by the
treatment and outcome absent any unmeasured confounder. In our example,
imagine both the treatment, source framing, and the outcome, source
credibility, affect religiosity measured at the end of the study.
Investigators measure religiosity at the end of the study and include
this measure as a covariate. However, doing so induces collider bias
such that if both the treatment and outcome are positively associated
with religiosity, the collider, they will be negatively associated with
each other. Conditioning on the collider risks the illusion of a
negative experimental effect in the absence of causality.

Table~\ref{tbl-terminologyelconfoundersexperiments}
\(\mathcal{G}_{3.2}\) clarifies a response: again, do not control
post-treatment variables! Here, `religiosity' measured after the end of
the study. If the scientific interest is in effect modification or
obtaining statistical precision, measure covariates before
randomisation.

\subsubsection{Example 4: Demographic Measures at End of Study Condition
on a Collider That Opens a Backdoor
Path}\label{example-4-demographic-measures-at-end-of-study-condition-on-a-collider-that-opens-a-backdoor-path}

Table~\ref{tbl-terminologyelconfoundersexperiments}
\(\mathcal{G}_{4.1}\) illustrates the threat of confounding bias by
adjusting for post-treatment variables, here affected only by the
treatment and an unmeasured cause of the outcome. Suppose source
credibility affects religiosity (religious people are reminded of their
faith), but there is no experimental effect of framing on credibility.
Imagine further that there is an unmeasured common cause of the
covariate religiosity and the outcome source credibility. This
unmeasured confounder might be religious education in childhood. In this
scenario, conditioning on the post-treatment variable religiosity will
open a backdoor path between the treatment and outcome, leading to an
association in the absence of causation. Again, we find that
randomisation did not prevent confounding.

Table~\ref{tbl-terminologyelconfoundersexperiments}
\(\mathcal{G}_{4.2}\) clarifies a response. Again, unless investigators
can rule out an effect of treatment, they should not condition on a
post-treatment covariate. The covariates of interest should be measured
before randomisation.

\subsubsection{Example 5: Treatment Affects Attrition Biasing Measure of
Outcome}\label{example-5-treatment-affects-attrition-biasing-measure-of-outcome}

Table~\ref{tbl-terminologyelconfoundersexperiments} \(\mathcal{G}_{5}\)
Suppose that the experimental condition affects measurement error in
self-reported source credibility \(U_{\Delta Y}\). For example, suppose
that source framing has no effect on credibility. However, those in the
scientific authority condition are more likely to express credibility
for science due to self-presentation bias. Likewise, perceiving the
investigators to be irreligious, participants in the religious source
framing condition might report less credibility for religious
authorities than they secretly harbour. Directed measurement error from
the treatment to the measurement error of the outcomes creates an
association in the absence of true causality, which we denote by
removing any arrow between the treatment \(A\) and the true outcome
\(Y\).

Table~\ref{tbl-terminologyelconfoundersexperiments} \(\mathcal{G}_{5}\)
reveals there is no easy solution to directed measurement error bias. If
the magnitude of bias were known, investigators could apply adjustments.
Additional experiments might be devised that are insensitive to directed
measurement error bias. Investigators might compute sensitivity analyses
to examine how much measurement error bias would be required to explain
away a result (refer to Linden \emph{et al.}
(\citeproc{ref-linden2020EVALUE}{2020}) for relatively easy-to-implement
sensitivity analysis). The point we make here is that randomisation does
not prevent confounding by directed measurement error bias.
Investigators must be vigilant.

\subsubsection{Example 6: Per Protocol Effect Lost in Sustained
Treatments Where Treatment Adherence Is Affected by a Measured
Confounder}\label{example-6-per-protocol-effect-lost-in-sustained-treatments-where-treatment-adherence-is-affected-by-a-measured-confounder}

Setting aside self-inflicted injuries of post-treatment conditioning and
directed measurement error, randomisation recovers unbiased causal
effect estimates for randomisation into treatment. Under perfect
adherence, these effect estimates correspond to the causal effects of
the treatments themselves. However, adherence is seldom perfect. The
following examples reveal challenges for recovering per-protocol effects
in settings where there is imperfect adherence.
Table~\ref{tbl-terminologyelconfoundersexperiments}
\(\mathcal{G}_{6-8}\) are adapted from Hern√°n \emph{et al.}
(\citeproc{ref-hernan2017per}{2017}).

Table~\ref{tbl-terminologyelconfoundersexperiments} \(\mathcal{G}_{6}\)
illustrates the threat for identifying the per-protocol effect in
sustained treatments where treatment adherence is affected by a measured
confounder. Consider a sequential experiment that investigates the
effects of sustained adherence to yoga on psychological distress,
measured at the end of the study. Suppose that inflexible people are
less likely to adhere to the protocols set out in the experiment and
therefore do not. Suppose that flexibility is measured by indicator
\(L\). If we do not condition on \(L\), there is an open path from
\(A_1 \associationred L_0 \associationred U \associationred Y_2\).
Although investigators may recover the effect of randomisation into
treatment, the per-protocol effect is confounded.

Table~\ref{tbl-terminologyelconfoundersexperiments} \(\mathcal{G}_{6}\)
also clarifies a response. Conditioning on \(L_0\) and \(L_1\) will
block the backdoor path, leading to an unbiased per-protocol effect
estimate.

\subsubsection{Example 7: Per protocol effect lost in sustained
treatments where past treatments affect measured confounder of future
treatment
adherence}\label{example-7-per-protocol-effect-lost-in-sustained-treatments-where-past-treatments-affect-measured-confounder-of-future-treatment-adherence}

Table~\ref{tbl-terminologyelconfoundersexperiments} \(\mathcal{G}_{7}\)
illustrates the threat for identifying the per-protocol effect in
sustained treatments where past treatments affect measured confounder of
future treatment adherence. Suppose that yoga affects flexibility. We
should condition on pre-treatment measures of flexibility to identify
the per-protocal effect. However, conditioning on the post-treatment
measure of flexibilty, \(\boxed{L_1}\) induces collider stratification
bias. This path runs from
\(A_1 \associationred L_1 \associationred U \associationred Y_3\).
However, if we do not condition on \(L_1\) there is an open backdoor
path from \(A_1 \associationred U \associationred Y_3\). We cannot
estimate a per-protocal effect by conditioning strategies.

Table~\ref{tbl-terminologyelconfoundersexperiments} \(\mathcal{G}_{7}\)
does not clarify the response. However, in a sequential treatment with
fixed strategies, in which there is sequential exchangeability -- or no
unmeasured confounding at each time point -- valid estimators for the
sequential treatments may be constructed (refer to Hernan and Robins
(\citeproc{ref-hernan2024WHATIF}{2024}); Dƒ±ÃÅaz \emph{et al.}
(\citeproc{ref-diaz2021nonparametric}{2021}); Hoffman \emph{et al.}
(\citeproc{ref-hoffman2023}{2023})). Although we may niavely obtain an
intention-to-treat effect estimate withouth special methods, infering an
effect of doing yoga on well-being -- the per-protocal effect, requires
special methods. These methods are not routinely used in the human
sciences.

\subsubsection{Example 8: Per Protocol Effect Lost in Sustained
Treatments Because Both Measured and Unmeasured Confounders Affect
Treatment
Adherence}\label{example-8-per-protocol-effect-lost-in-sustained-treatments-because-both-measured-and-unmeasured-confounders-affect-treatment-adherence}

Table~\ref{tbl-terminologyelconfoundersexperiments} \(\mathcal{G}_{8}\)
illustrates the threat for identifying the per-protocol effect in
sustained treatments where there are both measured and unmeasured
confounders. Suppose flexibility affects adherence, yoga affects
flexibility, and an unmeasured variable, such as prejudice toward
eastern spiritual practices, affects adherence. We have no measures for
this variable. There is unmeasured confounding.

If there were no effect of yoga on well-being except through
flexibility, and if flexibility were not affected by the unmeasured
antipathy toward eastern spiritual practices, and further, if the effect
of flexibility on yoga at each time point were conditionally independent
of all future counterfactual data, both for the treatments and the
outcomes, then it might be possible to construct special estimators that
identify the per-protocol effect of yoga on well-being in the presence
of unmeasured confounding that affects adherence (refer to Hern√°n
\emph{et al.} (\citeproc{ref-hernan2017per}{2017})). Clearly, we have
come a long way from the ANOVAs routinely deployed in experimental
studies. However, if we seek to understand the effect of yoga on
well-being and not the effect of random assignment to yoga on
well-being, we require special estimators.

\subsection{Conclusions}\label{conclusions}

The examples we have considered here hardly exhaust threats to causal
inference in experiments. Whenever the sample at the end of a study
differs in the distribution of effect modifiers from the sample
population at the start, results will not generalise as we hope. Such
bias goes by different names, such as selection bias or
target-restriction bias (refer to
(\citeproc{ref-yola}{\textbf{yola?}})). We have not considered these
threats here. However, I hope the eight examples presented persuade
investigators of the following:

First, confounding biases are possible in randomised experiments even
when randomisation succeeds.

Second, causal directed acyclic graphs are useful for clarifying these
biases.

Third, many such biases are self-inflicted, in the sense that it is easy
to destroy the benefits of randomisation through poor research designs.
These self-inflicted biases arise from conditioning on variables that
may be affected by treatment assignment. By the same token, the remedy
to self-inflicted injury is easily applied. Do not injury yourself. If
an experiment consists of a single treatment, unless you are certain
that treatments do not affect covariates, covariate data should be
collected before randomisation.

Fourth, `attention checks' should not be used to select participants
after treatments have been randomised. If attention is a relevant
covariate, measures should be taken before randomisation.

Fifth, investigators should not adopt naive practices of inferring
per-protocol effects from the portion of the sample that has followed
experimental protocols. Not only is such selection nearly guaranteed to
result in differences between the study population at the start and end
of the study, compromising external validity
(\citeproc{ref-yola}{\textbf{yola?}}), but we have also considered that
both measured and unmeasured confounders may invalidate per-protocol
results for the retained sample.

Sixth, methods for identifying causal effects in observational settings
may be useful for identifying causal effects in randomised experiments
because, after randomisation, every experiment becomes an observational
study.

Seventh, the points that we consider here for experiments apply to
observational studies that have obtained pseudorandomisation through
baseline adjustments. Standardly employed methods in observational
science, such as structural equation models or multi-level models, will
encounter the same problems that arise in experimental settings with
sustained treatment strategies. Sustained treatment strategies require
sequential randomisation (refer to Richardson and Robins
(\citeproc{ref-richardson2013}{2013}); Young \emph{et al.}
(\citeproc{ref-young2014identification}{2014});
(\citeproc{ref-yola}{\textbf{yola?}})).

Here, we have demonstrated that satisfying the assumptions for valid
causal inferences is typically much more challenging than many
experimental human scientists presently understand. Methods for causal
inference have wide scope for improving the design, analysis, and
interpretation of experimental research.

\newpage{}

\subsection{Funding}\label{funding}

This work is supported by a grant from the Templeton Religion Trust
(TRT0418) and RSNZ Marsden 3721245, 20-UOA-123; RSNZ 19-UOO-090. I also
received support from the Max Planck Institute for the Science of Human
History. The Funders had no role in preparing the manuscript or the
decision to publish it.

\newpage{}

\subsection{References}\label{references}

\phantomsection\label{refs}
\begin{CSLReferences}{1}{0}
\bibitem[\citeproctext]{ref-diaz2021nonparametric}
Dƒ±ÃÅaz, I, Hejazi, NS, Rudolph, KE, and Der Laan, MJ van (2021)
Nonparametric efficient causal mediation with intermediate confounders.
\emph{Biometrika}, \textbf{108}(3), 627--641.

\bibitem[\citeproctext]{ref-hernan2024WHATIF}
Hernan, MA, and Robins, JM (2024) \emph{Causal inference: What if?},
Taylor \& Francis. Retrieved from
\url{https://www.hsph.harvard.edu/miguel-hernan/causal-inference-book/}

\bibitem[\citeproctext]{ref-hernuxe1n2004}
Hern√°n, MA (2004) A definition of causal effect for epidemiological
research. \emph{Journal of Epidemiology \& Community Health},
\textbf{58}(4), 265--271.
doi:\href{https://doi.org/10.1136/jech.2002.006361}{10.1136/jech.2002.006361}.

\bibitem[\citeproctext]{ref-hernan2017per}
Hern√°n, MA, Robins, JM, et al. (2017) Per-protocol analyses of pragmatic
trials. \emph{N Engl J Med}, \textbf{377}(14), 1391--1398.

\bibitem[\citeproctext]{ref-hoffman2023}
Hoffman, KL, Salazar-Barreto, D, Rudolph, KE, and D√≠az, I (2023)
Introducing longitudinal modified treatment policies: A unified
framework for studying complex exposures.
doi:\href{https://doi.org/10.48550/arXiv.2304.09460}{10.48550/arXiv.2304.09460}.

\bibitem[\citeproctext]{ref-linden2020EVALUE}
Linden, A, Mathur, MB, and VanderWeele, TJ (2020) Conducting sensitivity
analysis for unmeasured confounding in observational studies using
e-values: The evalue package. \emph{The Stata Journal}, \textbf{20}(1),
162--175.

\bibitem[\citeproctext]{ref-pearl1995}
Pearl, J (1995) Causal diagrams for empirical research.
\emph{Biometrika}, \textbf{82}(4), 669--688.

\bibitem[\citeproctext]{ref-pearl2009a}
Pearl, J (2009) \emph{Causality}, Cambridge University Press.

\bibitem[\citeproctext]{ref-richardson2013}
Richardson, TS, and Robins, JM (2013) Single world intervention graphs:
A primer. In, Citeseer. Retrieved from
\url{https://core.ac.uk/display/102673558}

\bibitem[\citeproctext]{ref-suzuki2020}
Suzuki, E, Shinozaki, T, and Yamamoto, E (2020) Causal Diagrams:
Pitfalls and Tips. \emph{Journal of Epidemiology}, \textbf{30}(4),
153--162.
doi:\href{https://doi.org/10.2188/jea.JE20190192}{10.2188/jea.JE20190192}.

\bibitem[\citeproctext]{ref-tripepi2007}
Tripepi, G, Jager, KJ, Dekker, FW, Wanner, C, and Zoccali, C (2007)
Measures of effect: Relative risks, odds ratios, risk difference, and
{`}number needed to treat{'}. \emph{Kidney International},
\textbf{72}(7), 789--791.
doi:\href{https://doi.org/10.1038/sj.ki.5002432}{10.1038/sj.ki.5002432}.

\bibitem[\citeproctext]{ref-young2014identification}
Young, JG, Hern√°n, MA, and Robins, JM (2014) Identification, estimation
and approximation of risk under interventions that depend on the natural
value of treatment using observational data. \emph{Epidemiologic
Methods}, \textbf{3}(1), 1--19.

\end{CSLReferences}

\newpage{}

\subsection{Appendix A: Glossary}\label{id-app-a}

\begin{table}

\caption{\label{tbl-experiments}Glossary}

\centering{

\glossaryTerms

}

\end{table}%



\end{document}
