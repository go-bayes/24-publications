---
title: "Causal Inference in Environmental Psychology"
abstract: |
 This chapter offers a practical guide for environmental psychology researchers to investigate causal questions. We begin by introducing core concepts of causal inference and the assumptions needed to extract causal insights from observational data. We then explain how all causal diagrams can be built from five elementary relationships and use the question of whether greenspace access affects happiness to demonstrate use cases. When the confounding structure is complex, we recommend creating multiple causal diagrams to illustrate the uncertainty and report results under different scenarios. We offer guidelines for how to do this.
authors: 
  - name: Joseph A. Bulbulia
    orcid: 0000-0002-5861-2056
    email: joseph.bulbulia@vuw.ac.nz
    affiliation: 
      name: Victoria University of Wellington, New Zealand, School of Psychology, Centre for Applied Cross-Cultural Research
      department: Psychology/Centre for Applied Cross-Cultural Research
      city: Wellington
      country: New Zealand
      url: www.wgtn.ac.nz/cacr
  - name: Donald W Hine
    orcid: 0000-0002-3905-7026
    email: donald.hine@canterbury.ac.nz
    affiliation: 
      name: University of Canterbury, School of Psychology, Speech and Hearing
      city: Wellington
      country: New Zealand
      url: www.wgtn.ac.nz/cacr
keywords:
  - DAGS
  - Causal Inference
  - Confounding
  - Environmental
  - Psychology
  - Panel
format:
  pdf:
    sanitize: true
    keep-tex: true
    link-citations: true
    colorlinks: true
    documentclass: article
    classoption: [singlecolumn]
    lof: false
    lot: false
    number-sections: false
    number-depth: 4
    highlight-style: github
    geometry:
      - top=30mm
      - left=20mm
      - heightrounded
    header-includes:
      - \input{/Users/joseph/GIT/templates/latex/custom-commands.tex}
date: last-modified
execute:
  echo: false
  warning: false
  include: true
  eval: true
fontfamily: libertinus
bibliography: /Users/joseph/GIT/templates/bib/references.bib
csl: ./camb-a.csl
---

```{r}
#| label: load-libraries
#| echo: false
#| include: false
#| eval: false
#  fig-pos: 'htb'
#   html:
#    html-math-method: katex

# Include in YAML for Latex
# sanitize: true
# keep-tex: true
# include-in-header:
#       - text: |
#           \usepackage{cancel}


# for making graphs
library("tinytex")
library(extrafont)
loadfonts(device = "all")


# libraries for jb (when internet is not accessible)
# read libraries
source("/Users/joseph/GIT/templates/functions/libs2.R")

# read functions
source("/Users/joseph/GIT/templates/functions/funs.R")

# read data/ set to path in your computer
pull_path <-
  fs::path_expand(
    "/Users/joseph/v-project\ Dropbox/data/current/nzavs_13_arrow"
  )

# for saving models. # set path fo your computer
push_mods <-
  fs::path_expand(
    "/Users/joseph/v-project\ Dropbox/data/nzvs_mods/00drafts/23-causal-dags"
  )


# keywords: potential outcomes, DAGs, causal inference, evolution, religion, measurement, tutorial
# 10.31234/osf.io/b23k7

```

## Introduction

Causal inference seeks to answer a fundamental question: by how much does an intervention in the world (the "treatment") directly change another variable (the "outcome") -- if at all? It goes beyond identifying associations to quantifying the magnitude of causal effects. Although the ability to understand cause and effect exists across the animal kingdom [@mancuso2018revolutionary], methods for quantitatively estimating the size of these effects are relatively recent.

Randomised controlled experiments are often considered the "gold standard" for establishing causation; however, they can be expensive, impractical, and ethically complex, and bias may still affect results [@hernan2017per; @montgomery2018]. Observational data are abundant and offer a valuable alternative for accelerating knowledge – but only if we can draw valid causal inferences from them.

Unfortunately, many researchers still attempt to analyse observational data using workflows that do not address the confounding problem and can worsen bias [@westreich2013; @robins1986; @hernan2023]. This habit has led to a "causality crisis" in social sciences, where we acknowledge that associations are insufficient yet speculate about causality [@bulbulia2023a].

Recent advances in causal inference from biostatistics and computer science offer a solution [@vanderweele2015]. This approach has immense potential for environmental psychology, where randomised controlled experiments are often impractical or impossible. This chapter introduces causal inference methods to help environmental psychologists to extract robust causal insights from their observational data.

[**Part 1: An Overview of the Potential Outcomes Framework for Causal Inference**](#sec-part1) introduces the potential outcomes framework—the cornerstone of causal inference [@hernan2023]. Within the familiar context of randomised controlled experiments, we examine [**three fundamental assumptions**](#sec-three-fundamental-assumptions) that must be met to obtain causal inferences from data. Building intuition using the example of a randomised controlled experiment offers two benefits. First, it clarifies that even "gold standard" experiments rely on assumptions for their causal interpretations. Second, experiments clarify *why* these assumptions are necessary: they are essential because causal inference requires computing contrasts for partially counterfactual outcomes from data [@westreich2012berkson; @hernan2017per; @westreich2015; @robins2008estimation].

**Part 2: Causal Diagrams - Visually Understanding Confounding.** explains the fundamental structures of causal diagrams, aslo called "Directed Acyclic Graphs (DAGs)"  or "causal graphs". These graphs represent causal relationships and assumptions [@pearl2009a]. Although a complete examination of their capabilities goes beyond this chapter, we equip you with essential strategies for their construction and interpretation. We discover that all causal diagrams are built from [**the five elementary graphical structures of causality**](#sec-five-elementary) from which all complex causal relations are built. Here, we learn [**four elementary rules for evaluating confounding**](#sec-four-rules), which enable researchers evaluate whether valid causal inferences can be obtained from data. With this understanding, you can apply causal diagrams to your environmental psychology questions.

**Part 3: Using Causal Diagrams for Causal Identification - Worked Examples** reviews **seven everyday use cases** that illustrate [**four elementary rules for evaluating confounding**](#sec-four-rules)in action. We show how causal diagrams illuminate strategies researchers can employ to obtain consistent causal inferences with their data or in some cases, to clarify why valid inferences cannot be obtained. Some results align with intuition, but we show that causal diagrams can reveal options we may not initially consider. Throughout, we see that although rigorous mathematical proofs support causal diagrams, they are surprisingly accessible: they require no mathematical knowledge. As such, causal diagrams can empower many researchers beyond the computer scientists and statisticians who developed these tools.

**Part 4: Practical Guide For Constructing Causal Diagrams and Reporting Results When Causal Structure is Unclear** offers guidance for settings where investigators face uncertainties about the actual causal structure of the world or how accurately their data captures it.  Here, we explain the value of drawing multiple causal diagrams and simulating data to illustrate why this approach is powerful.  We also provide recommendations for reporting in two settings:

  (i)  Only cross-sectional data are available.
  (ii) Repeated measures longitudinal data are available.


## Part 1: An Overview of the Potential Outcomes Framework for Causal Inference {#section-part1}

The potential outcomes framework for causal inference originated in the work of Jerzy Neyman to evaluate the effectiveness of agricultural experiments [@neyman1923]. It was later extended by Harvard statistician Donald Rubin, who demonstrated the framework may also facilitate causal inferences in non-experimental settings [@rubin1976]. Jamie Robins further generalised this framework to assess confounding in complex scenarios involving multiple and time-varying treatments [@robins1986]. 

A core concept within this framework is a "counterfactual contrast" or "estimand." To quantitatively assess the magnitude of causality requires contrasting how the world would have turned out under two or more states, corresponding to different levels of intervention or treatment.  Notably, before any intervention, these states remain counterfactual. After any intervention, for every therapy applied, at most, only one of the two states of the world to be contrasted is realised. The other state remains counterfactual. Philosophers who have puzzled over the nature of causation have long realised that causality is **never directly observed** [@hume1902]. In a sense, we may think of causal inference as a form of counterfactual data science [@edwards2015; @bulbulia2023a] 

#### The Fundamental Problem of Causal Inference: Causal Contrasts are Not Directly Observed

To grasp the implications of counterfactual contrasts in causal inference, imagine yourself at a pivotal point in your life. Having just completed your undergraduate studies, you have been accepted into your dream Environmental Psychology program at the University of Canterbury. You are set to relocate to Christchurch, New Zealand, with Professor D.W. Hine. Suddenly, you receive a fantastic job offer from Acme Nuclear Fuels, a leader in renewable energy.  Both paths diverge considerably —  lifestyle, income, social networks, relationships, and perhaps even life purpose hang in the balance. Which choice aligns with your ideal future?

Formally, let $D$ denote the decision, where $D = 1$ means attending graduate school and $D = 0$ means joining the workforce.  Your two potential outcomes under each path are described as $Y_{\text{you}}(1)$ and $Y_{\text{you}}(0)$.  Importantly, we assume a definite outcome exists for each choice you *could* have made.  Conceptually, we need to measure the difference $Y_{\text{you}}(1) - Y_{\text{you}}(0)$ to quantify the *magnitude* of the effect of your choice. Yet, this difference remains fundamentally unobservable. Your life goes down one path, forever obscuring the alternative. In the following, the notation $A|B$ means "A conditional on B." 

$$
(Y_{\text{you}}|D_{\text{you}} = 1) = Y_{\text{you}}(1) \quad \text{implies} \quad Y_{\text{you}}(0)|D_{\text{you}} = 1~ \text{is counterfactual}.
$$


In plain English, this expression means: "The outcome that we observe under option $D = 1$ can be measured. However, because option $D = 0$ is not realised, the outcome under option $D=0$ cannot be measured. Thus, the contrast between these two outcomes cannot be computed. At least one outcome remains purely counterfactual.

The same probem arises if you select $D = 0$. Then, the outcome under $D=1$ remains counterfactual. And so we cannot compute the contrast:

$$
(Y_{\text{you}}|D_{\text{you}} = 0) = Y_{\text{you}}(0) \quad \text{implies} \quad Y_{\text{you}}(1)|D_{\text{you}} = 0~ \text{is counterfactual}.
$$

This example reflects the "fork-in-the-road" decisions we regularly encounter in life. "The fundamental problem of causal inference" [@rubin2005; @holland1986] highlights that directly comparing one individual's outcome under both potential paths is impossible. 

This problem persists for experiments. However, do not despair! Under certain assumptions, the data may illuminate *average treatment effects*. We next consider how experiments obtain average treatment effects despite individual-level causal effects being never directly observed.

### Causal inference in Experiments is a Missing Data Problem

Let us transition from the topic of life decisions to an example of relevance to environmental psychology, namely, estimating the average causal effect of easy access to urban green spaces on subjective happiness, hereafter referred to as "happiness." We assume this outcome is measurable and represent it with $Y$. 

We simply classify the "ample access to green space" intervention as a binary variable. Define $A = 1$ as "having ample access to green space" and $A = 0$ as "lacking ample access to green space." We assume these conditions are mutually exclusive. This simplification does not limit the generality of our conclusions; the points we make about experiments also apply to continuous treatments.

Next, it is crucial in causal inference to specify the population for whom we seek to evaluate causal effects, or the "target population." Suppose our target population is residents of New Zealand in the 2020s.

A preliminary causal question -- defined as a causal contrast or "estimand" might, therefore be:

"In contemporary New Zealand, does proximity to abundant green spaces increase self-perceived happiness compared to environments lacking such spaces?"

It would be unethical to experimentally randomise individuals into different green space access conditions, but let's set this ethical consideration aside. Assume we could assign people randomly to high and low green space access without objection or harm.

As alluded to earlier, the first point to note in the context of causal inference is that even well-designed experiments confront the challenge of missing values in the potential outcomes. Once an individual is assigned to one treatment condition, we cannot observe that individual's outcome for the condition not assigned. The fundamental problem of causal inference remains constant: for each individual, we can only observe one of the potential outcomes at any given time. Breaking down the Average Treatment Effect (ATE) into observed and unobserved outcomes yields the following equation:

$$
\text{Average Treatment Effect} = \left(\underbrace{\underbrace{\mathbb{E}[Y(1)|A = 1]}_{\text{observed}} + \underbrace{\mathbb{E}[Y(1)|A = 0]}_{\text{unobserved}}}_{\text{treated}}\right) - \left(\underbrace{\underbrace{\mathbb{E}[Y(0)|A = 0]}_{\text{observed}} + \underbrace{\mathbb{E}[Y(0)|A = 1]}_{\text{unobserved}}}_{\text{untreated}}\right).
$$

In this expression, $\mathbb{E}[Y(1)|A = 1]$ represents the average outcome when the treatment is given, which is observable. However, $\mathbb{E}[Y(1)|A = 0]$ represents the average outcome if the treatment had been given to those who were untreated, which remains unobservable. Similarly, the quantity $\mathbb{E}[Y(0)|A = 1]$ also remains unobservable.

It is hopefully evident from this brief application of the potential outcomes framework to experiments that the fundamental problem of causal inference is an ever-present concern even in experiments. For each participant, it is impossible to determine the outcome they would have experienced under an alternative treatment condition. You cannot quantitatively describe the life you would have led had you chosen the job at Acme Nuclear Fuels instead of attending the University of Canterbury.


### In Experiments, Random Treatment Assignment Balances Confounders Across Treatments

Experiments estimate average treatment effects by addressing the issue of confounding. A common scenario is "confounding by common cause," where a variable influences both the treatment and the outcome, leading to a non-causal association between them. Thus, if we were to change the treatment without altering the confounder, the outcome would remain unchanged. The common cause can create a spurious relationship that might be incorrectly interpreted as causal. For instance, if studying the effect of access to green space on happiness, income could explain the entire association. Hence, moving lower-income individuals to areas with more green space might not affect their happiness. Identifying and adjusting for such confounders is essential to reveal the genuine causal relationship, ensuring the association observed is not merely due to external factors. Though confounders are still present, this balance enables us to attribute differences in outcomes between groups directly to the treatment.

We can express this principle of no confounding mathematically in two complementary ways (where $A \coprod B$ signifies that $A$ is independent of $B$, and vice versa):

1. **Potential Outcomes Independent of Treatment (given L):** $Y(a) \coprod A \mid L$
2. **Treatment Assignment Independent of Potential Outcomes (given L):** $A \coprod Y(a) \mid L$

These formulations are crucial when working with causal diagrams, which visually encode these principles. The key idea is straightforward: ensuring a balance of confounders across treatment groups is fundamental to experimental and observational causal inference strategies. Randomisation facilitates this balance, achieving $A \coprod Y(a)$.

### The Three Fundamental Assumptions of Causal Inference {#sec-three-fundamental-assumptions}

Reviewing causal inference in experimental settings highlights three core assumptions essential for causal analysis.

#### Fundamental Assumption 1: Conditional Exchangeability

We say that conditional exchangeability holds if the potential outcomes and treatment assignments are statistically independent, considering all measured confounders. It enables us to attribute observed group differences directly to the treatment. Randomisation provides *unconditional* exchangeability, simplifying the analytical process.

#### Fundamental Assumption 2: Causal Consistency

We say that causal consistency holds if there is no heterogeneity in the treatments that would prevent us from assuming that the observed outcomes under treatments correspond to their potential outcomes. For an individual 'i', we must be able to assume:

$$
\begin{aligned}
Y_{i}(1) &= (Y_{i}|A_{i} = 1) \quad \text{(Potential outcome if treated)} \\
Y_{i}(0) &= (Y_{i}|A_{i} = 0) \quad \text{(Potential outcome if untreated)}
\end{aligned}
$$

If this assumption holds, as well as the assumptions of conditional exchangeability and positivity (reviewed below), we can calculate the Average Treatment Effect (ATE) from observed data as:

$$
\begin{aligned}
\text{ATE} &= \mathbb{E}[Y(1)] - \mathbb{E}[Y(0)] \\
&= \mathbb{E}(Y|A=1) - \mathbb{E}(Y|A=0)
\end{aligned}
$$

This contrast assumes that the potential outcome under treatment is observable when the treatment is administered, setting $Y_i(a)$ to $Y_i|A_i=a$. 

The standardisation of treatments in randomised controlled experiments generally ensures the validity of the causal consistency assumption, which is seldom disputed. However, in observational settings, we cannot typically control the treatments that people receive. This fact imposes considerable challenges for satisfying this assumption. (Discussed in [Appendix B](#appendix-b))

#### Fundamental Assumption 3: Positivity

For each set of confounder values, every individual must have a nonzero probability of receiving any treatment level. This assumption is naturally met in controlled experiments but may require explicit verification in observational studies.

These assumptions underscore the importance of thoughtful study design and rigorous analytical strategies to isolate true causal effects from observational data. Put differently, for observational data to recover causal inferences, the design must *emulate* an experiment. 

### Why Satisfying the Fundamental Assumptions of Causal Inference in Observational Settings is Challenging

Observational studies do not have the luxury of controlled treatment allocation inherent in experiments. The objective remains the same: to emulate the conditions of an experiment as accurately as possible. The primary obstacle is balancing confounders to enable fair group comparisons.

#### Challenge 1: Achieving Conditional Exchangeability in Observational Studies

The principle of conditional exchangeability necessitates that groups be comparable on all fronts, barring the treatment in question. However, real-world data, such as those examining the influence of green spaces on well-being, seldom present themselves in such an orderly manner:

- **Socioeconomic status**: the economic capacity of individuals often determines their living environments, thereby affecting their access to quality green spaces.
- **Age demographics**: different age groups have unique preferences and necessities regarding green spaces, which could influence the observed outcomes.
- **Mental health**: pre-existing conditions might lead individuals to seek out or avoid green spaces, complicating the causal pathway.
- **Lifestyle choices**: the proximity to green spaces could correlate with a more active, outdoor lifestyle preference. Is the observed effect on well-being a direct result of the green space, or is it indicative of a healthier lifestyle?
- **Personal values and social connections**: environmental values and community ties may influence both the choice of residence and the utilisation of green spaces.

These and other unobserved variables introduce bias into observational studies, complicating pinpointing the genuine effects of green spaces.

#### Challenge 2: Upholding the Causal Consistency Assumption Amidst Treatment Heterogeneity

The notion of 'proximity to green spaces' encompasses a wide array of variations, rendering the 'treatment' complex to measure consistently:

- **Diversity of green spaces**: Green spaces' ecological richness and visual appeal vary significantly. Equating well-maintained parks with neglected wild areas does not provide a like-for-like comparison.
- **Availability of amenities**: facilities such as walking paths and benches significantly impact the spaces' usability and enjoyment.
- **Size and type of green space**: the benefits derived from an urban garden versus a vast forest differ markedly, emphasising the need to consider the nature of the green space in the analysis.

#### Challenge 3: Meeting the Positivity Assumption in Observational Studies

Positivity demands that each individual have the possibility of experiencing any level of the treatment. However, real-world constraints, such as housing availability in specific locales, may preclude some groups from accessing varied green spaces, limiting the generalisation of findings across different settings.

**Take-Home Message**: although observational studies strive to replicate the experimental setup, achieving sufficient fidelity for robust causal conclusions is often formidable. In some instances, the limitations of our data preclude definitive causal assertions. Recognising the ideal of an experiment aids environmental psychologists in identifying and articulating the boundaries of observational causal inferences.

## Part 2: Causal Diagrams - A Visual Approach to Understanding Confounding

We now introduce causal diagrams, beginning with essential terminology. Grasping this vocabulary is crucial for effectively employing causal diagrams, though it may initially seem daunting. After laying out the terms, we will consider practical examples, uncovering the various forms of confounding embedded within four principal causal structures. Identifying and understanding these structures is vital for their application in real-world analyses. Refer to [**Appendix A**](#appendix-a) for a detailed glossary.

### Elements of Causal Diagrams

Causal diagrams distil the essence of causal relationships within a system into visual representations. At their core, these diagrams consist of:

#### 1. **Nodes**

Nodes represent variables or events within a causal framework. Each node stands for a distinct element that either exerts influence or is subject to influence within the system. Nodes encapsulate the components of our causal inquiry, from treatments to outcomes and confounders.

#### 2. **Arrows/Edges**

Arrows indicate the direction and presence of causal relationships between the variables denoted by nodes. Directed edges trace the flow of causal influence, with the originating variable termed the 'parent' and the receiving variable the 'child.' These arrows define the causal architecture of the system, illustrating how we assume one variable causally affects another. Importantly, the representation of causal relationships through arrows is agnostic to the specific nature of the relationship -- it remains the same whether the influence is linear, non-linear, or of any other form. 

#### 3. **Conditioning**

In causal data science, deciding which variables to adjust for is crucial for estimating the true causal effect unconfounded by other factors. We denote a decision to "control for" or equivalently "condition on" or equivalently "adjust for" a variable by enclosing it in a box. Conditioning on a variable means that we consider its influence in our attempt to causal association between the treatment and the outcome from statistical associations in the data.


### The Rules of D-separation 

Judea Pearl demonstrated how the rules of d-separation allow us to analyse relationships within causal diagrams [@pearl1995]. These rules allows us to identify confounders and develop strategies for obtaining valid causal inferences from statistical associations in the data [@pearl1995]. 

**Key Concepts**

#### Dependence

Denoted as $A \cancel\coprod B$, indicating that the probability distributions of $A$ and $B$ are interrelated. Knowledge about one variable provides insights into the other, suggesting a potential causal or associational link.

#### Independence

Denoted as $A \coprod B$, signifying that the probability distributions of $A$ and $B$ are independent. Information about one variable reveals nothing about the other, indicating no direct causal or associational connection.

#### Blocked Paths & D-separation 

We say a path is "blocked" if a node along it obstructs the causal influence from traversing between variables. If all paths between two variables are blocked, resulting in d-separation ($A \coprod B$), it implies the absence of a direct statistical association between them, facilitating unbiased causal inference.

#### Open Paths & D-connection

If at least one path between variables remains unblocked, allowing for the transmission of causal influence, the variables are considered d-connected ($A \cancel\coprod B$). This condition suggests a statistical association, warranting further analysis to understand the nature of the bias in the statistical association between the treatment and outcome.

These foundational elements and rules equip researchers with a robust framework for visually mapping and analysing causal relationships. They allow us to obtain causal inferences from complex, uncontrolled, observational data.

### The Five Elementary Graphical Structures of Causality {#sec-five-elementary}

To uncover causal insights from statistical relationships, it is essential to understand five basic graphical structures. Let's examine these structures, remembering that achieving balance in confounders across treatments necessitates ensuring independence between potential outcomes and treatment ($A\coprod Y(a)|L$) within groups defined by measured covariates $L$. This independence does not presuppose the treatment exerts no effect ($A\coprod Y |L$).

#### Causal Structure 1: Absence of Causality: Two Variables with No Arrows

When any arrows do not connect $A$ and $B$, they do not share a causal relationship and are statistically independent. Graphically, we represent this relationship as:

$$\xorxA$$

#### Causal Structure 2: Fundamental Causal Association: Two Variables with a Causal Arrow

The presence of a causal arrow ($A \to B$) indicates that alterations in $A$ will directly impact $B$, establishing a statistical dependence. Graphically, we represent this relationship as:

$$\xtoxA$$


#### Causal Structure 3: The Fork Structure: A Common Cause  

The fork structure, represented by $A \rightarrow B$ and $A \rightarrow C$, identifies $A$ as a common cause affecting both $B$ and $C$. Graphically, we represent this relationship as:

$$\fork$$

Pearl proved that when we condition on the common cause $A$ (indicated by $\boxed{A}$), $B$ and $C$ become conditionally independent [@pearl2009a]. By adjusting for the common cause, any non-causal association between $B$ and $C$ is effectively blocked at node $A$.

##### Motivating Example

Suppose observations reveal that areas with higher rates of public transportation usage also show reduced individual stress levels. Does using public transportation directly reduce stress? Not necessarily. A common environmental factor might influence both. Consider air quality as the common cause:

   - Better air quality ($A$) encourages the use of public transportation ($B$).
   - Better air quality ($A$) contributes to lower stress levels ($C$).

According to the rules of d-separation, if we account for the common cause (air quality), isolating days with similar air quality levels, the apparent link between public transportation usage and stress levels dissipates because they no longer share a direct influence. Adjusting for the fork's common cause eliminates the spurious connection.

#### Rule 1: The Fork Rule**:  

If interested in the causal effect of $B \to C$, condition on $\boxed{A}$.

#### Causal Structure 4. The Chain Structure: A Mediator

The chain structure ($A \rightarrow B \rightarrow C$) illustrates a setting in which $A$ causes $B$, and $B$ subsequently causes $C$. Conditioning on the intermediary variable $B$ (represented by $\boxed{B}$) interrupts the causal pathway, rendering $A$ and $C$ conditionally independent. Graphically, we represent this relationship as:

$$\chain$$

##### Motivating Example

Suppose we want to assess the effect of green space renovation in urban areas ($A$) on local community engagement ($B$), which subsequently reduces neighbourhood crime rates ($C$). Assume the renovation of green spaces $(A) \rightarrow$ boosts community engagement $(B) \rightarrow$, which then leads to a decrease in crime rates ($C$).

According to the rules of d-separation, controlling for the mediator, community engagement, in this case, might hide the broader effect of green space renovation. If the primary path through which green space renovation affects crime rates is via enhanced community engagement, then adjusting for community engagement could misleadingly suggest that green space renovation does not directly influence crime rates. This example underscores the necessity of carefully considering mediators when examining the effects of environmental changes on social outcomes.

##### **Rule 2. The Chain Rule:** 

If investigating the *total* causal effect of  $A\to C$, *avoid* conditioning on the mediator ($B$).  

**Important Note:** Remember, a "total" causal effect may combine several such causal chains in complex systems. Identifying a mediating role for focus ($B$)  is itself a valuable finding about the *mechanism* through which the supplement might operate. Assessing causal mediation requires further assumptions, which we will not discuss here [@vanderweele2015; @bulbulia2023].


#### Causal Structure 5: The Collider Structure: A Common Effect

The collider ($A\to C$, $B \to C$ ) features two factors independently causing a common effect. Initially, $A$ and $B$ lack association. Conditioning on the collider  $C$ (or its descendant) introduces a spurious statistical association between $A$ and $B$. Graphically:

$$\immorality$$

##### Motivating Example

Suppose we are interested in whether access to green spaces ($A$) causes people to become wealthier ($B$)? Suppose we decide to 'control' for well-being, which might cause people to seek out green spaces and also help them to obtain greater wealth. Call this variable ($C$). However, imagine that well-being is an effect of access to green space and wealth. This setting embodies a collider structure, where conditioning on well-being could misleadingly suggest a direct causal relationship between $A$ and $B$ that does not exist.

Initially, access to green spaces and community socioeconomic status do not directly influence one another. The presence of green spaces does not inherently make a community wealthier, nor does a higher socioeconomic status automatically lead to increased green space access.

However, when we specifically analyse individuals who report high levels of well-being ($\boxed{C}$), we might observe that:

1. Individuals in less wealthy neighbourhoods with ample green spaces could seem disproportionately happy, suggesting a strong beneficial effect of green spaces in these areas.
2. Meanwhile, individuals from wealthier neighbourhoods, also reporting high well-being, might not attribute their happiness as firmly to green spaces, implying that wealth contributes more to their well-being than their environment does.

In this context, examining the relationship between green space access and community wealth while controlling for well-being introduces a spurious association between $A$ and $B$. This spurious association arises because conditioning on well-being ($C$), the collider, artificially creates a statistical relationship between access to green spaces and socioeconomic status, leading to potential misinterpretation of the causal effect.

##### **Take-Home Message**

This example highlights the complexities of assessing causal relationships in observational studies. It illustrates the risk of confounding the analysis by conditioning on an outcome influenced by both variables of interest. Conditioning on well-being to explore the causal link between green space access and wealth could falsely suggest a direct relationship, underscoring the critical need for thoughtful analysis and interpretation in causal inference. Of course, we often do not know the actual structure of reality.  [In Part 4](#sec-part4), we consider how to address such uncertainties. For now, consider our third rule for confounding control:

#### **Rule 3: The Collider Rule:** when assessing the causal effect of $A\to B$, *avoid* conditioning on a collider ($C$) or its descendants.  Doing so may introduce an association that appears causal but is not.


#### We build all complex causal relationships from the five elemental structures of causation

All forms of confounding bias stem from combinations of the five basic causal structures we have outlined (absence/presence of cause, forks, chains, and colliders). Understanding these elements in isolation and combination allows us to identify potential confounders based on our assumptions about the world as encoded in a causal diagram.  Here we example a combination of two structures: the collider structure ($A \rightarrowred \boxed{C} \leftarrowred
B$ meets basic causality ($C\rightarrowNEW D$) to produce confounding by proxy: $A \rightarrowred \boxed{D} \leftarrowred B$.

THe reason relates to the first structure of causality.  Causation implies statistical association.  As such, causal inheritance implies *statistical Inheritance*. Descendants inherit statistical associations from their parents under the fundamental cause-effect relationship. The property of statistical inheritance makes descendants act as stand-ins for their parents.

##### Motivating Example 

Consider again the example of whether access to urban green spaces ($A$) affects wealth ($B$). Imagine they do not, but both independently contribute to well-being ($C$). Suppose that the only people who respond to our survey are those who are high in well-being. In effect, our survey is conditioning on one population stratum ($D$), which is a descendant of the collider -- well-being. Initially, green spaces and income independently affect well-being. However, when we specifically analyse data based on willingness to participate in the survey ($\boxed{D}$), we may inadvertently induce an association between green space access and socioeconomic status, inferring that those with access to green space tend to have a lower income.

$$\immoralityChildA$$


##### **Take-Home Message** 

Colliders and their descendants set subtle "traps" that might induce spurious associations. 

However, as we shall see in the next section, conditioning on proxies of unmeasured confounders opens up possibilities for confounding control beyond our measured variables. We can sometimes leverage proxies to reduce bias in our causal inferences.  


##### **Rule 4: The Proxy Rule:** 

Conditioning on a descendant is akin to conditioning on its parent. Put differently, a descendant is a *proxy$ for its parent. Avoid conditioning on descendants in settings where conditioning on the parent would induce misleading associations. 


### The Four Elementary Rules For Evaluating Confounding {#sec-four-rules}

Causal diagrams allow researchers to visualise and systematically identify potential confounders and strategies for adjusting for them. There are five basic graphical structures:

#### 1. **Causality Absent:**  $A$ does not cause $B$: absent any common causes, there is no statistical association between them.

$$\xorxA$$ 

#### 2. **Causality Present:**  $A$ causes $B$: absent conditioning that blocks them, $A$ and $B$ will be statistically associated.

$$\xtoxA$$

#### 3. **The Fork Structure:** $A$ causes $B$ and $A$ causes $C$: absent conditioning on $A$, $B$ and $C$ will be statistically associated. Conditional on $A$, $B$ and $C$ will be independent.

$$\forkTINY$$

#### 4. **The Chain Structure:**:  $A$ causes $B$ and $B$ causes $C$: absent conditioning on $B$, $A$ and $C$ will be statistically associated. Conditioning on $B$, $A$ and $C$ will be independent. 

$$\chainTINY$$ 

#### 5. **A Collider Structure:**  $A$ causes $C$ and $B$ causes $C$: absent conditioning on $C$, $A$ and $B$ will be statistically independant. Conditioning on $C$, $A$ and $B$ will be statistically associated. 

$$\immoralityTINY$$

From these five elementary structures, we discovered four rules that allow us to use these structures to evaluate confounding and its control:

#### 1. **The Fork Rule:** 

When a common cause influences treatment and outcome, to avoid bias, condition on the common cause.

#### 2. **The Chain Rule:**

  (i)  For total effect estimates, avoid conditioning on mediators within the causal path.
  (ii) For mediation analysis, ensure potential confounders do not introduce bias. )(Note: mediation analysis is complex [@vanderweele2015; @vansteelandt2012; @bulbulia2023].)

#### 3. **The Collider Rule**

Conditioning on a common effect opens a path between the two variables that cause it.  

#### 4. **The Proxy Rule**

Conditioning on a descendant is a proxy for conditioning on its parent. 


#### Role of Assumptions

Causal diagrams bring structure to complex environmental psychology systems. They promote critical thinking about relationships, improving study design and the chances of isolating true causal effects. However, causal diagrams cannot avoid assumptions. Observational data alone cannot prove causation: many diagrams are typically consistent with the data.  The power of causal diagrams lies in helping investigators understand how their assumptions and the data interact. However, we should create causal diagrams in collaboration with area experts because every path except the $A\to Y$ path is assumed. When experts disagree, we should propose multiple causal diagrams to reflect the implications of disagreements for causal inference and report the outcomes of their corresponding confounding control strategies. 


### Causal Diagrams and the Identification Problem in Causal Inference

The **identification problem** centres on whether we can derive the true causal effect of a treatment ($A$) on an outcome ($Y$) from observed data. Causal diagrams, rooted in **structural assumptions** about the underlying causal relationships, are indispensable for tackling this challenge. Crucially, we cannot prove these assumptions from data alone.

Addressing the identification problem has two core components:

#### 1. **Evaluating bias the absence of causality:** 

Before attributing any statistical association to causality, we must eliminate non-causal sources of correlation. We do this by:

* Identifying factors that influence both treatment ($A$) and outcome ($Y$).
* Developing adjustment strategies to control for confounders.
* Blocking backdoor paths that create indirect, non-causal links between $A$ and $Y$. By adjusting for confounders, we aim to achieve d-separation between $A$ and $Y$.

#### 2. **Evaluating bias in the presence of causality:** 

After addressing potential confounders, we must ensure any remaining association between $A$ and $Y$ reflects a true causal relationship. We address **over-conditioning bias** by:

* Avoiding mediator bias 
* Avoiding collider bias
* Verifying that any association between $A$ and $Y$ after in unbiased after all adjustments.

Thus, causal inference demands a delicate balance: identify and control for confounders but avoid introducing new biases. 

### How to Create Causal Diagrams to Address Identification Problems {sec-how-to-create-causal-diagrams}

#### Step 1. Clarify the Research Question Evaluated by the Diagram

State the problem your diagram addresses and the population for whom the problem applies.  Causal identification strategies may vary by question. For example, the confounding control strategy for evaluating the path $L\to Y$ will differ from that of assessing the path $A\to Y$.  For this reason, reporting coefficients other than the association between $A \to Y$ is typically ill-advised; see @westreich2013; @mcelreath2020; @bulbulia2023.


#### Step 2. Include all common causes of the exposure and outcome

Include both measured and unmeasured common causes and group functionally similar common causes under a single variable (e.g., $L_0$ for demographics).

#### Step 3. Include all ancestors of measured confounders linked with the treatment, the outcome, or both

Such inclusion helps address unmeasured confounding and reduce biases like M-bias. Again, group functionally similar variables for a simplified visual representation.

#### Step 4. Explicitly state assumptions about relative timing

Use time subscripts (e.g., $L_0$, $A_1$, $Y_2$) to denote the assumed order of events **Requirement:** causal diagrams must be acyclic (no feedback loops) for clear causal direction.

#### Step 5. Arrange temporal order of causality visually


*Time orders causality; the spatial layout of your causal diagram should therefore, respect time.*


Left-to-right or top-to-bottom flows aid understanding of causal assertions [@bulbulia2023]. As we shall repeatedly consider in [**Part 3**](#sec-part3), establishing temporal ordering is necessary for evaluating identification problems. 


#### Step 6. Box variables those variables that we adjust for to control confounding 

Typically, you will only box treatments and outcomes if they are measured with error.

#### Step 7. Represent paths structurally, not parametrically

Focus on whether paths exist, not their functional form (linear, non-linear, etc.). Parametric descriptions are not relevant for bias evaluation in a causal diagram. (For an explanation of causal interaction and causal diagrames, see: @bulbulia2023.)

#### Step 8. Minimise paths to those necessary for the identification problem

Reduce clutter; only include paths critical for a specific question (e.g., backdoor paths, mediators).

#### Step 9. Consider Potential Unmeasured Confounders

Use your domain knowledge to hypothesise where unmeasured confounders might occur. Draw unmeasured confounders on your causal diagram.

#### **Step 10. State Graphical Conventions**

Explain your symbol use (e.g., red for open backdoor paths).  Consistency aids interpretation, and verbal descriptions add accessibility.

## Part 3. Using Causal Diagrams for Causal Identification - Worked Examples {#section-part3}

### Notation

Causal diagrams use specific symbols to represent elements essential in causal inference [@pearl1995; @pearl2009; @greenland1999].  However, as mentioned, no agreed-upon convention exists for creating causal diagrams. We list the symbols and conventions we use in this chapter in @tbl-01.

* **$A$** is the treatment or exposure variable – the intervention or condition whose effect on an outcome is under investigation. **This symbol represents the cause**.
* **$Y$** is the outcome variable – the effect or result that is being studied. **This symbol represents the effect**.
* **$L$** includes all measured confounders – variables that may affect both the treatment and the outcome.
* **$U$** includes unmeasured confounders – variables not included in the analysis that could influence both the treatment and the outcome, potentially leading to biased conclusions.
* **$M$** is a mediator variable – a factor through which the treatment affects the outcome. The focus here is on identifying the total effect of treatment $A$ on an outcome $Y$. Still, it is also essential to understand how controlling for mediators can affect estimates of this total effect. 


::: {#tbl-01}

```{=latex}
\terminologylocalconventionssimple
```
Terminology that is used in this article for causal diagrams. The graph is adapted from [@bulbulia2023]. 
:::


We next describe our graphical conventions and causal diagrams.  Again, because conventions may differ, it is always important to state them explicitly when reporting causal diagrams. @tbl-02 describes the basic conventions that we employ in this chapter. 

::: {#tbl-02}

```{=latex}
\terminologygeneralbasic
```
Basic conventions for causal diagrams (adapted from [@bulbulia2023]). 
:::




### Graphical Table


@tbl-04 provides seven worked examples that put causal diagrams to work.  Our example will focus on the question of whether access to green space affects happiness and approach this question by focusing on how different assumptions about (i) the structure of the world and (ii) the observational data that have been collected may affect strategies for confounding control and the confidence in our results.  Each example refers to a row in the table. 

::: {#tbl-04}
```{=latex}
\terminologyelconfoundersLONG
```
Worked examples: This table is adapted from [@bulbulia2023].

:::



### 1. The problem of confounding by a common cause

@tbl-04 Row 1 describes the confounding problem of a common cause. We encountered this problem in Part 1. Such confounding arises when there is a variable or set of variables, denoted by $L$, that influence both the exposure, denoted by $A$, and the outcome, denoted by $Y.$ Because $L$ is a common cause of both $A$ and $Y$, $L$ may create a statistical association between $A$ and $Y$ that does not reflect a causal association.

For instance, in the context of green spaces, consider people who live closer to green spaces (exposure $A$) and their experience of improved happiness (outcome $Y$). A common cause might be socioeconomic status $L$. Individuals with higher socioeconomic status might have the financial capacity to afford housing near green spaces and simultaneously afford better healthcare and lifestyle choices, contributing to greater happiness. Thus, although the data may show a statistical association between living closer to green spaces $A$ and greater happiness $Y$, this association might not reflect a direct causal relationship owing to confounding by socioeconomic status $L$.

How might we obtain balance in this confounder to compare the treatments?  Addressing confounding by a common cause involves adjusting for the confounder in one's statistical model. We may adjust through regression, or more complicated methods, such as inverse probability of treatment weighting, marginal structural models, and others see @hernán2023. Such adjustment effectively closes the backdoor path from the exposure to the outcome. Equivalently, conditioning on $L$ d-separates $A$ and $Y$.  

@tbl-04 Row 1, Column 3, emphasises that a confounder by common cause must precede both the exposure and the outcome. While it is often clear that a confounder precedes the exposure (e.g., a person's country of birth), the timing might be uncertain in other cases. We assert its temporal precedence by positioning the confounder before the exposure in our causal diagrams. However, such a timing assumption might be strong when relying on cross-sectional data. Exploring causal scenarios where the confounder follows the treatment or outcome can be insightful in such cases. Causal diagrams are instrumental in examining possible timings and their implications for causal inference. 

Next, we examine the effects of conditioning on a variable that is an effect of the treatment.

### 2. Mediator Bias

 @tbl-04 Row 1 presents a problem of mediator bias. Consider again whether proximity to green spaces, $A$, affects happiness, $Y$. Suppose that physical activity is a mediator, $L$.

To fill out the example, imagine that living close to green spaces $A$ influences physical activity $L$, subsequently affecting happiness $Y$. If we were to condition on physical activity $L$, assuming it to be a confounder, we would then bias our estimates of the total effect of proximity to green spaces $A$ on happiness $Y$. Such a bias arises as a consequence of the chain rule. Conditioning on $L$ "d-separates" the total effect of $A$ on $Y$. This phenomenon is known as mediator bias. Notably, @montgomery2018 finds dozens of examples of mediator bias in *experiments* in which control is made for variables that occur after the treatment.  For example, obtaining demographic and other information from participants *after* a study is an invitation to mediator bias. If the treatment affects these variables, and the variables affect the outcome (as we assume by controlling for them), then researchers may induce mediator bias. 

To avoid mediator bias when estimating a total causal effect, we should, of course, never condition on a mediator! The surest way to prevent this problem is to ensure that $L$ occurs before the treatment $A$ and before the outcome $Y$.  We present this solution in @tbl-04 Row 2 Col 3. 


### 3. Confounding by Collider Stratification (Conditioning on a Common Effect)


 @tbl-04 Row 1 presents a problem of collider bias.  Conditioning on a common effect, or collider stratification, occurs when a variable, denoted by $L$, is influenced by both the exposure, denoted by $A$, and the outcome, denoted by $Y$.

Imagine, again, the context of an access to green space question, that an individual's choice to live closer to green spaces (exposure $A$) and their happiness (outcome $Y$) both affect the individual's overall sense of physical health (common effect $L$). Initially, $A$ and $Y$ could be independent, that is, $A \coprod Y(a)$, suggesting that the decision to live near green spaces is not directly a cause of happiness.

However, if we were to condition on physical health $L$ (the joint effect of $A$ and $Y$), we would open a backdoor path between $A$ and $Y$. Put another way, by "controlling for" physical health, we might *induce* a non-causal association between proximity to green spaces and happiness.

The reason that conditioning on physical health $L$ leads to confounding is that this variable provides information about the proximity to green spaces $A$ and one's happiness $Y$. Once we learn something about one's physical health, for example, that it is poor, it becomes more probable that a person is happy if they have low access to green spaces (assuming the relationship between green space access and happiness is positive.) 

Causal diagrams point a way to respond to the problem of collider stratification bias: we should generally ensure that:

1.  All confounders $L$ that are common causes of the exposure $A$ and the outcome $Y$ are measured before $A$ has occurred, and

2.  $A$ is measured before $Y$ has occurred.

If we preserve such temporal order, $L$ could not be an effect of $A$, and thus neither of $Y$.


### 4. Confounding by Conditioning on a Descendant of a Confounder  

 @tbl-04 Row 4 presents a problem of collider bias by decent. Recall the rules of d-separation also apply to conditioning on descendants of a confounder.  Thus, when conditioning on a measured descendant of an unmeasured collider, we may unwittingly evoke confounding by proxy. 
 
For example, if doctor visits were encoded in our data, and doctor visits were an effect of poor health, conditioning on doctor visits would function similarly to conditioning on poor health, introducing collider confounding. 


### 5. M-bias: Conditioning on Pre-Exposure Collider

There are only five elementary structures of causality. Any confounding scenario we might imagine can be developed from these five elementary structures. We next consider how we may combine these elementary causal relationships in causal diagrams to create effective strategies for confounding control. 


@tbl-04 Row 5 presents a form of pre-exposure over-conditioning confounding known as "M-bias".  This bias combines the collider structure and the fork structure, revealing what might not otherwise be obvious: it is possible to induce confounding even if we ensure that all variables have been measured **before** the treatment. The collider structure is evident in the path $U_Y \to L_0$ and $U_A \to L_0$. The collider rule shows that conditioning on $L_0$ opens a path between $U_Y$ and $U_A$. What is the result? We find that $U_Y$ is associated with the outcome $Y$ and $U_A$ is associated with treatment $A$. This is a fork (common cause) structure. The association between treatment and outcome that is opened by conditioning on $L$ arises from an open back-door path that occurs from the collider structure. We thus have confounding. How might such confounding play out in a real-world setting? 

In the context of green spaces, consider the scenario where an individual's level of physical activity $L$ is influenced by an unmeasured factor related to their propensity to live near green spaces $A$ -- say childhood upbringing. Suppose further that another unmeasured factor -- say a genetic factor -- increases both physical activity $L$ and happiness $Y$. Here, physical activity $L$ does not affect the decision to live near green spaces $A$ or happiness $Y$ but is a descendent of unmeasured variables that do. If we were to condition on physical activity $L$ in this scenario, we would create the bias just described --  "M-bias."  

How shall we respond to this problem? The solution is straightforward. If $L$ is neither a common cause of $A$ and $Y$ nor the effect of a shared common cause, then $L$ should not be included in a causal model. In terms of the conditional exchangeability principle, we find $A \coprod Y(a)$ yet $A \cancel{\coprod} Y(a)| L$. So we should not condition on $L$: do not control for exercise [@cole2010].[^3]

[^3]: Note that when we draw a chronologically ordered path from left to right the M shape for which "M-bias" takes its name changes to an E shape. We shall avoid proliferating jargon and retain the term "M bias."

### 6. Conditioning on a Descendent May Sometimes Reduce Confounding

 In @tbl-04 Row 6, we encounter a causal diagram in which an unmeasured confounder opens a back-door path that links the treatment and outcome.  Here, we consider how we may use the rules of d-separation to obtain unexpected strategies for confounding control. 
 
 Return to our green space example. Suppose an unmeasured genetic factor $U$ affects one's desire to seek out isolation in green spaces $A$ and also independently affects one's happiness $Y$.  Were such an unmeasured confounder to exist we could not obtain an unbiased estimate for the causal effect of green space access on happiness. It seems we have intractable confounding.  
 
However, imagine a variable $L^\prime$ that is a trait expressed later in life that arises from this genetic factor. If such a trait could be measured, even though the trait $L'$ is expressed after the treatment and outcome have occurred, controlling for $L'$ would enable investigators to close the backdoor path between the treatment and the outcome. This strategy works because a measured effect is a *proxy* for its cause $U$, the unmeasured confounder.  By conditioning on the late-adulthood trait, $L'$, we partially condition on its cause, $U$, the confounder of $A \to Y$. Thus, not all effective confounding control strategies need to rely on measuring pre-exposure variables. Thus, the elementary causal structures reveal a possibility for confounding control by condition on a post-outcome variable.  This strategy is not intuitive. Although a common cause must occur before a treatment (and outcome), its proxy need not! If we have a measure for the ladder but not the former, we should condition ont the post-treatment proxy of a pre-treatment common cause.

### 7. Confounding Control with Three Waves of Data is Powerful and Reveals Possibilities for Estimating an "Incident Exposure" Effect

@tbl-04 row 7 presents another setting in which there is unmeasured confounding. In response to this problem, we use the rules of d-separation to develop a data collection and modelling strategy that may greatly reduce the influence of unmeasured confounding.  @tbl-04 row 7 col 3, by collecting data for both the treatment and the outcome at baseline and controlling for baseline values of the treatment and outcome, any unmeasured association between the treatment $A_1$ and the outcome $Y_2$ would need to be *independent* of their baseline measurements. As such, including the baseline treatment and outcome, along with other measured covariates that might be measured descendants of unmeasured confounders, is a strategy that exerts considerable confounding control [@vanderweele2020]. 

Furthermore, this causal graph makes evident a second benefit of this strategy.  Returning to our example, a model that controls for baseline exposure would require that people initiate a change from the $A_0$ observed baseline level. Thus, by controlling for the baseline value of the treatment, we may learn about the causal effect of shifting one's access to green space status. This effect is called the "incident exposure effect." The incident exposure effect better emulates a "target trial" or the organisation of observational data into a hypothetical experiment in which there is a "time-zero" initiation of treatment in the data; see @hernán2016; @danaei2012; @vanderweele2020; @bulbulia2022.  Without controlling for the baseline treatment, we could only estimate a "prevalent exposure effect." If the initial exposure caused people some people to be miserable, we would not be able to track this outcome. The prevalent exposure effect would mask it, distorting causal inferences for the quantity of interest, namely, what would happen, on average, if people were to shift to having greater greenspace access. 

Finally, we obtain further control for unmeasured confounding by controlling for both the baseline treatment and the baseline outcome. For an unmeasured confounder to affect both the treatment and the outcome (and unmeasured fork structure), it would need to do so independently of the baseline measures of the treatment and exposure [@vanderweele2020]. 

Thus, we generally require repeated measures on the same unit over time intervals to obtain an incident exposure effect and exert more robust control for unmeasured confounding using past states of the treatment and outcome.  We must then model the treatments and outcomes as separate elements in our statistical model. 


## Part 4.  Practical Guide For Constructing Causal Diagrams and Reporting Results When Causal Structure is Unclear {#section-part4}

### Cross-sectional designs

In environmental psychology, researchers often grapple with whether causal inferences can be drawn from cross-sectional data, especially when longitudinal data are unavailable. The challenge is common to cross-sectional designs; even longitudinal studies require careful assumption management. We next discuss how causal diagrams can guide inference in both data types, with examples relevant to environmental psychologists.

#### 1. **Graphically encode causal assumptions** 

Causal inference turns on assumptions. Although cross-sectional analyses typically demand much stronger assumptions owing to the snapshot nature of data, these assumptions, when transparently articulated, do not permanently bar causal analysis. By stating different assumptions and modelling the data following these assumptions, we might find that certain causal conclusions are robust to these differences. Where the implications of different assumptions disagree, we can better determine the forms of data collection that would be required to settle such differences.  Below we consider an example where assumptions point to different conclusions, revealing the need to collect time-series data to assess whether a variable is a confounder or a mediator. 


#### 2. **Time-invariant confounders**: 

In cross-sectional studies, some confounders are inherently stable over time, such as ethnicity, year and place of birth, and biological gender. For environmental psychologists examining the relationship between access to natural environments and psychological well-being, these stable confounders can be adjusted for without concern for introducing bias from mediators or colliders. For example, conditioning on one’s year of birth can help isolate recent urban development's effect on mental health, independent of generational differences in attitudes toward green spaces.

#### 3. **Stable confounders**

While not immutable, other confounders are less likely to be influenced by the treatment. Variables such as sexual orientation, educational attainment, and often income level fall into this category. For instance, the effect of exposure to polluted environments on cognitive outcomes can be analysed by conditioning on education level, assuming that recent exposure to pollution is unlikely to change someone's educational history retroactively.

#### 4. **Timing and reverse causation**

The sequence of treatment and outcome is crucial. Sometimes, the temporal order is clear, reducing concerns about reverse causation. Mortality is a definitive outcome where the timing issue is unambiguous. If researching the effects of air quality on mortality, the causal direction (poor air quality leading to higher mortality rates) is straightforward.

#### 5. **Multiple causal diagrams**

Given the complexity of environmental influences on psychological outcomes, it's prudent to construct multiple causal diagrams to cover various hypothetical scenarios. For example, when studying the effect of community green space on stress reduction, one diagram might assume the direct benefits of green space on stress. At the same time, another might include potential mediators like physical activity. By analysing and reporting findings based on multiple diagrams, researchers can explore the robustness of their conclusions across different theoretical frameworks.

@tbl-cs describes ambiguous confounding control arising from cross-sectional data. Suppose again we are interested in the causal effect of access to greenspace, denoted by $A$ on "happiness," denoted by $Y$.   We are uncertain whether exercise, denoted by $L$, is a common cause of $A$ and $Y$ and thus a confounder or whether exercise is a mediator along the path from $A$ to $Y$. We may use causal diagrams to investigate the consequences of such ambiguity. 

**Assumption 1: Exercise is a common cause of $A$ and $Y$**, this scenario is presented in @tbl-cs row 1. Here, our strategy for confounding control is to estimate the effect of $A$ on $Y$ conditioning on $L$. 

**Assumption 2: Exercise is a mediator of $A$ and $Y$**, this scenario is presented in @tbl-cs row 2. Here, our strategy for confounding control is simply estimating the effect of $A$ on $Y$ without including $L$ (assuming there are no other common causes of the treatment and outcome). 

::: {#tbl-cs}

```{=latex}
\examplecrosssection
```
This table is adapted from [@bulbulia2023]
:::



To clarify how answers may differ, we can simulate data and run separate regressions, reflecting the different conditioning strategies embedded in the different assumptions. The following simulation generates data from a process in which exercise is a mediator (Scenario 2). (See Appendix C)



```{r}
#| label: simulation_cross_sectional-appendix
#| tbl-cap: "Code for a simulation of a data generating process in which the effect of excercise (L) fully mediates the effect of greenspace (A) on happiness (Y)."
#| out-width: 80%
#| echo: false
# load libraries
library(gtsummary) # gtsummary: nice tables
library(kableExtra) #  tables in latex/markdown
library(clarify) # simulate ATE

# simulation seed
set.seed(123) #  reproducibility

# define the parameters 
n = 1000 # Number of observations
p = 0.5  # Probability of A = 1 (access to greenspace)
alpha = 0 # Intercept for L (exercise)
beta = 2  # Effect of A on L 
gamma = 1 # Intercept for Y 
delta = 1.5 # Effect of L on Y
sigma_L = 1 # Standard deviation of L
sigma_Y = 1.5 # Standard deviation of Y

# simulate the data: fully mediated effect 
A = rbinom(n, 1, p) # binary exposure variable
L = alpha + beta*A + rnorm(n, 0, sigma_L) # continuous mediator
Y = gamma + delta*L + rnorm(n, 0, sigma_Y) # continuous outcome

# make the data frame
data = data.frame(A = A, L = L, Y = Y)

# fit regression in which L is assumed to be a mediator
fit_1 <- lm( Y ~ A + L, data = data)

# fit regression in which L is assumed to be a mediator
fit_2 <- lm( Y ~ A, data = data)

# create gtsummary tables for each regression model
table1 <- tbl_regression(fit_1)
table2 <- tbl_regression(fit_2)

# merge the tables for comparison
table_comparison <- tbl_merge(
  list(table1, table2),
  tab_spanner = c("Model: Exercise assumed confounder", 
                  "Model: Exercise assumed to be a mediator")
)
# make latex table
markdown_table_0 <- as_kable_extra(table_comparison, 
                                   format = "latex", 
                                   booktabs = TRUE)
# print                                   
markdown_table_0
```


This table presents the conditional treatment effect estimates.  We present code for obtaining marginal treatment effects in [Appendix C](#appendix-c) 

```{r}
#| label: ate_simulation_cross_sectional
#| fig-cap: ""
#| out-width: 100%
#| echo: false

# use `clarify` package to obtain ATE
library(clarify)
# simulate fit 1 ATE
set.seed(123)
sim_coefs_fit_1 <- sim(fit_1)
sim_coefs_fit_2 <- sim(fit_2)

# marginal risk difference ATE, simulation-based: model 1 (L is a confounder)
sim_est_fit_1 <-
  sim_ame(
    sim_coefs_fit_1,
    var = "A",
    subset = A == 1,
    contrast = "RD",
    verbose = FALSE
  )

# marginal risk difference ATE, simulation-based: model 2 (L is a mediator)
sim_est_fit_2 <-
  sim_ame(
    sim_coefs_fit_2,
    var = "A",
    subset = A == 1,
    contrast = "RD",
    verbose = FALSE
  )
# obtain summaries
summary_sim_est_fit_1 <- summary(sim_est_fit_1, null = c(`RD` = 0))
summary_sim_est_fit_2 <- summary(sim_est_fit_2, null = c(`RD` = 0))

# get coefficients for reporting
# ate for fit 1, with 95% CI
ATE_fit_1 <- glue::glue(
  "ATE =
                        {round(summary_sim_est_fit_1[3, 1], 2)},
                        CI = [{round(summary_sim_est_fit_1[3, 2], 2)},
                        {round(summary_sim_est_fit_1[3, 3], 2)}]"
)
# ate for fit 2, with 95% CI
ATE_fit_2 <-
  glue::glue(
    "ATE = {round(summary_sim_est_fit_2[3, 1], 2)},
                        CI = [{round(summary_sim_est_fit_2[3, 2], 2)},
                        {round(summary_sim_est_fit_2[3, 3], 2)}]"
  )
```



On the assumptions outlined in @tbl-cs row 1, in which we *assert* that exercise is a confounder, the average treatment effect of access to green space on happiness is `r ATE_fit_2`.

On the assumptions outlined in @tbl-cs row 2, in which we *assert* that exercise is a mediator, the average treatment effect of access to green space on happiness is `r ATE_fit_1`. 

Note that although the mediator $L$ is "highly significant", including it in the model is a mistake. We obtain a negative effect estimate for the causal effect of green space access on happiness.

With only cross-sectional data, we must infer the results are inconclusive. Such understanding, although not the definitive answer we sought, is progress. The result tells us we should not be overly confident with our analysis (whatever p-values we recover!), and it clarifies that longitudinal data are needed. 

These findings illustrate the role that assumptions about the relative timing of exercise as a confounder or as a mediator play. 

### Recommendations for Conducting and Reporting Causal Analyses with Cross-Sectional Data

When analysing and reporting analyses with cross-sectional data, researchers face the challenge of making causal inferences without the benefit of temporal information. 

The following recommendations aim to guide researchers in navigating these challenges effectively:

**Warning**: before proceeding with cross-sectional analysis, examine whether panel data are available. Longitudinal data can provide crucial temporal information that aids in establishing causality, offering a more robust framework for causal inference. If longitudinal data are unavailable, the recommendations above become even more critical for using cross-sectional data best.

#### 1. **Draw multiple causal diagrams**

Draw various causal diagrams to represent different theoretical assumptions about the relationships and timing of variables relevant to an identification problem. This approach facilitates a comprehensive exploration of potential causal pathways, clarifying variables' roles as confounders, mediators, or colliders. For example, in studying the effect of urban green spaces on mental health, consider diagrams that account for both direct effects and pathways involving mediators like physical activity or social interaction.

#### 2. **Perform and report analyses for each assumption**

Conduct and transparently report separate analyses for each scenario depicted by your causal diagrams. This practice ensures that your study is theoretically grounded for each model. Presenting results from each analytical approach and the underlying assumptions and statistical methods promotes a balanced interpretation of findings. Although this practice may be unfamiliar to some editors and reviewers, it is crucial to address the inherent challenges of cross-sectional analysis by expanding the scope of investigation beyond a single hypothesis.

#### 3. **Interpret findings with attention to ambiguities**

Interpret results carefully, highlighting any ambiguities or inconsistencies across analyses. Discuss how varying assumptions about structural relationships and the timing of events can lead to divergent conclusions. For instance, if access to green spaces appears to have a positive effect on mental health when considering exercise as a mediator but a negative effect when considered a confounder, explore the theoretical and empirical implications.

#### 4. **Report divergent findings**

Approach conclusions with caution, especially when findings suggest differing practical implications. Acknowledge the limitations of cross-sectional data in establishing causality and the potential for alternative explanations.

#### 5. **Identify avenues for future research**: 

Target future research that could clarify ambiguities. Consider the design of longitudinal studies or experiments capable of clarifying these ambiguities.

#### 6. **Supplement observational data with simulated data** 

Leverage data simulation to understand the complexities of causal inference. Simulating data based on various theoretical models allows researchers to explore the impact of different assumptions on their findings. This method tests analytical strategies under controlled conditions, assessing the robustness of conclusions against assumption violations or unobserved confounders.

#### 7. **Conduct sensitivity analyses to assess robustness**

implement sensitivity analyses to determine how dependent conclusions are on specific assumptions or parameters within your causal model. Use data simulation as a tool for these analyses, evaluating the sensitivity of results to various theoretical and methodological choices.


Cross-sectional data are limiting, however, by appropriately bounding uncertainties in your causal inferences you may use them to advance understanding. May your clarity, and caution, serve as an example for others.


### Longitudinal Designs

Causation occurs in time. Longitudinal designs offer a substantial advantage over cross-sectional designs for causal inference because sequential measurements allow us to capture causation and quantify its magnitude. We typically do not need to assert timing as in cross-sectional data settings. Because we know when variables have been measured, we can reduce ambiguity about the directionality of causal relationships. For instance, tracking changes in "happiness" following changes in access to green spaces over time can more definitively suggest causation than cross-sectional snapshots.


Despite this advantage, longitudinal researchers still face assumptions regarding the absence of unmeasured confounders or the stability of measured confounders over time. These assumptions must be explicitly stated.  As with cross-sectional designs, wherever assumptions differ, researchers should draw different causal diagrams that reflect these assumptions and subsequently conduct and report separate analyses. 


In this section, we simulate a dataset to demonstrate the benefits of incorporating both baseline exposure and baseline outcomes into analysing the effect of access to open green spaces on happiness. This approach allows us to control for initial levels of exposure and outcomes, offering a clearer understanding of the causal relationship. [Appendix D](#appendix-d-simulation-of-different-confounding-control-strategies) provides the code. [Appendix E](#appendix-e-non-parametric-estimation-of-average-treatment-effects-using-causal-forests-appendix-causal-forests) provides an example of a non-parametric estimator for the causal effect.  HAs mentioned before, by conditioning on baseline levels of access to green spaces and baseline mental health, researchers can more accurately estimate the *incident effect* of changes in green space access on changes in mental health. @tbl-lg offers an example of how we may use multiple causal diagrams to clarify the problem and our confounding control strategy. 


::: {#tbl-lg}

```{=latex}
\examplelongitudinal
```
This table is adapted from [@bulbulia2023]
:::


Our analysis assessed the average treatment effect (ATE) of access to green spaces on happiness across three distinct models: uncontrolled, standard controlled, and interaction controlled. These models were constructed using a hypothetical cohort of 10,000 individuals, incorporating baseline exposure to green spaces ($A_0$), baseline happiness ($Y_0$), baseline confounders ($L_0$), and an unmeasured confounder ($U$). The detailed simulation process and model construction are given in [Appendix D](#appendix-simulate-longitudinal-ate).



```{r}
#| label: codelg
#| echo: false
#| eval: true
# load libaries 
library(kableExtra)
if(!require(kableExtra)){install.packages("kableExtra")} # causal forest
if(!require(gtsummary)){install.packages("gtsummary")} # causal forest
if(!require(grf)){install.packages("grf")} # causal forest

# r_texmf()eproducibility
set.seed(123) 

# set number of observations
n <- 10000 

# baseline covariates
U <- rnorm(n) # Unmeasured confounder
A_0 <- rbinom(n, 1, prob = plogis(U)) # Baseline exposure
Y_0 <- rnorm(n, mean = U, sd = 1) # Baseline outcome
L_0 <- rnorm(n, mean = U, sd = 1) # Baseline confounders

# coefficients for treatment assignment
beta_A0 = 0.25
beta_Y0 = 0.3
beta_L0 = 0.2
beta_U = 0.1

# simulate treatment assignment
A_1 <- rbinom(n, 1, prob = plogis(-0.5 + 
                                    beta_A0 * A_0 +
                                    beta_Y0 * Y_0 + 
                                    beta_L0 * L_0 + 
                                    beta_U * U))

# coefficients for continuous outcome
delta_A1 = 0.3
delta_Y0 = 0.9
delta_A0 = 0.1
delta_L0 = 0.3
theta_A0Y0L0 = 0.5 # Interaction effect between A_1 and L_0
delta_U = 0.05

# simulate continuous outcome, including interaction
Y_2 <- rnorm(n,
             mean = 0 +
               delta_A1 * A_1 + 
               delta_Y0 * Y_0 + 
               delta_A0 * A_0 + 
               delta_L0 * L_0 + 
               theta_A0Y0L0 * Y_0 * 
               A_0 * L_0 + 
               delta_U * U,
             sd = .5)

# assemble data frame
data <- data.frame(Y_2, A_0, A_1, L_0, Y_0, U)

# model: no control
fit_no_control <- lm(Y_2 ~ A_1, data = data)

# model: standard covariate control
fit_standard <- lm(Y_2 ~ A_1 + L_0, data = data)

# model: interaction
fit_interaction  <- lm(Y_2 ~ A_1 + L_0 + A_0 + Y_0 + A_0:L_0:Y_0, data = data)

# create gtsummary tables for each regression model
tbl_fit_no_control<- tbl_regression(fit_no_control)  
tbl_fit_standard <- tbl_regression(fit_standard)
tbl_fit_interaction <- tbl_regression(fit_interaction)

# get only the treatment variable
tbl_list_modified <- lapply(list(
  tbl_fit_no_control,
  tbl_fit_standard,
  tbl_fit_interaction),
function(tbl) {
  tbl %>%
    modify_table_body(~ .x %>% dplyr::filter(variable == "A_1"))
})

# merge tables
table_comparison <- tbl_merge(
  tbls = tbl_list_modified,
  tab_spanner = c(
    "No Control",
    "Standard",
    "Interaction")
) |>
  modify_table_styling(
    column = c(p.value_1, p.value_2, p.value_3),
    hide = TRUE
  )

#create latex table for publication
markdown_table <-
  as_kable_extra(table_comparison, format = "latex", booktabs = TRUE) |>
  kable_styling(latex_options = "scale_down")
  
# print it
#markdown_table
```
```{r}
#| label: ate-sim-long
#| tbl-cap: "Code for calculating the average treatment effect."
#| echo: false
#| eval: true

# use `clarify` package to obtain ATE
if(!require(clarify)){install.packages("clarify")} # clarify package

# simulate fit 1 ATE
set.seed(123)
sim_coefs_fit_no_control<- sim(fit_no_control)  
sim_coefs_fit_std <- sim(fit_standard)
sim_coefs_fit_int <- sim(fit_interaction)

# marginal risk difference ATE, no controls
sim_est_fit_no_control <-
  sim_ame(
    sim_coefs_fit_no_control,
    var = "A_1",
    subset = A_1 == 1,
    contrast = "RD",
    verbose = FALSE
  )
# marginal risk difference ATE, simulation-based: model 1 (L is a confounder)
sim_est_fit_std <-
  sim_ame(
    sim_coefs_fit_std,
    var = "A_1",
    subset = A_1 == 1,
    contrast = "RD",
    verbose = FALSE
  )
# marginal risk difference ATE, simulation-based: model 2 (L is a mediator)
sim_est_fit_int <-
  sim_ame(
    sim_coefs_fit_int,
    var = "A_1",
    subset = A_1 == 1,
    contrast = "RD",
    verbose = FALSE
  )
# obtain summaries
summary_sim_coefs_fit_no_control <-
  summary(sim_est_fit_no_control, null = c(`RD` = 0))
summary_sim_est_fit_std <-
  summary(sim_est_fit_std, null = c(`RD` = 0))
summary_sim_est_fit_int <-
  summary(sim_est_fit_int, null = c(`RD` = 0))

# get coefficients for reporting
# ate for fit 1, with 95% CI
ATE_fit_no_control  <- glue::glue(
  "ATE = {round(summary_sim_coefs_fit_no_control[3, 1], 2)}, 
  CI = [{round(summary_sim_coefs_fit_no_control[3, 2], 2)},
  {round(summary_sim_coefs_fit_no_control[3, 3], 2)}]"
)
# ate for fit 2, with 95% CI
ATE_fit_std <- glue::glue(
  "ATE = {round(summary_sim_est_fit_std[3, 1], 2)}, 
  CI = [{round(summary_sim_est_fit_std[3, 2], 2)},
  {round(summary_sim_est_fit_std[3, 3], 2)}]"
)
# ate for fit 3, with 95% CI
ATE_fit_int <-
  glue::glue(
    "ATE = {round(summary_sim_est_fit_int[3, 1], 2)},
    CI = [{round(summary_sim_est_fit_int[3, 2], 2)},
    {round(summary_sim_est_fit_int[3, 3], 2)}]"
  )
# coefs
# ATE_fit_no_control
# ATE_fit_std
# ATE_fit_int
```

The ATE estimates from these models provide critical insights into the effects of green space exposure on individual happiness while accounting for various confounding factors. The model without control variables estimated `r ATE_fit_no_control`, significantly overestimating the treatment effect. Incorporating standard covariate control reduced this estimate to `r ATE_fit_std`, aligning more closely with the expected effect but still overestimating. Most notably, the model that included interactions among baseline exposure, outcome, and confounders yielded `r ATE_fit_int`,  approximating the true effect of 0.3. This finding underscores the importance of including baseline values of the exposure and outcome wherever these data are available. 

### Recommendations for Conducting and Reporting Causal Analyses with Longitudinal Data

Longitudinal data offer strong advantages for causal inference by enabling researchers to establish the relative timing of confounders, treatments and outcomes. The temporal sequence of events is crucial for establishing causality because causality occurs in time. The following recommendations aim to guide researchers in leveraging longitudinal data effectively to conduct and report causal analyses:

#### 1. **Draw multiple causal diagrams**:
   - **Identification problem diagram**: begin by constructing a causal diagram that outlines your initial assumptions about the relationships among variables, identifying potential confounders and mediators. This diagram should illustrate the complexity of the identification problem.
   - **Solution diagram**: next, create a separate causal diagram that proposes solutions to the identified problems. This may involve highlighting variables for conditioning to isolate the causal effect of interest or suggesting novel pathways for investigation. Having distinct diagrams for the problem and its proposed solutions clarifies the analytic strategy and theoretical underpinning of your study.

@tbl-lg provides an example of a table with multiple causal diagrams clarifying potential sources of confounding threats and reports strategies for addressing them. 

#### 2. **Attempt longitudinal designs with at least three waves of data.

Incorporating data from at least three time points considerably enhances your ability to infer causal relationships. This approach allows for the examination of temporal precedence and lagged effects. For example, by adjusting for physical activity measured before the treatment, we can ensure that physical activity is not the result of a new initiation to green spaces, which we establish by measuring green space access at baseline. Establishing chronological order in the temporal sequence of events allows us to avoid confounding problems 1-4 in @tbl-04. 

#### 3. Calculate Average Treatment Effects for a clearly specified target population**

Estimating the average treatment effect (ATE) across the entire study population provides a comprehensive measure of the intervention's effects. This step is crucial for understanding the treatment's overall effect and generalising findings to broader populations.

#### 4. Where causality is unclear, report results for multiple causal graphs

Given that the true causal structure may be complex and partially unknown, analysing and reporting results under each plausible causal diagram is prudent. This practice acknowledges the uncertainty inherent in causal modelling and demonstrates the robustness of findings across different theoretical frameworks.

#### 5. Conduct sensitivity analyses


Sensitivity analyses are essential for assessing the robustness of your findings to various assumptions within the causal model. These analyses can include simulations, as illustrated in Appendices C and D, to explore the impact of unmeasured confounding, model misspecification, and alternative causal pathways on the study conclusions. Sensitivity analyses help to identify the conditions under which the findings hold, enhancing the credibility of the causal inferences.(For more about addressing missing data, see: [@bulbulia2024PRACTICAL].) 

#### 6. **Address missing data at baseline and study attrition**: 

Longitudinal studies often need help with missing data and attrition, which can introduce bias and affect the validity of causal inferences. Implement and report strategies for handling missing data, such as multiple imputation or sensitivity analyses that assess the bias arising from missing responses at the study's conclusion. (For more about addressing missing data, see: [@bulbulia2024PRACTICAL]). 


By following these recommendations, you can more effectively navigate the inherent limitations of observational longitudinal data, improving the quality of your causal inferences.


## Summary 

This chapter has introduced the potential outcomes framework for causal inference and using directed acyclic graphs (DAGs) in environmental psychology. In [**Part 1**](#section-part1) we discussed three critical assumptions necessary for estimating average treatment effects from data:

1. **Conditional Exchangeability**: This assumption posits that treatment allocation is randomised and independent of potential outcomes, conditional on measured covariates.
2. **Causal Consistency**: This assumption asserts that the outcome observed under the treatment condition corresponds to the outcome that would have been observed had the unit received the treatment, and similarly for the control condition.
3. **Positivity**: This assumption asserts that every unit has a non-zero probability of receiving any treatments under comparison.

Although randomised controlled experiments naturally satisfy these assumptions through design—randomisation ensures exchangeability, control guarantees consistency, and design secures positivity -— observational studies typically do not. To obtain consistent causal estimates from observational data, we must assess the extent to which these assumptions can be satisfied.

In [**Part 2**](#sec-part2), we explained how causal diagrams work and described their utility in addressing the assumption of conditional exchangeability, or the "no unmeasured confounders" assumption. We identified [five fundamental structures](#sec-five-elementary) underlying all causal relationships. We discovered [four elementary rules](#sec-four-rules) for evaluating the implications of conditioning on elements within these structures regarding observable statistical associations in data. Thus, causal diagrams provide a simplified visual language for translating complex causal relationships into data observations. However, the relationships in these diagrams represent assertions that are not directly verifiable from the data. The causal relationships between treatments and outcomes are the only relationships not based on assertion. Causal diagrams help us identify structural sources of bias in the statistical associations between treatments and outcomes that may arise from assumed causal relationships, potentially associating treatments with outcomes irrespective of causal links.

In [**Part 3**](#section-part3)*, we applied causal diagrams to seven common confounding scenarios, demonstrating that a causal diagram needs to highlight only those aspects of a causal setting relevant for assessing structural sources of bias linking treatment and outcome in a non-causal manner. We focused on omitting nodes and paths not directly necessary for our stated identification problem, emphasising that causal diagrams are tailored to context-dependent questions and our assumptions about the world's causal structure.

In [**Part 4**](#section-part4), we showed how investigators may create multiple causal diagrams when the structure of a causal problem is ambiguous and illustrated the benefits of this approach through data simulation. We provided guidelines for reporting in scenarios where only cross-sectional data are available or when researchers have access to repeated measures of longitudinal data.

Although this discussion has focused on seven specific applications of causal diagrams, their applicability extends much further. The straightforward rules governing how variables become associated or disassociated through conditioning on nodes within basic structures enable the use of causal diagrams for quantitatively exploring causality in myriad questions. These applications transcend effective modelling strategies and informing data collection strategies, including adopting repeated measures designs.

We hope this chapter will inspire environmental psychologists to deepen their understanding of causal inference and incorporate causal diagrams into their research practices. The methodologies for distinguishing causation from correlation are well-established; powerful tools for causal inference are accessible. There is no longer any justification for reporting associations and speculating about causes. It is within your reach to quantify magnitudes of causality conditional on assumptions encoded in your causal graphs. 


{{< pagebreak >}}

## Funding

This work is supported by a grant from the Templeton Religion Trust (TRT0418). JB received support from the Max Planck Institute for the Science of Human History.
The funders had no role in preparing the manuscript or the decision to publish it.

## Contributions

DH proposed the chapter. JB developed the approach and wrote the first draft. Both authors contributed substantially to the final work.


## References

::: {#refs}
:::


{{< pagebreak >}}

## Appendix A: Glossary {#appendix-a}


This appendix provides a glossary of common terminology in causal inference.

**Acyclic**: a causal diagram cannot contain feedback loops. More precisely, no variable can be an ancestor or descendant of itself. If variables are repeatedly measured here, it is vital to index nodes by the relative timing of the nodes.

**Adjustment set**: a collection of variables we must either condition upon or deliberately avoid conditioning upon to obtain a consistent
causal estimate for the effect of interest [@pearl2009].

**Ancestor (parent)**: a node with a direct or indirect influence on others, positioned upstream in the causal chain.

**Arrow**denotes a causal relationship linking nodes.

**Backdoor path**: a "backdoor path" between a treatment variable, $A$, and an outcome variable, $Y$, is a sequence of links in a causal diagram that starts with an arrow into $A$ and reaches $Y$ through common causes, introducing potential confounding bias such that statistical association does not reflect causality. To estimate the causal effect of $A$ on $Y$ without bias, these paths must be blocked by adjusting for confounders. The backdoor criterion guides the selection of variables for adjustment to ensure unbiased causal inference.

**Conditioning**: explicitly accounting for a variable in our statistical analysis to address the identification problem. In causal diagrams, we usually represent conditioning by drawing a box around a node of the conditioned variable, for example, $\boxed{L_{0}}\to A_{1} \to L_{2}$. We do not box exposures and outcomes because we assume they are included in a model by default. Depending on the setting, we may condition by regression stratification, inverse probability of treatment weighting, g-methods, doubly robust
machine learning algorithms, or other methods. We do not cover such methods in this tutorial; however see @hernan2023.

**Counterfactual**: a hypothetical outcome that would have occurred for the same individuals under a different treatment condition than the one they experienced.

**Direct effect**: the portion of the total effect of a treatment on an outcome that is not mediated by other variables within the causal pathway.

**Collider**: a variable in a causal diagram at which two incoming paths meet head-to-head. For example if $A \rightarrowred \boxed{L} \leftarrowred Y$, then $L$ is a collider. If we do not condition on a collider (or its descendants), the path between $A$ and $Y$ remains closed. Conditioning on a collider (or its descendants) will induce an association between $A$ and $Y$.

**Confounder**: a member of an adjustment set. Notice a variable is a 'confounder' in relation to a specific adjustment set. 'Confounder' is a
relative concept [@lash2020].

**D-separation**: in a causal diagram, a path is 'blocked' or 'd-separated' if a node along it interrupts causation. Two variables are
d-separated if all paths connecting them are blocked, making them conditionally independent. Conversely, unblocked paths result in
'd-connected' variables, implying potential dependence [@pearl1995].

**Descendant (child)**: a node directly or indirectly influenced by upstream nodes (parents).

**Effect-modifier**: a variable is an effect-modifier, or 'effect-measure modifier' if its presence changes the magnitude or direction of the effect of an exposure or treatment on an outcome across the levels or values of this variable. In other words, the impact of the exposure is different at different levels of the effect modifier. 

**External validity**: the extent to which causal inferences can be generalised to other populations, settings, or times, also called "Target Validity."

**Identification problem**: the challenge of estimating the causal effect of a variable by adjusting for measured variables on units
in a study. Causal diagrams were developed to address the identification problem by application of the rules of d-separation to a causal diagram.

**Indirect effect (mediated effect)**: The portion of the total effect transmitted through a mediator variable.

**Internal validity**: the degree to which the design and conduct of a study are likely to have prevented bias, ensuring that the causal relationship observed can be confidently attributed to the treatment and not to other factors.

**Instrumental variable**: an ancestor of the exposure but not of the outcome. An instrumental variable affects the outcome only through its effect on the exposure and not otherwise. Whereas conditioning on a variable causally associated with the outcome rather than with the exposure will generally increase modelling precision, we should refrain from conditioning on instrumental variables [@cinelli2022].  Second, when an instrumental variable is the descendant of an unmeasured confounder, we should generally condition the instrumental variable to provide a partial adjustment for a confounder.

**Mediator**: a variable that transmits the effect of the treatment variable on the outcome variable, part of the causal pathway between treatment and outcome.

**Modified Disjunctive Cause Criterion**: @vanderweele2019 recommends obtaining a maximally efficient adjustment, which he calls a 'confounder set' A member of this set is any set of variables that can reduce or remove structural sources of bias. The strategy is as follows:

a.  Control for any variable that causes the exposure, the outcome, or
    both.
b.  Control for any proxy for an unmeasured variable that is a shared
    cause of both the exposure and outcome.
c.  Define an instrumental variable as a variable associated with the
    exposure but does not influence the outcome independently, except
    through the exposure. Exclude any instrumental variable that is not
    a proxy for an unmeasured confounder from the confounder set
    [@vanderweele2019].

Note that the concept of a 'confounder set' is broader than that of an
'adjustment set.' Every adjustment set is a member of a confounder set.
Hence, the Modified Disjunctive Cause Criterion will eliminate bias when
the data permit. However, a confounder set includes variables that 
reduce bias in cases where confounding cannot be eliminated.

**Node**: characteristic or features of units in a population ('variable') represented on a causal diagram. In a causal diagram, nodes are drawn with reference to variables defomed for the target population.

**Randomisation**: The process of randomly assigning subjects to different treatments or control groups to eliminate selection bias in experimental studies.

**Reverse causation**: $\atoyassert$, but in reality $\ytoa$

**Statistical model:** a mathematical representation of the relationships between variables in which we quantify covariances and
their corresponding uncertainties in the data. Statistical models typically correspond to multiple causal structures [@pearl2018;
@vanderweele2022b; @hernan2023]. That is, the causes of such covariances cannot be identified without assumptions.

**Structural model:** defines assumptions about causal relationships. Causal diagrams graphically encode these assumptions [@hernan2023],
leaving out the assumption about whether the exposure and outcome are causally associated. Outside of randomised experiments, we cannot
compute causal effects without structural models. A structural model is needed to interpret the statistical findings in causal terms.
Structural assumptions should be developed in consultation with experts. The role of structural assumptions when interpreting statistical results needs to be better understood across many human sciences and forms the motivation for my work here.

**Time-varying confounding:** occurs when a confounder that changes over time also acts as a mediator or collider in the causal pathway between
exposure and outcome. Controlling for such a confounder can introduce bias. Not controlling for it can retain bias.

{{< pagebreak >}}


## Appendix B: Causal Consistency in observational settings {#appendix-b}

In observational research, there are typically multiple versions of treatment. The theory of causal inference under multiple versions of treatment proves we can consistently estimate causal effects where the different versions of treatment are conditionally independent of the outcomes [@vanderweele2009, @vanderweele2009; @vanderweele2013; @vanderweele2018] 

Let $\coprod$ denote independence.
Where there are $K$ different versions of treatment $A$ and no confounding for $K$'s effect on $Y$ given measured confounders $L$ such that

$$
Y(k) \coprod K | L
$$

Then it can be proved that causal consistency follows. According to the theory of causal inference under multiple versions of treatment, the measured variable $A$ functions as a "coarsened indicator" for estimating the causal effect of the multiple versions of treatment $K$ on $Y(k)$ [@vanderweele2009; @vanderweele2013; @vanderweele2018].  

In the context of green spaces, let $A$ represent the general action of moving closer to any green space and $K$ represent the different versions of this treatment. For instance, $K$ could denote moving closer to different green spaces such as parks, forests, community gardens, or green spaces with varying amenities and features.

Here, the conditional independence implies that, given measured confounders $L$ (e.g. socioeconomic status, age, personal values), the type of green space one moves closer to ($K$) is independent of the outcomes $Y(k)$ (e.g. mental well-being under the $K$ conditions). In other words, the version of green space one chooses to live near does not affect the $K$ potential outcomes, provided the confounders $L$ are appropriately controlled for in our statistical models.

Put simply, strategies for confounding control and consistently estimating causal effects when multiple versions of treatment converge. However, the quantities we estimate under multiple treatment versions might need clearer interpretations.  For example, we cannot readily determine which of the many treatment versions is most causally efficacious and which lack any causal effect or are harmful.  

{{< pagebreak >}}

### Appendix C: Simulating Cross-Sectional Data to Compute the Average Treatment Effect When Conditioning on a Mediator {#appendix-c}

This appendix outlines a simulation designed to demonstrate the potential pitfalls of conditioning on a mediator in cross-sectional analyses. The simulation explores the scenario where the effect of access to green space ($A$) on happiness ($Y$) is fully mediated by exercise ($L$). This setup aims to illustrate how incorrect assumptions about the role of a variable (mediator vs. confounder) can lead to misleading estimates of the Average Treatment Effect (ATE).

#### Methodology

1. **Data Generation**: We simulate a dataset for 1,000 individuals, where access to green space ($A$) influences exercise ($L$), which in turn affects happiness ($Y$$). The simulation is based on predefined parameters that establish L as a mediator between $A$ and $Y$.

2. **Parameter Definitions**:
   - The probability of access to green space ($A$) is set at 0.5.
   - The effect of $A$ on $L$ (exercise) is quantified by $\beta = 2$.
   - The effect of $L$ on $Y$ (happiness) is quantified by $\delta = 1.5$.
   - Standard deviations for $L$ and $Y$ are set at 1 and 1.5, respectively.

3. **Model Specifications**:
   - **Model 1** (Correct Assumption): fits a linear regression model assuming $L$ as a mediator, including both $A$ and $L$ as regressors on $Y$. This model aligns with the data generating process and correctly identifies L as a mediator.
   - **Model 2** (Incorrect Assumption): fits a linear regression model including only $A$ as a regressors on $Y$, omitting the mediator $L$. This model assesses the direct effect of A on Y without accounting for mediation.

4. **Analysis and Comparison**: The analysis compares the estimated effects of $A$ on $Y$ under both model specifications. By including $L$ as a predictor in Model 1, we account for its mediating role, whereas Model 2 overlooks this aspect by excluding $L$ from the analysis.

5. **Presentation**: The results are displayed in a comparative table formatted for publication. The table contrasts the regression coefficients and significance levels obtained under each model, highlighting the impact of correctly versus incorrectly assuming the role of $L$ in the relationship between $A$ and $Y$.


```{r}
#| label: simulation_cross_sectional
#| tbl-cap: "Code for a simulation of a data generating process in which the effect of excercise (L) fully mediates the effect of greenspace (A) on happiness (Y)."
#| out-width: 80%
#| echo: true
#| eval: false

# load libraries
!require(kableExtra)){install.packages("kableExtra")} # tables
if(!require(gtsummary)){install.packages("gtsummary")} # tables

# simulation seed
set.seed(123) #  reproducibility

# define the parameters 
n = 1000 # Number of observations
p = 0.5  # Probability of A = 1 (access to greenspace)
alpha = 0 # Intercept for L (excercise)
beta = 2  # Effect of A on L 
gamma = 1 # Intercept for Y 
delta = 1.5 # Effect of L on Y
sigma_L = 1 # Standard deviation of L
sigma_Y = 1.5 # Standard deviation of Y

# simulate the data: fully mediated effect by L
A = rbinom(n, 1, p) # binary exposure variable
L = alpha + beta*A + rnorm(n, 0, sigma_L) # mediator L affect by A
Y = gamma + delta*L + rnorm(n, 0, sigma_Y) # Y affected only by L,

# make the data frame
data = data.frame(A = A, L = L, Y = Y)

# fit regression in which L is assume to be a mediator
# (cross-sectional data is consistent with this model)
fit_1 <- lm( Y ~ A + L, data = data)

# fit regression in which L is assume to be a mediator
# (cross-sectional data is also consistent with this model)
fit_2 <- lm( Y ~ A, data = data)

# create gtsummary tables for each regression model
table1 <- gtsummary::tbl_regression(fit_1)
table2 <- gtsummary::tbl_regression(fit_2)

# merge the tables for comparison
table_comparison <- gtsummary::tbl_merge(
  list(table1, table2),
  tab_spanner = c("Model: Exercise assumed confounder", 
                  "Model: Exercise assumed to be a mediator")
)
# make latex table (for publication)
markdown_table_0 <- as_kable_extra(table_comparison, 
                                   format = "latex", 
                                   booktabs = TRUE)
# print latex table (note, you might prefer "markdown" or another format)                                
markdown_table_0
```

The following code snippet is designed to estimate the Average Treatment Effect (ATE) using the `clarify` package in R, which is referenced here as [@greifer2023]. The procedure involves two primary steps: simulating coefficient distributions for regression models and then calculating the ATE based on these simulations. This process is applied to two distinct models to demonstrate the effects of including versus excluding a mediator variable in the analysis.


### Steps to Estimate the ATE

1. **Load the `clarify` Package**: This package provides functions to simulate regression coefficients and compute average marginal effects (AME), robustly facilitating the estimation of ATE.

2. **Set seed**: `set.seed(123)` ensures that the results of the simulations are reproducible, allowing for consistent outcomes across different runs of the code.

3. **Simulate the data distribution**:
   - `sim_coefs_fit_1` and `sim_coefs_fit_2` are generated using the `sim` function from the `clarify` package, applied to two fitted models (`fit_1` and `fit_2`). These functions simulate the distribution of coefficients based on the specified models, capturing the uncertainty around the estimated parameters.

4. **Calculate ATE**:
   - For both models, the `sim_ame` function calculates the ATE as the marginal risk difference (RD) when the treatment variable (`A`) is present (`A == 1`). This function uses the simulated coefficients to estimate the treatment effect across the simulated distributions, providing a comprehensive view of the ATE under each model.
   - To streamline the output, the function is set to verbose mode off (`verbose = FALSE`).

5. **Results**:
   - Summaries of these estimates (`summary_sim_est_fit_1` and `summary_sim_est_fit_2`) are obtained, providing detailed statistics including the estimated ATE and its 95% confidence intervals (CI).

6. **Presentation: report ATE and CIs**:
   - Using the `glue` package, the ATE along with its 95% CIs for both models, is formatted into a string for easy reporting. This step transforms the statistical output into a more interpretable form, highlighting the estimated treatment effect and its precision.


```{r}
#| label: ate-sim-crosstwo
#| tbl-cap: "Code for calculating the average treatment effect as contrasts between simulated outcomes for the entire population."
#| echo: true
#| eval: false

# use `clarify` package to obtain ATE
if(!require(clarify)){install.packages("clarify")} # clarify package
# simulate fit 1 ATE
set.seed(123)
sim_coefs_fit_1 <- sim(fit_1)
sim_coefs_fit_2 <- sim(fit_2)

# marginal risk difference ATE, simulation-based: model 1 (L is a confounder)
sim_est_fit_1 <-
  sim_ame(
    sim_coefs_fit_1,
    var = "A",
    subset = A == 1,
    contrast = "RD",
    verbose = FALSE
  )
# marginal risk difference ATE, simulation-based: model 2 (L is a mediator)
sim_est_fit_2 <-
  sim_ame(
    sim_coefs_fit_2,
    var = "A",
    subset = A == 1,
    contrast = "RD",
    verbose = FALSE
  )
# obtain summaries
summary_sim_est_fit_1 <- summary(sim_est_fit_1, null = c(`RD` = 0))
summary_sim_est_fit_2 <- summary(sim_est_fit_2, null = c(`RD` = 0))

# reporting 
# ate for fit 1, with 95% CI
ATE_fit_1 <- glue::glue(
  "ATE =
                        {round(summary_sim_est_fit_1[3, 1], 2)},
                        CI = [{round(summary_sim_est_fit_1[3, 2], 2)},
                        {round(summary_sim_est_fit_1[3, 3], 2)}]"
)
# ate for fit 2, with 95% CI
ATE_fit_2 <-
  glue::glue(
    "ATE = {round(summary_sim_est_fit_2[3, 1], 2)},
                        CI = [{round(summary_sim_est_fit_2[3, 2], 2)},
                        {round(summary_sim_est_fit_2[3, 3], 2)}]"
  )
```



### Upshot of the Simulation and Analysis

- **Model 1 (L as a Confounder)**: This analysis assumes that `L` is a confounder in the relationship between the treatment (`A`) and the outcome (`Y`), and thus, it includes `L` in the model. The ATE estimated here reflects the effect of `A` while controlling for `L`.

- **Model 2 (L as a Mediator)**: In contrast, this analysis considers `L` to be a mediator, and the model either includes `L` explicitly in its estimation process or excludes it to examine the direct effect of `A` on `Y`. The approach to mediation analysis here is crucial as it influences the interpretation of the ATE.

By comparing the ATEs from both models, researchers can understand the impact of mediation (or the lack thereof) on the estimated treatment effect. This comparison sheds light on how assumptions about variable roles (confounder vs. mediator) can significantly alter causal inferences drawn from cross-sectional data.

**Wherever it is uncertain whether a variable is a confounder or a mediator, we suggest creating two causal diagrams and reporting both analyses**

{{< pagebreak >}}

## Appendix D: Simulation of Different Confounding Control Strategies {#appendix-d}

This appendix outlines the methodology and results of a data simulation designed to compare different strategies for controlling confounding in the context of environmental psychology research. Specifically, the simulation examines the effect of access to open green spaces (treatment, $A_1$) on happiness (outcome, $Y_2$), while addressing the challenge of unmeasured confounding. The simulation incorporates baseline measures of exposure and outcome ($A_0$, $Y_0$), baseline confounders ($L_0$), and an unmeasured confounder ($U$), to evaluate the effectiveness of different analytical approaches.

###  Methodology

1.**Load Libraries `kableExtra`, `gtsummary`, and `grf`.**

1. **Target**: we simulate data for 10,000 individuals, including baseline exposure to green spaces ($A_0$), baseline happiness ($Y_0$), baseline confounders ($L_0$), and an unmeasured confounder ($U$). The simulation uses a logistic model for treatment assignment and a linear model for the continuous outcome, incorporating interactions to assess how baseline characteristics modify the treatment effect.

2. **Set seed and simulate the data distribution**:
   - Treatment assignment coefficients: $\beta_{A0} = 0.25$, $\beta_{Y0} = 0.3$, $\beta_{L0} = 0.2$, and $\beta_{U} = 0.1$.
   - Outcome model coefficients: $\delta_{A1} = 0.3$, $\delta_{Y0} = 0.9$, $\delta_{A0} = 0.1$, $\delta_{L0} = 0.3$, with an interaction effect ($\theta_{A0Y0L0} = 0.5$) indicating the combined influence of baseline exposure, outcome, and confounders on the follow-up outcome.

3. **Model comparison**:
   - **No control model**: estimates the effect of $A_1$ on $Y_2$ without controlling for any confounders.
   - **Standard covariate control model**: controls for baseline confounders ($L_0$) alongside treatment ($A_1$).
   - **Baseline exposure and outcome model**: extends the standard model by including baseline treatment and outcome ($A_0$, $Y_0$) and their interaction with $L_0$.

4. **Results**: each model's effectiveness in estimating the true treatment effect is assessed by comparing regression outputs. The simulation evaluates how well each model addresses the bias introduced by unmeasured confounding and the role of baseline characteristics in modifying treatment effects.

5. **Presentation**: the results are synthesised in a comparative table, formatted using the `kableExtra` {@zhu2021KableExtra] and `gtsummary` packages [@gtsummary2021], highlighting the estimated treatment effects and their statistical significance across models.

Overall, we use the simulation to illustrate the importance of incorporating baseline characteristics and their interactions to mitigate the influence of unmeasured confounding. 

Here is the simulation and modelling code:

```{r}
#| label: fig-codelg-appendix
#| echo: true
#| eval: false
library(kableExtra)
if(!require(kableExtra)){install.packages("kableExtra")} # causal forest
if(!require(gtsummary)){install.packages("gtsummary")} # causal forest
if(!require(grf)){install.packages("grf")} # causal forest

# r_texmf()eproducibility
set.seed(123) 

# set number of observations
n <- 10000 

# baseline covariates
U <- rnorm(n) # Unmeasured confounder
A_0 <- rbinom(n, 1, prob = plogis(U)) # Baseline exposure
Y_0 <- rnorm(n, mean = U, sd = 1) # Baseline outcome
L_0 <- rnorm(n, mean = U, sd = 1) # Baseline confounders

# coefficients for treatment assignment
beta_A0 = 0.25
beta_Y0 = 0.3
beta_L0 = 0.2
beta_U = 0.1

# simulate treatment assignment
A_1 <- rbinom(n, 1, prob = plogis(-0.5 + 
                                    beta_A0 * A_0 +
                                    beta_Y0 * Y_0 + 
                                    beta_L0 * L_0 + 
                                    beta_U * U))
# coefficients for continuous outcome
delta_A1 = 0.3
delta_Y0 = 0.9
delta_A0 = 0.1
delta_L0 = 0.3
theta_A0Y0L0 = 0.5 # Interaction effect between A_1 and L_0
delta_U = 0.05
# simulate continuous outcome including interaction
Y_2 <- rnorm(n,
             mean = 0 +
               delta_A1 * A_1 + 
               delta_Y0 * Y_0 + 
               delta_A0 * A_0 + 
               delta_L0 * L_0 + 
               theta_A0Y0L0 * Y_0 * 
               A_0 * L_0 + 
               delta_U * U,
             sd = .5)
# assemble data frame
data <- data.frame(Y_2, A_0, A_1, L_0, Y_0, U)

# model: no control
fit_no_control <- lm(Y_2 ~ A_1, data = data)

# model: standard covariate control
fit_standard <- lm(Y_2 ~ A_1 + L_0, data = data)

# model: interaction with baseline confounders, and baseline outcome and exposure
fit_interaction  <- lm(Y_2 ~ A_1 * (L_0 + A_0 + Y_0), data = data)

# create gtsummary tables for each regression model
tbl_fit_no_control<- tbl_regression(fit_no_control)  
tbl_fit_standard <- tbl_regression(fit_standard)
tbl_fit_interaction <- tbl_regression(fit_interaction)

# get only the treatment variable
tbl_list_modified <- lapply(list(
  tbl_fit_no_control,
  tbl_fit_standard,
  tbl_fit_interaction),
function(tbl) {
  tbl %>%
    modify_table_body(~ .x %>% dplyr::filter(variable == "A_1"))
})
# merge tables
table_comparison <- tbl_merge(
  tbls = tbl_list_modified,
  tab_spanner = c(
    "No Control",
    "Standard",
    "Interaction")
) |>
  modify_table_styling(
    column = c(p.value_1, p.value_2, p.value_3),
    hide = TRUE
  )
# latex table for publication
markdown_table <-
  as_kable_extra(table_comparison, format = "latex", booktabs = TRUE) |>
  kable_styling(latex_options = "scale_down")
print(markdown_table)
```

Next, in the following code snippet, we calculate the Average Treatment Effect (ATE) using simulation-based approaches for two distinct models: one with standard covariate control and another incorporating interaction. This approach leverages the `clarify` package in R, which facilitates the simulation and interpretation of estimated coefficients from linear models to derive ATEs under different modelling assumptions [@greifer2023].

First, we use the `sim` function from the `clarify` package to generate simulated coefficient distributions for the standard model (`fit_standard`) and the interaction model (`fit_interaction`). This step is crucial for capturing the uncertainty in our estimates arising from sampling variability.

Next, we employ the `sim_ame` function for each model to compute the average marginal effects (AME), focusing on the treatment variable (`A_1`). The calculation is done under the assumption that all individuals are treated (i.e., `A_1 == 1`), and we specify the contrast type as "RD" (Risk Difference) to directly obtain the ATE (Average Treatment Effect). The `sim_ame` function simulates the treatment effect across the distribution of simulated coefficients, providing a robust estimate of the ATE and its variability.

The summaries of these simulations (`summary_sim_est_fit_std` and `summary_sim_est_fit_int`) are then extracted to provide concise estimates of the ATE along with 95% confidence intervals (CIs) for both the standard and interaction models. This step is essential for understanding the magnitude and precision of the treatment effects estimated by the models.

Finally, we use the `glue` package to format these estimates into a human-readable form, presenting the ATE and its corresponding 95% CIs for each model. This presentation facilitates clear communication of the estimated treatment effects, allowing for direct comparison between the models and highlighting the impact of including baseline characteristics and their interactions on estimating the ATE [@hester2022GLUE].

This simulation-based approach to estimating the ATE underscores the importance of considering model complexity and the roles of confounders and mediators in causal inference analyses. By comparing the ATE estimates from different models, we can assess the sensitivity of our causal conclusions to various assumptions and modelling strategies.


```{r}
#| label: lst-atesimppendix
#| lst-cap: "Code."
#| out-width: 100%
#| tbl-cap: "Code for calculating the average treatment effect."
#| echo: true
#| eval: false

#| label: ate-sim-long
#| tbl-cap: "Code for calculating the average treatment effect."
#| echo: false
#| eval: true

# use `clarify` package to obtain ATE
if(!require(clarify)){install.packages("clarify")} # clarify package

# simulate fit 1 ATE
set.seed(123)
sim_coefs_fit_no_control<- sim(fit_no_control)  
sim_coefs_fit_std <- sim(fit_standard)
sim_coefs_fit_int <- sim(fit_interaction)

# marginal risk difference ATE, no controls
sim_est_fit_no_control <-
  sim_ame(
    sim_coefs_fit_no_control,
    var = "A_1",
    subset = A_1 == 1,
    contrast = "RD",
    verbose = FALSE
  )
# marginal risk difference ATE, simulation-based: model 1 (L is a confounder)
sim_est_fit_std <-
  sim_ame(
    sim_coefs_fit_std,
    var = "A_1",
    subset = A_1 == 1,
    contrast = "RD",
    verbose = FALSE
  )
# marginal risk difference ATE, simulation-based: model 2 (L is a mediator)
sim_est_fit_int <-
  sim_ame(
    sim_coefs_fit_int,
    var = "A_1",
    subset = A_1 == 1,
    contrast = "RD",
    verbose = FALSE
  )
# obtain summaries
summary_sim_coefs_fit_no_control <-
  summary(sim_est_fit_no_control, null = c(`RD` = 0))
summary_sim_est_fit_std <-
  summary(sim_est_fit_std, null = c(`RD` = 0))
summary_sim_est_fit_int <-
  summary(sim_est_fit_int, null = c(`RD` = 0))

# get coefficients for reporting
# ate for fit 1, with 95% CI
ATE_fit_no_control  <- glue::glue(
  "ATE = {round(summary_sim_coefs_fit_no_control[3, 1], 2)}, 
  CI = [{round(summary_sim_coefs_fit_no_control[3, 2], 2)},
  {round(summary_sim_coefs_fit_no_control[3, 3], 2)}]"
)
# ate for fit 2, with 95% CI
ATE_fit_std <- glue::glue(
  "ATE = {round(summary_sim_est_fit_std[3, 1], 2)}, 
  CI = [{round(summary_sim_est_fit_std[3, 2], 2)},
  {round(summary_sim_est_fit_std[3, 3], 2)}]"
)
# ate for fit 3, with 95% CI
ATE_fit_int <-
  glue::glue(
    "ATE = {round(summary_sim_est_fit_int[3, 1], 2)},
    CI = [{round(summary_sim_est_fit_int[3, 2], 2)},
    {round(summary_sim_est_fit_int[3, 3], 2)}]"
  )
# coefs they used in manuscript
```


Using the `clarify` package, we infer the ATE for the standard model is `r ATE_fit_std`.

Using the `clarify` package, we infer the ATE for the model that conditions on the baseline exposure and baseline outcome to be:  `r ATE_fit_int`, which is close to the values supplied to the data-generating mechanism. 

**Take-home message:** 
The baseline exposure and baseline outcome are often the most important variables to include for confounding control. The baseline exposure also allows us to estimate an incident-exposure effect. For this reason we should endeavour to obtain at least three waves of data such that these variables and other baseline confounders are included in at time 0, the exposure is included at time 1, and the outcome is included at time 2. 

{{< pagebreak >}}


## Appendix E: Non-parametric Estimation of Average Treatment Effects Using Causal Forests {#appendix-causal-forests} {#appendix-e}

This appendix provides a practical example of estimating average treatment effects (ATE) using a non-parametric approach, specifically through applying causal forests. Unlike traditional regression models, causal forests allow for estimating treatment effects without imposing strict assumptions about the form of the relationship between treatment, covariates, and outcomes. This flexibility makes them particularly useful for analysing complex datasets where the treatment effect may vary across observations.

#### Causal Forest Model Implementation

1. **Libraries**: the implementation begins with loading the necessary R libraries: `grf` for estimating conditional and average treatment effects using causal forests and `glue` for formatting the results for reporting.

2. 1. **Data generation**: the code assumes the presence of a data frame `data` generated from the previous code snippet containing the variables:
   - `A_1`: Treatment indicator.
   - `L_0`: A covariate.
   - `Y_2`: Outcome of interest.
   - `A_0` and `Y_0`: Baseline exposure and outcome, respectively. 
   
   Treatment (`W`) and outcome (`Y`) vectors are extracted from `data`, alongside a matrix `X` that includes covariates and baseline characteristics.

3. **Causal Forest model**: a causal forest model is fitted using the `causal_forest` function from the `grf` package [@grf2024]. This function takes the covariate matrix `X`, the outcome vector `Y`, and the treatment vector `W` as inputs, and it returns a model object that can be used for further analysis.

4. **Average Treatment Effect estimation**: the `average_treatment_effect` function computes the ATE from the fitted causal forest model. This step is crucial as it quantifies the overall impact of the treatment across the population, adjusting for covariates included in the model.

5. **Reporting**: The estimated ATE and its standard error (se) are extracted and formatted for reporting using the `glue` package [@hester2022GLUE]. This facilitates clear communication of the results, showing the estimated effect size and its uncertainty.

#### Key Takeaways

- **Flexibility and robustness**: causal forests offer a robust way to estimate treatment effects without making parametric solid assumptions. This approach is particularly advantageous in settings where the treatment effect may vary with covariates or across different subpopulations and we need to know the true functional form of the data-generating mechanism.

- **ATE estimation**: the model estimates the ATE as the difference in expected outcomes between treated and untreated units, averaged across the population. This estimate reflects the overall effect of the treatment, accounting for the distribution of covariates in the sample. 

- **Convergence to true value**: we find that the estimated ATE by the causal forest model converges to the actual value used in the data-generating process (assumed to be 0.3). This demonstrates the effectiveness of causal forests in uncovering the true treatment effect from complex data.

This example underscores the utility of semi-parametric and non-parametric methods, such as causal forests, in causal inference analyses.

```{r}
#| label: causal_forest
#| echo: true

# load causal forest library 
library(grf) # estimate conditional and average treatment effects
library(glue) # reporting 

#  'data' is our data frame with columns 'A_1' for treatment, 'L_0' for a covariate, and 'Y_2' for the outcome
#  we also have the baseline exposure 'A_0' and 'Y_0'
#  ensure W (treatment) and Y (outcome) are vectors
W <- as.matrix(data$A_1)  # Treatment
Y <- as.matrix(data$Y_2)  # Outcome
X <- as.matrix(data[, c("L_0", "A_0", "Y_0")])

# fit causal forest model 
fit_causal_forest <- causal_forest(X, Y, W)

# estimate the average treatment effect (ATE)
ate <- average_treatment_effect(fit_causal_forest)

# make data frame for reporting using "glue' 
ate<- data.frame(ate)

# obtain ate for report
ATE_fit_causal_forest <-
  glue::glue(
    "ATE = {round(ate[1, 1], 2)}, se = {round(ate[2, 1], 2)}"
  )
```

Causal forest estimates the average treatment effect as `r ATE_fit_causal_forest`. This approach converges to the true value supplied to the generating mechanism of 0.3



