% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames,x11names}{xcolor}
%
\documentclass[
  single column]{article}

\usepackage{amsmath,amssymb}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage[]{libertinus}
\ifPDFTeX\else  
    % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\usepackage[top=30mm,left=25mm,heightrounded,headsep=22pt,headheight=11pt,footskip=33pt,ignorehead,ignorefoot]{geometry}
\setlength{\emergencystretch}{3em} % prevent overfull lines
\setcounter{secnumdepth}{-\maxdimen} % remove section numbering
% Make \paragraph and \subparagraph free-standing
\makeatletter
\ifx\paragraph\undefined\else
  \let\oldparagraph\paragraph
  \renewcommand{\paragraph}{
    \@ifstar
      \xxxParagraphStar
      \xxxParagraphNoStar
  }
  \newcommand{\xxxParagraphStar}[1]{\oldparagraph*{#1}\mbox{}}
  \newcommand{\xxxParagraphNoStar}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
  \let\oldsubparagraph\subparagraph
  \renewcommand{\subparagraph}{
    \@ifstar
      \xxxSubParagraphStar
      \xxxSubParagraphNoStar
  }
  \newcommand{\xxxSubParagraphStar}[1]{\oldsubparagraph*{#1}\mbox{}}
  \newcommand{\xxxSubParagraphNoStar}[1]{\oldsubparagraph{#1}\mbox{}}
\fi
\makeatother


\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
% definitions for citeproc citations
\NewDocumentCommand\citeproctext{}{}
\NewDocumentCommand\citeproc{mm}{%
  \begingroup\def\citeproctext{#2}\cite{#1}\endgroup}
\makeatletter
 % allow citations to break across lines
 \let\@cite@ofmt\@firstofone
 % avoid brackets around text for \cite:
 \def\@biblabel#1{}
 \def\@cite#1#2{{#1\if@tempswa , #2\fi}}
\makeatother
\newlength{\cslhangindent}
\setlength{\cslhangindent}{1.5em}
\newlength{\csllabelwidth}
\setlength{\csllabelwidth}{3em}
\newenvironment{CSLReferences}[2] % #1 hanging-indent, #2 entry-spacing
 {\begin{list}{}{%
  \setlength{\itemindent}{0pt}
  \setlength{\leftmargin}{0pt}
  \setlength{\parsep}{0pt}
  % turn on hanging indent if param 1 is 1
  \ifodd #1
   \setlength{\leftmargin}{\cslhangindent}
   \setlength{\itemindent}{-1\cslhangindent}
  \fi
  % set entry spacing
  \setlength{\itemsep}{#2\baselineskip}}}
 {\end{list}}
\usepackage{calc}
\newcommand{\CSLBlock}[1]{\hfill\break\parbox[t]{\linewidth}{\strut\ignorespaces#1\strut}}
\newcommand{\CSLLeftMargin}[1]{\parbox[t]{\csllabelwidth}{\strut#1\strut}}
\newcommand{\CSLRightInline}[1]{\parbox[t]{\linewidth - \csllabelwidth}{\strut#1\strut}}
\newcommand{\CSLIndent}[1]{\hspace{\cslhangindent}#1}

\usepackage{booktabs}
\usepackage{longtable}
\usepackage{array}
\usepackage{multirow}
\usepackage{wrapfig}
\usepackage{float}
\usepackage{colortbl}
\usepackage{pdflscape}
\usepackage{tabu}
\usepackage{threeparttable}
\usepackage{threeparttablex}
\usepackage[normalem]{ulem}
\usepackage{makecell}
\usepackage{xcolor}
\input{/Users/joseph/GIT/latex/latex-for-quarto.tex}
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\AtBeginDocument{%
\ifdefined\contentsname
  \renewcommand*\contentsname{Table of contents}
\else
  \newcommand\contentsname{Table of contents}
\fi
\ifdefined\listfigurename
  \renewcommand*\listfigurename{List of Figures}
\else
  \newcommand\listfigurename{List of Figures}
\fi
\ifdefined\listtablename
  \renewcommand*\listtablename{List of Tables}
\else
  \newcommand\listtablename{List of Tables}
\fi
\ifdefined\figurename
  \renewcommand*\figurename{Figure}
\else
  \newcommand\figurename{Figure}
\fi
\ifdefined\tablename
  \renewcommand*\tablename{Table}
\else
  \newcommand\tablename{Table}
\fi
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Listing}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}
\makeatother
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\@ifpackageloaded{subcaption}{}{\usepackage{subcaption}}
\makeatother
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\usepackage{bookmark}

\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same} % disable monospaced font for URLs
\hypersetup{
  pdftitle={The Weirdest Causal Inferences in the World},
  pdfauthor={Joseph A. Bulbulia},
  colorlinks=true,
  linkcolor={blue},
  filecolor={Maroon},
  citecolor={Blue},
  urlcolor={Blue},
  pdfcreator={LaTeX via pandoc}}

\title{The Weirdest Causal Inferences in the World}

\usepackage{academicons}
\usepackage{xcolor}

  \author{Joseph A. Bulbulia}
            \affil{%
             \small{     Victoria University of Wellington, New Zealand
          ORCID \textcolor[HTML]{A6CE39}{\aiOrcid} ~0000-0002-5861-2056 }
              }
      


\date{2024-05-24}
\begin{document}
\maketitle
\begin{abstract}
Human scientists ask and answer questions about humans. Many of these
questions are causal. This study clarifies two failure modes in causal
inference. (1) Measurement error bias occurs when there is a discrepancy
between a variable's true value and its observed value. (2)
Sample-restriction bias arises when the association between cause and
effect in a study population does not reflect the causal association in
the target population. We use causal directed acyclic graphs (causal
DAGs) to show how these threats to valid inference relate to each other.
Our discussion addresses concerns that psycho-social datasets often draw
entirely from `Western, Educated, Industrialized, Rich, and Democratic
(WEIRD)' populations. We provide simple graphical tools to help
investigators evaluate when sample restriction is a feature and when it
is a bug.

\textbf{KEYWORDS}: \emph{Causal Inference}; \emph{Comparative};
\emph{Cross-Cultural}; \emph{DAGs};* \emph{Evolution},
\emph{Experiments}; *Measurement Error**; \emph{Selection Bias};
\end{abstract}

\subsection{Introduction}\label{introduction}

Human scientists ask and answer questions. To anchor answers in facts,
we collect data.

Most human scientists work in what Joseph Henrich, Steven Heine, and Ara
Norenzayan have termed `WEIRD' societies: `Western, Educated,
Industrialized, Rich, and Democratic Societies'
(\citeproc{ref-henrich2010weirdest}{Henrich \emph{et al.} 2010}).
Unsurprisingly, WEIRD samples are over-represented in human science
datasets (\citeproc{ref-arnett2008neglected}{Arnett 2008};
\citeproc{ref-sears1986college}{Sears 1986}). Henrich \emph{et al.}
(\citeproc{ref-henrich2010weirdest}{2010}) illustrate how WEIRD samples
differ from non-WEIRD samples in areas such as spatial cognition and
perceptions of fairness, while showing continuities in basic emotions
recognition, positive self-views, and motivation to punish anti-social
behaviour. Because science seeks generalisation wherever it can, Henrich
\emph{et al.} (\citeproc{ref-henrich2010weirdest}{2010}) urge that
sampling from non-WEIRD populations is desirable.

For certain questions, however, investigators might want to restrict
sampling further. For example, suppose investigators are interested in
the psychological effects of vasectomy on optimism. An efficient design
will restrict eligibility to those who have not already had vasectomy.
There might be other eligibility conditions. It might be desirable to
extend findings to the global eligible population. However, findings
sampling from the global population now need not extend to the ancestral
past. Where relevant scientific knowledge permits, it may be credible to
generalise with sample weights. Experimental designs may impose nested
or sequential restrictions, benefiting efficiency or ethics.

Sometimes, howeever, investigators need to restrict samples. For
instance, studying the psychological effects of vasectomy on optimism
would efficiently restrict eligibility to those who have not had a
vasectomy. Other eligibility conditions might apply. Extending findings
to the global eligible population can be desirable, but such findings
might not extend to the ancestral past. Where relevant scientific
knowledge allows, generalisation with sample weights may be credible.
Experimental designs may impose nested or sequential restrictions for
efficiency or ethics.

When is restriction desirable and when not? It depends on the question.
Here we assume the questions are causal.

\textbf{Part 1} uses causal diagrams to clarify five structural features
of measurement-error bias. Understanding measurement error bias is
essential in all research. In comparative research, we must distinguish
threats arising from measurement error bias, sample restriction
independently of measurement error bias, and threats combining both
biases.

\textbf{Part 2} introduces the concept of `target validity', focusing on
threats arising from censoring bias. Unlike measurement error bias,
which threatens validity irrespective of sampling restriction bias,
sample restriction bias is relative to a target population. We focus on
\textbf{censoring bias}, a form of sample restriction occurring within a
study.

\textbf{Part 3} considers target validity when there is a mismatch
between the sample population at baseline and the target population.
This threat to target validity is often termed `external validity'. In
comparative research, we assume populations are drawn from a larger
superpopulation. We evaluate the following threats to valid inference,
imagining a comparison between ``WEIRD'' and ``NOT-WEIRD'' groups:

\begin{itemize}
\tightlist
\item
  Mismatch between sample populations and the target stratum population.
\item
  Measurement error: If measurements work differently within strata of
  the superpopulation, comparative results will be invalid.
  Understanding measurement error bias clarifies the first failure mode:
  findings do not generalise because they are invalid for the
  comparative superpopulation.
\item
  Variation in effect modifiers between the sample populations to be
  compared.
\end{itemize}

We begin with a brief overview of causal inference and causal diagrams.

\subsubsection{Background: what is
causality}\label{background-what-is-causality}

To quantify a causal effect we must contrast the world as it has been
realised -- which is, in principle, observable -- with the world as it
might have been otherwise -- which is, in principle, not observable.

Consider a binary treatment variable \(A \in \{0,1\}\) representing the
randomised administration of a vaccine for individuals \(i\) in the set
\(\{1, 2, \ldots, n\}\). \(A_i = 1\) denotes administration in the
vaccine condition and \(A_i = 0\) denotes administration in the control
condition. The potential outcomes for each individual are denoted as
\(Y_i(0)\) and \(Y_i(1)\). These are the outcomes that are yet to be
realised before administration. For this reason they are called
`potential' or `counterfactual' outcomes. For an individual \(i\) we can
define a causal effect as a contrast between the outcome as it would
have been observed in response to one level of an intervention and the
outcome as it would have been observed in response to another level of
the intervention. Such a contrast for the \(i^{th}\) individual can be
expressed on the difference scale as:

\[
\delta_i = Y_i(1) - Y_i(0)
\]

where \(\delta_i\) defines the difference in respect of some predefined
measure \(Y\) in a scenario in which the treatment is received compared
with a scenario in which the treatment is not received, and
\(\delta_i \neq 0\) denotes a causal effect of \(A\) on \(Y\) for unit
\(i\). Similarly \(\delta_i = \frac{Y_i(1)}{Y(0)}\neq 1\) denotes a
causal effect of treatment \(A\) for unit \(i\) on the risk-ratio scale.
For any unit \(i\) these quantities cannot be computed from
observational data.

Suppose Alice is given vaccine: \(A_{\text{Alice}} = 1\). If we assume
the realised outcome \(Y_{Alice}| A = 1\) is equal to the counterfactual
outcome \(Y_{Alice}(1)\) then for Alice \(Y_{Alice}(1)\) is observed but
\(Y_{Alice}(0)\) remains counterfactual, and missing. Similarly, if Bob
is not given vaccine, for Bob \(Y_{Bob}(0)\) is observed but
\(Y_{Bob}(1)\) is not. That we cannot observe individual level causal
effects is called the \emph{Fundamental Problem of Causal Inference}
(\citeproc{ref-holland1986}{Holland 1986};
\citeproc{ref-rubin1976}{Rubin 1976}). This problem has long puzzled
philosophers(\citeproc{ref-hume1902}{Hume 1902};
\citeproc{ref-lewis1973}{Lewis 1973}), However, although individual
causal effects are generally unobservable, we may sometimes recover
average causal effects by treatment group.

\subsubsection{How we obtain average causal effect estimates from
ideally conducted randomised
experiments}\label{how-we-obtain-average-causal-effect-estimates-from-ideally-conducted-randomised-experiments}

The Average Treatment Effect (ATE), \(\Delta_{ATE}\), measures the
difference in outcomes between treated and control groups such that,

\[
\Delta_{ATE} = \mathbb{E}[Y(1)] - \mathbb{E}[Y(0)]
\]

where \(\mathbb{E}[Y(1)]\) and \(\mathbb{E}[Y(0)]\) are the average
outcomes in the treatment and control groups, respectively.

In a randomised experiment, we estimate \(\Delta_{ATE}\) by considering
both observed and unobserved outcomes:

\[
\text{ATE} = \left(\mathbb{E}[Y(1)|A = 1] + \mathbb{E}[Y(1)|A = 0]\right) - \left(\mathbb{E}[Y(0)|A = 0] + \mathbb{E}[Y(0)|A = 1]\right)
\]

Effective randomisation ensures that potential outcomes are similarly
distributed across both groups. Therefore, the average outcomes can be
expected to be equal across different treatment conditions:

\[
\widehat{\mathbb{E}}[Y(0) | A = 1] = \widehat{\mathbb{E}}[Y(0) | A = 0], \quad \widehat{\mathbb{E}}[Y(1) | A = 1] = \widehat{\mathbb{E}}[Y(1) | A = 0]
\]

This provides an unbiased estimate of the Average Treatment Effect
\(\widehat{\text{ATE}}\),

\[
\widehat{\text{ATE}} = \widehat{\mathbb{E}}[Y | A = 1] - \widehat{\mathbb{E}}[Y | A = 0]
\]

Randomised controlled experiments are powerful because they evenly
distribute potential explanatory factors across treatment groups. We
cannot observe Alice or Bob's individual causal effects. However, for
the groups to which they have been randomised, we recover average causal
effects by a Sherlock-Holmes process of inference by elimination. In an
ideally conducted randomised experiment, randomisation rules out every
explanation for treatment group differences except the treatment.

Note that in the context of our imagined experiment
\(\widehat{\text{ATE}}\) applies to the population from which the
experimental participants were drawn as calculated on the difference
scale. An more explict notation would define this effect estimate by
referencing it's scale and population:
\(\widehat{\text{ATE}}^{a'-a}_{\text{S}}\), where \(a'-a\) denotes the
difference scale, and \(S\) denotes source population. We will return to
this point in Part 2, but it is important to build intuition early that
in causal inference we must specifying a scale of contrast and
population for whom a causal effect estimate.

\subsubsection{To obtain average causal effect estimates from
observational studies requires three fundamental
assumptions}\label{to-obtain-average-causal-effect-estimates-from-observational-studies-requires-three-fundamental-assumptions}

An observational study aims to estimate the average treatment effects in
a setting where researchers do not control treatments or randomise the
treatment assignments. We may only consistently obtain and estimate the
counterfactual contrasts under strict assumptions. There are three
fundamental assumptions for obtaining from observational data the
counterfactual quantities required to compute causal contrasts.

\paragraph{Assumption 1. Causal
Consistency}\label{assumption-1.-causal-consistency}

Causal consistency states that the observed outcome for each individual
under the treatment they actually received is equal to their potential
outcome under that treatment. This means if an individual \(i\) received
treatment \(A_i = 1\), then their observed outcome \(Y_i\) is the same
as their potential outcome under treatment, denoted as \(Y_i(1)\).
Similarly, if they did not receive the treatment (\(A_i = 0\)), their
observed outcome is the same as their potential outcome without
treatment, denoted as \(Y_i(0)\), such that:

\[
Y_i = A_i \cdot Y_i(1) + (1 - A_i) \cdot Y_i(0)
\]

where:

\begin{itemize}
\tightlist
\item
  \(Y_i\) is the observed outcome for individual \(i\);
\item
  \(A_i\) is the treatment status for individual \(i\), with
  \(A_i = 1\); indicating treatment received and \(A_i = 0\) indicating
  no treatment;
\item
  \(Y_i(1)\) and \(Y_i(0)\) are the potential outcomes for individual
  \(i\) under treatment and no treatment, respectively.
\end{itemize}

The causal consistency assumption necessary to linking the theoretical
concept of potential outcomes -- the target quantities of interest --
with observable data (see Bulbulia \emph{et al.}
(\citeproc{ref-bulbulia2023a}{2023})).

\paragraph{Assumption 2. Conditional exchangeability (or
ignorability):}\label{assumption-2.-conditional-exchangeability-or-ignorability}

Conditional exchangeability states that given a set of measured
covariates \(L\), the potential outcomes are independent of the
treatment assignment. That is, once we control for \(L\), the treatment
assignment \(A\) is as good as random with respect to the potential
outcomes:

\[
Y(a) \coprod A | L
\]

where:

\begin{itemize}
\tightlist
\item
  \(Y(a)\) represents the potential outcomes for a particular treatment
  level \(a\).
\item
  \(\coprod\) denotes conditional independence.
\item
  \(A\) represents the treatment levels to be contrasted.
\item
  \(L\) represents the measured covariates.
\end{itemize}

Under the conditional exchangeability assumption, any differences in
outcomes between treatment groups can be attributed to the treatment.
Note that the conditional exchangeability assumption requires that all
confounding variables that affect both the treatment assignment \(A\)
and the potential outcomes \(Y(a)\) are measured and included in \(L\)
(For further clarification, see Appendix A).

\paragraph{Assumption 3. Positivity}\label{assumption-3.-positivity}

The positivity assumption requires that every individual in the
population has a non-zero probability of receiving each treatment level,
given their covariates. More formally,

\[
0 < Pr(A = a | L = l) < 1, \quad \forall a \in A, \, \forall l \in L \, \text{ such that } \, Pr(L = l) > 0
\]

where: - \(A\) is the treatment or exposure variable. - \(L\) is a
vector of covariates. - \(a\) and \(l\) represent specific values of
treatment and covariates, respectively.

The positivity assumption in causal inference, essential for ensuring
valid estimates of treatment effects, requires that every individual has
a non-zero chance of receiving each treatment across all covariate
patterns \(L\).\footnote{In practice, verifying this assumption faces
  two main challenges: a. \textbf{Data sparsity}: certain covariate
  combinations are rare or unobserved in the data, making it difficult
  to empirically confirm positivity for these groups. b. \textbf{Model
  dependence}: as a result of data sparsity researchers rely on
  statistical models to estimate treatment probabilities for all
  covariate patterns, but these assessments are only as reliable as the
  models used, which may be subject to misspecification or inaccuracies.}

\paragraph{Additional assumptions}\label{additional-assumptions}

There are additional practical and data assumptions for valid causal
inference (see Bulbulia \emph{et al.}
(\citeproc{ref-bulbulia2023a}{2023})). It is important to note that
these assumptions are theoretical and often challenging to verify in
practice. For example, the assumption of no unmeasured confounders
(implicit in conditional exchangeability) is particularly challenging
because it involves variables that are not observed. Note that even in
ideally conducted randomised experiments, the fundamental assumptions of
causal inference must be satisfied (see Imai \emph{et al.}
(\citeproc{ref-imai2008misunderstandings}{2008})).

\subsubsection{Fundamental Assumptions for Causal
Inference}\label{fundamental-assumptions-for-causal-inference}

We cannot generally observe individual causal effects, but we can
compute average treatment effects by aggregating individual observations
by treatment conditions. For a binary treatment, we express this as the
difference in mean outcomes by treatment condition:
\(E[Y(1)] - E[Y(0)]\) or the mean difference in outcomes by treatment
condition \(E[Y(1) - Y(0)]\). This counterfactual contrast represents
the quantity obtained for a sample population from an ideally conducted
randomised controlled trial --- an `experiment'. There are three
fundamental assumptions for computing average treatment effects:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Causal Consistency}: Treatment levels remain consistent within
  the treatment arms to be compared (implied by `control'). There must
  be at least two arms.
\item
  \textbf{(Conditional) Exchangeability}: Covariates that might affect
  outcomes under treatment are balanced across all arms (implied by
  `randomisation').
\item
  \textbf{Positivity}: Each covariate that might affect treatment in the
  target population has a non-zero probability of being observed within
  each treatment condition to be compared (implied by randomisation and
  a clearly defined target population).
\end{enumerate}

Although experiments often deviate from the ideal, potentially failing
these assumptions, in the ideal experiment, these assumptions are
satisfied. In observational or `real-world' settings, none of these
assumptions are guaranteed. Moreover, only the positivity assumption can
be verified by data.

\subsubsection{Graphical Conventions}\label{graphical-conventions}

\textbf{\(A\)}: Denotes the ``treatment'' or ``exposure'' - a random
variable.

This is the variable for which we seek to understand the effect of
intervening on it. It is the ``cause.''

\textbf{\(Y\)}: Denotes the outcome or response, measured at the end of
study.

It is the ``effect.''

\textbf{\(L\)}: Denotes a measured confounder or set of confounders.

\textbf{\(U\)}: Denotes an unmeasured confounder or confounders.

\textbf{Node}: a node or vertex represents characteristics or features
of units within a population on a causal diagram -- that is a
``variable.'' In causal directed acyclic graphs, we draw nodes with
respect to the \emph{target population}, which is the population for
whom investigators seek causal inferences
(\citeproc{ref-suzuki2020}{Suzuki \emph{et al.} 2020}). Time-indexed
node: \(X_t\) denotes relative chronology

\textbf{Arrow} (\(\rightarrowNEW\)): denotes causal relationship from
the node at the base of the arrow (a `parent') to the node at the tip of
the arrow (a `child'). In causal DAGS it is conventional to refrain from
drawing an arrow from treatment to outcome to avoid asserting a causal
path from \(A\) to \(Y\) because iyr purpose is to ascertain whether
causality can be identified for this path. All other nodes and paths --
including the absence of nodes and paths -- is typically assumed.

\textbf{Red Arrow} (\(\rightarrowred\)): path of non-causal association
between the treatment and outcome. Despite the arrows, this path is
associational and may flow against time.

\textbf{Dashed Arrow} (\(\rightarrowdotted\)): denotes a true
association between the treatment and outcome that becomes partially
obscured when conditioning on a mediator, assuming \(A\) causes \(Y\).

\textbf{Dashed Red Arrow} (\(\rightarrowdottedred\)): highlights
over-conditioning bias from conditioning on a mediator.

\textbf{Open Blue Arrow} (\(\rightarrowblue\)): highlights effect
modification, which occurs when the levels of the effect of treatment
vary within levels of a covariate. We do not assess the causal effect of
the effect-modifier on the outcome, recognising that it may be
incoherent to consider intervening on the effect-modifier.

\textbf{Boxed Variable} \(\big(\boxed{X}\big)\): conditioning or
adjustment for \(X\).

\textbf{Red-Boxed Variable} \(\big(\boxedred{X}\big)\): highlights the
source of confounding bias from adjustment.

\textbf{Dashed Circle} \(\big( \circledotted{X}\big)\): no adjustment is
made for a variable (implied for unmeasured confounders.)

\subsubsection{Causal Directed Acyclic Graphs (causal
DAGs)}\label{causal-directed-acyclic-graphs-causal-dags}

In the 1990s, Judea Pearl showed that we can evaluate causal
dependencies using observable probability distributions
(\citeproc{ref-pearl1995}{Pearl 1995}, \citeproc{ref-pearl2009a}{2009}).
He also demonstrated that causal directed acyclic graphs (causal DAGs)
clarify the conditional dependencies among variables
(\citeproc{ref-pearl1995}{Pearl 1995}). Based on assumptions about
causal structure, researchers can identify causal effects from joint
distributions of observed data.

Pearl developed graphical rules known as d-separation
(\citeproc{ref-pearl1995}{Pearl 1995}):

\begin{itemize}
\tightlist
\item
  \textbf{Fork rule} (\(B \leftarrowNEW \boxed{A} \rightarrowNEW C\)):
  \(B\) and \(C\) are independent when conditioned on \(A\)
  (\(B \coprod C \mid A\)).
\item
  \textbf{Chain rule} (\(A \rightarrowNEW \boxed{B} \rightarrowNEW C\)):
  Conditioning on \(B\) blocks the path between \(A\) and \(C\)
  (\(A \coprod C \mid B\)).
\item
  \textbf{Collider rule}
  (\(A \rightarrowNEW \boxed{C} \leftarrowNEW B\)): \(A\) and \(B\) are
  independent until conditioned on \(C\), which introduces dependence
  (\(A \cancel{\coprod} B \mid C\)).
\end{itemize}

These rules lead to the backdoor criterion and `backdoor adjustment'
theorem, which provide algorithms for identifying causal effects based
on the structural assumptions encoded in a causal DAG
(\citeproc{ref-pearl1995}{Pearl 1995}). We use the symbol
\(\mathcal{G}\) to name a graph.

Consider the following graphs from Table~\ref{tbl-terminologygeneral}:

\begin{itemize}
\tightlist
\item
  \textbf{\(\mathcal{G}_1\)}: If \(A\) and \(B\) are not causally
  related and share no common causes, \(A\) and \(B\) will not be
  statistically related.
\item
  \textbf{\(\mathcal{G}_2\)}: If \(A\) causes \(B\), and they share no
  common causes or their common causes are accounted for, \(A\) and
  \(B\) will be statistically related.
\item
  \textbf{\(\mathcal{G}_3\)}: If \(A\) causes \(B\) and \(A\) causes
  \(C\), then conditioning on \(A\) allows us to estimate the effect of
  \(B\) on \(C\).
\item
  \textbf{\(\mathcal{G}_4\)}: If \(A\) causes \(B\) and \(B\) causes
  \(C\), conditioning on \(B\) obscures the true causal effect of \(A\)
  on \(C\), making \(A\) independent of \(C\).
\item
  \textbf{\(\mathcal{G}_5\)}: If \(A\) causes \(C\) and \(B\) causes
  \(C\), conditioning on \(C\) associates \(A\) and \(B\), despite no
  direct causal effect.
\end{itemize}

If we assume that the variables in the graph correspond to Structural
Causal Models, all causal relationships can be defined by the elementary
structures presented above.

\subsubsection{Review of d-separation for Causal Identification on a
Graph}\label{review-of-d-separation-for-causal-identification-on-a-graph}

\begin{table}

\caption{\label{tbl-terminologygeneral}Elements of Causal Graphs}

\centering{

\terminologydirectedgraph

}

\end{table}%

\begin{table}

\caption{\label{tbl-terminologygeneral}Elements of Causal Graphs}

\centering{

\terminologyeffectmodification

}

\end{table}%

Table~\ref{tbl-terminologygeneral} clarifies how to ask a causal
question of effect modification. We assume no confounding of the
treatment on the outcome and that \(A\) has been randomised
(\(\mathcal{R} \rightarrowNEW A\)). We assume
\(\mathcal{R}  A \rightarrowNEW Y\).

To focus on effect modification, we do not draw a causal arrow from the
direct effect modifier \(F\) to the outcome \(Y\). This convention is
specific to this article (refer to Hernan and Robins
(\citeproc{ref-hernan2024WHATIF}{2024}), pp.~126-127, for a discussion
of `noncausal' arrows).

\subsection{Part 1 Measurement Error
Bias}\label{part-1-measurement-error-bias}

\begin{table}

\caption{\label{tbl-terminologymeasurementerror}Six Structural Sources
of Measurement Error Bias}

\centering{

\terminologymeasurementerror

}

\end{table}%

\subsubsection{Example 1: Uncorrelated errors under sharp null: no
treatment
effect}\label{example-1-uncorrelated-errors-under-sharp-null-no-treatment-effect}

Table~\ref{tbl-terminologymeasurementerror} \(\mathcal{G}_1\)
illustrates uncorrelated non-differential measurement error under the
`sharp-null,'\,' which arises when the error terms in the exposure and
outcome are independent. In this setting the structure of measurement
error is not expected to produce bias.

For example, consider a study investigating a causal effect of beliefs
in big Gods on social complexity in ancient societies. Imagine that
societies either randomly omitted or inaccurately recorded details about
their beliefs in big Gods and their social complexities. This might
happen from the varying preservation of records across cultures,
unrelated to the actual beliefs or social complexities. In this
scenario, the errors in historical record for beliefs in big Gods and
for social complexity would be independent. Such errors may generally
not introduce bias when there is no true effect.

\subsubsection{Example 2: Uncorrelated errors under treatment effect
biases true effects toward the
null.}\label{example-2-uncorrelated-errors-under-treatment-effect-biases-true-effects-toward-the-null.}

Table~\ref{tbl-terminologymeasurementerror} \(\mathcal{G}_2\)
illustrates uncorrelated non-differential measurement error, that is
bias that arises when the error terms in the exposure and outcome are
independent (information bias). In this setting, bias will typically
attenuate a true treatment effect.

Consider again the example of a study investigating a causal effect of
beliefs in big Gods on social complexity in ancient societies, were
there are uncorrelated errors in the treatment and outcome. In this
case, measurement error will make it seem that the true causal effects
of beliefs in big Gods is smaller that it is, or perhaps even that such
an effect is absent.

\subsubsection{Example 3: Correlated errors Non-Differential
(Undirected) Measurement
Errors}\label{example-3-correlated-errors-non-differential-undirected-measurement-errors}

Table~\ref{tbl-terminologymeasurementerror} \(\mathcal{G}_3\)
illustrates the structure of correlated non-differential (un-directed)
measurement error bias, which arises when the error terms of the
treatment and outcome share a common cause.

Consider an example: imagine that societies with more sophisticated
record-keeping systems tend to offer more precise and comprehensive
records both of beliefs in big Gods and of social complexity. In this
setting, it is the record-keeping systems that give an illusion of a
relationship between big Gods and social complexity. This might occur
without any effect of big-God beliefs on the measurement of social
complexity or vice versa. Nevertheless, the correlated sources of error
for both the exposure and outcome may suggest causation in its absence.

\subsubsection{Example 4: Uncorrelated Directed Measurement Error:
Exposure affects error of
outcome}\label{example-4-uncorrelated-directed-measurement-error-exposure-affects-error-of-outcome}

Table~\ref{tbl-terminologymeasurementerror} \(\mathcal{G}_4\)
illustrates the structure of uncorrelated differential (or directed)
measurement error, in the when a non-causal path is opened linking the
treatment, the outcome, or a common cause of the treatment an outcome.

Keeping with our previous example, imagine that beliefs in big Gods lead
to inflated records of social complexity in a culture's record keeping.
This might happen because the record keepers in societies that believe
in big Gods prefer societies to reflect the grandeur of their big Gods.
Suppose further that cultures lacking beliefs in big Gods prefer
Bacchanalian-style feasting to record keeping. In this scenario,
societies with record keepers who believe in big Gods would appear to
have more social complexity than equally complex societies without such
record keepers

\subsubsection{Example 5: Uncorrelated Directed error: Outcome affects
error of
exposure}\label{example-5-uncorrelated-directed-error-outcome-affects-error-of-exposure}

Table~\ref{tbl-terminologymeasurementerror} \(\mathcal{G}_5\)
illustrates the structure of uncorrelated differential (or directed)
measurement error, this time when the outcome affects the recording ot
the treatment that preceeded the outcome.

Consider if `history is written by the victors' how might this affect
measurement error bias? Suppose that social complexity causes beliefs in
big Gods. Perhaps kings make big Gods after the image of kings. If the
kings prefer a history in which big Gods were historically present, this
might bias the historical record, opening a path of association that
reverses the order of causation.

\subsubsection{Example 6: Directed error: outcome affects error of
exposure}\label{example-6-directed-error-outcome-affects-error-of-exposure}

Table~\ref{tbl-terminologymeasurementerror} \(\mathcal{G}_6\)
illustrates the structure of correlated differential (directed)
measurement error, which occurs when the exposure affects levels of
already correlated error terms.

Suppose social complexity produces a flattering class of religious
elites who tend to produce vainglorious depictions of kings and their
dominions, and also of the extend and scope of their societies beliefs
in big Gods. For example, such elites might tend to downplay widespread
cultural practices of worshiping lesser gods, inflate population
estimates, and overstate their the range of their economies. In this
scenario the errors of the exposure and of the outcome are both
correlated and differential.

We limit the biases of measurement error by reducing error in our
measures. Often, specialist knowledge can guide the expected direction
of measurement error associations, positive or negative (see: Suzuki
\emph{et al.} (\citeproc{ref-suzuki2020}{2020}), VanderWeele and Robins
(\citeproc{ref-vanderweele2010}{2010}), and VanderWeele and Robins
(\citeproc{ref-vanderweele2007a}{2007})). In some situations,
researchers might use causal diagrams with signed paths to refine causal
inferences, as suggested by VanderWeele
(\citeproc{ref-vanderweele2012}{2012}). These techniques extend beyond
the scope of this study. The point of these examples is to demonstrate
how causal diagrams can clarify sources of confounding from measurement
bias.

\subsection{Part 2: Selection-Restriction Bias From Right-Censoring
(Attrition)}\label{part-2-selection-restriction-bias-from-right-censoring-attrition}

There is much confusion about the topic of `selection bias', however,
there need not be. Some of this confusion is terminological. So we being
by avoiding the term `selection bias', and clarify our meanings.

\textbf{Unit/individual}: an entity, such as an object, person, or
culture. We will use the term `individual' in place of the more general
term `unit'. Think, `row' in one's dataset.

\begin{itemize}
\item
  \textbf{Variable}: a feature of an individual, transient or permenant.
  `John was sleepy but is no longer.'`Alice was born in December.'
\item
  \textbf{Treatment}: an event that might change a variable. `John was
  sleepy, we intervened with coffee, he's wide awake.' `Alice was born
  in December; there's nothing to change that.'
\item
  \textbf{Measurement}: a recorded trace of a variable, such as a column
  in one's dataset.
\item
  \textbf{Measurement error}: a misalignment between the true state of a
  variable and its recorded state. `Alice was born 30/Nov, her mother,
  preoccupied, lost track of time.' In Part 2, we considered a typology
  for structural sources of measurement.
\end{itemize}

\textbf{Population}: abstraction from statistics, denotes the set of all
indivuals defined by certain features. John belongs to the set of all
individuals who ignore instructions.

\begin{itemize}
\item
  \textbf{Super-population}: abstraction, the population of all possible
  individuals of a given kind, another abstract but useful concept. John
  and Alice belong to a super-population of hominins.
\item
  \textbf{Restricted population}: we say population \(p\) is restricted
  relative to another population \(P\), if the individuals \(\in p\)
  share some but not all features of \(P\). `The living' are a
  restriction of hominins.
\item
  \textbf{Target population}: a restriction of the super-population
  whose features interests investigators. An investigator who defines
  their interests is a member of the population of `good investators.'
\item
  \textbf{Source population}: the population from which the study's
  sample is drawn. Investigators wanted to recruit from a general
  population but recruited from the pool of first-year university
  psychology students conscripts.
\item
  \textbf{Baseline sample population}: the abstract set of individuals
  from which the units in one's study at treatment assigned belong,
  e.g.~`the set of all first-year university psychology students
  conscript might end up in this study'. To simplify we will think of
  the baseline population as the \emph{source population.}
\item
  \textbf{Selection into the sample}: the process by which individuals
  are included in a population or sample. Selection occurs, and is under
  investigator control, when a target population is defined from a
  super-population, or when investigators apply eligibility criteria for
  inclusion in the analytic sample. Selection into the sample is often
  out of investigator control. Investigators might aspire to answering
  questions about all of humanity but find themselves limited to
  undergraduate samples. Investigators might sample from a source
  population, but recover an analytic sample that differs from it in
  ways they cannot measure, such as mistrust of scientists. There is
  typically attrition of an analytic sample over time, and this is not
  typically fully within investigator control.
\item
  \textbf{Censored sample population}: the population from which the
  censored units are drawn. Censoring is uninformative if, for everyone
  in the baseline population, there is no effect of treatment (the sharp
  causal null hypothesis). Censoring is informative if there is an
  effect of the treatment, and this effect varies in at least one
  stratum of the baseline population. Note that uninformative censoring
  does not ensure valid inference for the target population even when
  valid inference is ensured for the baseline population. If the
  baseline population differs in the distribution of those features that
  modify the effect of the treatment, and no correction is applied,
  unbiased effect estimates for the baseline population will
  nevertheless be biased for the target population in at least one
  measure of effect (\citeproc{ref-greenland2009commentary}{Greenland
  2009}; \citeproc{ref-lash2020}{Lash \emph{et al.} 2020}). This is why
  it is important for investigators to state a causal effect of interest
  with respect to \emph{the full data} that includes the counterfactual
  quantities for the treatments to be compared in a clearly defined
  target population and with a specific causal contrast
  (\citeproc{ref-westreich2017}{Westreich \emph{et al.} 2017a}).
\end{itemize}

Please note that terminology slighly varies for these concepts (see:
Dahabreh \emph{et al.} (\citeproc{ref-dahabreh2021study}{2021}); Imai
\emph{et al.} (\citeproc{ref-imai2008misunderstandings}{2008}); Cole and
Stuart (\citeproc{ref-cole2010generalizing}{2010}); Westreich \emph{et
al.} (\citeproc{ref-westreich2017transportability}{2017b})). A clear
decomposition of key concepts need to assess generalisability is given
in Imai \emph{et al.} (\citeproc{ref-imai2008misunderstandings}{2008}).
For a less technical, pragmatically useful discussion see: Stuart
\emph{et al.} (\citeproc{ref-stuart2018generalizability}{2018}).

**In Part 2 we will assume that the baseline sample population is the
target population, and focus on biases arising from the
(right)-censoring of the sample population.

\begin{table}

\caption{\label{tbl-terminologycensoring}Five Structural Sources of
Right-Censoring Bias}

\centering{

\terminologycensoring

}

\end{table}%

\subsubsection{Example 1: Confounding by common cause of treatment and
attrition}\label{example-1-confounding-by-common-cause-of-treatment-and-attrition}

Table~\ref{tbl-terminologycensoring} \(\mathcal{G}_1\) illustrates
confounding by common cause of treatment and outcome in the censored
such that the potential outcomes of the population at baseline \(Y(a)\)
differ from those of the censored population at the end of study
\(Y'(a)\) such that \(Y'(a) \neq Y(a)\).

Suppose investigators are interested in whether religious service
attendance affects volunteering. Suppose that an unmeasured indicator,
loyality, affects religious both religious service attendance and
volunteering, and furthermore that loyality affects attrition. In this
case inferences might be biased for the target population at baseline
because loyalty, which affects religious service attendnance and
attrition, is kind of volunteering.

\subsubsection{Example 2: Confounding by common cause of treatment and
attrition}\label{example-2-confounding-by-common-cause-of-treatment-and-attrition}

Table~\ref{tbl-terminologycensoring} \(\mathcal{G}_2\) illustrates
confounding by common cause of treatment and the causes of attrition,
such that the potential outcomes of the population at baseline \(Y(a)\)
differ from those of the censored population at the end of study
\(Y'(a)\) such that \(Y'(a) \neq Y(a)\).

Suppose again that investigators are interested in whether religious
service attendance affects charitable giving. Suppose that physical
disability, an unmeasured variable, affects both whether one attends
religious service and whether one stays in the study. In this scenario,
observed responses under treatment from the retained sample differ from
the counterfactual responses of the baseline sample at the start of
study, opening a pathway for bias.

\subsubsection{Example 3: Treatment affects
censoring}\label{example-3-treatment-affects-censoring}

Table~\ref{tbl-terminologycensoring} \(\mathcal{G}_3\)

\subsubsection{Example 4: No treatment effect when outcome causing
censoring}\label{example-4-no-treatment-effect-when-outcome-causing-censoring}

Table~\ref{tbl-terminologycensoring} \(\mathcal{G}_4\)

\subsubsection{Example 5: Treatment effect when outcome causes
censoring}\label{example-5-treatment-effect-when-outcome-causes-censoring}

Table~\ref{tbl-terminologycensoring} \(\mathcal{G}_5\)

\subsubsection{Example 6: Treatment effect and effect-modifiers differ
in censored (restriction bias without
confounding)}\label{example-6-treatment-effect-and-effect-modifiers-differ-in-censored-restriction-bias-without-confounding}

Table~\ref{tbl-terminologycensoring} \(\mathcal{G}_6\)

\subsection{Part 3: Selection-Restriction Bias at Baseline
(Left-Censoring)}\label{part-3-selection-restriction-bias-at-baseline-left-censoring}

\subsubsection{Sample-Restriction Bias Considered as Collider
Stratification
Bias}\label{sample-restriction-bias-considered-as-collider-stratification-bias}

\begin{table}

\caption{\label{tbl-terminologyselectionrestrictionclassic}Collider-Stratification
bias at start of study (`M-bias')}

\centering{

\terminologyselectionrestrictionclassic

}

\end{table}%

Table~\ref{tbl-terminologycensoring} \(\mathcal{G}_6\)

\begin{table}

\caption{\label{tbl-terminologyselectionrestrictionbaseline}Collider-Stratification
bias at start of study (`M-bias')}

\centering{

\terminologyselectionrestrictionbaseline

}

\end{table}%

\subsubsection{Problem 1: Target population is not WEIRD; sample
population is
WEIRD}\label{problem-1-target-population-is-not-weird-sample-population-is-weird}

Table~\ref{tbl-terminologyselectionrestrictionbaseline}
\(\mathcal{G}_{1.1}\)

Table~\ref{tbl-terminologyselectionrestrictionbaseline}
\(\mathcal{G}_{1.2}\)

\subsubsection{Example 2: Target population is WEIRD; sample population
is not
WEIRD}\label{example-2-target-population-is-weird-sample-population-is-not-weird}

Table~\ref{tbl-terminologyselectionrestrictionbaseline}
\(\mathcal{G}_{2.1}\)

Table~\ref{tbl-terminologyselectionrestrictionbaseline}
\(\mathcal{G}_{2.2}\)

\subsubsection{Example 3: Correlated measurement error of
effect-modifiers for an overly ambitious target
population}\label{example-3-correlated-measurement-error-of-effect-modifiers-for-an-overly-ambitious-target-population}

Table~\ref{tbl-terminologyselectionrestrictionbaseline}
\(\mathcal{G}_{3.1}\)

Table~\ref{tbl-terminologyselectionrestrictionbaseline}
\(\mathcal{G}_{3.2}\)

\subsubsection{Example 4: Correlated measurement error of
effect-modifiers for an overly ambitious target
population}\label{example-4-correlated-measurement-error-of-effect-modifiers-for-an-overly-ambitious-target-population}

Table~\ref{tbl-terminologyselectionrestrictionbaseline}
\(\mathcal{G}_{4.1}\)

Table~\ref{tbl-terminologyselectionrestrictionbaseline}
\(\mathcal{G}_{4.2}\)

\subsection{Conclusions}\label{conclusions}

\begin{itemize}
\tightlist
\item
  Measurement first
\end{itemize}

\subsubsection{Building intuition for why definitions are insufficient
for understanding selection and source
biases}\label{building-intuition-for-why-definitions-are-insufficient-for-understanding-selection-and-source-biases}

When considering how source/target population mismatch imperiles
science, it is tempting to seek general definitions, typologies, and
identification criteria by which to spot the snakes.

Recently, a host of institutional diversity and inclusion initiatives
have been developed that commend researchers to obtain data from global
samples. In my view, the motivation for these mission statements is
ethically laudable. The injunction for a broader science of humanity
also accords with institutional missions. For example, the scientific
mission of the American Psychological Association (APA) is `to promote
the advancement, communication, and application of psychological science
and knowledge to benefit society and improve lives.' The APA does not
state that it wants to understand and benefit only North Atlantic
Societies.\footnote{https://www.apa.org/pubs/authors/equity-diversity-inclusion}.
It is therefore tempting to use such a mission statement as an ideal by
which to evaluate the samples used in human scientific research.

Suppose we agree that promoting a globally diverse science makes ethical
sense. Does the sampling of globally diverse populations always advance
this ideal? It is easy find examples in which restricting our source
population makes better scientific sense. Suppose we are interested in
the psychological effects of restorative justice among victims of
violent crime. Here, it would make little scientific sense to sample
from a population that has not experienced violent crime. Nor would it
make ethical sense. The scientific question, which my have important
ethical importance, is not served by casting a wider net. Suppose our
the interest were to investigate the health effects of calorie
restriction. It might be unethical to include children or the elderly.
It make little sense to investigate the psychological impact of
vasectomy in biological female or historectomy in biological males

In the cases we just considered, the scientific questions pertained to a
sub-sample of the human population and so could be sensibly restricted.
However even for questions that relate to all of humanity, sampling from
all of humanity might be undesirable. For example, if we were interested
in the effects of a vaccine on disease, sampling from one population
might be as good as sampling from all. Sampling from one population
might spare time and expense, which come with opportunity costs. We
might conclude that sampling universally, where unnecessary, is wasteful
and unethical.

We might agree with our mission statements in judging that ethical
aspirations must guide research at every phase. Yet, mistaking our
aspirations for sampling directives would result in wasteful science. If
such waste is avoidable, then the result is unethical science.

I present this examples to remind ourselves of the importance of
addressing questions of sampling in relation to its context.

During the past twenty years, the causal data sciences, also known as
causal inference, have enabled tremendous clarity for questions of
research design and analysis. The following hopes to clarify that it is
only within these workflows that we are able to understand the threats
to causal inferences from selection and source bias, and strategies for
addressing them.

\newpage{}

\subsection{Funding}\label{funding}

This work is supported by a grant from the Templeton Religion Trust
(TRT0418) and RSNZ Marsden 3721245, 20-UOA-123; RSNZ 19-UOO-090. I also
received support from the Max Planck Institute for the Science of Human
History. The Funders had no role in preparing the manuscript or the
decision to publish it.

\subsection{Acknowledgements}\label{acknowledgements}

Errors are my own.

\newpage{}

\subsection{Appendix A: Glossary}\label{appendix-a-glossary}

\begin{table}

\caption{\label{tbl-experiments}Glossary}

\centering{

\glossaryTerms

}

\end{table}%

\phantomsection\label{refs}
\begin{CSLReferences}{1}{0}
\bibitem[\citeproctext]{ref-arnett2008neglected}
Arnett, JJ (2008) The neglected 95\%: Why american psychology needs to
become less american. \emph{American Psychologist}, \textbf{63}(7),
602--614.
doi:\href{https://doi.org/10.1037/0003-066X.63.7.602}{10.1037/0003-066X.63.7.602}.

\bibitem[\citeproctext]{ref-bulbulia2023a}
Bulbulia, JA, Afzali, MU, Yogeeswaran, K, and Sibley, CG (2023)
Long-term causal effects of far-right terrorism in {N}ew {Z}ealand.
\emph{PNAS Nexus}, \textbf{2}(8), pgad242.

\bibitem[\citeproctext]{ref-cole2010generalizing}
Cole, SR, and Stuart, EA (2010) Generalizing evidence from randomized
clinical trials to target populations: The ACTG 320 trial.
\emph{American Journal of Epidemiology}, \textbf{172}(1), 107--115.

\bibitem[\citeproctext]{ref-dahabreh2021study}
Dahabreh, IJ, Haneuse, SJA, Robins, JM, \ldots{} Hernn, MA (2021) Study
designs for extending causal inferences from a randomized trial to a
target population. \emph{American Journal of Epidemiology},
\textbf{190}(8), 1632--1642.

\bibitem[\citeproctext]{ref-greenland2009commentary}
Greenland, S (2009) Commentary: Interactions in epidemiology: Relevance,
identification, and estimation. \emph{Epidemiology}, \textbf{20}(1),
14--17.

\bibitem[\citeproctext]{ref-henrich2010weirdest}
Henrich, J, Heine, SJ, and Norenzayan, A (2010) The weirdest people in
the world? \emph{Behavioral and Brain Sciences}, \textbf{33}(2-3),
61--83.

\bibitem[\citeproctext]{ref-hernan2024WHATIF}
Hernan, MA, and Robins, JM (2024) \emph{Causal inference: What if?},
Taylor \& Francis. Retrieved from
\url{https://www.hsph.harvard.edu/miguel-hernan/causal-inference-book/}

\bibitem[\citeproctext]{ref-holland1986}
Holland, PW (1986) Statistics and causal inference. \emph{Journal of the
American Statistical Association}, \textbf{81}(396), 945--960.

\bibitem[\citeproctext]{ref-hume1902}
Hume, D (1902) \emph{Enquiries Concerning the Human Understanding: And
Concerning the Principles of Morals}, Clarendon Press.

\bibitem[\citeproctext]{ref-imai2008misunderstandings}
Imai, K, King, G, and Stuart, EA (2008) Misunderstandings between
experimentalists and observationalists about causal inference.
\emph{Journal of the Royal Statistical Society Series A: Statistics in
Society}, \textbf{171}(2), 481--502.

\bibitem[\citeproctext]{ref-lash2020}
Lash, TL, Rothman, KJ, VanderWeele, TJ, and Haneuse, S (2020)
\emph{Modern epidemiology}, Wolters Kluwer. Retrieved from
\url{https://books.google.co.nz/books?id=SiTSnQEACAAJ}

\bibitem[\citeproctext]{ref-lewis1973}
Lewis, D (1973) Causation. \emph{The Journal of Philosophy},
\textbf{70}(17), 556--567.
doi:\href{https://doi.org/10.2307/2025310}{10.2307/2025310}.

\bibitem[\citeproctext]{ref-pearl1995}
Pearl, J (1995) Causal diagrams for empirical research.
\emph{Biometrika}, \textbf{82}(4), 669--688.

\bibitem[\citeproctext]{ref-pearl2009a}
Pearl, J (2009) \emph{Causality}, Cambridge University Press.

\bibitem[\citeproctext]{ref-rubin1976}
Rubin, DB (1976) Inference and missing data. \emph{Biometrika},
\textbf{63}(3), 581--592.
doi:\href{https://doi.org/10.1093/biomet/63.3.581}{10.1093/biomet/63.3.581}.

\bibitem[\citeproctext]{ref-sears1986college}
Sears, DO (1986) College sophomores in the laboratory: Influences of a
narrow data base on social psychology's view of human nature.
\emph{Journal of Personality and Social Psychology}, \textbf{51}(3),
515.

\bibitem[\citeproctext]{ref-stuart2018generalizability}
Stuart, EA, Ackerman, B, and Westreich, D (2018) Generalizability of
randomized trial results to target populations: Design and analysis
possibilities. \emph{Research on Social Work Practice}, \textbf{28}(5),
532--537.

\bibitem[\citeproctext]{ref-suzuki2020}
Suzuki, E, Shinozaki, T, and Yamamoto, E (2020) Causal Diagrams:
Pitfalls and Tips. \emph{Journal of Epidemiology}, \textbf{30}(4),
153--162.
doi:\href{https://doi.org/10.2188/jea.JE20190192}{10.2188/jea.JE20190192}.

\bibitem[\citeproctext]{ref-vanderweele2012}
VanderWeele, TJ (2012) Confounding and Effect Modification: Distribution
and Measure. \emph{Epidemiologic Methods}, \textbf{1}(1), 55--82.
doi:\href{https://doi.org/10.1515/2161-962X.1004}{10.1515/2161-962X.1004}.

\bibitem[\citeproctext]{ref-vanderweele2007a}
VanderWeele, TJ, and Robins, JM (2007) Directed acyclic graphs,
sufficient causes, and the properties of conditioning on a common
effect. \emph{American Journal of Epidemiology}, \textbf{166}(9),
1096--1104.
doi:\href{https://doi.org/10.1093/aje/kwm179}{10.1093/aje/kwm179}.

\bibitem[\citeproctext]{ref-vanderweele2010}
VanderWeele, TJ, and Robins, JM (2010) Signed directed acyclic graphs
for causal inference. \emph{Journal of the Royal Statistical Society
Series B: Statistical Methodology}, \textbf{72}(1), 111--127.
doi:\href{https://doi.org/10.1111/j.1467-9868.2009.00728.x}{10.1111/j.1467-9868.2009.00728.x}.

\bibitem[\citeproctext]{ref-westreich2017}
Westreich, D, Edwards, JK, Lesko, CR, Stuart, E, and Cole, SR (2017a)
Transportability of trial results using inverse odds of sampling
weights. \emph{American Journal of Epidemiology}, \textbf{186}(8),
1010--1014.
doi:\href{https://doi.org/10.1093/aje/kwx164}{10.1093/aje/kwx164}.

\bibitem[\citeproctext]{ref-westreich2017transportability}
Westreich, D, Edwards, JK, Lesko, CR, Stuart, E, and Cole, SR (2017b)
Transportability of trial results using inverse odds of sampling
weights. \emph{American Journal of Epidemiology}, \textbf{186}(8),
1010--1014.

\end{CSLReferences}



\end{document}
