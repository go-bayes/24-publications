% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames,x11names}{xcolor}
%
\documentclass[
  singlecolumn]{article}

\usepackage{amsmath,amssymb}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage[]{libertinus}
\ifPDFTeX\else  
    % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\usepackage[top=30mm,left=20mm,heightrounded]{geometry}
\setlength{\emergencystretch}{3em} % prevent overfull lines
\setcounter{secnumdepth}{-\maxdimen} % remove section numbering
% Make \paragraph and \subparagraph free-standing
\ifx\paragraph\undefined\else
  \let\oldparagraph\paragraph
  \renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
  \let\oldsubparagraph\subparagraph
  \renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi

\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\newenvironment{Shaded}{}{}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{1.00,0.33,0.33}{\textbf{#1}}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.42,0.45,0.49}{#1}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.84,0.23,0.29}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.36,0.77}{#1}}
\newcommand{\BuiltInTok}[1]{\textcolor[rgb]{0.84,0.23,0.29}{#1}}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.01,0.18,0.38}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.42,0.45,0.49}{#1}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.42,0.45,0.49}{#1}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.00,0.36,0.77}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.84,0.23,0.29}{#1}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.84,0.23,0.29}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.36,0.77}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.42,0.45,0.49}{#1}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{1.00,0.33,0.33}{\underline{#1}}}
\newcommand{\ExtensionTok}[1]{\textcolor[rgb]{0.84,0.23,0.29}{\textbf{#1}}}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.36,0.77}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.44,0.26,0.76}{#1}}
\newcommand{\ImportTok}[1]{\textcolor[rgb]{0.01,0.18,0.38}{#1}}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.42,0.45,0.49}{#1}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.84,0.23,0.29}{#1}}
\newcommand{\NormalTok}[1]{\textcolor[rgb]{0.14,0.16,0.18}{#1}}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.14,0.16,0.18}{#1}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.44,0.26,0.76}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.84,0.23,0.29}{#1}}
\newcommand{\RegionMarkerTok}[1]{\textcolor[rgb]{0.42,0.45,0.49}{#1}}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.00,0.36,0.77}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.01,0.18,0.38}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.01,0.18,0.38}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.89,0.38,0.04}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.01,0.18,0.38}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{1.00,0.33,0.33}{#1}}

\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
% definitions for citeproc citations
\NewDocumentCommand\citeproctext{}{}
\NewDocumentCommand\citeproc{mm}{%
  \begingroup\def\citeproctext{#2}\cite{#1}\endgroup}
\makeatletter
 % allow citations to break across lines
 \let\@cite@ofmt\@firstofone
 % avoid brackets around text for \cite:
 \def\@biblabel#1{}
 \def\@cite#1#2{{#1\if@tempswa , #2\fi}}
\makeatother
\newlength{\cslhangindent}
\setlength{\cslhangindent}{1.5em}
\newlength{\csllabelwidth}
\setlength{\csllabelwidth}{3em}
\newenvironment{CSLReferences}[2] % #1 hanging-indent, #2 entry-spacing
 {\begin{list}{}{%
  \setlength{\itemindent}{0pt}
  \setlength{\leftmargin}{0pt}
  \setlength{\parsep}{0pt}
  % turn on hanging indent if param 1 is 1
  \ifodd #1
   \setlength{\leftmargin}{\cslhangindent}
   \setlength{\itemindent}{-1\cslhangindent}
  \fi
  % set entry spacing
  \setlength{\itemsep}{#2\baselineskip}}}
 {\end{list}}
\usepackage{calc}
\newcommand{\CSLBlock}[1]{\hfill\break\parbox[t]{\linewidth}{\strut\ignorespaces#1\strut}}
\newcommand{\CSLLeftMargin}[1]{\parbox[t]{\csllabelwidth}{\strut#1\strut}}
\newcommand{\CSLRightInline}[1]{\parbox[t]{\linewidth - \csllabelwidth}{\strut#1\strut}}
\newcommand{\CSLIndent}[1]{\hspace{\cslhangindent}#1}

\usepackage{booktabs}
\usepackage{longtable}
\usepackage{array}
\usepackage{multirow}
\usepackage{wrapfig}
\usepackage{float}
\usepackage{colortbl}
\usepackage{pdflscape}
\usepackage{tabu}
\usepackage{threeparttable}
\usepackage{threeparttablex}
\usepackage[normalem]{ulem}
\usepackage{makecell}
\usepackage{xcolor}
\input{/Users/joseph/GIT/templates/latex/custom-commands.tex}
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\AtBeginDocument{%
\ifdefined\contentsname
  \renewcommand*\contentsname{Table of contents}
\else
  \newcommand\contentsname{Table of contents}
\fi
\ifdefined\listfigurename
  \renewcommand*\listfigurename{List of Figures}
\else
  \newcommand\listfigurename{List of Figures}
\fi
\ifdefined\listtablename
  \renewcommand*\listtablename{List of Tables}
\else
  \newcommand\listtablename{List of Tables}
\fi
\ifdefined\figurename
  \renewcommand*\figurename{Figure}
\else
  \newcommand\figurename{Figure}
\fi
\ifdefined\tablename
  \renewcommand*\tablename{Table}
\else
  \newcommand\tablename{Table}
\fi
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Listing}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}
\makeatother
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\@ifpackageloaded{subcaption}{}{\usepackage{subcaption}}
\makeatother
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same} % disable monospaced font for URLs
\hypersetup{
  pdftitle={Causal Inference in Environmental Psychology},
  pdfkeywords={DAGS, Causal
Inference, Confounding, Environmental, Psychology, Panel},
  colorlinks=true,
  linkcolor={blue},
  filecolor={Maroon},
  citecolor={Blue},
  urlcolor={Blue},
  pdfcreator={LaTeX via pandoc}}

\title{Causal Inference in Environmental Psychology}
\author{Joseph A. Bulbulia \and Donald W Hine}
\date{2024-02-19}

\begin{document}
\maketitle
\begin{abstract}
This chapter offers a practical guide for environmental psychology
researchers to investigate causal questions. We begin by introducing
core concepts of causal inference and the assumptions needed to extract
causal insights from observational data. We then explain how all causal
diagrams can be built from five elementary relationships, and use the
question of whether greenspace access affects happiness to demonstrate
use cases. When the confounding structure is complex, we recommend
creating multiple causal diagrams to illustrate the uncertainty and
report results under different scenarios. We offer guidelines for how to
do this.
\end{abstract}

\subsection{Introduction}\label{introduction}

Causal inference seeks to answer a fundamental question: by how much
does an intervention in the world (the ``treatment'') directly change
another variable (the ``outcome'') -- if at all? It goes beyond
identifying associations to quantifying the magnitude of causal effects.
Although the ability to understand cause and effect exists across the
animal kingdom (\citeproc{ref-mancuso2018revolutionary}{Mancuso 2018}),
methods for quantitatively estimating the size of these effects are a
relatively recent development.

Randomized controlled experiment are often considered the ``gold
standard'' for establishing causation, however, they can be expensive,
impractical, ethically complex, and results may still be affected by
bias (\citeproc{ref-hernan2017per}{Hernán \emph{et al.} 2017};
\citeproc{ref-montgomery2018}{Montgomery \emph{et al.} 2018}).
Observational data are abundant, and offer a valuable alternative for
accelerating knowledge -- but only if valid causal inferences can be
drawn from them.

Unfortunately, many researchers still attempt to analyze observational
data using workflows that do not address the problem of confounding and
can worsen bias (\citeproc{ref-hernan2023}{Hernan and Robins 2023};
\citeproc{ref-robins1986}{Robins 1986};
\citeproc{ref-westreich2013}{Westreich and Greenland 2013}). This has
led to a ``causality crisis'' in social sciences, where associations are
acknowledged as insufficient, yet firm causal statements remain elusive
(\citeproc{ref-bulbulia2023a}{Bulbulia \emph{et al.} 2023}).

Recent advances in causal inference from biostatistics and computer
science offer a solution (\citeproc{ref-vanderweele2015}{VanderWeele
2015}). This approach has immense potential for environmental
psychology, a field where randomized controlled experiments often are
impractical or impossible. This chapter introduces causal inference
methods, with the goal of helping environmental psychologists obtain the
tools they need to extract robust causal insights from their
observational data.

\hyperref[sec-part1]{\textbf{Part 1: An Overview of the Potential
Outcomes Framework for Causal Inference}} Here, we introduce the
potential outcomes framework---the cornerstone of causal inference
(\citeproc{ref-hernan2023}{Hernan and Robins 2023}). Within the familiar
context of randomized controlled experiments, we examine
\hyperref[sec-three-fundamental-assumptions]{\textbf{three fudamental
assumptions}} that must be met to obtain causal inferences from data.
Building intuition using the example of a randomized controlled
experiments offers two benefits. First, it makes clear that even ``gold
standard'' experiments rely on assumptions for their causal
interpretations. Second, experiments clarify \emph{why} these
assumptions are necessary: they are necessary because causal inference
requiers computating contrasts for partially counterfactual outcomes
from data (\citeproc{ref-hernan2017per}{Hernán \emph{et al.} 2017};
\citeproc{ref-robins2008estimation}{Robins and Hernan 2008};
\citeproc{ref-westreich2012berkson}{Westreich 2012};
\citeproc{ref-westreich2015}{Westreich \emph{et al.} 2015}).

\textbf{Part 2: Causal Diagrams - Visually Understanding Confounding.}
Here, we explain the fundamental structures of causal diagrams, aslo
called ``Directed Acyclic Graphs (DAGs)'' or ``causal graphs''. These
graphs represent causal relationships and assumptions
(\citeproc{ref-pearl2009a}{Pearl 2009b}). Although a full examination of
their capabilities goes beyond this chapter, we equip you with essential
strategies for their construction and interpretation. We discover that
all causal diagrams are built from
\hyperref[sec-five-elementary]{\textbf{the five elementary graphical
structures of causality}} from which all complex causal relations are
built. Here we learn \hyperref[sec-four-rules]{\textbf{four elementary
rules for evaluating confounding}}, which enable researchers evaluate
whether valid causal inferences can be obtained from data. With this
understanding in hand, you will be ready to apply causal diagrams to
your environmental psychology questions.

\textbf{Part 3: Using Causal Diagrams for Causal Identification - Worked
Examples}. Here, we develop \textbf{seven common use cases} that
demonstrate \textbf{four rules for evaluating confounding} in action. We
show how causal diagrams illuminate strategies that researchers can
employ to obtain consistent causal inferences with their data, or in
some cases, to clarify why valid inferences cannot be obtained. Some
results align with intuition, but we show that causal diagrams can
reveal options we may not initially consider. Throughout we see that
although causal diagrams are supported by rigorous mathematical proofs,
they are surprisingly accessible: they require no mathematical knowledge
to use. As such causal diagrams can be empowering a broad range of
researchers beyond the computer scientists and statistians that
developed these tools.

\textbf{Part 4: Practical Guide For Constructing Causal Diagrams and
Reporting Results When Causal Structure is Unclear}. Often researchers
will face uncertainties about the true causal structure of the world as
captured in their data. Here, we explain the value of drawing multiple
causal diagrams and simulate data to illustrate why this approach is
useful. We also provide recommendations for reporting in two settings:

\begin{enumerate}
\def\labelenumi{(\roman{enumi})}
\tightlist
\item
  Only cross-sectional data are available.
\item
  Repeated measures longitudinal data are available.
\end{enumerate}

\subsection{Part 1: An Overview of the Potential Outcomes Framework for
Causal Inference}\label{sec-part1}

The potential outcomes framework for causal inference originated in the
work of Jerzy Neyman for the purpose of evaluating the effectiveness of
agricultural experiments (\citeproc{ref-neyman1923}{Neyman 1923}). It
was later extended by Harvard statistician Donald Rubin, who
demonstrated the framework may also facilitate causal inferences in
non-experimental settings (\citeproc{ref-rubin1976}{Rubin 1976}). Jamie
Robins further generalized this framework to assess confounding in
complex scenarios involving multiple treatments and time-varying
treatments (\citeproc{ref-robins1986}{Robins 1986}).

A core concept within this framework is that of a ``counterfactual
contrast'' or ``estimand.'' To quantitatively assess the magnitude of
causality requires contrasting how the world would have turned out under
two or more states, corresponding to different levels of intervention or
treatment. Notably, before any intervention, these states remain
counterfactual. After any intervention, for every treatment applied, at
most, only one of the two states of the world to be contrasted is
realized. The other state remains counterfactual. Philosophers who have
puzzled over the nature of causation have long realized that causality
is \textbf{never directly observed} (\citeproc{ref-hume1902}{Hume
1902}). In a sense, we may think of causal inference as a form of
counterfactual data science (\citeproc{ref-bulbulia2023a}{Bulbulia
\emph{et al.} 2023}; \citeproc{ref-edwards2015}{Edwards \emph{et al.}
2015})

\paragraph{The Fundamental Problem of Causal Inference: Causal Contrasts
are Not Directly
Observed}\label{the-fundamental-problem-of-causal-inference-causal-contrasts-are-not-directly-observed}

To grasp the implications of counterfactual contrasts in causal
inference, imagine yourself at a pivotal point in your life. Having just
completed your undergraduate studies, you have been accepted into your
dream Environmental Psychology program at the University of Canterbury
and are set to relocate to Christchurch, New Zealand with Professor
D.W.Hine. Suddenly, you receive a fantastic job offer from Acme Nuclear
Fuels, a leader in renewable energy. Both paths diverge considerably ---
lifestyle, income, social networks, relationships, perhaps even life
purpose hang in the balance. Which choice aligns with your ideal future?

Formally, let \(D\) denote the decision, where \(D = 1\) means attending
graduate school and \(D = 0\) means joining the workforce. Your two
potential outcomes under each path are described as
\(Y_{\text{you}}(1)\) and \(Y_{\text{you}}(0)\). Importantly, we assume
a definite outcome exists for each choice you \emph{could} have made.
Conceptually, we need to measure the difference
\(Y_{\text{you}}(1) - Y_{\text{you}}(0)\) to quantify the
\emph{magnitude} of the effect of your choice. Yet, this difference
remains fundamentally unobservable. Your life goes down one path,
forever obscuring the alternative. In the following, the notation
\(A|B\) means ``A conditional on B.''

\[
(Y_{\text{you}}|D_{\text{you}} = 1) = Y_{\text{you}}(1) \quad \text{implies} \quad Y_{\text{you}}(0)|D_{\text{you}} = 1~ \text{is counterfactual}.
\]

In plain English, this expressions means: ``The outcome that we observe
under option \(D = 1\) can be measured. However, because option
\(D = 0\) is not realised, the outcome under option \(D=0\) cannot be
measured. Thus, the contrast between these two outcomes cannot be
computed. At least one outcome remains purely counterfaction.

The same probem arises if you select \(D = 0\). Then, the outcome under
\(D=1\) remains counterfactual. And so we cannot compute the contrast:

\[
(Y_{\text{you}}|D_{\text{you}} = 0) = Y_{\text{you}}(0) \quad \text{implies} \quad Y_{\text{you}}(1)|D_{\text{you}} = 0~ \text{is counterfactual}.
\]

This example reflects the ``fork-in-the-road'' decisions all of us
regularly encounter in life. ``The fundamental problem of causal
inference'' (\citeproc{ref-holland1986}{Holland 1986};
\citeproc{ref-rubin2005}{Rubin 2005}) highlights that directly comparing
one individual's outcome under both potential paths is impossible.

This problem persists for experiments. However, do not despair! Under
certain assumptions, the data may illuminate \emph{average treatment
effects}. We next consider how experiments obtain average treatment
effects despite the fact that individual-level causal effects are never
directly observed.

\subsubsection{Causal inference in Experiments is a Missing Data
Problem}\label{causal-inference-in-experiments-is-a-missing-data-problem}

Let us transition for the topic of life decisions to an example of
relevance to environmental psychology, namely, estimating the average
causal effect of easy access to urban green spaces on subjective
happiness, hereafter referred to as ``happiness.'' We assume this
outcome is measurable and represent it with the letter \(Y\).

For simplicity, we classify the intervention ``ample access to green
space'' as a binary variable. Define \(A = 1\) as ``having ample access
to green space'' and \(A = 0\) as ``lacking ample access to green
space.'' We assume these conditions are mutually exclusive. This
simplification does not limit the generality of our conclusions; the
points we make about experiments apply to continuous treatments as well.

Next, it is crucial in causal inference to specify the population for
whom we seek to evaluate causal effects, or the ``target population.''
Suppose our target population is residents of New Zealand in the 2020s.

A preliminary causal question -- defined as a causal contrast or
``estimand'' might therefore be:

``In contemporary New Zealand, does proximity to abundant green spaces
increase self-perceived happiness compared to environments lacking such
spaces?''

It would be unethical to experimentally randomise individuals into
different green-space access conditions, but let's set this ethical
consideration aside. Assume we could assign people randomly to high and
low green space access without objection or harm.

The first point to note in the context of causal inference, as alluded
to earlier, is that even well-designed experiments confront the
challenge of missing values in the potential outcomes. Once an
individual is assigned to one treatment condition, we cannot observe
that individual's outcome for the condition not assigned. The
fundamental problem of causal inference remains constant: for each
individual, we can only observe one of the potential outcomes at any
given time. Breaking down the Average Treatment Effect (ATE) into
observed and unobserved outcomes yields the following equation:

\[
\text{Average Treatment Effect} = \left(\underbrace{\underbrace{\mathbb{E}[Y(1)|A = 1]}_{\text{observed}} + \underbrace{\mathbb{E}[Y(1)|A = 0]}_{\text{unobserved}}}_{\text{treated}}\right) - \left(\underbrace{\underbrace{\mathbb{E}[Y(0)|A = 0]}_{\text{observed}} + \underbrace{\mathbb{E}[Y(0)|A = 1]}_{\text{unobserved}}}_{\text{untreated}}\right).
\]

In this expression, \(\mathbb{E}[Y(1)|A = 1]\) represents the average
outcome when the treatment is given, which is observable. However,
\(\mathbb{E}[Y(1)|A = 0]\) represents the average outcome if the
treatment had been given to those who were actually untreated, which
remains unobservable. Similarly, the quantity \(\mathbb{E}[Y(0)|A = 1]\)
also remains unobservable.

It is hopefully evident from this brief application of the potential
outcomes framework to experiments that the fundamental problem of causal
inference is an ever-present concern even in experiments. For each
participant, it is impossible to determine the outcome they would have
experienced under an alternative treatment condition, just as you cannot
quantitatively describe the life you would have led had you chosen the
job at Acme Nuclear Fuels instead of attending the University of
Canterbury.

\subsubsection{In Experiments, Random Treatment Assignment Balances
Confounders Across
Treatments}\label{in-experiments-random-treatment-assignment-balances-confounders-across-treatments}

Experiments estimate average treatment effects by addressing the issue
of confounding. A common scenario is ``confounding by common cause,''
where a variable influences both the treatment and the outcome, leading
to a non-causal association between them. This means if we were to
change the treatment without altering the confounder, the outcome would
remain unchanged. The common cause can create a spurious relationship
that might be incorrectly interpreted as causal. For instance, if
studying the effect of access to green space on happiness, it is
conceivable that income could explain the entire association. Hence,
moving lower-income individuals to areas with more green space might not
affect their happiness levels. Identifying and adjusting for such
confounders is essential to reveal the true causal relationship,
ensuring the association observed is not merely due to external factors.
Though confounders are still present, this balance enables us to
attribute differences in outcomes between groups directly to the
treatment.

We can express this principle of no confounding mathematically in two
complementary ways (where \(A \coprod B\) signifies that \(A\) is
independent of \(B\), and vice versa):

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Potential Outcomes Independent of Treatment (given L):}
  \(Y(a) \coprod A \mid L\)
\item
  \textbf{Treatment Assignment Independent of Potential Outcomes (given
  L):} \(A \coprod Y(a) \mid L\)
\end{enumerate}

These formulations are crucial when working with causal diagrams, which
visually encode these principles. The key idea is straightforward:
ensuring a balance of confounders across treatment groups is fundamental
to both experimental and observational causal inference strategies.
Randomization facilitates this balance, achieving \(A \coprod Y(a)\).

\subsubsection{The Three Fundamental Assumptions of Causal
Inference}\label{sec-three-fundamental-assumptions}

Reviewing causal inference in experimental settings highlights three
core assumptions essential for causal analysis.

\paragraph{Fundamental Assumption 1: Conditional
Exchangeability}\label{fundamental-assumption-1-conditional-exchangeability}

This assumes that potential outcomes and treatment assignment are
statistically independent, considering all measured confounders. It
enables us to attribute observed group differences directly to the
treatment. Randomization provides \emph{unconditional} exchangeability,
simplifying the analytical process.

\paragraph{Fundamental Assumption 2: Causal
Consistency}\label{fundamental-assumption-2-causal-consistency}

This assumption ensures that observed outcomes align with the potential
outcomes under the given treatment conditions. For an individual `i', we
have:

\[
\begin{aligned}
Y_{i}(1) &= (Y_{i}|A_{i} = 1) \quad \text{(Potential outcome if treated)} \\
Y_{i}(0) &= (Y_{i}|A_{i} = 0) \quad \text{(Potential outcome if untreated)}
\end{aligned}
\]

Given exchangeability, we can calculate the Average Treatment Effect
(ATE) from observed data as:

\[
\begin{aligned}
\text{ATE} &= \mathbb{E}[Y(1)] - \mathbb{E}[Y(0)] \\
&= \mathbb{E}(Y|A=1) - \mathbb{E}(Y|A=0)
\end{aligned}
\]

This contrast assumes that the potential outcome under treatment is
observable when the treatment is administered, setting \(Y_i(a)\) to
\(Y_i|A_i=a\). The standardization of treatments in randomized
controlled experiments generally ensures the validity of the causal
consistency assumption, which is seldom disputed. However in
observational settings we cannot typically control the treatments that
people receive. This fact imposes considerable challenges for satisfying
this assumption. (Discussed in \hyperref[appendix-b]{Appendix B})

\paragraph{Fundamental Assumption 3:
Positivity}\label{fundamental-assumption-3-positivity}

Every individual must have a nonzero probability of receiving any
treatment level, for each set of confounder values. This assumption,
naturally met in controlled experiments, may require explicit
verification in observational studies.

These assumptions underscore the importance of thoughtful study design
and rigorous analytical strategies to isolate true causal effects from
observational data. This is because for observational data to recover
causal inferences, the design must emulate an experiment.

\subsubsection{Why Satisfying the Fundamental Assumptions of Causal
Inference in Observational Settings is
Challenging}\label{why-satisfying-the-fundamental-assumptions-of-causal-inference-in-observational-settings-is-challenging}

Observational studies do not have the luxury of controlled treatment
allocation inherent in experiments. The objective remains the same: to
emulate the conditions of an experiment as accurately as possible. The
primary obstacle is establishing a balance in confounders to enable fair
group comparisons.

\paragraph{Challenge 1: Achieving Conditional Exchangeability in
Observational
Studies}\label{challenge-1-achieving-conditional-exchangeability-in-observational-studies}

The principle of conditional exchangeability necessitates that groups be
comparable on all fronts, barring the treatment in question. However,
real-world data, such as those examining the influence of green spaces
on well-being, seldom present themselves in such an orderly manner:

\begin{itemize}
\tightlist
\item
  \textbf{Socioeconomic status}: the economic capacity of individuals
  often determines their living environments, thereby affecting their
  access to quality green spaces.
\item
  \textbf{Age demographics}: different age groups have unique
  preferences and necessities regarding green spaces, which could
  influence the observed outcomes.
\item
  \textbf{Mental health}: pre-existing mental health conditions might
  lead individuals to seek out or avoid green spaces, complicating the
  causal pathway.
\item
  \textbf{Lifestyle choices}: the proximity to green spaces could
  correlate with a preference for a more active, outdoor lifestyle. Is
  the observed effect on well-being a direct result of the green space,
  or is it indicative of a generally healthier lifestyle?
\item
  \textbf{Personal values and social connections}: environmental values
  and community ties may influence both the choice of residence and the
  utilization of green spaces.
\end{itemize}

These and other unobserved variables introduce bias into observational
studies, complicating the task of pinpointing the genuine effects of
green spaces.

\paragraph{Challenge 2: Upholding the Causal Consistency Assumption
Amidst Treatment
Heterogeneity}\label{challenge-2-upholding-the-causal-consistency-assumption-amidst-treatment-heterogeneity}

The notion of `proximity to green spaces' encompasses a wide array of
variations, rendering the `treatment' difficult to measure consistently:

\begin{itemize}
\tightlist
\item
  \textbf{Diversity of green spaces}: the ecological richness and visual
  appeal of green spaces vary significantly. Equating well-maintained
  parks with neglected, wild areas does not provide a like-for-like
  comparison.
\item
  \textbf{Availability of amenities}: the presence of facilities such as
  walking paths and benches significantly impacts the usability and
  enjoyment of the spaces.
\item
  \textbf{Size and type of green space}: the benefits derived from an
  urban garden versus a vast forest differ markedly, emphasizing the
  need to consider the nature of the green space in the analysis.
\end{itemize}

\paragraph{Challenge 3: Meeting the Positivity Assumption in
Observational
Studies}\label{challenge-3-meeting-the-positivity-assumption-in-observational-studies}

Positivity demands that each individual have the possibility of
experiencing any level of the treatment. However, real-world
constraints, such as availability of housing in certain locales, may
preclude some groups from accessing varied green spaces, limiting the
ability to generalize findings across different settings.

\textbf{Take-Home Message}: although observational studies strive to
replicate the experimental setup, achieving sufficient fidelity for
robust causal conclusions is often formidable. In some instances, the
limitations of our data preclude definitive causal assertions.
Recognizing the ideal of an experiment aids environmental psychologists
in identifying and articulating the boundaries of observational causal
inferences.

\subsection{Part 2: Causal Diagrams - A Visual Approach to Understanding
Confounding}\label{part-2-causal-diagrams---a-visual-approach-to-understanding-confounding}

We now introduce causal diagrams, beginning with essential terminology.
Grasping this vocabulary is crucial for effectively employing causal
diagrams, though it may initially seem daunting. After laying out the
terms, we will consider into practical examples, uncovering the various
forms of confounding embedded within four principal causal structures.
Identifying and understanding these structures is vital for their
application in real-world analyses. For a detailed glossary, refer to
\hyperref[appendix-a]{\textbf{Appendix A}}.

\subsubsection{Elements of Causal
Diagrams}\label{elements-of-causal-diagrams}

Causal diagrams distill the essence of causal relationships within a
system into visual representations. At their core, these diagrams
consist of:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Nodes}: represent variables or events within a causal
  framework. Each node stands for a distinct element that either exerts
  influence or is subject to influence within the system. Nodes
  encapsulate the components of our causal inquiry, from treatments to
  outcomes and confounders.
\item
  \textbf{Arrows/Edges}: indicate the direction and presence of causal
  relationships between the variables denoted by nodes. Directed edges
  trace the flow of causal influence, with the originating variable
  termed the `parent' and the receiving variable the `child.' These
  arrows define the causal architecture of the system, illustrating how
  we assume one variable causally affects another. Importantly, the
  representation of causal relationships through arrows is agnostic to
  the specific nature of the relationship -- it remains the same whether
  the influence is linear, non-linear, or of any other form.
\item
  \textbf{Conditioning}: in causal data science, deciding which
  variables to adjust for is crucial for estimating the true causal
  effect unconfounded by other factors. We denote a decision to
  ``control for'' or equivalently ``condition on'' or equivalently
  ``adjust for'' a variable by enclising it in a box. Conditioning on a
  variable means that we are taking into account its influence in our
  attempt to causal association between the treatment on the outcome
  from statistical associations in the data.
\end{enumerate}

\subsubsection{The Rules of
D-separation}\label{the-rules-of-d-separation}

Judea Pearl demonstrated how the rules of d-separation allow us to
analyze relationships within causal diagrams
(\citeproc{ref-pearl1995}{Pearl 1995}). These rules allow us to identify
confounders and develop strategies for obtaining valid causal inferences
from statistical associations in the data
(\citeproc{ref-pearl1995}{Pearl 1995}).

\textbf{Key Concepts}

\begin{itemize}
\item
  \textbf{Dependence:} denoted as \(A \cancel\coprod B\), indicating
  that the probability distributions of \(A\) and \(B\) are
  interrelated. Knowledge about one variable provides insights into the
  other, suggesting a potential causal or associational link.
\item
  \textbf{Independence:} denoted as \(A \coprod B\), signifying that the
  probability distributions of \(A\) and \(B\) are independent.
  Information about one variable does not reveal anything about the
  other, indicating no direct causal or associational connection.
\item
  \textbf{Blocked Paths \& D-separation:} A path is considered
  ``blocked'' if a node along it obstructs the causal influence from
  traversing between variables. If all paths between two variables are
  blocked, resulting in d-separation (\(A \coprod B\)), it implies the
  absence of a direct statistical association between them, facilitating
  unbiased causal inference.
\item
  \textbf{Open Paths \& D-connection:} if at least one path between
  variables remains unblocked, allowing for the transmission of causal
  influence, the variables are considered d-connected
  (\(A \cancel\coprod B\)). This condition suggests the presence of a
  statistical association, warranting further analysis to understand the
  nature of the bias in the statistical association between the
  treatment and outcome.
\end{itemize}

These foundational elements and rules equip researchers with a robust
framework for visually mapping and analyzing causal relationships. They
allow us to obtain causal inferences from complex, uncontrolled,
observational data.

\subsubsection{The Five Elementary Graphical Structures of
Causality}\label{sec-five-elementary}

To uncover causal insights from statistical relationships, it is
essential to understand five basic graphical structures. Let's examine
these structures, keeping in mind that achieving balance in confounders
across treatments necessitates ensuring independence between potential
outcomes and treatment (\(A\coprod Y(a)|L\)) within groups defined by
measured covariates \(L\). This independence does not presuppose that
the treatment exerts no effect (\(A\coprod Y |L\)).

\paragraph{Causal Structure 1: Absence of Causality: Two Variables with
No
Arrows}\label{causal-structure-1-absence-of-causality-two-variables-with-no-arrows}

When \(A\) and \(B\) are not connected by any arrows, they do not share
a causal relationship and are statistically independent. Graphically, we
respresent this relationship as:

\[\xorxA\]

\paragraph{Causal Structure 2: Fundamental Causal Association: Two
Variables with a Causal
Arrow}\label{causal-structure-2-fundamental-causal-association-two-variables-with-a-causal-arrow}

The presence of a causal arrow (\(A \to B\)) indicates that alterations
in \(A\) will directly impact \(B\), establishing a statistical
dependence. Graphically, we respresent this relationship as:

\[\xtoxA\]

\paragraph{Causal Structure 3: The Fork Structure: A Common
Cause}\label{causal-structure-3-the-fork-structure-a-common-cause}

The fork structure, represented by \(A \rightarrow B\) and
\(A \rightarrow C\), identifies \(A\) as a common cause affecting both
\(B\) and \(C\). Graphically, we respresent this relationship as:

\[\fork\]

Pearl proved that when we condition on the common cause \(A\) (indicated
by \(\boxed{A}\)), \(B\) and \(C\) become conditionally independent
(\citeproc{ref-pearl2009a}{Pearl 2009b}). By adjusting for the common
cause, any non-causal association between \(B\) and \(C\) is effectively
blocked at node \(A\).

\textbf{Environmental Psychology Example}: suppose observations reveal
that areas with higher rates of public transportation usage also show
reduced levels of individual stress. Does using public transportation
directly reduce stress? Not necessarily. A common environmental factor
might influence both. Consider air quality as the common cause:

\begin{itemize}
\tightlist
\item
  Better air quality (\(A\)) encourages the use of public transportation
  (\(B\)).
\item
  Better air quality (\(A\)) contributes to lower stress levels (\(C\)).
\end{itemize}

According to the rules of d-separation, if we account for the common
cause (air quality), isolating days with similar air quality levels, the
apparent link between public transportation usage and stress levels
dissipates because they no longer share a direct influence. Adjusting
for the fork's common cause eliminates the spurious connection.

\textbf{Rule 1: The Fork Rule}: if interested in the causal effect of
\(B \to C\), condition on \(\boxed{A}\).

\paragraph{Causal Structure 4. The Chain Structure: A
Mediator}\label{causal-structure-4.-the-chain-structure-a-mediator}

The chain structure (\(A \rightarrow B \rightarrow C\)) illustrates a
setting in which \(A\) causes \(B\), and \(B\) subsequently causes
\(C\). Conditioning on the intermediary variable \(B\) (represented by
\(\boxed{B}\)) interrupts the causal pathway, rendering \(A\) and \(C\)
conditionally independent. Graphically, we respresent this relationship
as:

\[\chain\]

\textbf{Environmental Psychology Example}: Suppose we want to assess the
effect of green space renovation in urban areas (\(A\)) on local
community engagement (\(B\)), which subsequently reduces neighborhood
crime rates (\(C\)). Assume the renovation of green spaces
\((A) \rightarrow\) boosts community engagement \((B) \rightarrow\)
which then leads to a decrease in crime rates (\(C\)).

According to the rules of d-separation, controlling for the mediator,
community engagement in this case, might hide the broader effect of
green space renovation. If the primary path through which green space
renovation affects crime rates is via enhanced community engagement,
then adjusting for community engagement could misleadingly suggest that
green space renovation does not directly influence crime rates. This
example underscores the necessity of carefully considering mediators
when examining the effects of environmental changes on social outcomes.

\textbf{Rule 2. The Chain Rule:} if investigating the \emph{total}
causal effect of \(A\to C\), \emph{avoid} conditioning on the mediator
(\(B\)).

\textbf{Important Note:} remember, a ``total'' causal effect may combine
several such causal chains in complex systems. Identifying a mediating
role for focus (\(B\)) is itself a valuable finding about the
\emph{mechanism} through which the supplement might operate. Assessing
causal mediation requires further assumptions, which we will not discuss
here (\citeproc{ref-bulbulia2023}{Bulbulia 2023};
\citeproc{ref-vanderweele2015}{VanderWeele 2015}).

\paragraph{Causal Structure 5: The Collider Structure: A Common
Effect}\label{causal-structure-5-the-collider-structure-a-common-effect}

The collider (\(A\to C\), \(B \to C\) ) features two factors
independently causing a common effect. Initially, \(A\) and \(B\) lack
association. Conditioning on the collider \(C\) (or its descendant)
introduces a spurious statistical association between \(A\) and \(B\).
Graphically:

\[\immorality\]

\paragraph{Causal Structure 5: The Collider Structure: A Common Effect
and the Challenge of Conditioning on
Well-being}\label{causal-structure-5-the-collider-structure-a-common-effect-and-the-challenge-of-conditioning-on-well-being}

\textbf{Environmental Psychology Example}: Suppose we are interested in
whether access to green spaces (\(A\)) causes people to become wealthier
(\(B\))? Suppose we decide to `control' for well-being, which we believe
might cause people to seek out green spaces and also help them to obtain
greater wealth. Call this variable (\(C\)). However, imagine that
well-being is an effect of access to green space and also of wealth.
This setting embodies a collider structure, where conditioning on
well-being could misleadingly suggest a direct causal relationship
between \(A\) and \(B\) that does not exist.

Initially, access to green spaces and community socioeconomic status do
not directly influence one another. The presence of green spaces does
not inherently make a community wealthier, nor does a higher
socioeconomic status automatically lead to increased green space access.

However, when we specifically analyse individuals who report high levels
of well-being (\(\boxed{C}\)), we might observe that:

\begin{itemize}
\tightlist
\item
  Individuals in less wealthy neighborhoods with ample green spaces
  could seem disproportionately happy, suggesting a strong beneficial
  effect of green spaces in these areas.
\item
  Meanwhile, individuals from wealthier neighborhoods, also reporting
  high well-being, might not attribute their happiness as strongly to
  green spaces, implying that wealth contributes more to their
  well-being than their environment does.
\end{itemize}

In this context, examining the relationship between green space access
and community wealth while controlling for well-being introduces a
spurious association between \(A\) and \(B\). This is because
conditioning on well-being (\(C\)), the collider, artificially creates a
statistical relationship between access to green spaces and
socioeconomic status, leading to potential misinterpretation of the
causal effect.

\textbf{Take-Home Message}: this example highlights the complexities of
assessing causal relationships in observational studies. It illustrates
the risk of confounding the analysis by conditioning on an outcome
influenced by both variables of interest. Conditioning on well-being to
explore the causal link between green space access and wealth could
falsely suggest a direct relationship, underscoring the critical need
for thoughtful analysis and interpretation in causal inference. Of
course, we often do not know the true structure of reality.
\hyperref[sec-part4]{In Part 4} we consider how to address such
uncertainties. For now, consider our third rule for confounding control:

\textbf{Rule 3: The Collider Rule:} when assessing the causal effect of
\(A\to B\), \emph{avoid} conditioning on a collider (\(C\)) or its
descendants. Doing so may introduce an association that appears causal
but is not.

\paragraph{Causal Relationships Are Built From The Five Elemental
Structures of Causation: Example of a
Proxy}\label{causal-relationships-are-built-from-the-five-elemental-structures-of-causation-example-of-a-proxy}

All forms of confounding bias stem from combinations of the basic causal
structures we have outlined (absence/presence of cause, forks, chains,
colliders). Understanding these elements in isolation and combination
allows us identifying potential confounders, based on our assumptions
about the world as encoded in a causal diagram. Here we example a
combination of two structures: the collider structure
(\(A \rightarrowred \boxed{C} \leftarrowred B\) meets basic causality
(\(C\rightarrowNEW D\)) to produce confounding by proxy:
\(A \rightarrowred \boxed{D} \leftarrowred B\). Here is how it works:

\textbf{Conditioning a Descendant is akin to Conditioning on its
Ancestor}

\begin{itemize}
\tightlist
\item
  \textbf{Statistical Inheritance:} descendants inherit statistical
  associations from their parents by virtue of the basic cause-effect
  relationship. This makes descendants act as stand-ins for their
  parents.
\end{itemize}

Consider again the example of whether access to urban green spaces
(\(A\)) affects wealth (\(B\)). Imagine they do not, but that both
independently contribute to well-being (\(C\)). Suppose that the only
people who respond to our survey are those who are high in well-being.
In effect, our survey is conditioning on one stratum of the population
(\(D\)), and this stratum is a descendant of the collider -- well-being.
Initially, green spaces and income independently affect well-being.
However, when we specifically analyze data based on willingness to
participate in the survey (\(\boxed{D}\)), we may inadvertently induce
an association between green space access and socioeconomic status,
inferring that those with access to greenspace tend to have a lower
income.

\[\immoralityChildA\]

\textbf{Take-Home Message:} colliders and their descendants set subtle
``traps'' that might induce spurious associations. However, as we shall
see in the next section, conditioning on proxies of unmeasured
confounders opens up possibilities for confounding control beyond our
measured variables. We can sometimes leverage proxies to reduce bias in
our causal inferences.

\textbf{Rule 4: The Proxy Rule:} conditioning on a descendant is akin to
conditioning on its parent. Put differentlyy a descendant is a *proxy\$
for its parent. Avoid conditioning on descendant in settings where
conditioning on the parent would induce misleading associations.

\subsubsection{The Four Elementary Rules For Evaluating
Confounding}\label{sec-four-rules}

Causal diagrams allows researchers to visualise and and systematically
identify potential confounders and strategies for adjusting for them.
There are five basic graphical structures:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Causality Absent:} \(A\) does not cause \(B\): absent any
  common causes, there is no statistical association between them.
\end{enumerate}

\[\xorxA\]

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  \textbf{Causality Present:} \(A\) causes \(B\): absent conditioning
  that blocks them, \(A\) and \(B\) will be statistically associated.
\end{enumerate}

\[\xtoxA\]

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  \textbf{The Fork Structure:} \(A\) causes \(B\) and \(A\) causes
  \(C\): absent conditioning on \(A\), \(B\) and \(C\) will be
  statistically associated. Conditional on \(A\), \(B\) and \(C\) will
  be independent.
\end{enumerate}

\[\forkTINY\]

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{3}
\tightlist
\item
  \textbf{The Chain Structure:}: \(A\) causes \(B\) and \(B\) causes
  \(C\): absent conditioning on \(B\), \(A\) and \(C\) will be
  statistically associated. Conditioning on \(B\), \(A\) and \(C\) will
  be independent.
\end{enumerate}

\[\chainTINY\]

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{4}
\tightlist
\item
  \textbf{A Collider Structure:} \(A\) causes \(C\) and \(B\) causes
  \(C\): absent conditioning on \(C\), \(A\) and \(B\) will be
  statistically independant. Conditioning on \(C\), \(A\) and \(B\) will
  be statistically associated.
\end{enumerate}

\[\immoralityTINY\]

From these five elementary structures we discovered four rules that
allow us to use these structures to evaluate confounding and its
control:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{The Fork Rule:} When a common cause influences both treatment
  and outcome, condition on it to avoid bias.
\item
  \textbf{The Chain Rule:}
\end{enumerate}

\begin{enumerate}
\def\labelenumi{(\roman{enumi})}
\tightlist
\item
  For total effect estimates, avoid conditioning on mediators within the
  causal path.
\item
  For mediation analysis, ensure potential confounders do not introduce
  bias. )(Note: mediation analysis is complex
  (\citeproc{ref-bulbulia2023}{Bulbulia 2023};
  \citeproc{ref-vanderweele2015}{VanderWeele 2015};
  \citeproc{ref-vansteelandt2012}{Vansteelandt \emph{et al.} 2012}).)
\end{enumerate}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  \textbf{The Collider Rule}: conditioning on a common effect opens a
  path between the two variables that cause it.\\
\item
  \textbf{The Proxy Rule}: conditioning on a descendant is a proxy for
  conditioning on its parent.
\end{enumerate}

\textbf{Take-Home Message}: causal diagrams bring structure to complex
environmental psychology systems. They promote critical thinking about
relationships, improving study design and the chances of isolating true
causal effects. However, causal diagrams are built on assumptions.
\textbf{All structure relationships in a graph except the focal
treatment/outcome relationship must be assumed.} Observational data
alone cannot prove causation. This is because many diagrams are
typically consistent with the data. The power of causal diagrams lies in
helping investigators understand how their assumptions and the data
interact. However, because every path except the \(A\to Y\) path is
assumed, causal diagrams should be created in collaboration with area
experts. When experts disagree, multiple causal diagrams should be
proposed to reflect the implications of disagreements for causal
inference.

\subsubsection{Causal Diagrams and the Identification Problem in Causal
Inference}\label{causal-diagrams-and-the-identification-problem-in-causal-inference}

The \textbf{identification problem} centers on whether we can derive the
true causal effect of a treatment (\(A\)) on an outcome (\(Y\)) from
observed data. Causal diagrams, rooted in \textbf{structural
assumptions} about the underlying causal relationships, are
indispensable for tackling this challenge. Crucially, these assumptions
cannot be proven by data alone.

Addressing the identification problem has two core components:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Evaluating the Absence of Causality:} before attributing any
  statistical association to causality, we must eliminate non-causal
  sources of correlation. This means:

  \begin{itemize}
  \tightlist
  \item
    \textbf{Identifying confounders:} factors that influence both
    treatment (\(A\)) and outcome (\(Y\)).
  \item
    \textbf{Adjustment strategies:} employing methods to control for
    confounders.
  \item
    \textbf{Blocking backdoor paths:} backdoor paths are those that
    create indirect, non-causal links between \(A\) and \(Y\). Our goal
    is to achieve d-separation between \(A\) and \(Y\) by adjusting for
    confounders, effectively blocking these misleading paths.
  \item
    \textbf{Task 1:} ensure adjustments genuinely interrupt non-causal
    connections between treatment and outcome.
  \end{itemize}
\item
  \textbf{Assessing the Presence of Causality:} after addressing
  potential confounders, we must ensure any remaining association
  between \(A\) and \(Y\) reflects a true causal relationship. Here, the
  danger is \textbf{over-conditioning bias:}

  \begin{itemize}
  \tightlist
  \item
    \textbf{Caution:} as we have considered with mediator bias and
    collider bias, adjustments can sometimes introduce or worsen bias,
    making causal effects harder to isolate.
  \item
    \textbf{Task 2:} verify that any association between \(A\) and \(Y\)
    after adjustment is indeed causal, not an analysis artifact.
  \end{itemize}
\end{enumerate}

Thus, causal inference demands a delicate balance: identify and control
for confounders, but avoid introducing new biases. Causal diagrams help
visualize this complexity, ensuring our focus remains on genuine causal
effects within observational data.

\subsubsection{How to Create Causal Diagrams to Address Identification
Problems
\{sec-how-to-create-causal-diagrams\}}\label{how-to-create-causal-diagrams-to-address-identification-problems-sec-how-to-create-causal-diagrams}

\paragraph{Step 1. Clarify the Research Question Evaluated by the
Diagram}\label{step-1.-clarify-the-research-question-evaluated-by-the-diagram}

\begin{itemize}
\tightlist
\item
  State the problem your diagram addresses, and the population for whom
  the problem applies. Causal identification strategies may vary by
  question. For example, the confounding control strategy for evaluating
  the path \(L\to Y\) will differ from that of evaluating the path
  \(A\to Y\). For this reason, reporting co-efficients other than the
  association between \(A \to Y\) is typically ill-advised see Westreich
  and Greenland (\citeproc{ref-westreich2013}{2013}); McElreath
  (\citeproc{ref-mcelreath2020}{2020}); Bulbulia
  (\citeproc{ref-bulbulia2023}{2023}).
\end{itemize}

\paragraph{Step 2. Include all common causes of the exposure and
outcome}\label{step-2.-include-all-common-causes-of-the-exposure-and-outcome}

\begin{itemize}
\tightlist
\item
  Include both measured and unmeasured common causes.
\item
  Group functionally similar common causes under a single variable for
  clarity (e.g., \(L_0\) for demographics).
\end{itemize}

\paragraph{Step 3. Include all ancestors of measured confounders linked
with the treatment, the outcome, or
both}\label{step-3.-include-all-ancestors-of-measured-confounders-linked-with-the-treatment-the-outcome-or-both}

\begin{itemize}
\tightlist
\item
  This helps address unmeasured confounding and reduce biases like
  M-bias.
\item
  Again, group functionally similar variables for a simplified visual
  representation.
\end{itemize}

\paragraph{Step 4. Explicitly state assumptions about relative
timing}\label{step-4.-explicitly-state-assumptions-about-relative-timing}

\begin{itemize}
\tightlist
\item
  Use time subscripts (e.g., \(L_0\), \(A_1\), \(Y_2\)) to denote the
  assumed order of events.
\item
  \textbf{Key Requirement:} Diagrams must be acyclic (no feedback loops)
  for clear causal direction.
\end{itemize}

\paragraph{Step 5. Arrange temporal order of causality
visually}\label{step-5.-arrange-temporal-order-of-causality-visually}

\begin{itemize}
\tightlist
\item
  Left-to-right or top-to-bottom flows aid understanding of causal
  assertions (\citeproc{ref-bulbulia2023}{Bulbulia 2023}).
\item
  As we shall consider repeatedly in \hyperref[sec-part3]{\textbf{Part
  3}}, estabilishing temporal ordering is neccessary for evaluating
  identification problems. Time orders causality; the spatial layout of
  your causal diagram should therefore respect time.
\end{itemize}

\paragraph{Step 6. Box variables that are conditioned
on}\label{step-6.-box-variables-that-are-conditioned-on}

\begin{itemize}
\tightlist
\item
  Typically, you will not box treatments/outcome unless measured with
  error.
\end{itemize}

\paragraph{Step 7. Represent paths structurally, not
parametrically}\label{step-7.-represent-paths-structurally-not-parametrically}

\begin{itemize}
\tightlist
\item
  Focus on whether paths exist, not their exact functional form (linear,
  non-linear, etc.).
\item
  Parametric descriptions are not relevant for bias evaluation in a
  causal diagram. (For an explanation of causal interaction and causal
  diagrames see: Bulbulia (\citeproc{ref-bulbulia2023}{2023}).)
\end{itemize}

\paragraph{Step 8. Minimise paths to those necessary for the
identification
problem}\label{step-8.-minimise-paths-to-those-necessary-for-the-identification-problem}

\begin{itemize}
\tightlist
\item
  Reduce clutter; only include paths critical for a specific question
  (e.g., backdoor paths, mediators).
\end{itemize}

\paragraph{Step 9. Consider Potential Unmeasured
Confounders}\label{step-9.-consider-potential-unmeasured-confounders}

\begin{itemize}
\tightlist
\item
  Use your domain knowledge to hypothesize where in the diagram
  unmeasured confounders \emph{might} occur.
\item
  This aids in realistically assessing the limitations of your diagram.
\end{itemize}

\paragraph{\texorpdfstring{\textbf{Step 10. State Graphical
Conventions}}{Step 10. State Graphical Conventions}}\label{step-10.-state-graphical-conventions}

\begin{itemize}
\tightlist
\item
  Explain your symbol use (e.g., red for open backdoor paths).
\item
  Consistency aids interpretation, and verbal descriptions add
  accessibility.
\end{itemize}

\textbf{In the next section, we will consider see causal diagrams,
through rules about forks, chains, and colliders, help guide researchers
towards obtaining consistent causal estimates from observational data.}

\subsection{Part 3. Using Causal Diagrams for Causal Identification -
Worked Examples}\label{sec-part3}

\subsubsection{Notation}\label{notation}

Causal diagrams use specific symbols to represent elements important in
causal inference (\citeproc{ref-greenland1999}{Greenland \emph{et al.}
1999}; \citeproc{ref-pearl1995}{Pearl 1995},
\citeproc{ref-pearl2009}{2009a}). However, as mentioned, there is not a
single agreed convention for creating causal diagrams. The following
describes our symbols and meanings we use in this chapter are listed in
Table~\ref{tbl-01}:

\begin{itemize}
\tightlist
\item
  \textbf{\(A\)} is the treatment or exposure variable -- the
  intervention or condition whose effect on an outcome is under
  investigation. \textbf{This symbol represents the cause}.
\item
  \textbf{\(Y\)} is the outcome variable -- the effect or result that is
  being studied. \textbf{This symbol represents the effect}.
\item
  \textbf{\(L\)} includes all measured confounders -- variables that may
  affect both the treatment and the outcome.
\item
  \textbf{\(U\)} includes unmeasured confounders -- variables not
  included in the analysis that could influence both the treatment and
  the outcome, potentially leading to biased conclusions.
\item
  \textbf{\(M\)} is a mediator variable -- a factor through which the
  treatment affects the outcome. The focus here is on identifying the
  total effect of treatment \(A\) on an outcome \(Y\), but it is also
  important to understand how controlling for mediators can affect
  estimates of this total effect.
\end{itemize}

\begin{table}

\caption{\label{tbl-01}Terminology that is used in this article for
causal diagrams (adapted from (\citeproc{ref-bulbulia2023}{Bulbulia
2023})).}

\centering{

\terminologylocalconventionssimple

}

\end{table}%

We next describe our graphical conventions causal diagrams. Again,
because conventions may differ it is always important to state them
explicitly when reporting causal diagrams. Table~\ref{tbl-02} describes
the basic conventions that we employ in this chapter.

\begin{table}

\caption{\label{tbl-02}Basic conventions for causal diagrams (adapted
from (\citeproc{ref-bulbulia2023}{Bulbulia 2023})).}

\centering{

\terminologygeneralbasic

}

\end{table}%

\subsubsection{Graphical Table}\label{graphical-table}

Table~\ref{tbl-04} provides seven worked-examples that put causal
diagrams to work. Our example will focus on the question of whether
access to green space affects happiness, and approach this question by
focussing on how different assumptions about the (i) the structure of
the world and (ii) the observational data that have been collected may
affect strategies for confounding control and the confidence in our
results. Each example refers to a row in the table.

\begin{table}

\caption{\label{tbl-04}Worked examples: This table is adapted from
(\citeproc{ref-bulbulia2023}{Bulbulia 2023}).}

\centering{

\terminologyelconfoundersLONG

}

\end{table}%

\subsubsection{1. The problem of confounding by a common
cause}\label{the-problem-of-confounding-by-a-common-cause}

Table~\ref{tbl-04} 1 describes the problem of confounding by common
cause and its solution. We encountered this problem in Part 1. Such
confounding arises when there is a variable or set of variables, denoted
by \(L\), that influence both the exposure, denoted by \(A\), and the
outcome, denoted by \(Y.\) Because \(L\) is a common cause of both \(A\)
and \(Y\), \(L\) may create a statistical association between \(A\) and
\(Y\) that does not reflect a causal association.

For instance, in the context of green spaces, consider that people who
choose to live closer to green spaces (exposure \(A\)) and their
experience of improved happiness (outcome \(Y\)). A common cause might
be socioeconomic status \(L\). Individuals with higher socioeconomic
status might have the financial capacity to afford housing near green
spaces and simultaneously afford better healthcare and lifestyle
choices, contributing to greater happiness. Thus, although the data may
show a statistical association between living closer to green spaces
\(A\) and greater happiness \(Y\), this association might not reflect a
direct causal relationship owing to confounding by socioeconomic status
\(L\).

How might we obtain balance in this confounder for the treatments to be
compared? Addressing confounding by a common cause involves adjusting
for the confounder in one's statistical model. This may be done through
regression, or more complicated methods, such as inverse probability of
treatment weighting, marginal structural models, and others see Hernán
and Monge (\citeproc{ref-hernuxe1n2023}{2023}). Such adjustment
effectively closes the backdoor path from the exposure to the outcome.
Equivalently, conditioning on \(L\) d-separates \(A\) and \(Y\).

Table~\ref{tbl-04} Row 1, Column 3, emphasises that a confounder by
common cause must precede both the exposure and the outcome. While it is
often clear that a confounder precedes the exposure (e.g., a person's
country of birth), in other cases, the timing might be uncertain. By
positioning the confounder before the exposure in our causal diagrams,
we assert its temporal precedence. However, when relying on
cross-sectional data, such a timing assumption might be strong. In such
cases, exploring causal scenarios where the confounder follows the
treatment or outcome can be insightful. Causal diagrams are instrumental
in examining possible timings and their implications for causal
inference.

Next, we examine the effects of conditioning on a variable that is an
effect of the treatment.

\subsubsection{2. Mediator Bias}\label{mediator-bias}

Consider again the question of whether proximity to green spaces, \(A\),
affects happiness, \(Y\). Suppose that physical activity is a mediator,
\(L\).

To fill out the example, imagine that living close to green spaces \(A\)
influences physical activity \(L\), which subsequently affects happiness
\(Y\). Notice that if we were to condition on physical activity \(L\),
assuming it to be a confounder, we would then bias our estimates of the
total effect of proximity to green spaces \(A\) on happiness \(Y\). Such
a bias arises as a consequence of the chain rule. Conditioning on \(L\)
``d-separates'' the total effect of \(A\) on \(Y\). This phenomenon is
known as mediator bias. Notably, Montgomery \emph{et al.}
(\citeproc{ref-montgomery2018}{2018}) finds dozens of examples of
mediator bias in \emph{experiments} in which control is made for
variables that occur after the treatment. For example, the practice of
obtaining demographic and other information from participants
\emph{after} a study is an invitation to mediator bias. If the treatment
affects these variables, and the variables affect the outcome (as we
assume by controlling for them), then researchers may induce mediator
bias.

To avoid mediator bias when estimating a total causal effect we should,
of course, avoid conditioning on a mediator! The surest way to avoid
this problem is to ensure that \(L\) occurs before the treatment \(A\)
as well as before the outcome \(Y\). This solution is presented
Table~\ref{tbl-04} Row 2 Col 3.

\subsubsection{3. Confounding by Collider Stratification (Conditioning
on a Common
Effect)}\label{confounding-by-collider-stratification-conditioning-on-a-common-effect}

Conditioning on a common effect, also known as collider stratification,
occurs when a variable, denoted by \(L\), is influenced by both the
exposure, denoted by \(A\), and the outcome, denoted by \(Y\).

Imagine, again the context of an access to green space question, that an
individual's choice to live closer to green spaces (exposure \(A\)) and
their happiness (outcome \(Y\)) both affect the individual's overall
sense of physical health (common effect \(L\)). Initially, \(A\) and
\(Y\) could be independent, that is \(A \coprod Y(a)\), suggesting that
the decision to live near green spaces is not directly a cause of
happiness.

However, if we were to condition on physical health \(L\) (the common
effect of \(A\) and \(Y\)), a backdoor path between \(A\) and \(Y\)
would be opened. That is, by ``controlling for'' physical health we
might \emph{induce} a non-causal association between proximity to green
spaces and happiness.

The reason that conditioning on physical health \(L\) leads to
confounding is that this variable provides information about both the
proximity to green spaces \(A\) and one's happiness \(Y\). Once we learn
something about one's physical health, for example, that it is poor, it
becomes more probable that a person is happy if they have low access to
green spaces (assuming the relationship between green space access and
happiness is positive.)

Causal diagrams point a way to respond to the problem of collider
stratification bias: we should generally ensure that:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  All confounders \(L\) that are common causes of the exposure \(A\) and
  the outcome \(Y\) are measured before \(A\) has occurred, and
\item
  \(A\) is measured before \(Y\) has occurred.
\end{enumerate}

If such temporal order is preserved, \(L\) cannot be an effect of \(A\),
and thus neither of \(Y\).

\subsubsection{4. Confounding by Conditioning on a Descendant of a
Confounder}\label{confounding-by-conditioning-on-a-descendant-of-a-confounder}

The rules of d-separation also apply to conditioning on descendants of a
confounder. As shown in Table~\ref{tbl-04} Row 4, when conditioning on a
measured descendant of an unmeasured collider we may unwittingly evoke
confounding by proxy. For example, if doctor visits were encoded in our
data and doctor visits were an effect of poor health, then conditioning
on doctor visits would function in a similar way to conditioning on poor
health, introducing collider confounding.

There are only four elementary forms of confounding. Any confounding
scenario we might imagine can be developed from these elementary forms.
We next consider how we may combine these elementary causal
relationships in causal diagrams to develop effective strategies for
confounding control.

\subsubsection{5. M-bias: Conditioning on Pre-Exposure
Collider}\label{m-bias-conditioning-on-pre-exposure-collider}

Table~\ref{tbl-04} Row 5 presents a form of pre-exposure
over-conditioning confounding known as ``M-bias''. This bias combines
the collider structure and the fork structure revealing what might not
otherwise be obvious: it is possible to induce confounding even if we
ensure that all variables have been measured \textbf{before} the
treatment. The collider structure is evident in the path \(U_Y \to L_0\)
and \(U_A \to L_0\). We know from the collider rule that conditioning on
\(L_0\) opens a path between \(U_Y\) and \(U_A\). What is the result? We
find that \(U_Y\) is associated with the outcome \(Y\) and \(U_A\) is
associated with treatment \(A\). This is a fork (common cause)
structure. The association between treatment and outcome that is opened
by conditioning on \(L\) arises from an open back-door path that occurs
from the collider structure. We thus have confounding. How might such
confounding play out in a real-world setting?

In the context of green spaces, consider the scenario where an
individual's level of physical activity \(L\) is influenced by an
unmeasured factor related to their propensity to live near green spaces
\(A\) -- say childhood upbringing. Suppose further that another
unmeasured factor -- say a genetic factor -- increases both physical
activity \(L\) and happiness \(Y\). Here, physical activity \(L\) does
not affect the decision to live near green spaces \(A\) or happiness
\(Y\) but is a descendent of unmeasured variables that do. If we were to
condition on physical activity \(L\) in this scenario, we would create
the bias just described -- ``M-bias.''

How shall we respond to this problem? The solution is straightforward.
If \(L\) is neither a common cause of \(A\) and \(Y\) nor the effect of
a shared common cause, then \(L\) should not be included in a causal
model. In terms of the conditional exchangeability principle, we find
\(A \coprod Y(a)\) yet \(A \cancel{\coprod} Y(a)| L\). So we should not
condition on \(L\): do not control for exercise
(\citeproc{ref-cole2010}{Cole \emph{et al.} 2010}).\footnote{Note that
  when we draw a chronologically ordered path from left to right the M
  shape for which ``M-bias'' takes its name changes to an E shape We
  shall avoid proliferating jargon and retain the term ``M bias.''}

\subsubsection{6. Conditioning on a Descendent May Sometimes Reduce
Confounding}\label{conditioning-on-a-descendent-may-sometimes-reduce-confounding}

Consider how we may use the rules of d-separation to obtain unexpected
strategies for confounding control. In Table~\ref{tbl-04} Row 6, we
encounter a causal diagram in which an unmeasured confounder opens a
back-door path that links the treatment and outcome. We have what
appears to be intractable confounding. Return to our green space
example. Suppose an unmeasured genetic factor \(U\) affects one's desire
to seek out isolation in green spaces \(A\) and also independently
affects one's happiness \(Y\). Were such an unmeasured confounder to
exist, we could not obtain an unbiased estimate for the causal effect of
green space access on happiness. However, imagine a variable
\(L^\prime\) that is a trait that is expressed later in life, which
arises from this genetic factor. If such a trait could be measured, even
though the trait \(L'\) is expressed after the treatment and outcome
have occurred, controlling for \(L'\) would enable investigators to
close the backdoor path between the treatment and the outcome. The
reason this strategy works is that a measured effect is a \emph{proxy}
for its cause \(U\), the unmeasured confounder. By conditioning on the
late-adulthood trait, \(L'\), we partially condition on its cause,
\(U\), the confounder of \(A \to Y\). Thus, not all effective
confounding control strategies need to rely on measuring pre-exposure
variables.

\subsubsection{7. Confounding Control with Three Waves of Data is
Powerful and Reveals Possibilities for Estimating an ``Incident
Exposure''
Effect}\label{confounding-control-with-three-waves-of-data-is-powerful-and-reveals-possibilities-for-estimating-an-incident-exposure-effect}

Table~\ref{tbl-04} row 7 presents another setting in which there is
unmeasured confounding. In response to this problem, we use the rules of
d-separation to develop a strategy for data collection and modelling
that may greatly reduce the influence of unmeasured confounding.
Table~\ref{tbl-04} row 7 col 3, by collecting data for both the
treatment and the outcome at baseline and controlling for baseline
values of the treatment and outcome, any unmeasured association between
the treatment \(A_1\) and the outcome \(Y_2\) would need to be
\emph{independent} of their baseline measurements. As such, including
the baseline treatment and outcome, along with other measured covariates
that might be measured descendants of unmeasured confounders, is a
strategy that exerts considerable confounding control
(\citeproc{ref-vanderweele2020}{VanderWeele \emph{et al.} 2020}).

Furthermore, this causal graph makes evident a second benefit of this
strategy. Returning to our example, a model that controls for baseline
exposure would require that people initiate a change from the level of
\(A_0\) observed baseline. Thus, by controlling for the baseline value
of the treatment, we may learn about the causal effect of shifting one's
access to green space status. This effect is called the ``incident
exposure effect.'' The incident exposure effect better emulates a
``target trial'' or the the organisation of observational data into a
hypothetical experiment in which there is a ``time-zero'' initiation of
treatment in the data; see Hernán \emph{et al.}
(\citeproc{ref-hernuxe1n2016}{2016}); Danaei \emph{et al.}
(\citeproc{ref-danaei2012}{2012}); VanderWeele \emph{et al.}
(\citeproc{ref-vanderweele2020}{2020}); Bulbulia
(\citeproc{ref-bulbulia2022}{2022}). Without controlling for the
baseline treatment, we could only estimate a ``prevelant exposure
effect.'' If the initial exposure caused people some people to be very
unhappy, we would not be able to track this outcome. The prevelant
exposure effect would mask it, distorting causal inferences for the
quantity of interest, namely, what would happen, on average, if people
were to shift to having greater greenspace access.

Finally, by controlling for both the baseline treatment and the baseline
outcome, we obtain still further control for unmeasured confounding. For
an unmeasured confounder to affect both the treatment and the outcome
(and unmeasured fork structure) it would need to do so independently of
the baseline measures of the treatment and exposure
(\citeproc{ref-vanderweele2020}{VanderWeele \emph{et al.} 2020}).

Thus, to obtain an incident exposure effect, and to exert stronger
control for unmeasured confounding using past states of the treatment
and outcome, we generally require that events in the data can be
accurately classified into at least three relative time intervals. We
must must then model the treatments and outcomes as separate elements in
our statistical model.

\subsection{Part 4. Practical Guide For Constructing Causal Diagrams and
Reporting Results When Causal Structure is Unclear}\label{sec-part4}

\subsubsection{Cross-sectional designs}\label{cross-sectional-designs}

In environmental psychology, researchers often grapple with whether
causal inferences can be drawn from cross-sectional data, especially
when longitudinal data are not available. The challenge is not unique to
cross-sectional designs; even longitudinal studies require careful
assumption-management. We next discuss how causal diagrams can guide
inference in both data types, with examples relevant to environmental
psychologists.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Graphically encode causal assumptions}: causal inference turns
  on assumptions. Although cross-sectional analyses typically demand
  much stronger assumptions owing to the snapshot nature of data, these
  assumptions, when transparently articulated, do not always bar causal
  analysis. By stating different assumptions, and modelling the data in
  accordance with these assumptions, we might find that certain causal
  conclusions are robust to these differences. Where the implications of
  different assumptions disagree, we can often better determine the
  forms of data collection that would be required to settle such
  differences. Below we consider an example where assumptions point to
  different conclusions, revealing the need to collect time-series data
  to assess whether a variable is a confounder or a mediator.
\item
  \textbf{Stable confounders}: in cross-sectional studies, some
  confounders are inherently stable over time, such as ethnicity, year
  and place of birth, and biological gender. For environmental
  psychologists examining the relationship between access to natural
  environments and psychological well-being, these stable confounders
  can be adjusted for without concern for introducing bias from
  mediators or colliders. For example, conditioning on year of birth can
  help to isolate the effect of recent urban development on mental
  health, independent of generational differences in attitudes toward
  green spaces.
\item
  \textbf{Invariable confounders}: other confounders, while not
  immutable, are less likely to be influenced by the treatment.
  Variables such as sexual orientation, educational attainment, and
  often income level fall into this category. For instance, the effect
  of exposure to polluted environments on cognitive outcomes can be
  analysed by conditioning on education level, assuming that recent
  exposure to pollution is unlikely to retroactively change someone's
  educational history.
\item
  \textbf{Timing and reverse causation}: the sequence of treatment and
  outcome is crucial. In some cases, the temporal order is clear,
  reducing concerns about reverse causation. Mortality is a definitive
  outcome where the timing issue is unambiguous. If researching the
  effects of air quality on mortality, the causal direction (poor air
  quality leading to higher mortality rates) is straightforward.
\item
  \textbf{Multiple causal diagrams}: given the complexity of
  environmental influences on psychological outcomes, it's prudent to
  construct multiple causal diagrams to cover various hypothetical
  scenarios. For example, when studying the effect of community green
  space on stress reduction, one diagram might assume direct benefits of
  green space on stress, while another might include potential mediators
  like physical activity. By analysing and reporting findings based on
  multiple diagrams, researchers can explore the robustness of their
  conclusions across different theoretical frameworks.
\end{enumerate}

Table~\ref{tbl-cs} describes ambigious confounding control setting
arising from cross-sectional data. Suppose again we are interested in
the causal effect of access to greenspace denoted by \(A\) on
``happiness,'' denoted by \(Y\). We are uncertain whether excercise,
denoted by \(L\), is a common cause of \(A\) and \(Y\) and thus a
confounder, or whether excercise is a mediator along the path from \(A\)
to \(Y\). We may use causal diagrams to investigate the consequences of
such ambiguity.

\textbf{Assumption 1: Exercise is a common cause of \(A\) and \(Y\)},
this scenario is presented in Table~\ref{tbl-cs} row 1. Here, our
strategy for confounding control is to estimate the effect of \(A\) on
\(Y\) conditioning on \(L\).

\textbf{Assumption 2: Exercise is a mediator of \(A\) and \(Y\)}, this
scenario is presented in Table~\ref{tbl-cs} row 2. Here, our strategy
for confounding control is to simply estimate the effect of \(A\) on
\(Y\) without including \(L\) (assuming there are no other common causes
of the treatment and outcome).

\begin{table}

\caption{\label{tbl-cs}This table is adapted from
(\citeproc{ref-bulbulia2023}{Bulbulia 2023})}

\centering{

\examplecrosssection

}

\end{table}%

To clarify how answer may differ we can simulate data and run separate
regressions, reflecting the different conditioning strategies embedded
in the different assumptions. The following simulation generates data
from a process in which exercise is a mediator (Scenario 2). (See
Appendix C)

\begin{table}
\caption{Code for a simulation of a data generating process in which the effect
of excercise (L) fully mediates the effect of greenspace (A) on
happiness (Y).}\tabularnewline

\centering
\begin{tabular}{lcccccc}
\toprule
\multicolumn{1}{c}{ } & \multicolumn{3}{c}{Model: Exercise assumed confounder} & \multicolumn{3}{c}{Model: Exercise assumed to be a mediator} \\
\cmidrule(l{3pt}r{3pt}){2-4} \cmidrule(l{3pt}r{3pt}){5-7}
\textbf{Characteristic} & \textbf{Beta} & \textbf{95\% CI} & \textbf{p-value} & \textbf{Beta} & \textbf{95\% CI} & \textbf{p-value}\\
\midrule
A & -0.27 & -0.53, -0.01 & 0.043 & 2.9 & 2.6, 3.2 & <0.001\\
L & 1.6 & 1.5, 1.7 & <0.001 &  &  & \\
\bottomrule
\multicolumn{7}{l}{\rule{0pt}{1em}\textsuperscript{1} CI = Confidence Interval}\\
\end{tabular}
\end{table}

This table presents us the conditional treatment effect. Where an
outcome is continuous and there are no interactions, the coeficient for
the treatment (\(A_1\)) reflect the average treatment effect. With
covariates and interactions, to estimate an average treatment effects
requires additional steps. We present code for obtaining marginal
treatment effects in \hyperref[appendix-c]{Appendix C}

On the assumptions outlined in Table~\ref{tbl-cs} row 1, in which we
\emph{assert} that exercise is a confounder, the average treatment
effect of access to green space on happiness is ATE = 2.92, CI =
{[}2.66, 3.21{]}.

On the assumptions outlined in Table~\ref{tbl-cs} row 2, in which we
\emph{assert} that exercise is a mediator, the average treatment effect
of access to green space on happiness is ATE = -0.27, CI = {[}-0.52,
-0.01{]}.

Note that although the mediator \(L\) is ``highly significant'',
including it in the model turns out to be a mistake. We obtain a
negative effect estimate for the causal effect of greenspace access on
happiness.

With only cross-sectional data, we must infer the results are
inconclusive, and that longitudinal data must be collected to obtain
valid causal inferences. Such understanding, although perhaps not the
definitive answer we sought, is nevertheless progress. The result tells
us we should not be overly confident with our analysis (whatever
p-values we recover!), and it clarifies that longitudinal data are
needed.

These findings illustrate the role that assumptions about the relative
timing of exercise as a confounder or as a mediator plays.

\subsubsection{Recommendations for Conducting and Reporting Causal
Analyses with Cross-Sectional
Data}\label{recommendations-for-conducting-and-reporting-causal-analyses-with-cross-sectional-data}

When analyzing and reporting analyses with cross-sectional data,
researchers face the challenge of making causal inferences without the
benefit of temporal information.

The following recommendations aim to guide researchers in navigating
these challenges effectively:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Draw multiple causal diagrams}: draw a variety of causal
  diagrams to represent different theoretical assumptions about the
  relationships and timing of variables relevant to an identification
  problem. This approach facilitates a comprehensive exploration of
  potential causal pathways, clarifying the roles variables may play as
  confounders, mediators, or colliders. For example, in studying the
  effect of urban green spaces on mental health, consider diagrams that
  account for both direct effects and pathways involving mediators like
  physical activity or social interaction.
\item
  \textbf{Perform and report analyses for each assumption}: conduct and
  transparently report separate analyses for each scenario depicted by
  your causal diagrams. This ensures that the analysis is theoretically
  grounded for each model. Presenting results from each analytical
  approach, along with the underlying assumptions and statistical
  methods, promotes a balanced interpretation of findings. Although this
  practice may be unfamiliar to some editors and reviewers, it is
  crucial for addressing the inherent challenges of cross-sectional
  analysis by expanding the scope of investigation beyond a single
  hypothesis.
\item
  \textbf{Interpret findings with attention to ambiguities}: interpret
  results carefully, highlighting any ambiguities or inconsistencies
  across analyses. Discuss how varying assumptions about structural
  relationships and the timing of events can lead to divergent
  conclusions. For instance, if access to green spaces appears to have a
  positive effect on mental health when considering exercise as a
  mediator, but a negative effect when considered a confounder, explore
  the theoretical and empirical implications.
\item
  \textbf{Exercise caution with divergent dindings}: approach
  conclusions with caution, especially when findings suggest differing
  practical implications. Acknowledge the limitations of cross-sectional
  data in establishing causality and the potential for alternative
  explanations.
\item
  \textbf{Suggest specific avenues for future research}: identify and
  recommend specific, targeted future research efforts that could
  clarify the relationships among variables. Highlight the value of
  longitudinal studies or experiments in providing temporal information
  that cross-sectional studies lack.
\item
  \textbf{Supplement observational data with simulated data}: leverage
  data simulation to understand the complexities of causal inference.
  Simulating data based on various theoretical models allows researchers
  to explore the impact of different assumptions on their findings. This
  method tests analytical strategies under controlled conditions,
  assessing the robustness of conclusions against assumption violations
  or unobserved confounders.
\item
  \textbf{Conduct sensitivity analyses to assess robustness}: implement
  sensitivity analyses to determine how dependent conclusions are on
  specific assumptions or parameters within your causal model. Use data
  simulation as a tool for these analyses, evaluating the sensitivity of
  results to various theoretical and methodological choices.
\end{enumerate}

\textbf{Panel Data Consideration}: before proceeding with
cross-sectional analysis, examine whether panel data are available.
Longitudinal data can provide crucial temporal information that aids in
establishing causality, offering a more robust framework for causal
inference. If longitudinal data are not available, the recommendations
above become even more critical for making the best use of
cross-sectional data.

By following these recommendations, you can more effectively navigate
the inherent limitations of cross-sectional data by appropriatly
bounding uncertainties in your causal inferences.

\subsubsection{Longitudinal Designs}\label{longitudinal-designs}

Causation occurs in time. Longitudinal designs offer a substantial
advantage over cross-sectional designs for causal inference because
sequential measurements allow us to capture causation, and quantify its
magnitude. We typically do not need to assert timing as we do in
cross-sectional data settings. Because we know when variables have been
measured, we can reduce ambiguity about the directionality of causal
relationships. For instance, tracking changes in ``happiness'' following
changes in access to green spaces over time can more definitively
suggest causation than cross-sectional snapshots.

Despite this advantage, longitudinal researchers still face assumptions
regarding the absence of unmeasured confounders and the stability of
over time. These assumptions must be explicitly stated. As with
cross-sectional designs, wherever assumptions differ, researchers should
draw different causal diagrams that reflect these assumptions, and
subsequently conduct and report separate analyses.

In this section, we simulate a dataset to demonstrate the benefits of
incorporating both baseline exposure and baseline outcomes into the
analysis of the effect of access to open green spaces on happiness. This
approach allows us to control for initial levels of exposure and
outcomes, offering a clearer understanding of the causal relationship.
\hyperref[appendix-d-simulation-of-different-confounding-control-strategies]{Appendix
D} provides the code.
\hyperref[appendix-e-non-parametric-estimation-of-average-treatment-effects-using-causal-forests-appendix-causal-forests]{Appendix
E} provides an example of a non-parametric estimator for the causal
effect. HAs mentioned before, by conditioning on baseline levels of
access to green spaces and baseline mental health, researchers can more
accurately estimate the \emph{incident effect} of changes in green space
access on changes in mental health. Table~\ref{tbl-lg} offers an example
for how we may use multiple causal diagrams to clarify the problem, as
well as to clarify our confounding control strategy.

\begin{table}

\caption{\label{tbl-lg}This table is adapted from
(\citeproc{ref-bulbulia2023}{Bulbulia 2023})}

\centering{

\examplelongitudinal

}

\end{table}%

In our analysis, we assessed the average treatment effect (ATE) of
access to green spaces on happiness across three distinct models:
uncontrolled, standard controlled, and interaction controlled. These
models were constructed using a hypothetical cohort of 10,000
individuals, incorporating baseline exposure to green spaces (\(A_0\)),
baseline happiness (\(Y_0\)), baseline confounders (\(L_0\)), and an
unmeasured confounder (\(U\)). The detailed simulation process and model
construction are delineated in
\hyperref[appendix-simulate-longitudinal-ate]{Appendix D}.

The ATE estimates from these models provide critical insights into the
effects of green space exposure on individual happiness while accounting
for various confounding factors. The model without control variables
estimated ATE = 1.55, CI = {[}1.47, 1.63{]}, significantly
overestimating the treatment effect. Incorporating standard covariate
control reduced this estimate to ATE = 0.86, CI = {[}0.8, 0.92{]},
aligning more closely with the expected effect but still overestimating.
Most notably, the model that included interactions among baseline
exposure, outcome, and confounders yielded ATE = 0.29, CI = {[}0.27,
0.31{]}, approximating the true effect of 0.3. This finding underscores
the importance of including baseline values of the exposure and outcome
wherever at these data are available.

\subsubsection{Recommendations for Conducting and Reporting Causal
Analyses with Longitudinal
Data}\label{recommendations-for-conducting-and-reporting-causal-analyses-with-longitudinal-data}

Longitudinal data offer unique advantages for causal inference by
providing insights into the temporal sequence of events, which is
crucial for establishing causality. The following recommendations aim to
guide researchers in leveraging longitudinal data effectively to conduct
and report causal analyses:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Draw Multiple Causal Diagrams}:

  \begin{itemize}
  \tightlist
  \item
    \textbf{Identification Problem Diagram}: begin by constructing a
    causal diagram that outlines your initial assumptions about the
    relationships among variables, identifying potential confounders and
    mediators. This diagram should illustrate the complexity of the
    identification problem.
  \item
    \textbf{Solution Diagram}: next, create a separate causal diagram
    that proposes solutions to the identified problems. This may involve
    highlighting variables for conditioning to isolate the causal effect
    of interest or suggesting novel pathways for investigation. Having
    distinct diagrams for the problem and its proposed solutions
    clarifies the analytic strategy and theoretical underpinning of your
    study.
  \end{itemize}
\item
  \textbf{Attempt Three-Wave Longitudinal Designs} a three-wave design,
  incorporating data from at least three time points, significantly
  enhances the ability to infer causal relationships. This approach
  allows for the examination of temporal precedence and lagged effects.
  For example, establishing that access to green spaces precedes
  increased physical activity, which then leads to improvements in
  mental health, offers compelling evidence of a causal pathway. This
  temporal sequence is key to substantiating causal claims.
\item
  \textbf{Calculate Average Treatment Effects for the Entire
  Population}: estimating the average treatment effect (ATE) across the
  entire study population provides a comprehensive measure of the
  intervention's effects. This step is crucial for understanding the
  overall effect of the treatment and for generalizing findings to
  broader populations.
\item
  \textbf{Report Results for Each Possible Structure}: given that the
  true causal structure may be complex and partially unknown, it is
  prudent to analyze and report results under each plausible causal
  diagram. This practice acknowledges the uncertainty inherent in causal
  modeling and demonstrates the robustness of findings across different
  theoretical frameworks.
\item
  \textbf{Conduct Sensitivity Analyses}: sensitivity analyses are
  essential for assessing the robustness of your findings to various
  assumptions within the causal model. These analyses can include
  simulations, as illustrated in Appendices C and D, to explore the
  impact of unmeasured confounding, model misspecification, and
  alternative causal pathways on the study conclusions. Sensitivity
  analyses help to identify the conditions under which the findings
  hold, enhancing the credibility of the causal inferences.(For more
  about how to address missing data, see:
  (\citeproc{ref-bulbulia2024PRACTICAL}{Bulbulia 2024}).)
\item
  \textbf{Address Missing Data and Attrition}: longitudinal studies
  often face challenges related to missing data and attrition, which can
  introduce bias and affect the validity of causal inferences. Implement
  and clearly report strategies for handling missing data, such as
  multiple imputation or sensitivity analyses that assess the bias
  arising from missing responses at the study's conclusion. (For more
  about how to address missing data, see:
  (\citeproc{ref-bulbulia2024PRACTICAL}{Bulbulia 2024})).
\end{enumerate}

By following these recommendations, you can more effectively navigate
the inherent limitations of observational longitudinal data, improving
the quality of your causal inferences.

\subsection{Summary}\label{summary}

This chapter has provided an introduction to the potential outcomes
framework for causal inference and the use of directed acyclic graphs
(DAGs) in environmental psychology. In \textbf{Part 1} we discussed
three critical assumptions necessary for estimating average treatment
effects from data:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Conditional Exchangeability}: This assumption posits that the
  allocation of treatment is randomized and independent of potential
  outcomes, conditional on measured covariates.
\item
  \textbf{Causal Consistency}: This assumption asserts that the outcome
  observed under the treatment condition corresponds to the outcome that
  would have been observed had the unit received the treatment, and
  similarly for the control condition.
\item
  \textbf{Positivity}: This assumption asserts that every unit has a
  non-zero probability of receiving any of the treatments under
  comparison.
\end{enumerate}

Although randomized controlled experiments naturally satisfy these
assumptions through design---randomization ensures exchangeability,
control guarantees consistency, and design secures positivity -\/---
observational studies typically do not. To obtain consistent causal
estimates from observational data, we must assess the extent to which
these assumptions can be satisfied.

In \textbf{Part 2}, we explained how causal diagrams work and described
their utility in addressing the assumption of conditional
exchangeability, or the ``no unmeasured confounders'' assumption. We
identified \hyperref[sec-five-elementary]{five fundamental structures}
underlying all causal relationships and discovered
\hyperref[sec-four-rules]{four elementary rules} for evaluating the
implications of conditioning on elements within these structures
regarding observable statistical associations in data. Thus, causal
diagrams provide a simplified visual language for translating complex
causal relationships into data observations. However, the relationships
depicted in these diagrams represent assertions not directly verifiable
from the data. The causal relationships between treatments and outcomes
are the only relationships not based on assertion. Causal diagrams help
us identify structural sources of bias in the statistical associations
between treatments and outcomes that may arise from assumed causal
relationships in the world, potentially associating treatments with
outcomes irrespective of causal links.

In \textbf{Part 3}, we applied causal diagrams to seven common
confounding scenarios, demonstrating that a causal diagram needs to
highlight only those aspects of a causal setting relevant for assessing
structural sources of bias linking treatment and outcome in a non-causal
manner. We focussed on omitting nodes and paths not directly necessary
for our stated identification problem, emphasizing that causal diagrams
are tailored not just to context-dependent questions but also to our
assumptions about the world's causal structure.

In \textbf{Part 4}, we showed how investigators may create multiple
causal diagrams when the structure of a causal problem is ambiguous, and
illustrated the benefits of this approach through data simulation. We
provided guidelines for reporting in scenarios where either only
cross-sectional data are available or when researchers have access to
repeated measures longitudinal data.

Although this discussion has focused on seven specific applications of
causal diagrams, their applicability extends much further. The
straightforward rules governing how variables become associated or
disassociated through conditioning on nodes within basic structures
enable the use of causal diagrams for quantitatively exploring causality
in a myriad of questions. These applications transcend effective
modeling strategies, informing data collection strategies, including the
adoption of repeated measures designs.

It is our hope that this material will inspire environmental
psychologists to deepen their understanding of causal inference and
incorporate causal diagrams into their research practices. The
methodologies for distinguishing causation from correlation are
well-established; there is no longer any justification for overlooking
these tools.

\newpage{}

\subsection{Funding}\label{funding}

This work is supported by a grant from the Templeton Religion Trust
(TRT0418). JB received support from the Max Planck Institute for the
Science of Human History. The funders had no role in preparing the
manuscript or the decision to publish it.

\subsection{Contributions}\label{contributions}

DH proposed the chapter. JB developed the approach and wrote the first
draft. Both authors contributed substantially to the final work.

\subsection{References}\label{references}

\phantomsection\label{refs}
\begin{CSLReferences}{1}{0}
\bibitem[\citeproctext]{ref-bulbulia2024PRACTICAL}
Bulbulia, J (2024) A practical guide to causal inference in three-wave
panel studies. \emph{PsyArXiv Preprints}.
doi:\href{https://doi.org/10.31234/osf.io/uyg3d}{10.31234/osf.io/uyg3d}.

\bibitem[\citeproctext]{ref-bulbulia2022}
Bulbulia, JA (2022) A workflow for causal inference in cross-cultural
psychology. \emph{Religion, Brain \& Behavior}, \textbf{0}(0), 1--16.
doi:\href{https://doi.org/10.1080/2153599X.2022.2070245}{10.1080/2153599X.2022.2070245}.

\bibitem[\citeproctext]{ref-bulbulia2023}
Bulbulia, JA (2023) Causal diagrams (directed acyclic graphs): A
practical guide.

\bibitem[\citeproctext]{ref-bulbulia2023a}
Bulbulia, JA, Afzali, MU, Yogeeswaran, K, and Sibley, CG (2023)
Long-term causal effects of far-right terrorism in {N}ew {Z}ealand.
\emph{PNAS Nexus}, \textbf{2}(8), pgad242.

\bibitem[\citeproctext]{ref-cinelli2022}
Cinelli, C, Forney, A, and Pearl, J (2022) A Crash Course in Good and
Bad Controls. \emph{Sociological Methods \& Research},
00491241221099552.
doi:\href{https://doi.org/10.1177/00491241221099552}{10.1177/00491241221099552}.

\bibitem[\citeproctext]{ref-cole2010}
Cole, SR, Platt, RW, Schisterman, EF, \ldots{} Poole, C (2010)
Illustrating bias due to conditioning on a collider. \emph{International
Journal of Epidemiology}, \textbf{39}(2), 417--420.
doi:\href{https://doi.org/10.1093/ije/dyp334}{10.1093/ije/dyp334}.

\bibitem[\citeproctext]{ref-danaei2012}
Danaei, G, Tavakkoli, M, and Hernán, MA (2012) Bias in observational
studies of prevalent users: lessons for comparative effectiveness
research from a meta-analysis of statins. \emph{American Journal of
Epidemiology}, \textbf{175}(4), 250--262.
doi:\href{https://doi.org/10.1093/aje/kwr301}{10.1093/aje/kwr301}.

\bibitem[\citeproctext]{ref-edwards2015}
Edwards, JK, Cole, SR, and Westreich, D (2015) All your data are always
missing: Incorporating bias due to measurement error into the potential
outcomes framework. \emph{International Journal of Epidemiology},
\textbf{44}(4), 14521459.

\bibitem[\citeproctext]{ref-greenland1999}
Greenland, S, Pearl, J, and Robins, JM (1999) Causal diagrams for
epidemiologic research. \emph{Epidemiology (Cambridge, Mass.)},
\textbf{10}(1), 37--48.

\bibitem[\citeproctext]{ref-greifer2023}
Greifer, N, Worthington, S, Iacus, S, and King, G (2023) \emph{Clarify:
Simulation-based inference for regression models}. Retrieved from
\url{https://iqss.github.io/clarify/}

\bibitem[\citeproctext]{ref-hernan2023}
Hernan, MA, and Robins, JM (2023) \emph{Causal inference}, Taylor \&
Francis. Retrieved from
\url{https://books.google.co.nz/books?id=/_KnHIAAACAAJ}

\bibitem[\citeproctext]{ref-hernuxe1n2023}
Hernán, MA, and Monge, S (2023) Selection bias due to conditioning on a
collider. \emph{BMJ}, \textbf{381}, p1135.
doi:\href{https://doi.org/10.1136/bmj.p1135}{10.1136/bmj.p1135}.

\bibitem[\citeproctext]{ref-hernan2017per}
Hernán, MA, Robins, JM, et al. (2017) Per-protocol analyses of pragmatic
trials. \emph{N Engl J Med}, \textbf{377}(14), 1391--1398.

\bibitem[\citeproctext]{ref-hernuxe1n2016}
Hernán, MA, Sauer, BC, Hernández-Díaz, S, Platt, R, and Shrier, I (2016)
Specifying a target trial prevents immortal time bias and other
self-inflicted injuries in observational analyses. \emph{Journal of
Clinical Epidemiology}, \textbf{79}, 7075.

\bibitem[\citeproctext]{ref-hester2022GLUE}
Hester, J, and Bryan, J (2022) \emph{Glue: Interpreted string literals}.
Retrieved from \url{https://CRAN.R-project.org/package=glue}

\bibitem[\citeproctext]{ref-holland1986}
Holland, PW (1986) Statistics and causal inference. \emph{Journal of the
American Statistical Association}, \textbf{81}(396), 945960.

\bibitem[\citeproctext]{ref-hume1902}
Hume, D (1902) \emph{Enquiries Concerning the Human Understanding: And
Concerning the Principles of Morals}, Clarendon Press.

\bibitem[\citeproctext]{ref-lash2020}
Lash, TL, Rothman, KJ, VanderWeele, TJ, and Haneuse, S (2020)
\emph{Modern epidemiology}, Wolters Kluwer. Retrieved from
\url{https://books.google.co.nz/books?id=SiTSnQEACAAJ}

\bibitem[\citeproctext]{ref-mancuso2018revolutionary}
Mancuso, S (2018) \emph{The revolutionary genius of plants: A new
understanding of plant intelligence and behavior}, Simon; Schuster.

\bibitem[\citeproctext]{ref-mcelreath2020}
McElreath, R (2020) \emph{Statistical rethinking: A {B}ayesian course
with examples in r and stan}, CRC press.

\bibitem[\citeproctext]{ref-montgomery2018}
Montgomery, JM, Nyhan, B, and Torres, M (2018) How conditioning on
posttreatment variables can ruin your experiment and what to do about
It. \emph{American Journal of Political Science}, \textbf{62}(3),
760--775.
doi:\href{https://doi.org/10.1111/ajps.12357}{10.1111/ajps.12357}.

\bibitem[\citeproctext]{ref-neyman1923}
Neyman, JS (1923) On the application of probability theory to
agricultural experiments. Essay on principles. Section 9.(tlanslated and
edited by dm dabrowska and tp speed, statistical science (1990), 5,
465-480). \emph{Annals of Agricultural Sciences}, \textbf{10}, 151.

\bibitem[\citeproctext]{ref-pearl1995}
Pearl, J (1995) Causal diagrams for empirical research.
\emph{Biometrika}, \textbf{82}(4), 669--688.

\bibitem[\citeproctext]{ref-pearl2009}
Pearl, J (2009a) \emph{\href{https://doi.org/10.1214/09-SS057}{Causal
inference in statistics: An overview}}.

\bibitem[\citeproctext]{ref-pearl2009a}
Pearl, J (2009b) \emph{Causality}, Cambridge University Press.

\bibitem[\citeproctext]{ref-pearl2018}
Pearl, J, and Mackenzie, D (2018) \emph{The book of why: The new science
of cause and effect}, Basic books.

\bibitem[\citeproctext]{ref-robins1986}
Robins, J (1986) A new approach to causal inference in mortality studies
with a sustained exposure period---application to control of the healthy
worker survivor effect. \emph{Mathematical Modelling}, \textbf{7}(9-12),
1393--1512.

\bibitem[\citeproctext]{ref-robins2008estimation}
Robins, J, and Hernan, M (2008) Estimation of the causal effects of
time-varying exposures. \emph{Chapman \& Hall/CRC Handbooks of Modern
Statistical Methods}, 553--599.

\bibitem[\citeproctext]{ref-rubin1976}
Rubin, DB (1976) Inference and missing data. \emph{Biometrika},
\textbf{63}(3), 581--592.
doi:\href{https://doi.org/10.1093/biomet/63.3.581}{10.1093/biomet/63.3.581}.

\bibitem[\citeproctext]{ref-rubin2005}
Rubin, DB (2005) Causal inference using potential outcomes: Design,
modeling, decisions. \emph{Journal of the American Statistical
Association}, \textbf{100}(469), 322--331. Retrieved from
\url{https://www.jstor.org/stable/27590541}

\bibitem[\citeproctext]{ref-gtsummary2021}
Sjoberg, DD, Whiting, K, Curry, M, Lavery, JA, and Larmarange, J (2021)
Reproducible summary tables with the gtsummary package. \emph{{The R
Journal}}, \textbf{13}, 570--580.
doi:\href{https://doi.org/10.32614/RJ-2021-053}{10.32614/RJ-2021-053}.

\bibitem[\citeproctext]{ref-grf2024}
Tibshirani, J, Athey, S, Sverdrup, E, and Wager, S (2024) \emph{Grf:
Generalized random forests}. Retrieved from
\url{https://github.com/grf-labs/grf}

\bibitem[\citeproctext]{ref-vanderweele2009}
VanderWeele, TJ (2009) Concerning the consistency assumption in causal
inference. \emph{Epidemiology}, \textbf{20}(6), 880.
doi:\href{https://doi.org/10.1097/EDE.0b013e3181bd5638}{10.1097/EDE.0b013e3181bd5638}.

\bibitem[\citeproctext]{ref-vanderweele2015}
VanderWeele, TJ (2015) \emph{Explanation in causal inference: Methods
for mediation and interaction}, Oxford University Press.

\bibitem[\citeproctext]{ref-vanderweele2018}
VanderWeele, TJ (2018) On well-defined hypothetical interventions in the
potential outcomes framework. \emph{Epidemiology}, \textbf{29}(4), e24.
doi:\href{https://doi.org/10.1097/EDE.0000000000000823}{10.1097/EDE.0000000000000823}.

\bibitem[\citeproctext]{ref-vanderweele2019}
VanderWeele, TJ (2019) Principles of confounder selection.
\emph{European Journal of Epidemiology}, \textbf{34}(3), 211219.

\bibitem[\citeproctext]{ref-vanderweele2013}
VanderWeele, TJ, and Hernan, MA (2013) Causal inference under multiple
versions of treatment. \emph{Journal of Causal Inference},
\textbf{1}(1), 120.

\bibitem[\citeproctext]{ref-vanderweele2020}
VanderWeele, TJ, Mathur, MB, and Chen, Y (2020) Outcome-wide
longitudinal designs for causal inference: A new template for empirical
studies. \emph{Statistical Science}, \textbf{35}(3), 437466.

\bibitem[\citeproctext]{ref-vanderweele2022b}
VanderWeele, TJ, and Vansteelandt, S (2022) A statistical test to reject
the structural interpretation of a latent factor model. \emph{Journal of
the Royal Statistical Society Series B: Statistical Methodology},
\textbf{84}(5), 20322054.

\bibitem[\citeproctext]{ref-vansteelandt2012}
Vansteelandt, S, Bekaert, M, and Lange, T (2012) Imputation strategies
for the estimation of natural direct and indirect effects.
\emph{Epidemiologic Methods}, \textbf{1}(1), 131158.

\bibitem[\citeproctext]{ref-westreich2012berkson}
Westreich, D (2012) Berkson's bias, selection bias, and missing data.
\emph{Epidemiology (Cambridge, Mass.)}, \textbf{23}(1), 159.

\bibitem[\citeproctext]{ref-westreich2015}
Westreich, D, Edwards, JK, Cole, SR, Platt, RW, Mumford, SL, and
Schisterman, EF (2015) Imputation approaches for potential outcomes in
causal inference. \emph{International Journal of Epidemiology},
\textbf{44}(5), 17311737.

\bibitem[\citeproctext]{ref-westreich2013}
Westreich, D, and Greenland, S (2013) The table 2 fallacy: Presenting
and interpreting confounder and modifier coefficients. \emph{American
Journal of Epidemiology}, \textbf{177}(4), 292298.

\bibitem[\citeproctext]{ref-zhu2021KableExtra}
Zhu, H (2021) \emph{kableExtra: Construct complex table with 'kable' and
pipe syntax}. Retrieved from
\url{https://CRAN.R-project.org/package=kableExtra}

\end{CSLReferences}

\newpage{}

\subsection{Appendix A: Glossary}\label{appendix-a}

This appendix provides a glossary of common terminology in causal
inference.

\textbf{Acyclic}: a causal diagram cannot contain feedback loops. More
precisely, no variable can be an ancestor or descendant of itself. If
variables are repeatedly measured here, it is especially important to
index nodes by the relative timing of the nodes.

\textbf{Adjustment set}: a collection of variables we must either
condition upon or deliberately avoid conditioning upon to obtain a
consistent causal estimate for the effect of interest
(\citeproc{ref-pearl2009}{Pearl 2009a}).

\textbf{Ancestor (parent)}: a node with a direct or indirect influence
on others, positioned upstream in the causal chain.

\textbf{Arrow}: denotes a causal relationship linking nodes.

\textbf{Backdoor path}: a ``backdoor path'' between a treatment
variable, \(A\), and an outcome variable, \(Y\), is a sequence of links
in a causal diagram that starts with an arrow into \(A\) and reaches
\(Y\) through common causes, introducing potential confounding bias such
that stastical association does not reflect causality. To estimate the
causal effect of \(A\) on \(Y\) without bias, these paths must be
blocked by adjusting for confounders. The backdoor criterion guides the
selection of variables for adjustment to ensure unbiased causal
inference.

\textbf{Conditioning}: the process of explicitly accounting for a
variable in our statistical analysis to address the identification
problem. In causal diagrams, we usually represent conditioning by
drawing a box around a node of the conditioned variable, for example,
\(\boxed{L_{0}}\to A_{1} \to L_{2}\). We do not box exposures and
outcomes, because we assume they are included in a model by default.
Depending on the setting, we may condition by regression stratification,
inverse-probability of treatment weighting, g-methods, doubly robust
machine learning algorithms, or other methods. We do not cover such
methods in this tutorial, however see Hernan and Robins
(\citeproc{ref-hernan2023}{2023}).

\textbf{Counterfactual}: a hypothetical outcome that would have occurred
for the same individuals under a different treatment condition than the
one they actually experienced.

\textbf{Direct effect}: the portion of the total effect of a treatment
on an outcome that is not mediated by other variables within the causal
pathway.

\textbf{Collider}: a variable in a causal diagram at which two incoming
paths meet head-to-head. For example if
\(A \rightarrowred \boxed{L} \leftarrowred Y\), then \(L\) is a
collider. If we do not condition on a collider (or its descendants), the
path between \(A\) and \(Y\) remains closed. Conditioning on a collider
(or its descendants) will induce an association between \(A\) and \(Y\).

\textbf{Confounder}: a member of an adjustment set. Notice a variable is
a `confounder' in relation to a specific adjustment set. `Confounder' is
a relative concept (\citeproc{ref-lash2020}{Lash \emph{et al.} 2020}).

\textbf{D-separation}: in a causal diagram, a path is `blocked' or
`d-separated' if a node along it interrupts causation. Two variables are
d-separated if all paths connecting them are blocked, making them
conditionally independent. Conversely, unblocked paths result in
`d-connected' variables, implying potential dependence
(\citeproc{ref-pearl1995}{Pearl 1995}).

\textbf{Descendant (child)}: a node influenced, directly or indirectly,
by upstream nodes (parents).

\textbf{Effect-modifier}: a variable is an effect-modifier, or
`effect-measure modifier' if its presence changes the magnitude or
direction of the effect of an exposure or treatment on an outcome across
the levels or values of this variable. In other words, the effect of the
exposure is different at different levels of the effect-modifier.

\textbf{External validity}: the extent to which causal inferences can be
generalizsd to other populations, settings, or times, also called
``Target Validity.''

\textbf{Identification problem}: the challenge of estimating the causal
effect of a variable using by adjusting for measured variables on units
in a study. Causal diagrams were developed to address the identification
problem by application of the rules of d-separation to a causal diagram.

\textbf{Indirect effect (Mediated effect)}: The portion of the total
effect that is transmitted through a mediator variable.

\textbf{Internal validity}: the degree to which the design and conduct
of a study are likely to have prevented bias, ensuring that the causal
relationship observed can be confidently attributed to the treatment and
not to other factors.

\textbf{Instrumental variable}: an ancestor of the exposure but not of
the outcome. An instrumental variable affects the outcome only through
its effect on the exposure and not otherwise. Whereas conditioning on a
variable causally associated with the outcome but not with the exposure
will generally increase modelling precision, we should avoid
conditioning on instrumental variables
(\citeproc{ref-cinelli2022}{Cinelli \emph{et al.} 2022}). Second, when
an instrumental variable is the descendant of an unmeasured confounder,
we should generally condition the instrumental variable to provide a
partial adjustment for a confounder.

\textbf{Mediator}: a variable that transmits the effect of the treatment
variable on the outcome variable, part of the causal pathway between
treatment and outcome.

\textbf{Modified Disjunctive Cause Criterion}: VanderWeele
(\citeproc{ref-vanderweele2019}{2019}) recommends obtaining a maximally
efficient adjustment which he calls a `confounder set' A member of this
set is any set of variables that can reduce or remove a structural
sources of bias. The strategy is as follows:

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\tightlist
\item
  Control for any variable that causes the exposure, the outcome, or
  both.
\item
  Control for any proxy for an unmeasured variable that is a shared
  cause of both the exposure and outcome.
\item
  Define an instrumental variable as a variable associated with the
  exposure but does not influence the outcome independently, except
  through the exposure. Exclude any instrumental variable that is not a
  proxy for an unmeasured confounder from the confounder set
  (\citeproc{ref-vanderweele2019}{VanderWeele 2019}).
\end{enumerate}

Note that the concept of a `confounder set' is broader than that of an
`adjustment set.' Every adjustment set is a member of a confounder set.
Hence, the Modified Disjunctive Cause Criterion will eliminate bias when
the data permit. However, a confounder set includes variables that will
reduce bias in cases where confounding cannot be eliminated.

\textbf{Node}: characteristic or features of units in a population
(`variable') represented on a causal diagram. In a causal diagram, nodes
are drawn with reference to variables defomed for the target population.

\textbf{Randomisation}: The process of randomly assigning subjects to
different treatments or control groups, aiming to eliminate selection
bias in experimental studies.

\textbf{Reverse Causation}: \(\atoyassert\), but in reality \(\ytoa\)

\textbf{Statistical model:} a mathematical representation of the
relationships between variables in which we quantify covariances and
their corresponding uncertainties in the data. Statistical models
typically correspond to multiple causal structures
(\citeproc{ref-hernan2023}{Hernan and Robins 2023};
\citeproc{ref-pearl2018}{Pearl and Mackenzie 2018};
\citeproc{ref-vanderweele2022b}{VanderWeele and Vansteelandt 2022}).
That is, the causes of such covariances cannot be identified without
assumptions.

\textbf{Structural model:} defines assumptions about causal
relationships. Causal diagrams graphically encode these assumptions
(\citeproc{ref-hernan2023}{Hernan and Robins 2023}), leaving out the
assumption about whether the exposure and outcome are causally
associated. Outside of randomised experiments, we cannot compute causal
effects in the absence of structural models. A structural model is
needed to interpret the statistical findings in causal terms. Structural
assumptions should be developed in consultation with experts. The role
of structural assumptions when interpreting statistical results remains
poorly understood across many human sciences and forms the motivation
for my work here.

\textbf{Time-varying confounding:} occurs when a confounder that changes
over time also acts as a mediator or collider in the causal pathway
between exposure and outcome. Controlling for such a confounder can
introduce bias. Not controlling for it can retain bias.

\newpage{}

\subsection{Appendix B: Causal Consistency in observational
settings}\label{appendix-b}

In observational research, there are typically multiple versions of
treatment. The theory of causal inference under multiple versions of
treatment proves we can consistently estimate causal effects where the
different versions of treatment are conditionally independent of the
outcomes VanderWeele (\citeproc{ref-vanderweele2009}{2009})

Let \(\coprod\) denote independence. Where there are \(K\) different
versions of treatment \(A\) and no confounding for \(K\)'s effect on
\(Y\) given measured confounders \(L\) such that

\[
Y(k) \coprod K | L
\]

Then it can be proved that causal consistency follows. According to the
theory of causal inference under multiple versions of treatment, the
measured variable \(A\) functions as a ``coarsened indicator'' for
estimating the causal effect of the multiple versions of treatment \(K\)
on \(Y(k)\) (\citeproc{ref-vanderweele2009}{VanderWeele 2009},
\citeproc{ref-vanderweele2018}{2018};
\citeproc{ref-vanderweele2013}{VanderWeele and Hernan 2013}).

In the context of green spaces, let \(A\) represent the general action
of moving closer to any green space and \(K\) represent the different
versions of this treatment. For instance, \(K\) could denote moving
closer to different types of green spaces such as parks, forests,
community gardens, or green spaces with varying amenities and features.

Here, the conditional independence implies that, given measured
confounders \(L\) (e.g.~socioeconomic status, age, personal values), the
type of green space one moves closer to (\(K\)) is independent of the
outcomes \(Y(k)\) (e.g.~mental well-being under the \(K\) conditions).
In other words, the version of green space one chooses to live near does
not affect the \(K\) potential outcomes, provided the confounders \(L\)
are properly controlled for in our statistical models.

Put simply, strategies for confounding control and for consistently
estimating causal effects when there are multiple versions of treatment
converge. However, the quantities we estimate under multiple versions of
treatment might lack any clear interpretations. For example, we cannot
readily determine which of the many versions of treatment is most causal
efficacious and which lack any causal effect, or are harmful.

\newpage{}

\subsubsection{Appendix C: Simulating Cross-Sectional Data to Compute
the Average Treatment Effect When Conditioning on a
Mediator}\label{appendix-c}

This appendix outlines a simulation designed to demonstrate the
potential pitfalls of conditioning on a mediator in cross-sectional
analyses. The simulation explores the scenario where the effect of
access to green space (\(A\)) on happiness (\(Y\)) is fully mediated by
exercise (\(L\)). This setup aims to illustrate how incorrect
assumptions about the role of a variable (mediator vs.~confounder) can
lead to misleading estimates of the Average Treatment Effect (ATE).

\paragraph{Simulation Design}\label{simulation-design}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Data Generation}: We simulate a dataset for 1,000 individuals,
  where access to green space (\(A\)) influences exercise (\(L\)), which
  in turn affects happiness (\(Y\)\$). The simulation is based on
  predefined parameters that establish L as a mediator between \(A\) and
  \(Y\).
\item
  \textbf{Parameter Definitions}:

  \begin{itemize}
  \tightlist
  \item
    Probability of access to green space (\(A\)) is set at 0.5.
  \item
    The effect of \(A\) on \(L\) (exercise) is quantified by
    \(\beta = 2\).
  \item
    The effect of \(L\) on \(Y\) (happiness) is quantified by
    \(\delta = 1.5\).
  \item
    Standard deviations for \(L\) and \(Y\) are set at 1 and 1.5,
    respectively.
  \end{itemize}
\item
  \textbf{Model Specifications}:

  \begin{itemize}
  \tightlist
  \item
    \textbf{Model 1} (Correct Assumption): fits a linear regression
    model assuming \(L\) as a mediator, including both \(A\) and \(L\)
    as regressors on \(Y\). This model aligns with the data generating
    process and correctly identifies L as a mediator.
  \item
    \textbf{Model 2} (Incorrect Assumption): fits a linear regression
    model including only \(A\) as a regressors on \(Y\), omitting the
    mediator \(L\). This model assesses the direct effect of A on Y
    without accounting for mediation.
  \end{itemize}
\item
  \textbf{Analysis and Comparison}: The analysis compares the estimated
  effects of \(A\) on \(Y\) under both model specifications. By
  including \(L\) as a predictor in Model 1, we account for its
  mediating role, whereas Model 2 overlooks this aspect by excluding
  \(L\) from the analysis.
\item
  \textbf{Presentation}: The results are displayed in a comparative
  table formatted for publication. The table contrasts the regression
  coefficients and significance levels obtained under each model,
  highlighting the impact of correctly versus incorrectly assuming the
  role of \(L\) in the relationship between \(A\) and \(Y\).
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# load libraries}
\SpecialCharTok{!}\FunctionTok{require}\NormalTok{(kableExtra)}\ErrorTok{)}\NormalTok{\{}\FunctionTok{install.packages}\NormalTok{(}\StringTok{"kableExtra"}\NormalTok{)\} }\CommentTok{\# tables}
\ControlFlowTok{if}\NormalTok{(}\SpecialCharTok{!}\FunctionTok{require}\NormalTok{(gtsummary))\{}\FunctionTok{install.packages}\NormalTok{(}\StringTok{"gtsummary"}\NormalTok{)\} }\CommentTok{\# tables}

\CommentTok{\# simulation seed}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{) }\CommentTok{\#  reproducibility}

\CommentTok{\# define the parameters }
\NormalTok{n }\OtherTok{=} \DecValTok{1000} \CommentTok{\# Number of observations}
\NormalTok{p }\OtherTok{=} \FloatTok{0.5}  \CommentTok{\# Probability of A = 1 (access to greenspace)}
\NormalTok{alpha }\OtherTok{=} \DecValTok{0} \CommentTok{\# Intercept for L (excercise)}
\NormalTok{beta }\OtherTok{=} \DecValTok{2}  \CommentTok{\# Effect of A on L }
\NormalTok{gamma }\OtherTok{=} \DecValTok{1} \CommentTok{\# Intercept for Y }
\NormalTok{delta }\OtherTok{=} \FloatTok{1.5} \CommentTok{\# Effect of L on Y}
\NormalTok{sigma\_L }\OtherTok{=} \DecValTok{1} \CommentTok{\# Standard deviation of L}
\NormalTok{sigma\_Y }\OtherTok{=} \FloatTok{1.5} \CommentTok{\# Standard deviation of Y}

\CommentTok{\# simulate the data: fully mediated effect by L}
\NormalTok{A }\OtherTok{=} \FunctionTok{rbinom}\NormalTok{(n, }\DecValTok{1}\NormalTok{, p) }\CommentTok{\# binary exposure variable}
\NormalTok{L }\OtherTok{=}\NormalTok{ alpha }\SpecialCharTok{+}\NormalTok{ beta}\SpecialCharTok{*}\NormalTok{A }\SpecialCharTok{+} \FunctionTok{rnorm}\NormalTok{(n, }\DecValTok{0}\NormalTok{, sigma\_L) }\CommentTok{\# mediator L affect by A}
\NormalTok{Y }\OtherTok{=}\NormalTok{ gamma }\SpecialCharTok{+}\NormalTok{ delta}\SpecialCharTok{*}\NormalTok{L }\SpecialCharTok{+} \FunctionTok{rnorm}\NormalTok{(n, }\DecValTok{0}\NormalTok{, sigma\_Y) }\CommentTok{\# Y affected only by L,}

\CommentTok{\# make the data frame}
\NormalTok{data }\OtherTok{=} \FunctionTok{data.frame}\NormalTok{(}\AttributeTok{A =}\NormalTok{ A, }\AttributeTok{L =}\NormalTok{ L, }\AttributeTok{Y =}\NormalTok{ Y)}

\CommentTok{\# fit regression in which L is assume to be a mediator}
\CommentTok{\# (cross{-}sectional data is consistent with this model)}
\NormalTok{fit\_1 }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{( Y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ A }\SpecialCharTok{+}\NormalTok{ L, }\AttributeTok{data =}\NormalTok{ data)}

\CommentTok{\# fit regression in which L is assume to be a mediator}
\CommentTok{\# (cross{-}sectional data is also consistent with this model)}
\NormalTok{fit\_2 }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{( Y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ A, }\AttributeTok{data =}\NormalTok{ data)}

\CommentTok{\# create gtsummary tables for each regression model}
\NormalTok{table1 }\OtherTok{\textless{}{-}}\NormalTok{ gtsummary}\SpecialCharTok{::}\FunctionTok{tbl\_regression}\NormalTok{(fit\_1)}
\NormalTok{table2 }\OtherTok{\textless{}{-}}\NormalTok{ gtsummary}\SpecialCharTok{::}\FunctionTok{tbl\_regression}\NormalTok{(fit\_2)}

\CommentTok{\# merge the tables for comparison}
\NormalTok{table\_comparison }\OtherTok{\textless{}{-}}\NormalTok{ gtsummary}\SpecialCharTok{::}\FunctionTok{tbl\_merge}\NormalTok{(}
  \FunctionTok{list}\NormalTok{(table1, table2),}
  \AttributeTok{tab\_spanner =} \FunctionTok{c}\NormalTok{(}\StringTok{"Model: Exercise assumed confounder"}\NormalTok{, }
                  \StringTok{"Model: Exercise assumed to be a mediator"}\NormalTok{)}
\NormalTok{)}
\CommentTok{\# make latex table (for publication)}
\NormalTok{markdown\_table\_0 }\OtherTok{\textless{}{-}} \FunctionTok{as\_kable\_extra}\NormalTok{(table\_comparison, }
                                   \AttributeTok{format =} \StringTok{"latex"}\NormalTok{, }
                                   \AttributeTok{booktabs =} \ConstantTok{TRUE}\NormalTok{)}
\CommentTok{\# print latex table (note, you might prefer "markdown" or another format)                                }
\NormalTok{markdown\_table\_0}
\end{Highlighting}
\end{Shaded}

The following code snippet is designed to estimate the Average Treatment
Effect (ATE) using the \texttt{clarify} package in R, which is
referenced here as (\citeproc{ref-greifer2023}{Greifer \emph{et al.}
2023}). The procedure involves two primary steps: simulating coefficient
distributions for regression models and then calculating the ATE based
on these simulations. This process is applied to two distinct models to
demonstrate the effects of including versus excluding a mediator
variable in the analysis.

\subsubsection{Steps to Estimate the
ATE}\label{steps-to-estimate-the-ate}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Load the \texttt{clarify} Package}: This package provides
  functions to simulate regression coefficients and compute average
  marginal effects (AME), facilitating the estimation of ATE in a robust
  manner.
\item
  \textbf{Set Seed for Reproducibility}: \texttt{set.seed(123)} ensures
  that the results of the simulations are reproducible, allowing for
  consistent outcomes across different runs of the code.
\item
  \textbf{Simulate Coefficient Distributions}:

  \begin{itemize}
  \tightlist
  \item
    \texttt{sim\_coefs\_fit\_1} and \texttt{sim\_coefs\_fit\_2} are
    generated using the \texttt{sim} function from the \texttt{clarify}
    package, applied to two fitted models (\texttt{fit\_1} and
    \texttt{fit\_2}). These functions simulate the distribution of
    coefficients based on the specified models, capturing the
    uncertainty around the estimated parameters.
  \end{itemize}
\item
  \textbf{Calculate ATE}:

  \begin{itemize}
  \tightlist
  \item
    For both models, the \texttt{sim\_ame} function calculates the ATE
    as the marginal risk difference (RD) when the treatment variable
    (\texttt{A}) is present (\texttt{A\ ==\ 1}). This function uses the
    simulated coefficients to estimate the treatment effect across the
    simulated distributions, providing a comprehensive view of the ATE
    under each model.
  \item
    The function is set to verbose mode off (\texttt{verbose\ =\ FALSE})
    to streamline the output.
  \end{itemize}
\item
  \textbf{Summarize Results}:

  \begin{itemize}
  \tightlist
  \item
    Summaries of these estimates (\texttt{summary\_sim\_est\_fit\_1} and
    \texttt{summary\_sim\_est\_fit\_2}) are obtained, providing detailed
    statistics including the estimated ATE and its 95\% confidence
    intervals (CI).
  \end{itemize}
\item
  \textbf{Report ATE and CIs}:

  \begin{itemize}
  \tightlist
  \item
    Using the \texttt{glue} package, the ATE along with its 95\% CIs for
    both models is formatted into a string for easy reporting. This step
    transforms the statistical output into a more interpretable form,
    highlighting the estimated treatment effect and its precision.
  \end{itemize}
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# use \textasciigrave{}clarify\textasciigrave{} package to obtain ATE}
\ControlFlowTok{if}\NormalTok{(}\SpecialCharTok{!}\FunctionTok{require}\NormalTok{(clarify))\{}\FunctionTok{install.packages}\NormalTok{(}\StringTok{"clarify"}\NormalTok{)\} }\CommentTok{\# clarify package}
\CommentTok{\# simulate fit 1 ATE}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}
\NormalTok{sim\_coefs\_fit\_1 }\OtherTok{\textless{}{-}} \FunctionTok{sim}\NormalTok{(fit\_1)}
\NormalTok{sim\_coefs\_fit\_2 }\OtherTok{\textless{}{-}} \FunctionTok{sim}\NormalTok{(fit\_2)}

\CommentTok{\# marginal risk difference ATE, simulation{-}based: model 1 (L is a confounder)}
\NormalTok{sim\_est\_fit\_1 }\OtherTok{\textless{}{-}}
  \FunctionTok{sim\_ame}\NormalTok{(}
\NormalTok{    sim\_coefs\_fit\_1,}
    \AttributeTok{var =} \StringTok{"A"}\NormalTok{,}
    \AttributeTok{subset =}\NormalTok{ A }\SpecialCharTok{==} \DecValTok{1}\NormalTok{,}
    \AttributeTok{contrast =} \StringTok{"RD"}\NormalTok{,}
    \AttributeTok{verbose =} \ConstantTok{FALSE}
\NormalTok{  )}

\CommentTok{\# marginal risk difference ATE, simulation{-}based: model 2 (L is a mediator)}
\NormalTok{sim\_est\_fit\_2 }\OtherTok{\textless{}{-}}
  \FunctionTok{sim\_ame}\NormalTok{(}
\NormalTok{    sim\_coefs\_fit\_2,}
    \AttributeTok{var =} \StringTok{"A"}\NormalTok{,}
    \AttributeTok{subset =}\NormalTok{ A }\SpecialCharTok{==} \DecValTok{1}\NormalTok{,}
    \AttributeTok{contrast =} \StringTok{"RD"}\NormalTok{,}
    \AttributeTok{verbose =} \ConstantTok{FALSE}
\NormalTok{  )}

\CommentTok{\# obtain summaries}
\NormalTok{summary\_sim\_est\_fit\_1 }\OtherTok{\textless{}{-}} \FunctionTok{summary}\NormalTok{(sim\_est\_fit\_1, }\AttributeTok{null =} \FunctionTok{c}\NormalTok{(}\StringTok{\textasciigrave{}}\AttributeTok{RD}\StringTok{\textasciigrave{}} \OtherTok{=} \DecValTok{0}\NormalTok{))}
\NormalTok{summary\_sim\_est\_fit\_2 }\OtherTok{\textless{}{-}} \FunctionTok{summary}\NormalTok{(sim\_est\_fit\_2, }\AttributeTok{null =} \FunctionTok{c}\NormalTok{(}\StringTok{\textasciigrave{}}\AttributeTok{RD}\StringTok{\textasciigrave{}} \OtherTok{=} \DecValTok{0}\NormalTok{))}

\CommentTok{\# get coefficients for reporting}
\CommentTok{\# ate for fit 1, with 95\% CI}
\NormalTok{ATE\_fit\_1 }\OtherTok{\textless{}{-}}\NormalTok{ glue}\SpecialCharTok{::}\FunctionTok{glue}\NormalTok{(}
  \StringTok{"ATE =}
\StringTok{                        \{round(summary\_sim\_est\_fit\_1[3, 1], 2)\},}
\StringTok{                        CI = [\{round(summary\_sim\_est\_fit\_1[3, 2], 2)\},}
\StringTok{                        \{round(summary\_sim\_est\_fit\_1[3, 3], 2)\}]"}
\NormalTok{)}
\CommentTok{\# ate for fit 2, with 95\% CI}
\NormalTok{ATE\_fit\_2 }\OtherTok{\textless{}{-}}
\NormalTok{  glue}\SpecialCharTok{::}\FunctionTok{glue}\NormalTok{(}
    \StringTok{"ATE = \{round(summary\_sim\_est\_fit\_2[3, 1], 2)\},}
\StringTok{                        CI = [\{round(summary\_sim\_est\_fit\_2[3, 2], 2)\},}
\StringTok{                        \{round(summary\_sim\_est\_fit\_2[3, 3], 2)\}]"}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

\subsubsection{Upshot of the Simulation and
Analysis}\label{upshot-of-the-simulation-and-analysis}

\begin{itemize}
\item
  \textbf{Model 1 (L as a Confounder)}: This analysis assumes that
  \texttt{L} is a confounder in the relationship between the treatment
  (\texttt{A}) and the outcome (\texttt{Y}), and thus, it includes
  \texttt{L} in the model. The ATE estimated here reflects the effect of
  \texttt{A} while controlling for \texttt{L}.
\item
  \textbf{Model 2 (L as a Mediator)}: In contrast, this analysis
  considers \texttt{L} to be a mediator, and the model either includes
  \texttt{L} explicitly in its estimation process or excludes it to
  examine the direct effect of \texttt{A} on \texttt{Y}. The approach to
  mediation analysis here is crucial as it influences the interpretation
  of the ATE.
\end{itemize}

By comparing the ATEs from both models, researchers can understand the
impact of mediation (or the lack thereof) on the estimated treatment
effect. This comparison sheds light on how assumptions about variable
roles (confounder vs.~mediator) can significantly alter causal
inferences drawn from cross-sectional data.

\textbf{Wherever it is uncertain whether a variable is a confounder or a
mediator, we suggest creating two causal diagrams and reporting both
anlyses}

\newpage{}

\subsection{Appendix D: Simulation of Different Confounding Control
Strategies}\label{appendix-d}

This appendix outlines the methodology and results of a data simulation
designed to compare different strategies for controlling confounding in
the context of environmental psychology research. Specifically, the
simulation examines the effect of access to open green spaces
(treatment, \(A_1\)) on happiness (outcome, \(Y_2\)), while addressing
the challenge of unmeasured confounding. The simulation incorporates
baseline measures of exposure and outcome (\(A_0\), \(Y_0\)), baseline
confounders (\(L_0\)), and an unmeasured confounder (\(U\)), to evaluate
the effectiveness of different analytical approaches.

\subsubsection{Methodology}\label{methodology}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Data generation}: we simulate data for 10,000 individuals,
  including baseline exposure to green spaces (\(A_0\)), baseline
  happiness (\(Y_0\)), baseline confounders (\(L_0\)), and an unmeasured
  confounder (\(U\)). The simulation uses a logistic model for treatment
  assignment and a linear model for the continuous outcome,
  incorporating interactions to assess how baseline characteristics
  modify the treatment effect.
\item
  \textbf{Coefficient specifications}:

  \begin{itemize}
  \tightlist
  \item
    Treatment assignment coefficients: \(\beta_{A0} = 0.25\),
    \(\beta_{Y0} = 0.3\), \(\beta_{L0} = 0.2\), and \(\beta_{U} = 0.1\).
  \item
    Outcome model coefficients: \(\delta_{A1} = 0.3\),
    \(\delta_{Y0} = 0.9\), \(\delta_{A0} = 0.1\), \(\delta_{L0} = 0.3\),
    with an interaction effect (\(\theta_{A0Y0L0} = 0.5\)) indicating
    the combined influence of baseline exposure, outcome, and
    confounders on the follow-up outcome.
  \end{itemize}
\item
  \textbf{Model comparison}:

  \begin{itemize}
  \tightlist
  \item
    \textbf{No control model}: estimates the effect of \(A_1\) on
    \(Y_2\) without controlling for any confounders.
  \item
    \textbf{Standard covariate control model}: controls for baseline
    confounders (\(L_0\)) alongside treatment (\(A_1\)).
  \item
    \textbf{Baseline exposure and outcome model}: extends the standard
    model by including baseline treatment and outcome (\(A_0\), \(Y_0\))
    and their interaction with \(L_0\).
  \end{itemize}
\item
  \textbf{Analysis}: each model's effectiveness in estimating the true
  treatment effect is assessed by comparing regression outputs. The
  simulation specifically evaluates how well each model addresses the
  bias introduced by unmeasured confounding and the role of baseline
  characteristics in modifying treatment effects.
\item
  \textbf{Presentation}: the results are synthesised in a comparative
  table, formatted using the \texttt{kableExtra} \{Zhu
  (\citeproc{ref-zhu2021KableExtra}{2021}){]} and \texttt{gtsummary}
  packages (\citeproc{ref-gtsummary2021}{Sjoberg \emph{et al.} 2021}),
  highlighting the estimated treatment effects and their statistical
  significance across models.
\end{enumerate}

Overall, we use the simulation to illustrate the importance of
incorporating baseline characteristics and their interactions to
mitigate the influence of unmeasured confounding.

Here is the simulation and modelling code:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(kableExtra)}
\ControlFlowTok{if}\NormalTok{(}\SpecialCharTok{!}\FunctionTok{require}\NormalTok{(kableExtra))\{}\FunctionTok{install.packages}\NormalTok{(}\StringTok{"kableExtra"}\NormalTok{)\} }\CommentTok{\# causal forest}
\ControlFlowTok{if}\NormalTok{(}\SpecialCharTok{!}\FunctionTok{require}\NormalTok{(gtsummary))\{}\FunctionTok{install.packages}\NormalTok{(}\StringTok{"gtsummary"}\NormalTok{)\} }\CommentTok{\# causal forest}
\ControlFlowTok{if}\NormalTok{(}\SpecialCharTok{!}\FunctionTok{require}\NormalTok{(grf))\{}\FunctionTok{install.packages}\NormalTok{(}\StringTok{"grf"}\NormalTok{)\} }\CommentTok{\# causal forest}

\CommentTok{\# r\_texmf()eproducibility}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{) }

\CommentTok{\# set number of observations}
\NormalTok{n }\OtherTok{\textless{}{-}} \DecValTok{10000} 

\CommentTok{\# baseline covariates}
\NormalTok{U }\OtherTok{\textless{}{-}} \FunctionTok{rnorm}\NormalTok{(n) }\CommentTok{\# Unmeasured confounder}
\NormalTok{A\_0 }\OtherTok{\textless{}{-}} \FunctionTok{rbinom}\NormalTok{(n, }\DecValTok{1}\NormalTok{, }\AttributeTok{prob =} \FunctionTok{plogis}\NormalTok{(U)) }\CommentTok{\# Baseline exposure}
\NormalTok{Y\_0 }\OtherTok{\textless{}{-}} \FunctionTok{rnorm}\NormalTok{(n, }\AttributeTok{mean =}\NormalTok{ U, }\AttributeTok{sd =} \DecValTok{1}\NormalTok{) }\CommentTok{\# Baseline outcome}
\NormalTok{L\_0 }\OtherTok{\textless{}{-}} \FunctionTok{rnorm}\NormalTok{(n, }\AttributeTok{mean =}\NormalTok{ U, }\AttributeTok{sd =} \DecValTok{1}\NormalTok{) }\CommentTok{\# Baseline confounders}

\CommentTok{\# coefficients for treatment assignment}
\NormalTok{beta\_A0 }\OtherTok{=} \FloatTok{0.25}
\NormalTok{beta\_Y0 }\OtherTok{=} \FloatTok{0.3}
\NormalTok{beta\_L0 }\OtherTok{=} \FloatTok{0.2}
\NormalTok{beta\_U }\OtherTok{=} \FloatTok{0.1}

\CommentTok{\# simulate treatment assignment}
\NormalTok{A\_1 }\OtherTok{\textless{}{-}} \FunctionTok{rbinom}\NormalTok{(n, }\DecValTok{1}\NormalTok{, }\AttributeTok{prob =} \FunctionTok{plogis}\NormalTok{(}\SpecialCharTok{{-}}\FloatTok{0.5} \SpecialCharTok{+} 
\NormalTok{                                    beta\_A0 }\SpecialCharTok{*}\NormalTok{ A\_0 }\SpecialCharTok{+}
\NormalTok{                                    beta\_Y0 }\SpecialCharTok{*}\NormalTok{ Y\_0 }\SpecialCharTok{+} 
\NormalTok{                                    beta\_L0 }\SpecialCharTok{*}\NormalTok{ L\_0 }\SpecialCharTok{+} 
\NormalTok{                                    beta\_U }\SpecialCharTok{*}\NormalTok{ U))}
\CommentTok{\# coefficients for continuous outcome}
\NormalTok{delta\_A1 }\OtherTok{=} \FloatTok{0.3}
\NormalTok{delta\_Y0 }\OtherTok{=} \FloatTok{0.9}
\NormalTok{delta\_A0 }\OtherTok{=} \FloatTok{0.1}
\NormalTok{delta\_L0 }\OtherTok{=} \FloatTok{0.3}
\NormalTok{theta\_A0Y0L0 }\OtherTok{=} \FloatTok{0.5} \CommentTok{\# Interaction effect between A\_1 and L\_0}
\NormalTok{delta\_U }\OtherTok{=} \FloatTok{0.05}

\CommentTok{\# simulate continuous outcome including interaction}
\NormalTok{Y\_2 }\OtherTok{\textless{}{-}} \FunctionTok{rnorm}\NormalTok{(n,}
             \AttributeTok{mean =} \DecValTok{0} \SpecialCharTok{+}
\NormalTok{               delta\_A1 }\SpecialCharTok{*}\NormalTok{ A\_1 }\SpecialCharTok{+} 
\NormalTok{               delta\_Y0 }\SpecialCharTok{*}\NormalTok{ Y\_0 }\SpecialCharTok{+} 
\NormalTok{               delta\_A0 }\SpecialCharTok{*}\NormalTok{ A\_0 }\SpecialCharTok{+} 
\NormalTok{               delta\_L0 }\SpecialCharTok{*}\NormalTok{ L\_0 }\SpecialCharTok{+} 
\NormalTok{               theta\_A0Y0L0 }\SpecialCharTok{*}\NormalTok{ Y\_0 }\SpecialCharTok{*} 
\NormalTok{               A\_0 }\SpecialCharTok{*}\NormalTok{ L\_0 }\SpecialCharTok{+} 
\NormalTok{               delta\_U }\SpecialCharTok{*}\NormalTok{ U,}
             \AttributeTok{sd =}\NormalTok{ .}\DecValTok{5}\NormalTok{)}

\CommentTok{\# assemble data frame}
\NormalTok{data }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(Y\_2, A\_0, A\_1, L\_0, Y\_0, U)}

\CommentTok{\# model: no control}
\NormalTok{fit\_no\_control }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(Y\_2 }\SpecialCharTok{\textasciitilde{}}\NormalTok{ A\_1, }\AttributeTok{data =}\NormalTok{ data)}

\CommentTok{\# model: standard covariate control}
\NormalTok{fit\_standard }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(Y\_2 }\SpecialCharTok{\textasciitilde{}}\NormalTok{ A\_1 }\SpecialCharTok{+}\NormalTok{ L\_0, }\AttributeTok{data =}\NormalTok{ data)}

\CommentTok{\# model: interaction with baseline confounders, and baseline outcome and exposure}
\NormalTok{fit\_interaction  }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(Y\_2 }\SpecialCharTok{\textasciitilde{}}\NormalTok{ A\_1 }\SpecialCharTok{*}\NormalTok{ (L\_0 }\SpecialCharTok{+}\NormalTok{ A\_0 }\SpecialCharTok{+}\NormalTok{ Y\_0), }\AttributeTok{data =}\NormalTok{ data)}

\CommentTok{\# create gtsummary tables for each regression model}
\NormalTok{tbl\_fit\_no\_control}\OtherTok{\textless{}{-}} \FunctionTok{tbl\_regression}\NormalTok{(fit\_no\_control)  }
\NormalTok{tbl\_fit\_standard }\OtherTok{\textless{}{-}} \FunctionTok{tbl\_regression}\NormalTok{(fit\_standard)}
\NormalTok{tbl\_fit\_interaction }\OtherTok{\textless{}{-}} \FunctionTok{tbl\_regression}\NormalTok{(fit\_interaction)}

\CommentTok{\# get only the treatment variable}
\NormalTok{tbl\_list\_modified }\OtherTok{\textless{}{-}} \FunctionTok{lapply}\NormalTok{(}\FunctionTok{list}\NormalTok{(}
\NormalTok{  tbl\_fit\_no\_control,}
\NormalTok{  tbl\_fit\_standard,}
\NormalTok{  tbl\_fit\_interaction),}
\ControlFlowTok{function}\NormalTok{(tbl) \{}
\NormalTok{  tbl }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{modify\_table\_body}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\NormalTok{ .x }\SpecialCharTok{\%\textgreater{}\%}\NormalTok{ dplyr}\SpecialCharTok{::}\FunctionTok{filter}\NormalTok{(variable }\SpecialCharTok{==} \StringTok{"A\_1"}\NormalTok{))}
\NormalTok{\})}
\CommentTok{\# merge tables}
\NormalTok{table\_comparison }\OtherTok{\textless{}{-}} \FunctionTok{tbl\_merge}\NormalTok{(}
  \AttributeTok{tbls =}\NormalTok{ tbl\_list\_modified,}
  \AttributeTok{tab\_spanner =} \FunctionTok{c}\NormalTok{(}
    \StringTok{"No Control"}\NormalTok{,}
    \StringTok{"Standard"}\NormalTok{,}
    \StringTok{"Interaction"}\NormalTok{)}
\NormalTok{) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{modify\_table\_styling}\NormalTok{(}
    \AttributeTok{column =} \FunctionTok{c}\NormalTok{(p.value\_1, p.value\_2, p.value\_3),}
    \AttributeTok{hide =} \ConstantTok{TRUE}
\NormalTok{  )}

\CommentTok{\# latex table for publication}
\NormalTok{markdown\_table }\OtherTok{\textless{}{-}}
  \FunctionTok{as\_kable\_extra}\NormalTok{(table\_comparison, }\AttributeTok{format =} \StringTok{"latex"}\NormalTok{, }\AttributeTok{booktabs =} \ConstantTok{TRUE}\NormalTok{) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{kable\_styling}\NormalTok{(}\AttributeTok{latex\_options =} \StringTok{"scale\_down"}\NormalTok{)}
\FunctionTok{print}\NormalTok{(markdown\_table)}
\end{Highlighting}
\end{Shaded}

Next, in the following code snippet, we calculate the Average Treatment
Effect (ATE) using simulation-based approaches for two distinct models:
one with standard covariate control and another incorporating
interactions. This approach leverages the \texttt{clarify} package in R,
which facilitates the simulation and interpretation of estimated
coefficients from linear models to derive ATEs under different modeling
assumptions (\citeproc{ref-greifer2023}{Greifer \emph{et al.} 2023}).

First, we use the \texttt{sim} function from the \texttt{clarify}
package to generate simulated coefficient distributions for the standard
model (\texttt{fit\_standard}) and the interaction model
(\texttt{fit\_interaction}). This step is crucial for capturing the
uncertainty in our estimates arising from sampling variability.

Next, for each model, we employ the \texttt{sim\_ame} function to
compute the average marginal effects (AME), focusing on the treatment
variable (\texttt{A\_1}). The calculation is done under the assumption
that all individuals are treated (i.e., \texttt{A\_1\ ==\ 1}), and we
specify the contrast type as ``RD'' (Risk Difference) to directly obtain
the ATE (Average Treatment Effect). The \texttt{sim\_ame} function
simulates the treatment effect across the distribution of simulated
coefficients, providing a robust estimate of the ATE along with its
variability.

The summaries of these simulations (\texttt{summary\_sim\_est\_fit\_std}
and \texttt{summary\_sim\_est\_fit\_int}) are then extracted to provide
concise estimates of the ATE along with 95\% confidence intervals (CIs)
for both the standard and interaction models. This step is essential for
understanding the magnitude and precision of the treatment effects
estimated by the models.

Finally, we use the \texttt{glue} package to format these estimates into
a human-readable form, presenting the ATE and its corresponding 95\% CIs
for each model. This presentation facilitates clear communication of the
estimated treatment effects, allowing for direct comparison between the
models and highlighting the impact of including baseline characteristics
and their interactions on the estimation of the ATE
(\citeproc{ref-hester2022GLUE}{Hester and Bryan 2022}).

This simulation-based approach to estimating the ATE underscores the
importance of considering model complexity and the roles of confounders
and mediators in causal inference analyses. By comparing the ATE
estimates from different models, we can assess the sensitivity of our
causal conclusions to various assumptions and modeling strategies.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#| label: ate{-}sim{-}long}
\CommentTok{\#| tbl{-}cap: "Code for calculating the average treatment effect."}
\CommentTok{\#| echo: false}
\CommentTok{\#| eval: true}

\CommentTok{\# use \textasciigrave{}clarify\textasciigrave{} package to obtain ATE}
\ControlFlowTok{if}\NormalTok{(}\SpecialCharTok{!}\FunctionTok{require}\NormalTok{(clarify))\{}\FunctionTok{install.packages}\NormalTok{(}\StringTok{"clarify"}\NormalTok{)\} }\CommentTok{\# clarify package}

\CommentTok{\# simulate fit 1 ATE}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}
\NormalTok{sim\_coefs\_fit\_no\_control}\OtherTok{\textless{}{-}} \FunctionTok{sim}\NormalTok{(fit\_no\_control)  }
\NormalTok{sim\_coefs\_fit\_std }\OtherTok{\textless{}{-}} \FunctionTok{sim}\NormalTok{(fit\_standard)}
\NormalTok{sim\_coefs\_fit\_int }\OtherTok{\textless{}{-}} \FunctionTok{sim}\NormalTok{(fit\_interaction)}

\CommentTok{\# marginal risk difference ATE, no controls}
\NormalTok{sim\_est\_fit\_no\_control }\OtherTok{\textless{}{-}}
  \FunctionTok{sim\_ame}\NormalTok{(}
\NormalTok{    sim\_coefs\_fit\_no\_control,}
    \AttributeTok{var =} \StringTok{"A\_1"}\NormalTok{,}
    \AttributeTok{subset =}\NormalTok{ A\_1 }\SpecialCharTok{==} \DecValTok{1}\NormalTok{,}
    \AttributeTok{contrast =} \StringTok{"RD"}\NormalTok{,}
    \AttributeTok{verbose =} \ConstantTok{FALSE}
\NormalTok{  )}
\CommentTok{\# marginal risk difference ATE, simulation{-}based: model 1 (L is a confounder)}
\NormalTok{sim\_est\_fit\_std }\OtherTok{\textless{}{-}}
  \FunctionTok{sim\_ame}\NormalTok{(}
\NormalTok{    sim\_coefs\_fit\_std,}
    \AttributeTok{var =} \StringTok{"A\_1"}\NormalTok{,}
    \AttributeTok{subset =}\NormalTok{ A\_1 }\SpecialCharTok{==} \DecValTok{1}\NormalTok{,}
    \AttributeTok{contrast =} \StringTok{"RD"}\NormalTok{,}
    \AttributeTok{verbose =} \ConstantTok{FALSE}
\NormalTok{  )}
\CommentTok{\# marginal risk difference ATE, simulation{-}based: model 2 (L is a mediator)}
\NormalTok{sim\_est\_fit\_int }\OtherTok{\textless{}{-}}
  \FunctionTok{sim\_ame}\NormalTok{(}
\NormalTok{    sim\_coefs\_fit\_int,}
    \AttributeTok{var =} \StringTok{"A\_1"}\NormalTok{,}
    \AttributeTok{subset =}\NormalTok{ A\_1 }\SpecialCharTok{==} \DecValTok{1}\NormalTok{,}
    \AttributeTok{contrast =} \StringTok{"RD"}\NormalTok{,}
    \AttributeTok{verbose =} \ConstantTok{FALSE}
\NormalTok{  )}
\CommentTok{\# obtain summaries}
\NormalTok{summary\_sim\_coefs\_fit\_no\_control }\OtherTok{\textless{}{-}}
  \FunctionTok{summary}\NormalTok{(sim\_est\_fit\_no\_control, }\AttributeTok{null =} \FunctionTok{c}\NormalTok{(}\StringTok{\textasciigrave{}}\AttributeTok{RD}\StringTok{\textasciigrave{}} \OtherTok{=} \DecValTok{0}\NormalTok{))}
\NormalTok{summary\_sim\_est\_fit\_std }\OtherTok{\textless{}{-}}
  \FunctionTok{summary}\NormalTok{(sim\_est\_fit\_std, }\AttributeTok{null =} \FunctionTok{c}\NormalTok{(}\StringTok{\textasciigrave{}}\AttributeTok{RD}\StringTok{\textasciigrave{}} \OtherTok{=} \DecValTok{0}\NormalTok{))}
\NormalTok{summary\_sim\_est\_fit\_int }\OtherTok{\textless{}{-}}
  \FunctionTok{summary}\NormalTok{(sim\_est\_fit\_int, }\AttributeTok{null =} \FunctionTok{c}\NormalTok{(}\StringTok{\textasciigrave{}}\AttributeTok{RD}\StringTok{\textasciigrave{}} \OtherTok{=} \DecValTok{0}\NormalTok{))}

\CommentTok{\# get coefficients for reporting}
\CommentTok{\# ate for fit 1, with 95\% CI}
\NormalTok{ATE\_fit\_no\_control  }\OtherTok{\textless{}{-}}\NormalTok{ glue}\SpecialCharTok{::}\FunctionTok{glue}\NormalTok{(}
  \StringTok{"ATE = \{round(summary\_sim\_coefs\_fit\_no\_control[3, 1], 2)\}, }
\StringTok{  CI = [\{round(summary\_sim\_coefs\_fit\_no\_control[3, 2], 2)\},}
\StringTok{  \{round(summary\_sim\_coefs\_fit\_no\_control[3, 3], 2)\}]"}
\NormalTok{)}
\CommentTok{\# ate for fit 2, with 95\% CI}
\NormalTok{ATE\_fit\_std }\OtherTok{\textless{}{-}}\NormalTok{ glue}\SpecialCharTok{::}\FunctionTok{glue}\NormalTok{(}
  \StringTok{"ATE = \{round(summary\_sim\_est\_fit\_std[3, 1], 2)\}, }
\StringTok{  CI = [\{round(summary\_sim\_est\_fit\_std[3, 2], 2)\},}
\StringTok{  \{round(summary\_sim\_est\_fit\_std[3, 3], 2)\}]"}
\NormalTok{)}
\CommentTok{\# ate for fit 3, with 95\% CI}
\NormalTok{ATE\_fit\_int }\OtherTok{\textless{}{-}}
\NormalTok{  glue}\SpecialCharTok{::}\FunctionTok{glue}\NormalTok{(}
    \StringTok{"ATE = \{round(summary\_sim\_est\_fit\_int[3, 1], 2)\},}
\StringTok{    CI = [\{round(summary\_sim\_est\_fit\_int[3, 2], 2)\},}
\StringTok{    \{round(summary\_sim\_est\_fit\_int[3, 3], 2)\}]"}
\NormalTok{  )}
\CommentTok{\# coefs they used in manuscript}
\end{Highlighting}
\end{Shaded}

Using the \texttt{clarify} package, we infer the ATE for the standard
model is ATE = 0.86, CI = {[}0.8, 0.92{]}.

Using the \texttt{clarify} package, we infer the ATE for the model that
conditions on the baseline exposure and baseline outcome to be: ATE =
0.29, CI = {[}0.27, 0.31{]}, which is close to the values supplied to
the data-generating mechanism.

\textbf{Take-home message:} The baseline exposure and baseline outcome
are often the most important variables to include for confounding
control. The baseline exposure also allows us to estimate an
incident-exposure effect. For this reason should endevour to obtain at
least three waves of data such that these variables along with other
baseline confounders are included in at time 0, the exposure is included
at time 1, and the outcome is included at time 2.

\newpage{}

\subsection{Appendix E: Non-parametric Estimation of Average Treatment
Effects Using Causal Forests
\{\#appendix-causal-forests\}}\label{appendix-e}

This appendix provides a practical example of estimating average
treatment effects (ATE) using a non-parametric approach, specifically
through the application of causal forests. Unlike traditional regression
models, causal forests allow for the estimation of treatment effects
without imposing strict assumptions about the form of the relationship
between treatment, covariates, and outcomes. This flexibility makes them
particularly useful for analyzing complex datasets where the treatment
effect may vary across observations.

\paragraph{Causal Forest Model
Implementation}\label{causal-forest-model-implementation}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Libraries}: the implementation begins with loading the
  necessary R libraries: \texttt{grf} for estimating conditional and
  average treatment effects using causal forests, and \texttt{glue} for
  formatting the results for reporting.
\item
  \begin{enumerate}
  \def\labelenumii{\arabic{enumii}.}
  \tightlist
  \item
    \textbf{Data generation}: the code assumes the presence of a
    dataframe \texttt{data} generated from the previous code snippet
    containing the variables:
  \end{enumerate}

  \begin{itemize}
  \tightlist
  \item
    \texttt{A\_1}: Treatment indicator.
  \item
    \texttt{L\_0}: A covariate.
  \item
    \texttt{Y\_2}: Outcome of interest.
  \item
    \texttt{A\_0} and \texttt{Y\_0}: Baseline exposure and outcome,
    respectively.
  \end{itemize}

  Treatment (\texttt{W}) and outcome (\texttt{Y}) vectors are extracted
  from \texttt{data}, alongside a matrix \texttt{X} that includes
  covariates and baseline characteristics.
\item
  \textbf{Causal Forest model}: a causal forest model is fitted using
  the \texttt{causal\_forest} function from the \texttt{grf} package
  (\citeproc{ref-grf2024}{Tibshirani \emph{et al.} 2024}). This function
  takes the covariate matrix \texttt{X}, the outcome vector \texttt{Y},
  and the treatment vector \texttt{W} as inputs, and it returns a model
  object that can be used for further analysis.
\item
  \textbf{Average Treatment Effect estimation}: yhe
  \texttt{average\_treatment\_effect} function computes the ATE from the
  fitted causal forest model. This step is crucial as it quantifies the
  overall impact of the treatment across the population, adjusting for
  covariates included in the model.
\item
  \textbf{Reporting}: The estimated ATE and its standard error (se) are
  extracted and formatted for reporting using the \texttt{glue} package
  (\citeproc{ref-hester2022GLUE}{Hester and Bryan 2022}). This
  facilitates clear communication of the results, showing the estimated
  effect size and its uncertainty.
\end{enumerate}

\paragraph{Key Takeaways}\label{key-takeaways}

\begin{itemize}
\item
  \textbf{Flexibility and robustness}: causal forests offer a robust way
  to estimate treatment effects without making strong parametric
  assumptions. This approach is particularly advantageous in settings
  where the treatment effect may vary with covariates or across
  different subpopulations and we do not know the true functional form
  of the data-generating mechanism.
\item
  \textbf{ATE estimation}: the model estimates the ATE as the difference
  in expected outcomes between treated and untreated units, averaged
  across the population. This estimate reflects the overall effect of
  the treatment, accounting for the distribution of covariates in the
  sample.
\item
  \textbf{Convergence to true value}: we find that the estimated ATE by
  the causal forest model converges to the true value used in the data
  generating process (assumed to be 0.3). This demonstrates the
  effectiveness of causal forests in uncovering the true treatment
  effect from complex data.
\end{itemize}

This example underscores the utility of semi-parametric and
non-parametric methods, such as causal forests, in causal inference
analyses.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# load causal forest library }
\FunctionTok{library}\NormalTok{(grf) }\CommentTok{\# estimate conditional and average treatment effects}
\FunctionTok{library}\NormalTok{(glue) }\CommentTok{\# reporting }

\CommentTok{\#  \textquotesingle{}data\textquotesingle{} is our data frame with columns \textquotesingle{}A\_1\textquotesingle{} for treatment, \textquotesingle{}L\_0\textquotesingle{} for a covariate, and \textquotesingle{}Y\_2\textquotesingle{} for the outcome}
\CommentTok{\#  we also have the baseline exposure \textquotesingle{}A\_0\textquotesingle{} and \textquotesingle{}Y\_0\textquotesingle{}}
\CommentTok{\#  ensure W (treatment) and Y (outcome) are vectors}
\NormalTok{W }\OtherTok{\textless{}{-}} \FunctionTok{as.matrix}\NormalTok{(data}\SpecialCharTok{$}\NormalTok{A\_1)  }\CommentTok{\# Treatment}
\NormalTok{Y }\OtherTok{\textless{}{-}} \FunctionTok{as.matrix}\NormalTok{(data}\SpecialCharTok{$}\NormalTok{Y\_2)  }\CommentTok{\# Outcome}
\NormalTok{X }\OtherTok{\textless{}{-}} \FunctionTok{as.matrix}\NormalTok{(data[, }\FunctionTok{c}\NormalTok{(}\StringTok{"L\_0"}\NormalTok{, }\StringTok{"A\_0"}\NormalTok{, }\StringTok{"Y\_0"}\NormalTok{)])}

\CommentTok{\# fit causal forest model }
\NormalTok{fit\_causal\_forest }\OtherTok{\textless{}{-}} \FunctionTok{causal\_forest}\NormalTok{(X, Y, W)}

\CommentTok{\# estimate the average treatment effect (ATE)}
\NormalTok{ate }\OtherTok{\textless{}{-}} \FunctionTok{average\_treatment\_effect}\NormalTok{(fit\_causal\_forest)}

\CommentTok{\# make data frame for reporting using "glue\textquotesingle{} }
\NormalTok{ate}\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(ate)}

\CommentTok{\# obtain ate for report}
\NormalTok{ATE\_fit\_causal\_forest }\OtherTok{\textless{}{-}}
\NormalTok{  glue}\SpecialCharTok{::}\FunctionTok{glue}\NormalTok{(}
    \StringTok{"ATE = \{round(ate[1, 1], 2)\}, se = \{round(ate[2, 1], 2)\}"}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

Causal forest estimates the average treatment effect as ATE = 0.3, se =
0.01. This converges to to the true value supplied to the generating
mechanism of 0.3



\end{document}
