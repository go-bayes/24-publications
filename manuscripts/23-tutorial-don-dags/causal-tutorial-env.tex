% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames,x11names}{xcolor}
%
\documentclass[
  singlecolumn]{article}

\usepackage{amsmath,amssymb}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage[]{libertinus}
\ifPDFTeX\else  
    % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\usepackage[top=30mm,left=20mm,heightrounded]{geometry}
\setlength{\emergencystretch}{3em} % prevent overfull lines
\setcounter{secnumdepth}{-\maxdimen} % remove section numbering
% Make \paragraph and \subparagraph free-standing
\makeatletter
\ifx\paragraph\undefined\else
  \let\oldparagraph\paragraph
  \renewcommand{\paragraph}{
    \@ifstar
      \xxxParagraphStar
      \xxxParagraphNoStar
  }
  \newcommand{\xxxParagraphStar}[1]{\oldparagraph*{#1}\mbox{}}
  \newcommand{\xxxParagraphNoStar}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
  \let\oldsubparagraph\subparagraph
  \renewcommand{\subparagraph}{
    \@ifstar
      \xxxSubParagraphStar
      \xxxSubParagraphNoStar
  }
  \newcommand{\xxxSubParagraphStar}[1]{\oldsubparagraph*{#1}\mbox{}}
  \newcommand{\xxxSubParagraphNoStar}[1]{\oldsubparagraph{#1}\mbox{}}
\fi
\makeatother

\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\newenvironment{Shaded}{}{}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{1.00,0.33,0.33}{\textbf{#1}}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.42,0.45,0.49}{#1}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.84,0.23,0.29}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.36,0.77}{#1}}
\newcommand{\BuiltInTok}[1]{\textcolor[rgb]{0.84,0.23,0.29}{#1}}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.01,0.18,0.38}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.42,0.45,0.49}{#1}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.42,0.45,0.49}{#1}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.00,0.36,0.77}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.84,0.23,0.29}{#1}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.84,0.23,0.29}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.36,0.77}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.42,0.45,0.49}{#1}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{1.00,0.33,0.33}{\underline{#1}}}
\newcommand{\ExtensionTok}[1]{\textcolor[rgb]{0.84,0.23,0.29}{\textbf{#1}}}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.36,0.77}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.44,0.26,0.76}{#1}}
\newcommand{\ImportTok}[1]{\textcolor[rgb]{0.01,0.18,0.38}{#1}}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.42,0.45,0.49}{#1}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.84,0.23,0.29}{#1}}
\newcommand{\NormalTok}[1]{\textcolor[rgb]{0.14,0.16,0.18}{#1}}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.14,0.16,0.18}{#1}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.44,0.26,0.76}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.84,0.23,0.29}{#1}}
\newcommand{\RegionMarkerTok}[1]{\textcolor[rgb]{0.42,0.45,0.49}{#1}}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.00,0.36,0.77}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.01,0.18,0.38}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.01,0.18,0.38}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.89,0.38,0.04}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.01,0.18,0.38}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{1.00,0.33,0.33}{#1}}

\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
% definitions for citeproc citations
\NewDocumentCommand\citeproctext{}{}
\NewDocumentCommand\citeproc{mm}{%
  \begingroup\def\citeproctext{#2}\cite{#1}\endgroup}
\makeatletter
 % allow citations to break across lines
 \let\@cite@ofmt\@firstofone
 % avoid brackets around text for \cite:
 \def\@biblabel#1{}
 \def\@cite#1#2{{#1\if@tempswa , #2\fi}}
\makeatother
\newlength{\cslhangindent}
\setlength{\cslhangindent}{1.5em}
\newlength{\csllabelwidth}
\setlength{\csllabelwidth}{3em}
\newenvironment{CSLReferences}[2] % #1 hanging-indent, #2 entry-spacing
 {\begin{list}{}{%
  \setlength{\itemindent}{0pt}
  \setlength{\leftmargin}{0pt}
  \setlength{\parsep}{0pt}
  % turn on hanging indent if param 1 is 1
  \ifodd #1
   \setlength{\leftmargin}{\cslhangindent}
   \setlength{\itemindent}{-1\cslhangindent}
  \fi
  % set entry spacing
  \setlength{\itemsep}{#2\baselineskip}}}
 {\end{list}}
\usepackage{calc}
\newcommand{\CSLBlock}[1]{\hfill\break\parbox[t]{\linewidth}{\strut\ignorespaces#1\strut}}
\newcommand{\CSLLeftMargin}[1]{\parbox[t]{\csllabelwidth}{\strut#1\strut}}
\newcommand{\CSLRightInline}[1]{\parbox[t]{\linewidth - \csllabelwidth}{\strut#1\strut}}
\newcommand{\CSLIndent}[1]{\hspace{\cslhangindent}#1}

\usepackage{booktabs}
\usepackage{longtable}
\usepackage{array}
\usepackage{multirow}
\usepackage{wrapfig}
\usepackage{float}
\usepackage{colortbl}
\usepackage{pdflscape}
\usepackage{tabu}
\usepackage{threeparttable}
\usepackage{threeparttablex}
\usepackage[normalem]{ulem}
\usepackage{makecell}
\usepackage{xcolor}
\input{/Users/joseph/GIT/latex/latex-for-quarto.tex}
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\AtBeginDocument{%
\ifdefined\contentsname
  \renewcommand*\contentsname{Table of contents}
\else
  \newcommand\contentsname{Table of contents}
\fi
\ifdefined\listfigurename
  \renewcommand*\listfigurename{List of Figures}
\else
  \newcommand\listfigurename{List of Figures}
\fi
\ifdefined\listtablename
  \renewcommand*\listtablename{List of Tables}
\else
  \newcommand\listtablename{List of Tables}
\fi
\ifdefined\figurename
  \renewcommand*\figurename{Figure}
\else
  \newcommand\figurename{Figure}
\fi
\ifdefined\tablename
  \renewcommand*\tablename{Table}
\else
  \newcommand\tablename{Table}
\fi
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Listing}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}
\makeatother
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\@ifpackageloaded{subcaption}{}{\usepackage{subcaption}}
\makeatother
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\usepackage{bookmark}

\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same} % disable monospaced font for URLs
\hypersetup{
  pdftitle={Causal Inference in Environmental Psychology},
  pdfkeywords={DAGS, Causal
Inference, Confounding, Environmental, Psychology, Panel},
  colorlinks=true,
  linkcolor={blue},
  filecolor={Maroon},
  citecolor={Blue},
  urlcolor={Blue},
  pdfcreator={LaTeX via pandoc}}

\title{Causal Inference in Environmental Psychology}
\author{Joseph A Bulbulia \and Donald W Hine}
\date{2024-05-16}

\begin{document}
\maketitle
\begin{abstract}
This chapter offers an introduction to causal inference within
environmental psychology, emphasising the use of causal diagrams
(Directed Acyclic Graphs --- DAGs) to evaluate strategies for
confounding control. It is structured into four sections: (1) a primer
on principles of causal inference, (2) a tutorial on constructing causal
diagrams, (3) practical examples of their application, and (4)
guidelines for effective reporting. In settings where the structure of
confounding is unclear or contested, we recommend creating multiple
causal diagrams and performing corresponding analyses. That is, we
recommend presenting a spectrum of results linked to each credible
causal diagram.
\end{abstract}

\subsection{Introduction}\label{introduction}

Causal inference seeks to answer the question: How does intervening on a
``treatment'' variable affect an ``outcome'' variable? Although causal
understanding is widespread across species
(\citeproc{ref-mancuso2018revolutionary}{Mancuso 2018}), methods for
systematically quantitatively magnitudes of causal effect are relatively
recent developments.

Randomised controlled experiments, considered the ``gold standard'' for
measuring causal effects, face limitations related to cost,
practicality, ethics, and potential biases
(\citeproc{ref-hernan2017per}{Hernán \emph{et al.} 2017};
\citeproc{ref-montgomery2018}{Montgomery \emph{et al.} 2018}).
Observational data, which are more readily available, offer alternatives
-- provided that the causal inferences we draw from observational data
are valid. Regrettably, however, the practice of analysing observational
data and reporting associations without adequately addressing
confounding biases is misleading (\citeproc{ref-hernan2023}{Hernan and
Robins 2023}; \citeproc{ref-robins1986}{Robins 1986};
\citeproc{ref-westreich2013}{Westreich and Greenland 2013}). Widespread
practices of analysing observational data and reporting correlations as
hinting at causality, yet without adequately controlling for these
biases, are misleading (\citeproc{ref-bulbulia2023a}{Bulbulia \emph{et
al.} 2023}). Converging advances in causal inference from biostatistics,
economomics, and computer science allow us to leverage observational
data to better ask, and answer, causal questions
(\citeproc{ref-hernan2023}{Hernan and Robins 2023}). This chapter
provides and overview of these methods and their applications for
environmental psychology.

\hyperref[section-part1]{\textbf{Part 1}} introduces the potential
outcomes framework and the
\hyperref[sec-three-fundamental-assumptions]{\textbf{three fundamental
assumptions}} necessary for causal inferences
(\citeproc{ref-hernan2023}{Hernan and Robins 2023}). Here, we discover
that causal inference hinges on our ability to use data to compute
contrasts between \emph{counterfactual} outcomes
(\citeproc{ref-hernan2017per}{Hernán \emph{et al.} 2017};
\citeproc{ref-robins2008estimation}{Robins and Hernan 2008};
\citeproc{ref-westreich2012berkson}{Westreich 2012};
\citeproc{ref-westreich2015}{Westreich \emph{et al.} 2015}).

\hyperref[section-part2]{\textbf{Part 2}} offers an introduction to
causal diagrams or ``Directed Acyclic Graphs (DAGs)'' as tools for
developing strategies for confounding control.

\hyperref[section-part3]{\textbf{Part 3}} presents seven worked examples
that put causal diagrams to practice.

\hyperref[section-part4]{\textbf{Part 4}} offers reporting guidelines.
Here we advocate for reporting multiple causal diagrams and analysis
wherever causal assumptions are uncertain or the timing of variables in
one's data is unclear.

\subsection{Part 1: An Overview of the Potential Outcomes Framework for
Causal Inference}\label{section-part1}

The potential outcomes framework for causal inference originated in the
work of Jerzy Neyman to evaluate the effectiveness of agricultural
experiments (\citeproc{ref-neyman1923}{Splawa-Neyman \emph{et al.}
1990}). It was later extended by Harvard statistician Donald Rubin, who
demonstrated the framework may also facilitate causal inferences in
non-experimental settings (\citeproc{ref-rubin1976}{Rubin 1976}). Jamie
Robins further generalised this framework to assess confounding in
complex scenarios involving multiple and time-varying treatments
(\citeproc{ref-robins1986}{Robins 1986}).

A fundamental principle in the potential outcomes framework is the
concept of ``counterfactual contrast'' or ``estimand.'' To quantify
causal effects, one must compare the outcomes under different
intervention or treatment scenarios. Notably, prior to any intervention,
these scenarios are purely hypothetical. Post-intervention, only one
scenario is actualised for each realised treatment, leaving the
alternative as a non-observed counterfactual. For any individual unit to
be treated, that only one of the two possible outcomes is realised
underscores a critical property of causality: causality is \textbf{not
directly observable} (\citeproc{ref-hume1902}{Hume 1902}). Causal
inference, therefore, can only quantify causal effects by combining data
with counterfactual simulation (\citeproc{ref-bulbulia2023a}{Bulbulia
\emph{et al.} 2023}; \citeproc{ref-edwards2015}{Edwards \emph{et al.}
2015}). The concept of a counterfactual data science -- may sound
strange. However, anyone who has encountered a randomised experiment has
encountered counterfactual data science. Before building intuitions for
causal inference from the familiar example of experiments, let's first
build intuitions for the idea that causal quantities are never directly
observed.

\paragraph{The Fundamental Problem of Causal Inference: Causal Contrasts
are Not Directly
Observed}\label{the-fundamental-problem-of-causal-inference-causal-contrasts-are-not-directly-observed}

Imagine you are at a pivotal juncture in your life. You have just
completed your undergraduate studies and have been accepted into your
dream Environmental Psychology program at the University of Canterbury.
You are set to relocate to Christchurch, New Zealand. However, while
making preparations, you receive a job offer from Acme Nuclear Fuels, a
leader in renewable energy. Should you embark on graduate study or take
the job? The course your life will take under each decision would appear
to differ --- your lifestyle, income, social networks, relationships,
and perhaps even sense of life purpose hang in the balance. Which choice
aligns with your ideal future?

Formally, let \(D\) denote your decision, where \(D = 1\) means
attending graduate school and \(D = 0\) means joining the workforce. Let
\(Y\) denote your life outcome. We use the symbol \(|\) to denote
conditionality such that the outcome. \(Y(1)\) denotes your life outcome
when \(Y|D=1\); \(Y(0)\) denotes your life outcome when \(Y|D=0\). Your
two potential outcomes under each path are described are given as
\(Y_{\text{you}}(1)\) and \(Y_{\text{you}}(0)\). Conceptually, to
quantify the \emph{magnitude} of the effect of your decision on your
life outcome, we must calculate the difference:

\[Y_{\text{you}}(1) - Y_{\text{you}}(0)\]

Yet, this difference cannot be calculated from data because when you
choose one path, you obscure the other:

\[
(Y_{\text{you}}|D_{\text{you}} = 1) = Y_{\text{you}}(1) \quad \text{implies} \quad Y_{\text{you}}(0)|D_{\text{you}} = 1~ \text{is counterfactual}.
\]

This expression means: ``The outcome we observe under option \(D = 1\)
can be measured. However, because option \(D = 0\) is not realised, the
outcome under option \(D=0\) cannot be measured. Thus, the contrast
between these two outcomes cannot be computed. At least one outcome
remains purely counterfactual.

The same problem arises if you select \(D = 0\). Then, the outcome under
\(D=1\) remains counterfactual. And so we cannot compute the contrast:

\[
(Y_{\text{you}}|D_{\text{you}} = 0) = Y_{\text{you}}(0) \quad \text{implies} \quad Y_{\text{you}}(1)|D_{\text{you}} = 0~ \text{is counterfactual}.
\]

Of course, you regularly make principled decisions about your life based
on past experiences, instincts, and knowledge. Nevertheless, the
\emph{data} that you require to quantitatively compare life outcomes
under one decision as opposed to the other is not available. Life, as it
would have unfolded under the option you do not select, remains
counterfactual -- it cannot be directly measured. A quantitative causal
contrast here is not a matter of factual data science. This example,
although contrived, perhaps resonates with similar crossroads you have
encountered in your life. The dilemmas that you faced at these
crossroads underscore what is known as ``The fundamental problem of
causal inference'' (\citeproc{ref-holland1986}{Holland 1986};
\citeproc{ref-rubin2005}{Rubin 2005}): for any individual case, we
cannot observe the potential outcomes that we require to quantify the
magnitude of an individual causal contrast.

The fundamental problem of causal inference never goes away. However, by
collecting, organising, and aggregating data under certain assumptions,
we can obtain valid quantitative causal contrasts from data for
\emph{average treatment effects}. To clarify these assumptions, we next
consider how experiments attach magnitudes to missing counterfactual
outcomes to obtain average treatment effects.

\subsubsection{Causal inference in Experiments is a Missing Data
Problem}\label{causal-inference-in-experiments-is-a-missing-data-problem}

Let us transition from the topic of life decisions to an example of
relevance to environmental psychology, namely, estimating the average
causal effect of easy access to urban green spaces on subjective
happiness, hereafter referred to as ``happiness.'' We assume this
outcome is measurable and represent it with \(Y\).

For simplicity, we classify the intervention ``ample access to green
space'' as a binary variable. Define \(A = 1\) as ``having ample access
to green space'' and \(A = 0\) as ``lacking ample access to green
space.'' We assume these conditions are mutually exclusive. This
simplification does not limit the generality of our conclusions; the
points we make about experiments also apply to continuous treatments. It
is crucial in causal inference to specify the population for whom we
seek to evaluate causal effects, or the ``target population.'' In this
case, our target population is residents of New Zealand in the 2020s.

A preliminary causal question -- defined as a causal contrast or
``estimand'' might, therefore be:

``In New Zealand, does proximity to abundant green spaces increase
self-perceived happiness compared to environments lacking such spaces?''

Of course it would be unethical to experimentally randomise individuals
into different green-space access conditions. However, for the purposes
of illustration, assume experimentalists could assign people randomly to
high and low green space access without objection or harm.

As alluded to earlier, the first point to note in the context of causal
inference is that even well-designed experiments confront the challenge
of missing values in the potential outcomes. Once an individual is
assigned to one treatment condition, we cannot observe that individual's
outcome for the condition not assigned. The fundamental problem of
causal inference remains constant: for each individual, we can only
observe one of the potential outcomes at any given time. Breaking down
the Average Treatment Effect (ATE) into observed and unobserved outcomes
yields the following equation:

\[
\text{Average Treatment Effect} = \left(\underbrace{\underbrace{\mathbb{E}[Y(1)|A = 1]}_{\text{observed}} + \underbrace{\mathbb{E}[Y(1)|A = 0]}_{\text{unobserved}}}_{\text{treated}}\right) - \left(\underbrace{\underbrace{\mathbb{E}[Y(0)|A = 0]}_{\text{observed}} + \underbrace{\mathbb{E}[Y(0)|A = 1]}_{\text{unobserved}}}_{\text{untreated}}\right).
\]

In this expression, \(\mathbb{E}[Y(1)|A = 1]\) represents the average
outcome when the treatment is given, which is observable. However,
\(\mathbb{E}[Y(1)|A = 0]\) represents the average outcome if the
treatment had been given to those who were untreated, which remains
unobservable. Similarly, the quantity \(\mathbb{E}[Y(0)|A = 1]\) also
remains unobservable.

It is hopefully evident from this brief application of the potential
outcomes framework to experiments that the fundamental problem of causal
inference is an ever-present concern even in experiments. For each
participant, it is impossible to determine the outcome they would have
experienced under an alternative treatment condition. You cannot
quantitatively describe the life you would have led had you chosen the
job at Acme Nuclear Fuels instead of attending the University of
Canterbury. Nor, if you lived your life in a leafy suburb, could you
determine how happy you would have been if your life had been devoid of
green space?

\subsubsection{In Experiments, Random Treatment Assignment Balances
Confounders Across
Treatments}\label{in-experiments-random-treatment-assignment-balances-confounders-across-treatments}

How do experiments manage to estimate average treatment effects despite
the inherent challenges? The solution involves addressing the concept of
``confounding.'' Consider the concept of ``confounding by common
cause.'' This occurs when one or more variables causally affect both the
intervention under study (the ``treatment'' or ``exposure'') and the
outcome of interest, leading to a non-causal association between the
treatment and outcome. By ``non-causal,'' we mean that if we intervened
in the treatment but not the confounder, the outcome would not change.
The common cause creates a misleading or exaggerated relationship that
may be mistakenly interpreted as causal. For instance, when assessing
the effect of access to green space on happiness, it is possible that
the association could be entirely explained by income. If so, then an
observed association between access to green space and happiness would
be entirely misleading. Were we to relocate low-income individuals to
high-access green areas, we might not affect subjective happiness at
all. Thus, accurately identifying and adjusting for confounding by
common cause is crucial for determining the true causal relationship
between two variables, ensuring that the observed association is not
merely a result of extraneous influences.

We can express this principle of no confounding mathematically in two
complementary ways (where \(A \coprod B\) signifies that \(A\) is
independent of \(B\), and vice versa):

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Potential Outcomes Independent of Treatment (given L):}
  \(Y(a) \coprod A \mid L\)
\item
  \textbf{Treatment Assignment Independent of Potential Outcomes (given
  L):} \(A \coprod Y(a) \mid L\)
\end{enumerate}

These formulations are crucial when working with causal diagrams, which
visually encode these principles. The key idea is straightforward:
ensuring a balance of confounders across treatment groups is fundamental
to experimental and observational causal inference strategies.
Randomisation facilitates this balance, achieving \(A \coprod Y(a)\).

\subsubsection{The Three Fundamental Assumptions of Causal
Inference}\label{sec-three-fundamental-assumptions}

Reviewing causal inference in experimental settings highlights three
core assumptions essential for causal analysis.

\paragraph{Fundamental Assumption 1: Conditional
Exchangeability}\label{fundamental-assumption-1-conditional-exchangeability}

We say that conditional exchangeability holds if the potential outcomes
and treatment assignments are statistically independent, considering all
measured confounders. It enables us to attribute observed group
differences directly to the treatment. Randomisation provides
\emph{unconditional} exchangeability, simplifying the analytical
process.

\subparagraph{Challenge in Satisfying Conditional Exchangeability in
Observational
Settings}\label{challenge-in-satisfying-conditional-exchangeability-in-observational-settings}

Achieving conditional exchangeability is challenging in observational
studies. This condition requires the groups being compared to be similar
in every aspect except for the treatment. Consider the example of the
effect of living near green spaces on subjective happiness. In
real-world data, individuals with access to green spaces may differ from
those without access in several ways:

\begin{itemize}
\tightlist
\item
  \textbf{Socioeconomic status}: the economic capacity of individuals
  often determines their living environments, thereby affecting their
  access to quality green spaces.
\item
  \textbf{Age demographics}: different age groups have unique
  preferences and necessities regarding green spaces, which could
  influence the observed outcomes.
\item
  \textbf{Mental health}: pre-existing conditions might lead individuals
  to seek out or avoid green spaces, complicating the causal pathway.
\item
  \textbf{Lifestyle choices}: the proximity to green spaces could
  correlate with a more active, outdoor lifestyle preference. Does the
  observed effect on well-being directly result from the green space, or
  does it indicate a healthier lifestyle?
\item
  \textbf{Personal values and social connections}: environmental values
  and community ties may influence both the choice of residence and the
  utilisation of green spaces.
\end{itemize}

These and other unmeasured factors can introduce biases, complicating
the interpretation of causal relationships in observational studies.

\paragraph{Fundamental Assumption 2: Causal
Consistency}\label{fundamental-assumption-2-causal-consistency}

We say that causal consistency holds if there is no heterogeneity in the
treatments that would prevent us from assuming that the observed
outcomes under treatments correspond to their potential outcomes. For an
individual `i', we must be able to assume:

\[
\begin{aligned}
Y_{i}(1) &= (Y_{i}|A_{i} = 1) \quad \text{(Potential outcome if treated)} \\
Y_{i}(0) &= (Y_{i}|A_{i} = 0) \quad \text{(Potential outcome if untreated)}
\end{aligned}
\]

If this assumption holds, as well as the assumptions of conditional
exchangeability and positivity (reviewed below), we can calculate the
Average Treatment Effect (ATE) from observed data as:

\[
\begin{aligned}
\text{ATE} &= \mathbb{E}[Y(1)] - \mathbb{E}[Y(0)] \\
&= \mathbb{E}(Y|A=1) - \mathbb{E}(Y|A=0)
\end{aligned}
\]

This contrast assumes that the potential outcome under treatment is
observable when the treatment is administered, setting \(Y_i(a)\) to
\(Y_i|A_i=a\).

The standardisation of treatments in randomised controlled experiments
generally ensures the validity of the causal consistency assumption,
which is seldom disputed. However, in observational settings, we cannot
typically control the treatments that people receive. This fact imposes
considerable challenges for satisfying this assumption. (Discussed in
\hyperref[appendix-b]{Appendix B})

\subparagraph{Challenges in Satisfying the Causal Consistency Assumption
in Observational
Settings}\label{challenges-in-satisfying-the-causal-consistency-assumption-in-observational-settings}

Again we consider our interest in quantifying the causal effect of
living near green spaces. The definition of ``proximity to green
spaces'' itself, varies significantly, leading to a diverse range of
experiences classified under the same ``treatment.'' Focussing on the
variability of the green spaces themselves, this includes:

\begin{itemize}
\tightlist
\item
  \textbf{Diversity of green spaces}: Green spaces' ecological richness
  and visual appeal vary considerable. Equating well-maintained parks
  with neglected wild areas does not provide a like-for-like comparison.
\item
  \textbf{Availability of amenities}: facilities such as walking paths
  and benches considerably affects the spaces' usability and enjoyment.
\item
  \textbf{Size and type of green space}: the benefits derived from an
  urban garden versus a vast forest differ markedly, emphasising the
  need to consider the nature of the green space in the analysis.
\end{itemize}

\paragraph{Fundamental Assumption 3:
Positivity}\label{fundamental-assumption-3-positivity}

We must assume that there is a non-zero probability of receiving each
treatment level within covariate-defined subgroups.

\[
P(A = a | L= l) > 0
\]

This assumption is also met by the \emph{control} that experimentalists
exert over randomised controlled experiments and is rarely stated
explicitly. However, in observational settings, this condition must be
verified to avoid extrapolating results beyond observed data.

\subparagraph{Challenges in Satisfying The Positivity Assumption in
Observational Settings: The Relevant Treatments Do Not Exist in The
Data}\label{challenges-in-satisfying-the-positivity-assumption-in-observational-settings-the-relevant-treatments-do-not-exist-in-the-data}

Positivity demands that each individual has the possibility of
experiencing \emph{every} level of the treatment to be compared.
However, real-world constraints, such as housing availability in
specific locales, may preclude some groups from accessing varied green
spaces. Where the treatments of interest are absent or scarcely
represented in our dataset, the resulting causal inferences will lack
empirical support. Consequently, any coefficients derived will represent
extrapolations from statistical models, challenging the validity of our
causal inferences (\citeproc{ref-hernan2023}{Hernan and Robins 2023};
\citeproc{ref-westreich2010}{Westreich and Cole 2010}).

\subsubsection{Summary Part 1}\label{summary-part-1}

In Part 1, we described the fundamental problem of causal inference,
focusing on the critical distinction between correlation -- associations
in the data, and causation -- the contrast between potential outcomes
only one of which, at most, can be observed. We discussed how controlled
experiments facilitate the estimation of average treatment effects (ATE)
by systematically manipulating the variable of interest, allowing for
the distribution of variables that might affect the outcome to be
balanced across the treatment conditions. The discussion then shifted to
observational data, emphasising the challenges inherent in extracting
causal relationships from data without the benefit of controlled
interventions. We underscored the necessity of three key assumptions
---- conditional exchangeability, causal consistency, and
positivity---for inferring average treatment effects from observational
data. These assumptions ensure that the treatment groups are comparable,
the treatment effect is consistent across the population, and every
individual has a non-zero probability of receiving each treatment level,
respectively. As we transition to discussing causal diagrams, it is
essential to recognise that these graphical tools offer a systematic
approach to identifying and controlling for confounding variables, thus
aiding researchers in satisfying the first of the three fundamental
assumptions required for causal inference. Causal diagrams, however, do
not obviate the need for these assumptions; instead, they provide a
framework for evaluating whether and how the first of the three
assumptions -- conditional exchangeability -- may be satisfied in a
given study.

\subsection{Part 2: Causal Diagrams - A Visual Approach to Understanding
Confounding}\label{section-part2}

We now introduce causal diagrams, beginning with essential terminology.
Grasping this vocabulary is crucial for effectively employing causal
diagrams, though it may initially seem daunting. After laying out the
terms, we will consider practical examples, uncovering the various forms
of confounding embedded within four principal causal structures.
Identifying and understanding these structures is vital for their
application in real-world analyses. Refer to
\hyperref[appendix-a]{\textbf{Appendix A}} for a detailed glossary.

\subsubsection{Elements of Causal
Diagrams}\label{elements-of-causal-diagrams}

Causal diagrams distil the essence of causal relationships within a
system into visual representations. At their core, these diagrams
consist of:

\paragraph{\texorpdfstring{1. \textbf{Nodes}}{1. Nodes}}\label{nodes}

Nodes represent variables or events within a causal framework. Each node
stands for a distinct element that either exerts influence or is subject
to influence within the system. Nodes encapsulate the components of our
causal inquiry. They denote: (1) treatment(s) (2) outcome(s) or (3)
confounders.

\paragraph{\texorpdfstring{2.
\textbf{Arrows/Edges}}{2. Arrows/Edges}}\label{arrowsedges}

Arrows indicate the direction and presence of causal relationships
between the variables denoted by nodes. Directed edges trace the assumed
flow of causal influence. The originating variable is called a
``parent,'' and the receiving variable is called a ``child.'' These
arrows define the causal architecture of the system, illustrating how we
assume one variable causally affects another. Notably, the
representation of causal relationships through arrows remains the same
whether the assumed influence is linear or non-linear.

\paragraph{\texorpdfstring{3.
\textbf{Conditioning}}{3. Conditioning}}\label{conditioning}

In causal data science, deciding which variables to adjust for is
crucial for estimating the true causal effect unconfounded by other
factors. We denote a decision to ``control for'' or equivalently
``condition on'' or equivalently ``adjust for'' a variable by enclosing
it in a box.

\subsubsection{The Rules of
D-separation}\label{the-rules-of-d-separation}

Judea Pearl demonstrated how the rules of d-separation allow us to
analyse relationships within causal diagrams
(\citeproc{ref-pearl1995}{Pearl 1995}). These rules allow us to identify
confounders and develop strategies for obtaining valid causal inferences
from statistical associations in the data
(\citeproc{ref-pearl1995}{Pearl 1995}).

\textbf{Key Concepts}

\paragraph{Dependence}\label{dependence}

Denoted as \(A \cancel\coprod B\), indicating that the probability
distributions of \(A\) and \(B\) are interrelated. Knowledge about one
variable provides insights into the other, suggesting a potential causal
or associational link.

\paragraph{Independence}\label{independence}

Denoted as \(A \coprod B\), signifying that the probability
distributions of \(A\) and \(B\) are independent. Information about one
variable reveals nothing about the other, indicating no direct causal or
associational connection.

\paragraph{Blocked Paths and
D-Separation}\label{blocked-paths-and-d-separation}

A path is considered ``blocked'' when a node on this path prevents
causal influence from propagating between variables. D-separation occurs
when all paths between two variables are blocked (\(A \coprod B\)),
indicating no direct statistical association between them. This
condition is crucial for enabling unbiased causal inference.

\paragraph{Open Paths \& D-connection}\label{open-paths-d-connection}

If at least one path between variables remains unblocked, allowing for
the transmission of causal influence, the variables are considered
d-connected (\(A \cancel\coprod B\)). This condition suggests a
statistical association, warranting further analysis to understand the
nature of the bias in the statistical association between the treatment
and outcome.

\subsubsection{The Five Elementary Graphical Structures of Causality and
Four Rules for Confounding Control}\label{sec-five-elementary}

To uncover causal insights from statistical relationships, it is
essential to understand five basic graphical structures. We next examine
these structures, remembering that achieving balance in confounders
across treatments requires ensuring statistical independence between
potential outcomes and treatment (\(A\coprod Y(a)|L\)) within groups
defined by measured covariates \(L\).

\paragraph{Causal Structure 1: Absence of Causality: Two Variables with
No
Arrows}\label{causal-structure-1-absence-of-causality-two-variables-with-no-arrows}

When any arrows do not connect \(A\) and \(B\), we assume do not share a
causal relationship and are statistically independent. Graphically, we
represent this relationship as:

\[\xorxA\]

\paragraph{Causal Structure 2: Direct Causation Between Two
Variables}\label{causal-structure-2-direct-causation-between-two-variables}

A causal arrow (\(A \to B\)) signifies that changes in variable \(A\)
directly cause changes in variable \(B\), creating a statistical
dependence between them. This direct causal link is graphically depicted
as:

\[\xtoxA\]

\paragraph{Causal Structure 3: The Fork Structure - Common Cause
Scenario}\label{causal-structure-3-the-fork-structure---common-cause-scenario}

The fork structure, indicated by \(A \rightarrow B\) and
\(A \rightarrow C\), denotes the assumption that \(A\) is a common cause
that influences both \(B\) and \(C\). Graphically:

\[\fork\]

Pearl proved that when we condition on the common cause \(A\) (indicated
by \(\boxed{A}\)), \(B\) and \(C\) become conditionally independent
(\citeproc{ref-pearl2009a}{Pearl 2009b}). By adjusting for the common
cause, any non-causal association between \(B\) and \(C\) is effectively
blocked at node \(A\).

\subparagraph{Motivating Example}\label{motivating-example}

Suppose our observations reveal that areas with higher rates of bicycle
commuting also present lower average levels of psychological distress.
Does using bicycle commuting directly reduce average psychological
distress? Not necessarily. A common environmental factor might influence
both. Consider sunshine hours as the common cause:

\begin{itemize}
\tightlist
\item
  Sunshine (\(A\)) encourages the use of bicycles (\(B\)).
\item
  Sunshine (\(A\)) contributes to lower psychological distress (\(C\)).
\end{itemize}

According to the rules of d-separation, if we were to account for the
common cause (sunshine hours), isolating days with similar levels of
sunshine, the apparent link between bicycle commuting and psychological
distress levels would dissipate. Adjusting for the fork's common cause
would eliminate the spurious association we have assumed in this
example.

\paragraph{Rule 1: The Fork Rule}\label{sec-four-rules}

If interested in the causal effect of \(B \to C\), condition on
\(\boxed{A}\).

\paragraph{Causal Structure 4. The Chain Structure: A
Mediator}\label{causal-structure-4.-the-chain-structure-a-mediator}

The chain structure (\(A \rightarrow B \rightarrow C\)) illustrates a
setting in which \(A\) causes \(B\), and \(B\) subsequently causes
\(C\). Conditioning on the intermediary variable \(B\) (denoted:
\(\boxed{B}\)) interrupts the causal pathway, rendering \(A\) and \(C\)
conditionally independent. Graphically, we represent this relationship
as:

\[\chain\]

\subparagraph{Motivating Example}\label{motivating-example-1}

Suppose we wanted to assess the effect of green space renovation in
urban areas (\(A\)) on local community engagement (\(B\)), which
subsequently reduces neighbourhood crime rates (\(C\)). Assume the
renovation of green spaces \((A) \rightarrow\) boosts community
engagement \((B) \rightarrow\), which then leads to a decrease in crime
rates (\(C\)).

According to the rules of d-separation, controlling for the mediator,
community engagement, in this case, might hide the broader effect of
green space renovation. If the primary path through which green space
renovation affects crime rates is via enhanced community engagement,
then adjusting for community engagement could misleadingly suggest that
green space renovation does not directly influence crime rates. This
example underscores the necessity of carefully considering mediators
when examining the effects of environmental changes on social outcomes.

\subparagraph{Rule 2. The Chain Rule}\label{rule-2.-the-chain-rule}

If investigating the \emph{total} causal effect of \(A\to C\),
\emph{avoid} conditioning on the mediator (\(B\)).

\textbf{Important note:} A ``total'' causal effect may combine several
causal chains in complex systems. Identifying a mediating role for focus
(\(B\)) is a valuable finding about the \emph{mechanism} through which
the supplement might operate. Assessing causal mediation requires
further assumptions, which we will not discuss here
(\citeproc{ref-bulbulia2023}{Bulbulia 2023};
\citeproc{ref-vanderweele2015}{VanderWeele 2015}).

\paragraph{Causal Structure 5: The Collider Structure: A Common
Effect}\label{causal-structure-5-the-collider-structure-a-common-effect}

The collider (\(A\to C\), \(B \to C\)) features two factors
independently causing a common effect. Initially, \(A\) and \(B\) lack
association. Conditioning on the collider \(C\) (or its descendant)
introduces a spurious statistical association between \(A\) and \(B\).
Graphically:

\[\immorality\]

\subparagraph{Motivating Example}\label{motivating-example-2}

Suppose we were interested in whether access to green spaces (\(A\))
causes people to become happier (\(B\))? Suppose we were to control for
a variable \(C\) -- say health -- which is a common effect of the
treatment \(A\) (access to green space) and the outcome \(B\)
(happiness). Here, conditioning on \(C\) would open a non-causal path
between \(A\) and \(B\). That is, if \(A\) causes \(C\) and \(B\) also
causes \(C\), controlling for \(C\) can induce a spurious association
between \(A\) and \(B\). Consider:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Among unhealthy individuals (low \(C\)), those with high access to
  green space (\(A\)) will appear less happy (\(B\)). When we stratify
  by \(C\), a negative association between green space and happiness is
  observed, even if \(A\) and \(B\) are not causally asssociated.
\item
  Among healthy individuals (high \(C\)), those with low access to green
  space (\(A\)) will be happier (\(B\)). Again, when we stratify by
  \(C\) a negative association between green space and happiness is
  observed.
\end{enumerate}

In this scenario, the relationship between green space access and
happiness, when health is controlled for, introduces a spurious negative
association. This association is non-causal because it arises from
controlling for a collider, health. Without controlling for health,
assuming there are no other confounding paths between green space access
and happiness, the misleading statistical association would not be
present.

This example illustrates the risk of confounding the analysis by
conditioning on an outcome influenced by both variables of interest.

\paragraph{\texorpdfstring{\textbf{Rule 3: The Collider Rule:} when
assessing the causal effect of \(A\to B\), do not conditioning on a
collider (\(C\)) or its descendants. Doing so may introduce an
association that appears causal but is
not.}{Rule 3: The Collider Rule: when assessing the causal effect of A\textbackslash to B, do not conditioning on a collider (C) or its descendants. Doing so may introduce an association that appears causal but is not.}}\label{rule-3-the-collider-rule-when-assessing-the-causal-effect-of-ato-b-do-not-conditioning-on-a-collider-c-or-its-descendants.-doing-so-may-introduce-an-association-that-appears-causal-but-is-not.}

\paragraph{We build all complex causal relationships from the five
elemental structures of
causation}\label{we-build-all-complex-causal-relationships-from-the-five-elemental-structures-of-causation}

All forms of confounding bias stem from combinations of the five basic
causal structures we have outlined (absence/presence of cause, forks,
chains, and colliders). Understanding these elements in isolation and
combination allows us to identify potential confounders based on our
assumptions about the world as encoded in a causal diagram. Here we
consider an example that combines two structures: the collider structure
(\(A \rightarrowred \boxed{C} \leftarrowred
B\) and basic causality (\(C\rightarrowNEW D\)), This combination
produce confounding by proxy:
\(A \rightarrowred \boxed{D} \leftarrowred B\).

Causation implies statistical association. As such, causal inheritance
implies \emph{statistical dependence by inheritance}. The property of
statistical inheritance makes descendants act as stand-ins or proxies
for their parents.

\subparagraph{Motivating Example}\label{motivating-example-3}

Consider again the example of whether access to urban green spaces
(\(A\)) affects wealth (\(B\)). Imagine they do not, but both
independently contribute to well-being (\(C\)). Suppose that the only
people who respond to our survey are those who are high in well-being.
In effect, our survey is conditioning on one population stratum (\(D\)),
which is a descendant of the collider -- well-being. Initially, green
spaces and income independently affect well-being. However, when we
specifically analyse data based on willingness to participate in the
survey (\(\boxed{D}\)), we may inadvertently induce an association
between green space access and socioeconomic status, inferring that
those with access to green space tend to have a lower income.

\[\immoralityChildA\]

Colliders and their descendants set subtle ``traps'' that might induce
spurious associations.

However, as we shall see in the next section, conditioning on proxies of
unmeasured confounders opens possibilities for confounding control
beyond our measured variables. We can sometimes leverage proxies to
reduce bias in our causal inferences.

\subparagraph{Rule 4: The Proxy Rule}\label{rule-4-the-proxy-rule}

Conditioning on a descendant is akin to conditioning on its parent. Put
differently, a descendant is a *proxy\$ for its parent. Avoid
conditioning on descendants in settings where conditioning on the parent
would induce misleading associations.

\paragraph{Role of Assumptions}\label{role-of-assumptions}

Causal diagrams bring structure to complex environmental psychology
systems. They promote critical thinking about relationships, improving
study design and the chances of isolating true causal effects. However,
causal diagrams cannot avoid assumptions. Observational data alone
cannot prove causation: many diagrams are typically consistent with the
data. The power of causal diagrams lies in helping investigators
understand how their assumptions and the data interact. However, we
should create causal diagrams in collaboration with subject area experts
because every path except the \(A\to Y\) path is assumed. When experts
disagree, we should propose multiple causal diagrams to reflect the
implications of disagreements for causal inference and report the
outcomes of their corresponding confounding control strategies.

\subsubsection{How to Create Causal Diagrams to Address Causal
Identification Problems
\{sec-how-to-create-causal-diagrams\}}\label{how-to-create-causal-diagrams-to-address-causal-identification-problems-sec-how-to-create-causal-diagrams}

The \textbf{identification problem} centres on whether we can derive the
true causal effect of a treatment (\(A\)) on an outcome (\(Y\)) from
observed data. Addressing the identification problem has two core
components:

\paragraph{First, evaluate bias in the absence of a treatment
effect}\label{first-evaluate-bias-in-the-absence-of-a-treatment-effect}

Before attributing any statistical association to causality, we must
eliminate non-causal sources of correlation. We do this by:

\begin{itemize}
\tightlist
\item
  Identifying factors that influence both treatment (\(A\)) and outcome
  (\(Y\)).
\item
  Developing adjustment strategies to control for confounders.
\item
  Blocking backdoor paths that create indirect, non-causal links between
  \(A\) and \(Y\). By adjusting for confounders, we aim to achieve
  d-separation between \(A\) and \(Y\).
\end{itemize}

\paragraph{Second, evaluate bias in the presence of a treatment
effect}\label{second-evaluate-bias-in-the-presence-of-a-treatment-effect}

After addressing potential confounders, we must ensure any remaining
association between \(A\) and \(Y\) reflects a true causal relationship.
We address \textbf{over-conditioning bias} by:

\begin{itemize}
\tightlist
\item
  Avoiding mediator bias
\item
  Avoiding collider bias
\item
  Verifying that any association between \(A\) and \(Y\) after in
  unbiased after all adjustments.
\end{itemize}

Thus, causal inference demands a delicate balance: identify and control
for confounders but avoid introducing new biases. Here is how
investigators should construct their causal diagrams.

\paragraph{Step 1. Clarify the research question evaluated by the
diagram}\label{step-1.-clarify-the-research-question-evaluated-by-the-diagram}

Before attempting to draw any causal diagram, state the problem your
diagram addresses and the population to whom the problem applies. Causal
identification strategies may vary by question. For example, the
confounding control strategy for evaluating the path \(L\to Y\) will
differ from that of assessing the path \(A\to Y\). For this reason,
reporting coefficients other than the association between \(A \to Y\) is
typically ill-advised; see Westreich and Greenland
(\citeproc{ref-westreich2013}{2013}); McElreath
(\citeproc{ref-mcelreath2020}{2020}); Bulbulia
(\citeproc{ref-bulbulia2023}{2023}).

\paragraph{Step 2. Include all common causes of the exposure and
outcome}\label{step-2.-include-all-common-causes-of-the-exposure-and-outcome}

Incorporate all common causes (confounders) of both the exposure and the
outcome into your diagram. This includes both measured and unmeasured
variables. Where possible, aggregate functionally similar common causes
into a single variable notation (e.g., \(L_0\) for demographic
variables).

\paragraph{Step 3. Include all ancestors of measured confounders linked
with the treatment, the outcome, or
both}\label{step-3.-include-all-ancestors-of-measured-confounders-linked-with-the-treatment-the-outcome-or-both}

Include any ancestors (precursors) of measured confounders that are
associated with either the treatment, the outcome, or both. This step is
crucial for addressing hidden biases arising from unmeasured
confounding. Simplify the diagram by grouping similar variables.

\paragraph{Step 4. Explicitly state assumptions about relative
timing}\label{step-4.-explicitly-state-assumptions-about-relative-timing}

Explicitly annotate the temporal sequence of events using subscripts
(e.g., \(L_0\), \(A_1\), \(Y_2\)). It is imperative that causal diagrams
are acyclic.

\paragraph{Step 5. Arrange temporal order of causality
visually}\label{step-5.-arrange-temporal-order-of-causality-visually}

Arrange your diagram to reflect the temporal progression of causality,
either left-to-right or top-to-bottom. This arrangement enhances the
comprehensibility of causal relations and is vital for dissecting
identification issues as discussed in \hyperref[sec-part3]{\textbf{Part
3}}, establishing temporal ordering is necessary for evaluating
identification problems.

\paragraph{Step 6. Box variables are those variables that we adjust for
to control
confounding}\label{step-6.-box-variables-are-those-variables-that-we-adjust-for-to-control-confounding}

Mark variables for adjustment (e.g., confounders) with boxes.

\paragraph{Step 7. Represent paths structurally, not
parametrically}\label{step-7.-represent-paths-structurally-not-parametrically}

Focus on whether paths exist, not their functional form (linear,
non-linear, etc.). Parametric descriptions are not relevant for bias
evaluation in a causal diagram. (For an explanation of causal
interaction and diagrams, see: Bulbulia
(\citeproc{ref-bulbulia2023}{2023}).)

\paragraph{Step 8. Minimise paths to those necessary for the
identification
problem}\label{step-8.-minimise-paths-to-those-necessary-for-the-identification-problem}

Reduce clutter; only include paths critical for a specific question
(e.g., backdoor paths, mediators).

\paragraph{Step 9. Consider Potential Unmeasured
Confounders}\label{step-9.-consider-potential-unmeasured-confounders}

Leverage domain expertise to clarify potential unmeasured confounders
and represent them in your diagram. This proactive step aids in
anticipating and addressing \emph{all} possible sources of confounding
bias.

\paragraph{\texorpdfstring{\textbf{Step 10. State Graphical
Conventions}}{Step 10. State Graphical Conventions}}\label{step-10.-state-graphical-conventions}

Establish and explain the graphical conventions used in your diagram
(e.g., using red to highlight open backdoor paths). Consistency in
symbol use enhances interpretability, while explicit descriptions
improve accessibility and understanding.

\subsection{Part 3. Using Causal Diagrams for Causal Identification -
Worked Examples}\label{section-part3}

\subsubsection{Notation}\label{notation}

Causal diagrams use specific symbols to represent elements essential in
causal inference (\citeproc{ref-greenland1999}{Greenland \emph{et al.}
1999}; \citeproc{ref-pearl1995}{Pearl 1995},
\citeproc{ref-pearl2009}{2009a}). However, as mentioned, no agreed-upon
convention exists for creating causal diagrams. We list the symbols and
conventions we use in this chapter in Table~\ref{tbl-01}.

\begin{itemize}
\tightlist
\item
  \textbf{\(A\)} is the treatment or exposure variable -- the
  intervention or condition whose effect on an outcome is under
  investigation. \textbf{This symbol represents the cause}.
\item
  \textbf{\(Y\)} is the outcome variable -- the effect or result that is
  being studied. \textbf{This symbol represents the effect}.
\item
  \textbf{\(L\)} includes all measured confounders -- variables that may
  affect both the treatment and the outcome.
\item
  \textbf{\(U\)} includes unmeasured confounders -- variables not
  included in the analysis that could influence both the treatment and
  the outcome, potentially leading to biased conclusions.
\item
  \textbf{\(M\)} is a mediator variable -- a factor through which the
  treatment affects the outcome. The focus here is on identifying the
  total effect of treatment \(A\) on an outcome \(Y\). Still, it is also
  essential to understand how controlling for mediators can affect
  estimates of this total effect.
\end{itemize}

\begin{table}

\caption{\label{tbl-01}Terminology used in this article for causal
diagrams. The graph is adapted from
(\citeproc{ref-bulbulia2023}{Bulbulia 2023}).}

\centering{

\terminologylocalconventionssimple

}

\end{table}%

We next describe our graphical conventions and causal diagrams. Again,
because conventions may differ, it is always important to state them
explicitly when reporting causal diagrams. Table~\ref{tbl-02} describes
the basic conventions that we employ in this chapter.

\begin{table}

\caption{\label{tbl-02}Basic conventions for causal diagrams (adapted
from (\citeproc{ref-bulbulia2023}{Bulbulia 2023})).}

\centering{

\terminologygeneralbasic

}

\end{table}%

\subsubsection{Graphical Table}\label{graphical-table}

Table~\ref{tbl-04} provides seven worked examples that put causal
diagrams to work. Our example will focus on the question of whether
access to green space affects happiness and approach this question by
focusing on how different assumptions about (i) the structure of the
world and (ii) the observational data that have been collected may
affect strategies for confounding control and the confidence in our
results. Each example refers to a row in the table.

\begin{table}

\caption{\label{tbl-04}Worked examples: This table is adapted from
(\citeproc{ref-bulbulia2023}{Bulbulia 2023}).}

\centering{

\terminologyelconfoundersLONG

}

\end{table}%

\subsubsection{1. The problem of confounding by a common
cause}\label{the-problem-of-confounding-by-a-common-cause}

Table~\ref{tbl-04} Row 1 describes the confounding problem of a common
cause. We encountered this problem in Part 1. Such confounding arises
when there is a variable or set of variables, denoted by \(L\), that
influence both the exposure, denoted by \(A\), and the outcome, denoted
by \(Y.\) Because \(L\) is a common cause of both \(A\) and \(Y\), \(L\)
may create a statistical association between \(A\) and \(Y\) that does
not reflect a causal association.

For instance, in the context of green spaces, consider people who live
closer to green spaces (exposure \(A\)) and their experience of improved
happiness (outcome \(Y\)). A common cause might be socioeconomic status
\(L\). Individuals with higher socioeconomic status might have the
financial capacity to afford housing near green spaces and
simultaneously afford better healthcare and lifestyle choices,
contributing to greater happiness. Thus, although the data may show a
statistical association between living closer to green spaces \(A\) and
greater happiness \(Y\), this association might not reflect a direct
causal relationship owing to confounding by socioeconomic status \(L\).

How might we obtain balance in this confounder to compare the
treatments? Addressing confounding by a common cause involves adjusting
for the confounder in one's statistical model. We may adjust through
regression, or more complicated methods, such as the inverse probability
of treatment weighting, marginal structural models, and others see
Hernán and Monge (\citeproc{ref-hernuxe1n2023}{2023}). Such adjustment
effectively closes the backdoor path from the exposure to the outcome.
Equivalently, conditioning on \(L\) d-separates \(A\) and \(Y\).

Table~\ref{tbl-04} Row 1, Column 3, emphasises that a confounder by
common cause must precede both the exposure and the outcome. While it is
often clear that a confounder precedes the exposure (e.g., a person's
country of birth), the timing might be uncertain in other cases. We
assert its temporal precedence by positioning the confounder before the
exposure in our causal diagrams. However, such a timing assumption might
be strong when relying on cross-sectional data. Exploring causal
scenarios where the confounder follows the treatment or outcome can be
insightful in such cases. Causal diagrams are instrumental in examining
possible timings and their implications for causal inference.

Next, we examine the effects of conditioning on a variable that is an
effect of the treatment.

\subsubsection{2. Mediator Bias}\label{mediator-bias}

Table~\ref{tbl-04} Row 1 presents a problem of mediator bias. Consider
again whether proximity to green spaces, \(A\), affects happiness,
\(Y\). Suppose that physical activity is a mediator, \(L\).

To fill out the example, imagine that living close to green spaces \(A\)
influences physical activity \(L\), subsequently affecting happiness
\(Y\). If we were to condition on physical activity \(L\), assuming it
to be a confounder, we would then bias our estimates of the total effect
of proximity to green spaces \(A\) on happiness \(Y\). Such a bias
arises because of the chain rule. Conditioning on \(L\) ``d-separates''
the total effect of \(A\) on \(Y\). This phenomenon is known as mediator
bias. Notably, Montgomery \emph{et al.}
(\citeproc{ref-montgomery2018}{2018}) finds dozens of examples of
mediator bias in \emph{experiments} in which control is made for
variables that occur after the treatment. For example, obtaining
demographic and other information from participants \emph{after} a study
is an invitation to mediator bias. If the treatment affects these
variables, and the variables affect the outcome (as we assume by
controlling for them), then researchers may induce mediator bias.

To avoid mediator bias when estimating a total causal effect, we should
never condition on a mediator! The surest way to prevent this problem is
to ensure that \(L\) occurs before the treatment \(A\) and before the
outcome \(Y\). We present this solution in Table~\ref{tbl-04} Row 2 Col
3.

\subsubsection{3. Confounding by Collider Stratification (Conditioning
on a Common
Effect)}\label{confounding-by-collider-stratification-conditioning-on-a-common-effect}

Table~\ref{tbl-04} Row 1 presents a problem of collider bias.
Conditioning on a common effect, or collider stratification, occurs when
a variable, denoted by \(L\), is influenced by both the exposure,
denoted by \(A\), and the outcome, denoted by \(Y\).

Let us assume initial independence: the choice to live closer to green
spaces (exposure \(A\)) and happiness (outcome \(Y\)) are independent:
\(A \coprod Y(a)\).

We furthermore assume physical health \(L\) is an effect of green space
access, and happiness increases physical health. Thus, \(L\) is an
effect of \(A\) and \(Y\). If we were to condition on \(L\) in this
setting, we would introduce \emph{collider stratification bias}. When we
control for the common effect \(L\) (physical health), we may
inadvertently introduce confounding. This happens because knowing
something about \(L\) gives us information about both \(A\) and \(Y\).
If someone were high on physical health but low an access to greenspace,
this would imply that they are higher in happiness. Likewise, if someone
were low in physical health but high in access to green space, this
would imply lower happiness. As a result of our conditioning strategy,
it would appear that access to green space and happiness are negatively
associated. However, if we were to avoid conditioning on the common
outcome, we would find that the treatment and outcome are not
associated.

How can we avoid collider bias, the temporal sequence of measurement
affords a powerful strategy:

Ensure all common causes of \(A\) and \(Y\) -- call them \(L\) -- are
measured before the treatment \(A\) occurs. Ensure further that \(Y\)
occurs after \(A\) occurs. If the confounder \(L\) is not measured,
ensure that conditioning on its downstream proxy, \(L'\) does not induce
collider or mediator biases.

By adhering to this sequence, we can mitigate the risk of collider
stratification bias and better understand the causal relationships
between exposure, outcome, and their common effects.

\subsubsection{4. Confounding by Conditioning on a Descendant of a
Confounder}\label{confounding-by-conditioning-on-a-descendant-of-a-confounder}

Table~\ref{tbl-04} Row 4 presents a problem of collider bias by decent.
Recall the rules of d-separation also apply to conditioning on
descendants of a confounder. Thus, we may unwittingly evoke confounding
by proxy when conditioning on a measured descendant of an unmeasured
collider.

For example, if doctor visits were encoded in our data, and doctor
visits were an effect of poor health, conditioning on doctor visits
would function similarly to conditioning on poor health in the previous
example, introducing collider confounding.

\subsubsection{5. M-bias: Conditioning on Pre-Exposure
Collider}\label{m-bias-conditioning-on-pre-exposure-collider}

There are only five elementary structures of causality. Every
confounding scenario can be developed from these five elementary
structures. We next consider how we may combine these elementary causal
relationships in causal diagrams to create effective strategies for
confounding control.

Table~\ref{tbl-04} Row 5 presents a form of pre-exposure
over-conditioning confounding known as ``M-bias''. This bias combines
the collider structure and the fork structure, revealing what might not
otherwise be obvious: it is possible to induce confounding even if we
ensure that all variables have been measured \textbf{before} the
treatment. The collider structure is evident in the path \(U_Y \to L_0\)
and \(U_A \to L_0\). The collider rule shows that conditioning on
\(L_0\) opens a path between \(U_Y\) and \(U_A\). What is the result? We
find that \(U_Y\) is associated with the outcome \(Y\) and \(U_A\) is
associated with treatment \(A\). This is a fork (common cause)
structure. The association between treatment and outcome opened by
conditioning on \(L\) arises from an open back-door path that occurs
from the collider structure. We thus have confounding. How might such
confounding play out in a real-world setting?

In the context of green spaces, consider the scenario where an
individual's level of physical activity \(L\) is influenced by an
unmeasured factor related to their propensity to live near green spaces
\(A\) -- say childhood upbringing. Suppose further that another
unmeasured factor -- say a genetic factor -- increases both physical
activity \(L\) and happiness \(Y\). Here, physical activity \(L\) does
not affect the decision to live near green spaces \(A\) or happiness
\(Y\) but is a descendent of unmeasured variables that do. If we were to
condition on physical activity \(L\) in this scenario, we would create
the bias just described -- ``M-bias.''

How shall we respond to this problem? The solution is straightforward.
If \(L\) is neither a common cause of \(A\) and \(Y\) nor the effect of
a shared common cause, then \(L\) should not be included in a causal
model. In terms of the conditional exchangeability principle, we find
\(A \coprod Y(a)\) yet \(A \cancel{\coprod} Y(a)| L\). So we should not
condition on \(L\): do not control for exercise
(\citeproc{ref-cole2010}{Cole \emph{et al.} 2010}).\footnote{Note that
  when we draw a chronologically ordered path from left to right, the M
  shape for which ``M-bias'' takes its name changes to an E shape. We
  shall avoid proliferating jargon and retain the term ``M bias.''}

\subsubsection{6. Conditioning on a Descendent May Sometimes Reduce
Confounding}\label{conditioning-on-a-descendent-may-sometimes-reduce-confounding}

In Table~\ref{tbl-04} Row 6, we encounter a causal diagram in which an
unmeasured confounder opens a back-door path that links the treatment
and outcome. Here, we consider how we may use the rules of d-separation
to obtain unexpected strategies for confounding control.

Returning to our green space example, suppose an unmeasured genetic
factor \(U\) affects one's desire to seek out isolation in green spaces
\(A\) and independently affects one's happiness \(Y\). Were such an
unmeasured confounder to exist we could not obtain an unbiased estimate
for the causal effect of green space access on happiness. We have, it
seems, intractable confounding.

However, imagine a variable \(L^\prime\), a trait expressed later in
life that arises from this genetic factor. If such a trait could be
measured, even though the trait \(L'\) is expressed after the treatment
and outcome have occurred, controlling for \(L'\) would enable
investigators to close the backdoor path between the treatment and the
outcome. This strategy works because a measured effect is a \emph{proxy}
for its cause \(U\), the unmeasured confounder. By conditioning on the
late-adulthood trait, \(L'\), we partially condition on its cause,
\(U\), the confounder of \(A \to Y\). Thus, not all effective
confounding control strategies need to rely on measuring pre-exposure
variables. Thus, the elementary causal structures reveal a possibility
for confounding control by condition on a post-outcome variable. This
strategy is not intuitive. Although a common cause must occur before a
treatment (and outcome), its proxy need not! If we have a measure for
the latter but not the former, we should condition on the post-treatment
proxy of a pre-treatment common cause.

\subsubsection{7. Confounding Control with Three Waves of Data is
Powerful and Reveals Possibilities for Estimating an ``Incident
Exposure''
Effect}\label{confounding-control-with-three-waves-of-data-is-powerful-and-reveals-possibilities-for-estimating-an-incident-exposure-effect}

Table~\ref{tbl-04} row 7 presents another setting in which there is
unmeasured confounding. In response to this problem, we use the rules of
d-separation to develop a data collection and modelling strategy that
may greatly reduce the influence of unmeasured confounding.
Table~\ref{tbl-04} row 7 col 3, by collecting data for both the
treatment and the outcome at baseline and controlling for baseline
values of the treatment and outcome, any unmeasured association between
the treatment \(A_1\) and the outcome \(Y_2\) would need to be
\emph{independent} of their baseline measurements. As such, including
the baseline treatment and outcome, along with other measured covariates
that might be measured descendants of unmeasured confounders, is a
strategy that exerts considerable confounding control
(\citeproc{ref-vanderweele2020}{VanderWeele \emph{et al.} 2020}).

Furthermore, this causal graph makes evident a second benefit of this
strategy. Returning to our example, a model that controls for baseline
exposure would require that people initiate a change from the \(A_0\)
observed baseline level. Thus, by controlling for the baseline value of
the treatment, we may learn about the causal effect of shifting one's
access to green space status. This effect is called the ``incident
exposure effect.'' The incident exposure effect better emulates a
``target trial'' or the organisation of observational data into a
hypothetical experiment in which there is a ``time-zero'' initiation of
treatment in the data; see Hernán \emph{et al.}
(\citeproc{ref-hernuxe1n2016}{2016}); Danaei \emph{et al.}
(\citeproc{ref-danaei2012}{2012}); VanderWeele \emph{et al.}
(\citeproc{ref-vanderweele2020}{2020}); Bulbulia
(\citeproc{ref-bulbulia2022}{2022}). Without controlling for the
baseline treatment, we could only estimate a ``prevalent exposure
effect.'' If the initial exposure caused people some people to be
miserable, we would not be able to track this outcome. The prevalent
exposure effect would mask it, distorting causal inferences for the
quantity of interest, namely, what would happen, on average, if people
were to shift to having greater greenspace access.

Finally, we obtain further control for unmeasured confounding by
controlling for both the baseline treatment and the baseline outcome.
For an unmeasured confounder to affect both the treatment and the
outcome (and unmeasured fork structure), it would need to do so
independently of the baseline measures of the treatment and exposure
(\citeproc{ref-vanderweele2020}{VanderWeele \emph{et al.} 2020}).

Thus, we generally require repeated measures on the same unit over time
intervals to obtain an incident exposure effect and exert more robust
control for unmeasured confounding using past states of the treatment
and outcome. We must then model the treatments and outcomes as separate
elements in our statistical model.

\subsection{Part 4. Practical Guide For Constructing Causal Diagrams and
Reporting Results When Causal Structure is Unclear}\label{section-part4}

\subsubsection{Cross-sectional designs}\label{cross-sectional-designs}

In environmental psychology, researchers often grapple with whether
causal inferences can be drawn from cross-sectional data, especially
when longitudinal data are unavailable. The challenge is common to
cross-sectional designs. However, it is important to appreciate that
even longitudinal studies require careful assumption management. We next
discuss how causal diagrams can guide inference in both data types, with
examples relevant to environmental psychologists.

\paragraph{1. Graphically encode causal
assumptions}\label{graphically-encode-causal-assumptions}

Causal inference turns on assumptions. Although cross-sectional analyses
typically demand much stronger assumptions owing to the snapshot nature
of data, these assumptions, when transparently articulated, do not
permanently bar causal analysis. By stating different assumptions and
modelling the data following these assumptions, we might find that
certain causal conclusions are robust to these differences. Where the
implications of different assumptions disagree, we can better determine
the forms of data collection that would be required to settle such
differences. Below we consider an example where assumptions point to
different conclusions, revealing the benefits of collecting time-series
data to assess whether a variable is a confounder or a mediator.

\paragraph{2. Time-invariant
confounders}\label{time-invariant-confounders}

In cross-sectional studies, some confounders are inherently stable over
time, such as ethnicity, year and place of birth, and biological gender.
For environmental psychologists examining the relationship between
access to natural environments and psychological well-being, these
stable confounders can be adjusted for without concern for introducing
bias from mediators or colliders. For example, conditioning on one's
year of birth can help isolate recent urban development's effect on
mental health, independent of generational differences in attitudes
toward green spaces.

\paragraph{3. Stable confounders}\label{stable-confounders}

While not immutable, other confounders are less likely to be influenced
by the treatment. Variables such as sexual orientation, educational
attainment, and often income level fall into this category. For
instance, the effect of exposure to polluted environments on cognitive
outcomes can be analysed by conditioning on education level, assuming
that recent exposure to pollution is unlikely to change someone's
educational history retroactively.

\paragraph{4. Timing and reverse
causation}\label{timing-and-reverse-causation}

The sequence of treatment and outcome is crucial. Sometimes, the
temporal order is clear, reducing concerns about reverse causation.
Mortality is a definitive outcome where the timing issue is unambiguous.
If researching the effects of air quality on mortality, the causal
direction (poor air quality leading to higher mortality rates) is
straightforward. However, consider the relationship between
socio-economic status and health outcomes; the direction of causality is
complex because socioeconomic factors can influence health (through
access to resources), and poor health can affect socio-economic status
(through reduced earning capacity).

\paragraph{5. Create causal diagrams}\label{create-causal-diagrams}

Given the complexity of environmental influences on psychological
outcomes, it's prudent to construct multiple causal diagrams to cover
various hypothetical scenarios. For example, when studying the effect of
community green space on stress reduction, one diagram might assume the
direct benefits of green space on stress. At the same time, another
might include potential mediators like physical activity. By analysing
and reporting findings based on multiple diagrams, researchers can
examine the robustness of their conclusions across different theoretical
frameworks and sets of assumptions.

Table~\ref{tbl-cs} describes ambiguous confounding control arising from
cross-sectional data. Suppose again we are interested in the causal
effect of access to greenspace, denoted by \(A\) on ``happiness,''
denoted by \(Y\). We are uncertain whether exercise, denoted by \(L\),
is a common cause of \(A\) and \(Y\) and thus a confounder or whether
exercise is a mediator along the path from \(A\) to \(Y\). That is: (1)
those who exercise might seek access to green space, and (2) exercise
might increase happiness. Alternatively, the availability of green space
might encourage physical activity, which could subsequently affect
happiness. Causal diagrams can disentangle these relationships by
explicitly representing potential paths, thereby guiding appropriate
strategies for confounding control selection. We recommend using
multiple causal diagrams to investigate the consequences of different
plausible structural assumptions.

\textbf{Assumption 1: Exercise is a common cause of \(A\) and \(Y\)},
this scenario is presented in Table~\ref{tbl-cs} row 1. Here, our
strategy for confounding control is to estimate the effect of \(A\) on
\(Y\) conditioning on \(L\).

\textbf{Assumption 2: Exercise is a mediator of \(A\) and \(Y\)}, this
scenario is presented in Table~\ref{tbl-cs} row 2. Here, our strategy
for confounding control is simply estimating the effect of \(A\) on
\(Y\) without including \(L\) (assuming there are no other common causes
of the treatment and outcome).

\begin{table}

\caption{\label{tbl-cs}This table is adapted from
(\citeproc{ref-bulbulia2023}{Bulbulia 2023})}

\centering{

\examplecrosssection

}

\end{table}%

We can simulate data and run separate regressions to clarify how answers
may differ, reflecting the different conditioning strategies embedded in
the different assumptions. The following simulation generates data from
a process in which exercise is a mediator (Scenario 2). (See Appendix C)

\begin{table}
\caption{Code for a simulation of a data generating process in which the effect
of exercise (L) fully mediates the effect of greenspace (A) on happiness
(Y).}\tabularnewline

\centering
\begin{tabular}{lcccccc}
\toprule
\multicolumn{1}{c}{ } & \multicolumn{3}{c}{Model: Exercise assumed confounder} & \multicolumn{3}{c}{Model: Exercise assumed to be a mediator} \\
\cmidrule(l{3pt}r{3pt}){2-4} \cmidrule(l{3pt}r{3pt}){5-7}
\textbf{Characteristic} & \textbf{Beta} & \textbf{95\% CI} & \textbf{p-value} & \textbf{Beta} & \textbf{95\% CI} & \textbf{p-value}\\
\midrule
A & -0.27 & -0.53, -0.01 & 0.043 & 2.9 & 2.6, 3.2 & <0.001\\
L & 1.6 & 1.5, 1.7 & <0.001 &  &  & \\
\bottomrule
\multicolumn{7}{l}{\rule{0pt}{1em}\textsuperscript{1} CI = Confidence Interval}\\
\end{tabular}
\end{table}

This table presents the conditional treatment effect estimates. We
present code for obtaining marginal treatment effects in
\hyperref[appendix-c]{Appendix C}

On the assumptions outlined in Table~\ref{tbl-cs} row 1, in which we
\emph{assert} that exercise is a confounder, the average treatment
effect of access to green space on happiness is ATE = 2.92, CI =
{[}2.66, 3.21{]}.

On the assumptions outlined in Table~\ref{tbl-cs} row 2, in which we
\emph{assert} that exercise is a mediator, the average treatment effect
of access to green space on happiness is ATE = -0.27, CI = {[}-0.52,
-0.01{]}.

Note that although the mediator \(L\) is ``highly statistically
significant'', including it in the model is a mistake. We obtain a
negative effect estimate for the causal effect of green space access on
happiness.

With only cross-sectional data, we must infer the results are
inconclusive. Such understanding, although not the definitive answer we
sought, is progress. The result tells us we should not be overly
confident with our analysis (whatever p-values we recover!), and it
clarifies that longitudinal data are needed.

These findings illustrate the role that assumptions about the relative
timing of exercise as a confounder or as a mediator play.

\subsubsection{Recommendations for Conducting and Reporting Causal
Analyses with Cross-Sectional
Data}\label{recommendations-for-conducting-and-reporting-causal-analyses-with-cross-sectional-data}

When analysing and reporting analyses with cross-sectional data,
researchers face the challenge of making causal inferences without the
benefit of temporal information.

The following recommendations aim to guide researchers in navigating
these challenges effectively:

\textbf{Warning}: before proceeding with cross-sectional analysis,
examine whether panel data are available. Longitudinal data can provide
crucial temporal information that aids in establishing causality,
offering a more robust framework for causal inference. If longitudinal
data are unavailable, the recommendations above become even more
critical for using cross-sectional data best.

\paragraph{\texorpdfstring{1. \textbf{Draw multiple causal
diagrams}}{1. Draw multiple causal diagrams}}\label{draw-multiple-causal-diagrams}

Draw various causal diagrams to represent different theoretical
assumptions about the relationships and timing of variables relevant to
an identification problem. This approach comprehensively examines
possible causal pathways, clarifying variables' roles as confounders,
mediators, or colliders. For example, in studying the effect of urban
green spaces on mental health, consider diagrams that account for both
direct effects and pathways involving mediators like physical activity
or social interaction.

\paragraph{\texorpdfstring{2. \textbf{Perform and report analyses for
each
assumption}}{2. Perform and report analyses for each assumption}}\label{perform-and-report-analyses-for-each-assumption}

Conduct and transparently report separate analyses for each scenario
your causal diagrams depict. This practice ensures that your study is
theoretically grounded for each model. Presenting results from each
analytical approach and the underlying assumptions and statistical
methods promotes a balanced interpretation of findings. Although this
practice may be unfamiliar to some editors and reviewers, it is crucial
to address the inherent challenges of cross-sectional analysis by
expanding the scope of investigation beyond a single hypothesis.

\paragraph{\texorpdfstring{3. \textbf{Interpret findings with attention
to
ambiguities}}{3. Interpret findings with attention to ambiguities}}\label{interpret-findings-with-attention-to-ambiguities}

Interpret results carefully, highlighting any ambiguities or
inconsistencies across analyses. Discuss how varying assumptions about
structural relationships and the timing of events can lead to divergent
conclusions. For instance, exploring the theoretical and empirical
implications of access to green spaces appears to positively affect
mental health when considering exercise as a mediator but a negative
effect when considered a confounder.

\paragraph{\texorpdfstring{4. \textbf{Report divergent
findings}}{4. Report divergent findings}}\label{report-divergent-findings}

Approach conclusions with caution, especially when findings suggest
differing practical implications. Acknowledge the limitations of
cross-sectional data in establishing causality and the potential for
alternative explanations.

\paragraph{\texorpdfstring{5. \textbf{Identify avenues for future
research}}{5. Identify avenues for future research}}\label{identify-avenues-for-future-research}

Target future research that could clarify ambiguities. Consider the
design of longitudinal studies or experiments capable of clarifying
these ambiguities.

\paragraph{\texorpdfstring{6. \textbf{Supplement observational data with
simulated
data}}{6. Supplement observational data with simulated data}}\label{supplement-observational-data-with-simulated-data}

Leverage data simulation to understand the complexities of causal
inference. Simulating data based on various theoretical models allows
researchers to examine the effect of different assumptions on their
findings. This method tests analytical strategies under controlled
conditions, assessing the robustness of conclusions against assumption
violations or unobserved confounders.

\paragraph{\texorpdfstring{7. \textbf{Conduct sensitivity analyses to
assess
robustness}}{7. Conduct sensitivity analyses to assess robustness}}\label{conduct-sensitivity-analyses-to-assess-robustness}

implement sensitivity analyses to determine how dependent conclusions
are on specific assumptions or parameters within your causal model. Use
data simulation as a tool for these analyses, evaluating the sensitivity
of results to various theoretical and methodological choices.

Cross-sectional data are limiting; however, by appropriately bounding
uncertainties in your causal inferences, you may use them to advance
understanding. May your clarity and caution serve as an example for
others.

\subsubsection{Longitudinal Designs}\label{longitudinal-designs}

Causation occurs in time. Longitudinal designs offer a substantial
advantage over cross-sectional designs for causal inference because
sequential measurements allow us to capture causation and quantify its
magnitude. We typically do not need to assert timing as in
cross-sectional data settings. Because we know when variables have been
measured, we can reduce ambiguity about the directionality of causal
relationships. For instance, tracking changes in ``happiness'' following
changes in access to green spaces over time can more definitively
suggest causation than cross-sectional snapshots.

Despite this advantage, longitudinal researchers still face assumptions
regarding the absence of unmeasured confounders or the stability of
measured confounders over time. These assumptions must be explicitly
stated. As with cross-sectional designs, wherever assumptions differ,
researchers should draw different causal diagrams that reflect these
assumptions and subsequently conduct and report separate analyses.

In this section, we simulate a dataset to demonstrate the benefits of
incorporating both baseline exposure and baseline outcomes into
analysing the effect of access to open green spaces on happiness. This
approach allows us to control for initial levels of exposure and
outcomes, offering a clearer understanding of the causal relationship.
\hyperref[appendix-d-simulation-of-different-confounding-control-strategies]{Appendix
D} provides the code.
\hyperref[appendix-e-non-parametric-estimation-of-average-treatment-effects-using-causal-forests]{Appendix
E} provides an example of a non-parametric estimator for the causal
effect. As mentioned before, by conditioning on baseline levels of
access to green spaces and baseline mental health, researchers can more
accurately estimate the \emph{incident effect} of changes in green space
access on changes in mental health. Table~\ref{tbl-lg} offers an example
of how we may use multiple causal diagrams to clarify the problem and
our confounding control strategy.

\begin{table}

\caption{\label{tbl-lg}This table is adapted from
(\citeproc{ref-bulbulia2023}{Bulbulia 2023})}

\centering{

\examplelongitudinal

}

\end{table}%

Our analysis assessed the average treatment effect (ATE) of access to
green spaces on happiness across three distinct models: uncontrolled,
standard controlled, and interaction controlled. These models were
constructed using a hypothetical cohort of 10,000 individuals,
incorporating baseline exposure to green spaces (\(A_0\)), baseline
happiness (\(Y_0\)), baseline confounders (\(L_0\)), and an unmeasured
confounder (\(U\)). The detailed simulation process and model
construction are given in
\hyperref[appendix-simulate-longitudinal-ate]{Appendix D}.

The ATE estimates from these models provide critical insights into the
effects of green space exposure on individual happiness while accounting
for various confounding factors. The model without control variables
estimated ATE = 1.55, CI = {[}1.47, 1.63{]}, significantly
overestimating the treatment effect. Incorporating standard covariate
control reduced this estimate to ATE = 0.86, CI = {[}0.8, 0.92{]},
aligning more closely with the expected effect but still overestimating.
Most notably, the model that included interactions among baseline
exposure, outcome, and confounders yielded ATE = 0.29, CI = {[}0.27,
0.31{]}, approximating the true effect of 0.3. This finding underscores
the importance of including baseline values of the exposure and outcome
wherever these data are available.

\subsubsection{Recommendations for Conducting and Reporting Causal
Analyses with Longitudinal
Data}\label{recommendations-for-conducting-and-reporting-causal-analyses-with-longitudinal-data}

Longitudinal data offer strong advantages for causal inference by
enabling researchers to establish the relative timing of confounders,
treatments, and outcomes. The temporal sequence of events is crucial for
establishing causality because causality occurs in time. The following
recommendations aim to guide researchers in leveraging longitudinal data
effectively to conduct and report causal analyses:

\paragraph{1. Draw multiple causal
diagrams}\label{draw-multiple-causal-diagrams-1}

\begin{itemize}
\tightlist
\item
  \textbf{Identification problem diagram}: begin by constructing a
  causal diagram that outlines your initial assumptions about the
  relationships among variables, identifying potential confounders and
  mediators. This diagram should illustrate the complexity of the
  identification problem.
\item
  \textbf{Solution diagram}: next, create a separate causal diagram that
  proposes solutions to the identified problems. This may involve
  highlighting variables for conditioning to isolate the causal effect
  of interest or suggesting novel pathways for investigation. Having
  distinct diagrams for the problem and its proposed solutions clarifies
  your study's analytic strategy and theoretical underpinning.
\end{itemize}

Table~\ref{tbl-lg} provides an example of a table with multiple causal
diagrams clarifying potential sources of confounding threats and reports
strategies for addressing them.

\paragraph{2. Attempt longitudinal designs with at least three waves of
data}\label{attempt-longitudinal-designs-with-at-least-three-waves-of-data}

Incorporating data from at least three intervals considerably enhances
your ability to infer causal relationships. This approach allows for the
examination of temporal precedence and lagged effects. For example, by
adjusting for physical activity measured before the treatment, we can
ensure that physical activity does not result from a new initiation to
green spaces, which we establish by measuring green space access at
baseline. Establishing chronological order in the temporal sequence of
events allows us to avoid confounding problems 1-4 in
Table~\ref{tbl-04}.

\paragraph{3. Calculate Average Treatment Effects for a clearly
specified target
population}\label{calculate-average-treatment-effects-for-a-clearly-specified-target-population}

Estimating the average treatment effect (ATE) across the entire study
population provides a comprehensive measure of the intervention's
effects. This step is crucial for understanding the treatment's overall
effect and generalising findings to broader populations.

\paragraph{4. Where causality is unclear, report results for multiple
causal
graphs}\label{where-causality-is-unclear-report-results-for-multiple-causal-graphs}

Given that the true causal structure may be complex and partially
unknown, analysing and reporting results under each plausible causal
diagram is prudent. This practice acknowledges the uncertainty inherent
in causal modelling and demonstrates the robustness of findings across
different theoretical frameworks.

\paragraph{5. Conduct sensitivity
analyses}\label{conduct-sensitivity-analyses}

Sensitivity analyses are essential for assessing the robustness of your
findings to various assumptions within the causal model. These analyses
can include simulations, as illustrated in Appendices C and D, to
examine bias arising of unmeasured confounding, model misspecification,
and alternative causal pathways on the study conclusions. Sensitivity
analyses help to identify the conditions under which the findings hold,
enhancing the credibility of the causal inferences. (For more about
addressing missing data, see:
(\citeproc{ref-bulbulia2024PRACTICAL}{Bulbulia 2024}).)

\paragraph{6. Address missing data at baseline and study
attrition}\label{address-missing-data-at-baseline-and-study-attrition}

Longitudinal studies often need help with missing data and attrition,
which can introduce bias and affect the validity of causal inferences.
Implement and report strategies for handling missing data, such as
multiple imputation or sensitivity analyses that assess the bias arising
from missing responses at the study's conclusion. (For more about
addressing missing data, see:
(\citeproc{ref-bulbulia2024PRACTICAL}{Bulbulia 2024})).

By following these recommendations, you will more effectively navigate
the inherent limitations of observational longitudinal data, improving
the quality of your causal inferences.

\subsection{Summary}\label{summary}

This chapter has introduced the potential outcomes framework for causal
inference and using directed acyclic graphs (DAGs) in environmental
psychology.

In \hyperref[section-part1]{\textbf{Part 1}} we discussed three critical
assumptions necessary for estimating average treatment effects from
data:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Conditional Exchangeability}: This assumption posits that
  treatment allocation is randomised and independent of potential
  outcomes, conditional on measured covariates.
\item
  \textbf{Causal Consistency}: This assumption asserts that the outcome
  observed under the treatment condition corresponds to the outcome that
  would have been observed had the unit received the treatment, and
  similarly for the control condition.
\item
  \textbf{Positivity}: This assumption asserts that every unit has a
  non-zero probability of receiving any treatments under comparison.
\end{enumerate}

Although randomised controlled experiments naturally satisfy these
assumptions through design---randomisation ensures exchangeability,
control guarantees consistency, and design secures positivity -\/---
observational studies typically do not. To obtain consistent causal
estimates from observational data, we must assess the extent to which
these assumptions can be satisfied.

In \hyperref[section-part2]{\textbf{Part 2}}, we explained how causal
diagrams work and described their utility in addressing the assumption
of conditional exchangeability, or the ``no unmeasured confounders''
assumption. We identified \hyperref[sec-five-elementary]{five
fundamental structures} underlying all causal relationships. We
discovered \hyperref[sec-four-rules]{four elementary rules} for
evaluating the implications of conditioning on elements within these
structures regarding observable statistical associations in data. Thus,
causal diagrams provide a simplified visual language for translating
complex causal relationships into data observations. However, the
relationships in these diagrams represent assertions that are not
directly verifiable from the data. The causal relationships between
treatments and outcomes are the only relationships not based on
assertion. Causal diagrams help us identify structural sources of bias
in the statistical associations between treatments and outcomes that may
arise from assumed causal relationships, potentially associating
treatments with outcomes irrespective of causal links.

In \hyperref[section-part3]{\textbf{Part 3}}, we applied causal diagrams
to seven common confounding scenarios, demonstrating that a causal
diagram needs to highlight only those aspects of a causal setting
relevant for assessing structural sources of bias linking treatment and
outcome in a non-causal manner. We focused on omitting nodes and paths
not directly necessary for our stated identification problem,
emphasising that causal diagrams are tailored to context-dependent
questions and our assumptions about the world's causal structure.

In \hyperref[section-part4]{\textbf{Part 4}}, we showed how
investigators might create multiple causal diagrams when the structure
of a causal problem is ambiguous and illustrated the benefits of this
approach through data simulation. We provided guidelines for reporting
in scenarios where only cross-sectional data are available or when
researchers have access to repeated measures of longitudinal data.

Although this discussion has focused on seven specific applications of
causal diagrams, their applicability extends much further. The
straightforward rules governing how variables become associated or
disassociated through conditioning on nodes within basic structures
enable the use of causal diagrams for quantitatively exploring causality
in myriad questions. These applications transcend effective modelling
strategies and informing data collection strategies, including adopting
repeated measures designs.

We hope this chapter will inspire environmental psychologists to deepen
their understanding of causal inference and incorporate causal diagrams
into their research practices. The methodologies for distinguishing
causation from correlation are well-established; powerful tools for
causal inference are accessible. There is no longer any justification
for reporting associations and speculating about causes. It is within
your reach to quantify magnitudes of causality conditional on
assumptions encoded in your causal graphs.

\newpage{}

\subsection{Funding}\label{funding}

This work is supported by a grant from the Templeton Religion Trust
(TRT0418). JB received support from the Max Planck Institute for the
Science of Human History. The funders had no role in preparing the
manuscript or deciding to publish it.

\subsection{Contributions}\label{contributions}

DH proposed the chapter. JB developed the approach and wrote the first
draft. Both authors contributed substantially to the final work.

\subsection{References}\label{references}

\phantomsection\label{refs}
\begin{CSLReferences}{1}{0}
\bibitem[\citeproctext]{ref-bulbulia2024PRACTICAL}
Bulbulia, J (2024) A practical guide to causal inference in three-wave
panel studies. \emph{PsyArXiv Preprints}.
doi:\href{https://doi.org/10.31234/osf.io/uyg3d}{10.31234/osf.io/uyg3d}.

\bibitem[\citeproctext]{ref-bulbulia2022}
Bulbulia, JA (2022) A workflow for causal inference in cross-cultural
psychology. \emph{Religion, Brain \& Behavior}, \textbf{0}(0), 1--16.
doi:\href{https://doi.org/10.1080/2153599X.2022.2070245}{10.1080/2153599X.2022.2070245}.

\bibitem[\citeproctext]{ref-bulbulia2023}
Bulbulia, JA (2023) Causal diagrams (directed acyclic graphs): A
practical guide.

\bibitem[\citeproctext]{ref-bulbulia2023a}
Bulbulia, JA, Afzali, MU, Yogeeswaran, K, and Sibley, CG (2023)
Long-term causal effects of far-right terrorism in {N}ew {Z}ealand.
\emph{PNAS Nexus}, \textbf{2}(8), pgad242.

\bibitem[\citeproctext]{ref-cinelli2022}
Cinelli, C, Forney, A, and Pearl, J (2022) A Crash Course in Good and
Bad Controls. \emph{Sociological Methods \&Research}, 00491241221099552.
doi:\href{https://doi.org/10.1177/00491241221099552}{10.1177/00491241221099552}.

\bibitem[\citeproctext]{ref-cole2010}
Cole, SR, Platt, RW, Schisterman, EF, \ldots{} Poole, C (2010)
Illustrating bias due to conditioning on a collider. \emph{International
Journal of Epidemiology}, \textbf{39}(2), 417--420.
doi:\href{https://doi.org/10.1093/ije/dyp334}{10.1093/ije/dyp334}.

\bibitem[\citeproctext]{ref-danaei2012}
Danaei, G, Tavakkoli, M, and Hernán, MA (2012) Bias in observational
studies of prevalent users: lessons for comparative effectiveness
research from a meta-analysis of statins. \emph{American Journal of
Epidemiology}, \textbf{175}(4), 250--262.
doi:\href{https://doi.org/10.1093/aje/kwr301}{10.1093/aje/kwr301}.

\bibitem[\citeproctext]{ref-edwards2015}
Edwards, JK, Cole, SR, and Westreich, D (2015) All your data are always
missing: Incorporating bias due to measurement error into the potential
outcomes framework. \emph{International Journal of Epidemiology},
\textbf{44}(4), 1452--1459.

\bibitem[\citeproctext]{ref-greenland1999}
Greenland, S, Pearl, J, and Robins, JM (1999) Causal diagrams for
epidemiologic research. \emph{Epidemiology (Cambridge, Mass.)},
\textbf{10}(1), 37--48.

\bibitem[\citeproctext]{ref-greifer2023}
Greifer, N, Worthington, S, Iacus, S, and King, G (2023) \emph{Clarify:
Simulation-based inference for regression models}. Retrieved from
\url{https://iqss.github.io/clarify/}

\bibitem[\citeproctext]{ref-hernan2023}
Hernan, MA, and Robins, JM (2023) \emph{Causal inference}, Taylor \&
Francis. Retrieved from
\url{https://books.google.co.nz/books?id=/_KnHIAAACAAJ}

\bibitem[\citeproctext]{ref-hernuxe1n2023}
Hernán, MA, and Monge, S (2023) Selection bias due to conditioning on a
collider. \emph{BMJ}, \textbf{381}, p1135.
doi:\href{https://doi.org/10.1136/bmj.p1135}{10.1136/bmj.p1135}.

\bibitem[\citeproctext]{ref-hernan2017per}
Hernán, MA, Robins, JM, et al. (2017) Per-protocol analyses of pragmatic
trials. \emph{N Engl J Med}, \textbf{377}(14), 1391--1398.

\bibitem[\citeproctext]{ref-hernuxe1n2016}
Hernán, MA, Sauer, BC, Hernández-Díaz, S, Platt, R, and Shrier, I (2016)
Specifying a target trial prevents immortal time bias and other
self-inflicted injuries in observational analyses. \emph{Journal of
Clinical Epidemiology}, \textbf{79}, 70--75.

\bibitem[\citeproctext]{ref-hester2022GLUE}
Hester, J, and Bryan, J (2022) \emph{Glue: Interpreted string literals}.
Retrieved from \url{https://CRAN.R-project.org/package=glue}

\bibitem[\citeproctext]{ref-holland1986}
Holland, PW (1986) Statistics and causal inference. \emph{Journal of the
American Statistical Association}, \textbf{81}(396), 945--960.

\bibitem[\citeproctext]{ref-hume1902}
Hume, D (1902) \emph{Enquiries Concerning the Human Understanding: And
Concerning the Principles of Morals}, Clarendon Press.

\bibitem[\citeproctext]{ref-lash2020}
Lash, TL, Rothman, KJ, VanderWeele, TJ, and Haneuse, S (2020)
\emph{Modern epidemiology}, Wolters Kluwer. Retrieved from
\url{https://books.google.co.nz/books?id=SiTSnQEACAAJ}

\bibitem[\citeproctext]{ref-mancuso2018revolutionary}
Mancuso, S (2018) \emph{The revolutionary genius of plants: A new
understanding of plant intelligence and behavior}, Simon; Schuster.

\bibitem[\citeproctext]{ref-mcelreath2020}
McElreath, R (2020) \emph{Statistical rethinking: A {B}ayesian course
with examples in r and stan}, CRC press.

\bibitem[\citeproctext]{ref-montgomery2018}
Montgomery, JM, Nyhan, B, and Torres, M (2018) How conditioning on
posttreatment variables can ruin your experiment and what to do about
It. \emph{American Journal of Political Science}, \textbf{62}(3),
760--775.
doi:\href{https://doi.org/10.1111/ajps.12357}{10.1111/ajps.12357}.

\bibitem[\citeproctext]{ref-pearl1995}
Pearl, J (1995) Causal diagrams for empirical research.
\emph{Biometrika}, \textbf{82}(4), 669--688.

\bibitem[\citeproctext]{ref-pearl2009}
Pearl, J (2009a) \emph{\href{https://doi.org/10.1214/09-SS057}{Causal
inference in statistics: An overview}}.

\bibitem[\citeproctext]{ref-pearl2009a}
Pearl, J (2009b) \emph{Causality}, Cambridge University Press.

\bibitem[\citeproctext]{ref-pearl2018}
Pearl, J, and Mackenzie, D (2018) \emph{The book of why: The new science
of cause and effect}, Basic books.

\bibitem[\citeproctext]{ref-robins1986}
Robins, J (1986) A new approach to causal inference in mortality studies
with a sustained exposure period---application to control of the healthy
worker survivor effect. \emph{Mathematical Modelling}, \textbf{7}(9-12),
1393--1512.

\bibitem[\citeproctext]{ref-robins2008estimation}
Robins, J, and Hernan, M (2008) Estimation of the causal effects of
time-varying exposures. \emph{Chapman \& Hall/CRC Handbooks of Modern
Statistical Methods}, 553--599.

\bibitem[\citeproctext]{ref-rubin1976}
Rubin, DB (1976) Inference and missing data. \emph{Biometrika},
\textbf{63}(3), 581--592.
doi:\href{https://doi.org/10.1093/biomet/63.3.581}{10.1093/biomet/63.3.581}.

\bibitem[\citeproctext]{ref-rubin2005}
Rubin, DB (2005) Causal inference using potential outcomes: Design,
modeling, decisions. \emph{Journal of the American Statistical
Association}, \textbf{100}(469), 322--331. Retrieved from
\url{https://www.jstor.org/stable/27590541}

\bibitem[\citeproctext]{ref-gtsummary2021}
Sjoberg, DD, Whiting, K, Curry, M, Lavery, JA, and Larmarange, J (2021)
Reproducible summary tables with the gtsummary package. \emph{{The R
Journal}}, \textbf{13}, 570--580.
doi:\href{https://doi.org/10.32614/RJ-2021-053}{10.32614/RJ-2021-053}.

\bibitem[\citeproctext]{ref-neyman1923}
Splawa-Neyman, J, Dabrowska, DM, and Speed, TP (1990) On the application
of probability theory to agricultural experiments. Essay on principles.
Section 9. \emph{Statistical Science}, 465--472.

\bibitem[\citeproctext]{ref-grf2024}
Tibshirani, J, Athey, S, Sverdrup, E, and Wager, S (2024) \emph{Grf:
Generalized random forests}. Retrieved from
\url{https://github.com/grf-labs/grf}

\bibitem[\citeproctext]{ref-vanderweele2009}
VanderWeele, TJ (2009) Concerning the consistency assumption in causal
inference. \emph{Epidemiology}, \textbf{20}(6), 880.
doi:\href{https://doi.org/10.1097/EDE.0b013e3181bd5638}{10.1097/EDE.0b013e3181bd5638}.

\bibitem[\citeproctext]{ref-vanderweele2015}
VanderWeele, TJ (2015) \emph{Explanation in causal inference: Methods
for mediation and interaction}, Oxford University Press.

\bibitem[\citeproctext]{ref-vanderweele2018}
VanderWeele, TJ (2018) On well-defined hypothetical interventions in the
potential outcomes framework. \emph{Epidemiology}, \textbf{29}(4), e24.
doi:\href{https://doi.org/10.1097/EDE.0000000000000823}{10.1097/EDE.0000000000000823}.

\bibitem[\citeproctext]{ref-vanderweele2019}
VanderWeele, TJ (2019) Principles of confounder selection.
\emph{European Journal of Epidemiology}, \textbf{34}(3), 211--219.

\bibitem[\citeproctext]{ref-vanderweele2013}
VanderWeele, TJ, and Hernan, MA (2013) Causal inference under multiple
versions of treatment. \emph{Journal of Causal Inference},
\textbf{1}(1), 1--20.

\bibitem[\citeproctext]{ref-vanderweele2020}
VanderWeele, TJ, Mathur, MB, and Chen, Y (2020) Outcome-wide
longitudinal designs for causal inference: A new template for empirical
studies. \emph{Statistical Science}, \textbf{35}(3), 437--466.

\bibitem[\citeproctext]{ref-vanderweele2022b}
VanderWeele, TJ, and Vansteelandt, S (2022) A statistical test to reject
the structural interpretation of a latent factor model. \emph{Journal of
the Royal Statistical Society Series B: Statistical Methodology},
\textbf{84}(5), 2032--2054.

\bibitem[\citeproctext]{ref-westreich2012berkson}
Westreich, D (2012) Berkson's bias, selection bias, and missing data.
\emph{Epidemiology (Cambridge, Mass.)}, \textbf{23}(1), 159.

\bibitem[\citeproctext]{ref-westreich2010}
Westreich, D, and Cole, SR (2010) Invited commentary: positivity in
practice. \emph{American Journal of Epidemiology}, \textbf{171}(6).
doi:\href{https://doi.org/10.1093/aje/kwp436}{10.1093/aje/kwp436}.

\bibitem[\citeproctext]{ref-westreich2015}
Westreich, D, Edwards, JK, Cole, SR, Platt, RW, Mumford, SL, and
Schisterman, EF (2015) Imputation approaches for potential outcomes in
causal inference. \emph{International Journal of Epidemiology},
\textbf{44}(5), 1731--1737.

\bibitem[\citeproctext]{ref-westreich2013}
Westreich, D, and Greenland, S (2013) The table 2 fallacy: Presenting
and interpreting confounder and modifier coefficients. \emph{American
Journal of Epidemiology}, \textbf{177}(4), 292--298.

\bibitem[\citeproctext]{ref-zhu2021KableExtra}
Zhu, H (2021) \emph{kableExtra: Construct complex table with 'kable' and
pipe syntax}. Retrieved from
\url{https://CRAN.R-project.org/package=kableExtra}

\end{CSLReferences}

\newpage{}

\subsection{Appendix A: Glossary}\label{appendix-a}

This appendix provides a glossary of common terminology in causal
inference.

\textbf{Acyclic}: a causal diagram cannot contain feedback loops. More
precisely, no variable can be an ancestor or descendant of itself. If
variables are repeatedly measured here, it is vital to index nodes by
the relative timing of the nodes.

\textbf{Adjustment set}: a collection of variables we must either
condition upon or deliberately avoid conditioning upon to obtain a
consistent causal estimate for the effect of interest
(\citeproc{ref-pearl2009}{Pearl 2009a}).

\textbf{Ancestor (parent)}: a node with a direct or indirect influence
on others, positioned upstream in the causal chain.

\textbf{Arrow}: denotes a causal relationship linking nodes.

\textbf{Backdoor path}: a ``backdoor path'' between a treatment
variable, \(A\), and an outcome variable, \(Y\), is a sequence of links
in a causal diagram that starts with an arrow into \(A\) and reaches
\(Y\) through common causes, introducing potential confounding bias such
that statistical association does not reflect causality. To estimate the
causal effect of \(A\) on \(Y\) without bias, these paths must be
blocked by adjusting for confounders. The backdoor criterion guides the
selection of variables for adjustment to ensure unbiased causal
inference.

\textbf{Conditioning}: explicitly accounting for a variable in our
statistical analysis to address the identification problem. In causal
diagrams, we usually represent conditioning by drawing a box around a
node of the conditioned variable, for example,
\(\boxed{L_{0}}\to A_{1} \to L_{2}\). We do not box exposures and
outcomes because we assume they are included in a model by default.
Depending on the setting, we may condition by regression stratification,
inverse probability of treatment weighting, g-methods, doubly robust
machine learning algorithms, or other methods. We do not cover such
methods in this tutorial; however, see Hernan and Robins
(\citeproc{ref-hernan2023}{2023}).

\textbf{Counterfactual}: a hypothetical outcome that would have occurred
for the same individuals under a different treatment condition than the
one they experienced.

\textbf{Direct effect}: the portion of the total effect of a treatment
on an outcome that is not mediated by other variables within the causal
pathway.

\textbf{Collider}: a variable in a causal diagram at which two incoming
paths meet head-to-head. For example, if
\(A \rightarrowred \boxed{L} \leftarrowred Y\), then \(L\) is a
collider. If we do not condition on a collider (or its descendants), the
path between \(A\) and \(Y\) remains closed. Conditioning on a collider
(or its descendants) will induce an association between \(A\) and \(Y\).

\textbf{Confounder}: a member of an adjustment set. Notice a variable is
a ``confounder'' in relation to a specific adjustment set.
``Confounder'' is a relative concept (\citeproc{ref-lash2020}{Lash
\emph{et al.} 2020}).

\textbf{d-separation}: in a causal diagram, a path is ``blocked'' or
``d-separated'' if a node along it interrupts causation. Two variables
are d-separated if all paths connecting them are blocked, making them
conditionally independent. Conversely, unblocked paths result in
``d-connected'' variables, implying potential dependence
(\citeproc{ref-pearl1995}{Pearl 1995}).

\textbf{Descendant (child)}: a node directly or indirectly influenced by
upstream nodes (parents).

\textbf{Effect-modifier}: a variable is an effect-modifier, or
``effect-measure modifie'' if its presence changes the magnitude or
direction of the effect of an exposure or treatment on an outcome across
the levels or values of this variable. In other words, the effect of the
exposure is different at different levels of the effect modifier.

\textbf{External validity}: the extent to which causal inferences can be
generalised to other populations, settings, or times, also called
``Target Validity.''

\textbf{Identification problem}: the challenge of estimating the causal
effect of a variable by adjusting for measured variables on units in a
study. Causal diagrams were developed to address the identification
problem by application of the rules of d-separation to a causal diagram.

\textbf{Indirect effect (mediated effect)}: The portion of the total
effect transmitted through a mediator variable.

\textbf{Internal validity}: the degree to which the design and conduct
have prevented bias, ensuring that the causal relationship observed can
be confidently attributed to the treatment and not to other factors.

\textbf{Instrumental variable}: an ancestor of the exposure but not of
the outcome. An instrumental variable affects the outcome only through
its effect on the exposure and not otherwise. Whereas conditioning on a
variable causally associated with the outcome rather than with the
exposure will generally increase modelling precision, we should refrain
from conditioning on instrumental variables
(\citeproc{ref-cinelli2022}{Cinelli \emph{et al.} 2022}). Second, when
an instrumental variable is the descendant of an unmeasured confounder,
we should generally condition the instrumental variable to provide a
partial adjustment for a confounder.

\textbf{Mediator}: a variable that transmits the effect of the treatment
variable on the outcome variable, part of the causal pathway between
treatment and outcome.

\textbf{Modified Disjunctive Cause Criterion}: VanderWeele
(\citeproc{ref-vanderweele2019}{2019}) recommends obtaining a maximally
efficient adjustment, which he calls a ``confounder set.'' A member of
this set is any set of variables that can reduce or remove structural
sources of bias. The strategy is as follows:

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\tightlist
\item
  Control for any variable that causes the exposure, the outcome, or
  both.
\item
  Control for any proxy for an unmeasured variable that is a shared
  cause of the exposure and outcome.
\item
  Define an instrumental variable as a variable associated with the
  exposure but does not influence the outcome independently, except
  through the exposure. Exclude any instrumental variable that is not a
  proxy for an unmeasured confounder from the confounder set
  (\citeproc{ref-vanderweele2019}{VanderWeele 2019}).
\end{enumerate}

Note that the concept of a ``confounder set'' is broader than that of an
``adjustment set'' Every adjustment set is a member of a confounder set.
Hence, the Modified Disjunctive Cause Criterion will eliminate bias when
the data permit. However, a confounder set includes variables that
reduce bias in cases where confounding cannot be eliminated.

\textbf{Node}: characteristic or features of units in a population (a
variable) represented on a causal diagram. In a causal diagram, nodes
are drawn with reference to variable distributions for the target
population.

\textbf{Randomisation}: the process of randomly assigning subjects to
different treatments or control groups to eliminate selection bias in
experimental studies.

\textbf{Reverse causation}: \(\atoyassert\), but in reality \(\ytoa\)

\textbf{Statistical model:} a mathematical representation of the
relationships between variables in which we quantify covariances and
their corresponding uncertainties in the data. Statistical models
typically correspond to multiple causal structures
(\citeproc{ref-hernan2023}{Hernan and Robins 2023};
\citeproc{ref-pearl2018}{Pearl and Mackenzie 2018};
\citeproc{ref-vanderweele2022b}{VanderWeele and Vansteelandt 2022}).
That is, the causes of such covariances cannot be identified without
assumptions.

\textbf{Structural model:} defines assumptions about causal
relationships. Causal diagrams graphically encode these assumptions
(\citeproc{ref-hernan2023}{Hernan and Robins 2023}), leaving out the
assumption about whether the exposure and outcome are causally
associated. We can only compute causal effects outside of randomised
experiments with structural models. A structural model is needed to
interpret the statistical findings in causal terms. Structural
assumptions should be developed in consultation with experts. The role
of structural assumptions when interpreting statistical results needs to
be better understood across many human sciences and forms the motivation
for my work here.

\textbf{Time-varying confounding:} occurs when a confounder that changes
over time acts as a mediator or collider in the causal pathway between
exposure and outcome. Controlling for such a confounder can introduce
bias. Not controlling for it can retain bias.

\newpage{}

\subsection{Appendix B: Causal Consistency in observational
settings}\label{appendix-b}

In observational research, there are typically multiple versions of the
treatment. The theory of causal inference under multiple versions of
treatment proves we can consistently estimate causal effects where the
different versions of treatment are conditionally independent of the
outcomes VanderWeele (\citeproc{ref-vanderweele2009}{2009})

Let \(\coprod\) denote independence. Where there are \(K\) different
versions of treatment \(A\) and no confounding for \(K\)'s effect on
\(Y\) given measured confounders \(L\) such that

\[
Y(k) \coprod K | L
\]

Then it can be proved that causal consistency follows. According to the
theory of causal inference under multiple versions of treatment, the
measured variable \(A\) functions as a ``coarsened indicator'' for
estimating the causal effect of the multiple versions of treatment \(K\)
on \(Y(k)\) (\citeproc{ref-vanderweele2009}{VanderWeele 2009},
\citeproc{ref-vanderweele2018}{2018};
\citeproc{ref-vanderweele2013}{VanderWeele and Hernan 2013}).

In the context of green spaces, let \(A\) represent the general action
of moving closer to any green space and \(K\) represent the different
versions of this treatment. For instance, \(K\) could denote moving
closer to different green spaces such as parks, forests, community
gardens, or green spaces with varying amenities and features.

Here, the conditional independence implies that, given measured
confounders \(L\) (e.g.~socioeconomic status, age, personal values), the
type of green space one moves closer to (\(K\)) is independent of the
outcomes \(Y(k)\) (e.g.~mental well-being under the \(K\) conditions).
In other words, the version of green space one chooses to live near does
not affect the \(K\) potential outcomes, provided the confounders \(L\)
are appropriately controlled for in our statistical models.

Put simply, strategies for confounding control and consistently
estimating causal effects when multiple treatment versions converge.
However, the quantities we estimate under multiple treatment versions
might need clearer interpretations. For example, we cannot readily
determine which of the many treatment versions is most causally
efficacious and which lack any causal effect or are harmful.

\newpage{}

\subsubsection{Appendix C Simulation of Cross-Sectional Data to Compute
the Average Treatment Effect When Conditioning on a
Mediator}\label{appendix-c}

This appendix outlines a simulation designed to demonstrate the
potential pitfalls of conditioning on a mediator in cross-sectional
analyses. The simulation examines the scenario where the effect of
access to green space (\(A\)) on happiness (\(Y\)) is fully mediated by
exercise (\(L\)). This setup aims to illustrate how incorrect
assumptions about the role of a variable (mediator vs.~confounder) can
lead to misleading estimates of the Average Treatment Effect (ATE).

\paragraph{Methodology}\label{methodology}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Data Generation}: we simulate a dataset for 1,000 individuals,
  where access to green space (\(A\)) influences exercise (\(L\)), which
  in turn affects happiness (\(Y\)\$). The simulation is based on
  predefined parameters that establish L as a mediator between \(A\) and
  \(Y\).
\item
  \textbf{Parameter Definitions}:

  \begin{itemize}
  \tightlist
  \item
    The probability of access to green space (\(A\)) is set at 0.5.
  \item
    The effect of \(A\) on \(L\) (exercise) is given by \(\beta = 2\).
  \item
    The effect of \(L\) on \(Y\) (happiness) is given by
    \(\delta = 1.5\).
  \item
    Standard deviations for \(L\) and \(Y\) are set at 1 and 1.5,
    respectively.
  \end{itemize}
\item
  \textbf{Model Specifications}:

  \begin{itemize}
  \tightlist
  \item
    \textbf{Model 1} (Correct Assumption): fits a linear regression
    model assuming \(L\) as a mediator, including both \(A\) and \(L\)
    as regressors on \(Y\). This model aligns with the data-generating
    process, and, by the rules of d-separation, induces mediator bias
    for the \(A\to Y\) path.
  \item
    \textbf{Model 2} (Incorrect Assumption): fits a linear regression
    model including only \(A\) as a regressor on \(Y\), omitting the
    mediator \(L\). This model assesses the direct effect of A on Y
    without accounting for mediation.
  \end{itemize}
\item
  \textbf{Analysis and Comparison}: the analysis compares the estimated
  effects of \(A\) on \(Y\) under both model specifications.
\item
  \textbf{Presentation}: the results are displayed in a comparative
  table.
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# load libraries}
\SpecialCharTok{!}\FunctionTok{require}\NormalTok{(kableExtra)}\ErrorTok{)}\NormalTok{\{}\FunctionTok{install.packages}\NormalTok{(}\StringTok{"kableExtra"}\NormalTok{)\} }\CommentTok{\# tables}
\ControlFlowTok{if}\NormalTok{(}\SpecialCharTok{!}\FunctionTok{require}\NormalTok{(gtsummary))\{}\FunctionTok{install.packages}\NormalTok{(}\StringTok{"gtsummary"}\NormalTok{)\} }\CommentTok{\# tables}

\CommentTok{\# simulation seed}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{) }\CommentTok{\#  reproducibility}

\CommentTok{\# define the parameters }
\NormalTok{n }\OtherTok{=} \DecValTok{1000} \CommentTok{\# Number of observations}
\NormalTok{p }\OtherTok{=} \FloatTok{0.5}  \CommentTok{\# Probability of A = 1 (access to greenspace)}
\NormalTok{alpha }\OtherTok{=} \DecValTok{0} \CommentTok{\# Intercept for L (exercise)}
\NormalTok{beta }\OtherTok{=} \DecValTok{2}  \CommentTok{\# Effect of A on L }
\NormalTok{gamma }\OtherTok{=} \DecValTok{1} \CommentTok{\# Intercept for Y }
\NormalTok{delta }\OtherTok{=} \FloatTok{1.5} \CommentTok{\# Effect of L on Y}
\NormalTok{sigma\_L }\OtherTok{=} \DecValTok{1} \CommentTok{\# Standard deviation of L}
\NormalTok{sigma\_Y }\OtherTok{=} \FloatTok{1.5} \CommentTok{\# Standard deviation of Y}

\CommentTok{\# simulate the data: fully mediated effect by L}
\NormalTok{A }\OtherTok{=} \FunctionTok{rbinom}\NormalTok{(n, }\DecValTok{1}\NormalTok{, p) }\CommentTok{\# binary exposure variable}
\NormalTok{L }\OtherTok{=}\NormalTok{ alpha }\SpecialCharTok{+}\NormalTok{ beta}\SpecialCharTok{*}\NormalTok{A }\SpecialCharTok{+} \FunctionTok{rnorm}\NormalTok{(n, }\DecValTok{0}\NormalTok{, sigma\_L) }\CommentTok{\# mediator L affect by A}
\NormalTok{Y }\OtherTok{=}\NormalTok{ gamma }\SpecialCharTok{+}\NormalTok{ delta}\SpecialCharTok{*}\NormalTok{L }\SpecialCharTok{+} \FunctionTok{rnorm}\NormalTok{(n, }\DecValTok{0}\NormalTok{, sigma\_Y) }\CommentTok{\# Y affected only by L,}

\CommentTok{\# make the data frame}
\NormalTok{data }\OtherTok{=} \FunctionTok{data.frame}\NormalTok{(}\AttributeTok{A =}\NormalTok{ A, }\AttributeTok{L =}\NormalTok{ L, }\AttributeTok{Y =}\NormalTok{ Y)}

\CommentTok{\# fit regression in which we control for L, a mediator}
\CommentTok{\# (cross{-}sectional data is consistent with this model)}
\NormalTok{fit\_1 }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{( Y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ A }\SpecialCharTok{+}\NormalTok{ L, }\AttributeTok{data =}\NormalTok{ data)}

\CommentTok{\# fit regression in which L is assumed to be a mediator, not a confounder.}
\CommentTok{\# (cross{-}sectional data is also consistent with this model)}
\NormalTok{fit\_2 }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{( Y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ A, }\AttributeTok{data =}\NormalTok{ data)}

\CommentTok{\# create gtsummary tables for each regression model}
\NormalTok{table1 }\OtherTok{\textless{}{-}}\NormalTok{ gtsummary}\SpecialCharTok{::}\FunctionTok{tbl\_regression}\NormalTok{(fit\_1)}
\NormalTok{table2 }\OtherTok{\textless{}{-}}\NormalTok{ gtsummary}\SpecialCharTok{::}\FunctionTok{tbl\_regression}\NormalTok{(fit\_2)}

\CommentTok{\# merge the tables for comparison}
\NormalTok{table\_comparison }\OtherTok{\textless{}{-}}\NormalTok{ gtsummary}\SpecialCharTok{::}\FunctionTok{tbl\_merge}\NormalTok{(}
  \FunctionTok{list}\NormalTok{(table1, table2),}
  \AttributeTok{tab\_spanner =} \FunctionTok{c}\NormalTok{(}\StringTok{"Model: Exercise assumed confounder"}\NormalTok{, }
                  \StringTok{"Model: Exercise assumed to be a mediator"}\NormalTok{)}
\NormalTok{)}
\CommentTok{\# make latex table (for publication)}
\NormalTok{markdown\_table\_0 }\OtherTok{\textless{}{-}} \FunctionTok{as\_kable\_extra}\NormalTok{(table\_comparison, }
                                   \AttributeTok{format =} \StringTok{"latex"}\NormalTok{, }
                                   \AttributeTok{booktabs =} \ConstantTok{TRUE}\NormalTok{)}
\CommentTok{\# print latex table (note, you might prefer "markdown" or another format)                                }
\NormalTok{markdown\_table\_0}
\end{Highlighting}
\end{Shaded}

The following code snippet is designed to estimate the Average Treatment
Effect (ATE) using the \texttt{clarify} package in R, which is
referenced here as (\citeproc{ref-greifer2023}{Greifer \emph{et al.}
2023}). The procedure involves two primary steps: simulating coefficient
distributions for regression models and then calculating the ATE based
on these simulations. This process is applied to two distinct models to
demonstrate the effects of including versus excluding a mediator
variable in the analysis.

\subsubsection{Steps to Estimate the
ATE}\label{steps-to-estimate-the-ate}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Load the \texttt{clarify} Package}: this package provides
  functions to simulate regression coefficients and compute average
  marginal effects (AME), robustly facilitating the estimation of ATE.
\item
  \textbf{Set seed}: \texttt{set.seed(123)} ensures that the results of
  the simulations are reproducible, allowing for consistent outcomes
  across different code runs.
\item
  \textbf{Simulate the data distribution}:

  \begin{itemize}
  \tightlist
  \item
    \texttt{sim\_coefs\_fit\_1} and \texttt{sim\_coefs\_fit\_2} are
    generated using the \texttt{sim} function from the \texttt{clarify}
    package, applied to two fitted models (\texttt{fit\_1} and
    \texttt{fit\_2}). These functions simulate the distribution of
    coefficients based on the specified models, capturing the
    uncertainty around the estimated parameters.
  \end{itemize}
\item
  \textbf{Calculate ATE}:

  \begin{itemize}
  \tightlist
  \item
    For both models, the \texttt{sim\_ame} function calculates the ATE
    as the marginal risk difference (RD) when the treatment variable
    (\texttt{A}) is present (\texttt{A\ ==\ 1}). This function uses the
    simulated coefficients to estimate the treatment effect across the
    simulated distributions, providing a comprehensive view of the ATE
    under each model.
  \item
    To streamline the output, the function is set to verbose mode off
    (\texttt{verbose\ =\ FALSE}).
  \end{itemize}
\item
  \textbf{Results}:

  \begin{itemize}
  \tightlist
  \item
    Summaries of these estimates (\texttt{summary\_sim\_est\_fit\_1} and
    \texttt{summary\_sim\_est\_fit\_2}) are obtained, providing detailed
    statistics including the estimated ATE and its 95\% confidence
    intervals (CI).
  \end{itemize}
\item
  \textbf{Presentation: report ATE and CIs}:

  \begin{itemize}
  \tightlist
  \item
    Using the \texttt{glue} package, the ATE and its 95\% CIs for both
    models are formatted into a string for easy reporting. This step
    transforms the statistical output into a more interpretable form,
    highlighting the estimated treatment effect and its precision.
  \end{itemize}
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# use \textasciigrave{}clarify\textasciigrave{} package to obtain ATE}
\ControlFlowTok{if}\NormalTok{(}\SpecialCharTok{!}\FunctionTok{require}\NormalTok{(clarify))\{}\FunctionTok{install.packages}\NormalTok{(}\StringTok{"clarify"}\NormalTok{)\} }\CommentTok{\# clarify package}
\CommentTok{\# simulate fit 1 ATE}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}
\NormalTok{sim\_coefs\_fit\_1 }\OtherTok{\textless{}{-}} \FunctionTok{sim}\NormalTok{(fit\_1)}
\NormalTok{sim\_coefs\_fit\_2 }\OtherTok{\textless{}{-}} \FunctionTok{sim}\NormalTok{(fit\_2)}

\CommentTok{\# marginal risk difference ATE, simulation{-}based: model 1 (L is a confounder)}
\NormalTok{sim\_est\_fit\_1 }\OtherTok{\textless{}{-}}
  \FunctionTok{sim\_ame}\NormalTok{(}
\NormalTok{    sim\_coefs\_fit\_1,}
    \AttributeTok{var =} \StringTok{"A"}\NormalTok{,}
    \AttributeTok{subset =}\NormalTok{ A }\SpecialCharTok{==} \DecValTok{1}\NormalTok{,}
    \AttributeTok{contrast =} \StringTok{"RD"}\NormalTok{,}
    \AttributeTok{verbose =} \ConstantTok{FALSE}
\NormalTok{  )}
\CommentTok{\# marginal risk difference ATE, simulation{-}based: model 2 (L is a mediator)}
\NormalTok{sim\_est\_fit\_2 }\OtherTok{\textless{}{-}}
  \FunctionTok{sim\_ame}\NormalTok{(}
\NormalTok{    sim\_coefs\_fit\_2,}
    \AttributeTok{var =} \StringTok{"A"}\NormalTok{,}
    \AttributeTok{subset =}\NormalTok{ A }\SpecialCharTok{==} \DecValTok{1}\NormalTok{,}
    \AttributeTok{contrast =} \StringTok{"RD"}\NormalTok{,}
    \AttributeTok{verbose =} \ConstantTok{FALSE}
\NormalTok{  )}
\CommentTok{\# obtain summaries}
\NormalTok{summary\_sim\_est\_fit\_1 }\OtherTok{\textless{}{-}} \FunctionTok{summary}\NormalTok{(sim\_est\_fit\_1, }\AttributeTok{null =} \FunctionTok{c}\NormalTok{(}\StringTok{\textasciigrave{}}\AttributeTok{RD}\StringTok{\textasciigrave{}} \OtherTok{=} \DecValTok{0}\NormalTok{))}
\NormalTok{summary\_sim\_est\_fit\_2 }\OtherTok{\textless{}{-}} \FunctionTok{summary}\NormalTok{(sim\_est\_fit\_2, }\AttributeTok{null =} \FunctionTok{c}\NormalTok{(}\StringTok{\textasciigrave{}}\AttributeTok{RD}\StringTok{\textasciigrave{}} \OtherTok{=} \DecValTok{0}\NormalTok{))}

\CommentTok{\# reporting }
\CommentTok{\# ate for fit 1, with 95\% CI}
\NormalTok{ATE\_fit\_1 }\OtherTok{\textless{}{-}}\NormalTok{ glue}\SpecialCharTok{::}\FunctionTok{glue}\NormalTok{(}
  \StringTok{"ATE =}
\StringTok{                        \{round(summary\_sim\_est\_fit\_1[3, 1], 2)\},}
\StringTok{                        CI = [\{round(summary\_sim\_est\_fit\_1[3, 2], 2)\},}
\StringTok{                        \{round(summary\_sim\_est\_fit\_1[3, 3], 2)\}]"}
\NormalTok{)}
\CommentTok{\# ate for fit 2, with 95\% CI}
\NormalTok{ATE\_fit\_2 }\OtherTok{\textless{}{-}}
\NormalTok{  glue}\SpecialCharTok{::}\FunctionTok{glue}\NormalTok{(}
    \StringTok{"ATE = \{round(summary\_sim\_est\_fit\_2[3, 1], 2)\},}
\StringTok{                        CI = [\{round(summary\_sim\_est\_fit\_2[3, 2], 2)\},}
\StringTok{                        \{round(summary\_sim\_est\_fit\_2[3, 3], 2)\}]"}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

\subsubsection{Upshot of the Simulation and
Analysis}\label{upshot-of-the-simulation-and-analysis}

\begin{itemize}
\item
  \textbf{Model 1 (L as a Confounder)}: this analysis assumes that
  \texttt{L} is a confounder in the relationship between the treatment
  (\texttt{A}) and the outcome (\texttt{Y}), and thus, it includes
  \texttt{L} in the model. The ATE estimated here reflects the effect of
  \texttt{A} while controlling for \texttt{L}.
\item
  \textbf{Model 2 (L as a Mediator)}: in contrast, this analysis
  considers \texttt{L} to be a mediator, and the model either includes
  \texttt{L} explicitly in its estimation process or excludes it to
  examine the direct effect of \texttt{A} on \texttt{Y}. The approach to
  mediation analysis here is crucial as it influences the interpretation
  of the ATE.
\end{itemize}

By comparing the ATEs from both models, researchers can understand the
effect of mediation (or the lack thereof) on the estimated treatment
effect. This comparison sheds light on how assumptions about variable
roles (confounder vs.~mediator) can significantly alter causal
inferences drawn from cross-sectional data.

\textbf{Wherever it is uncertain whether a variable is a confounder or a
mediator, we suggest creating two causal diagrams and reporting both
analyses.}

\newpage{}

\subsection{Appendix D: Simulation of Different Confounding Control
Strategies}\label{appendix-d}

This appendix outlines the methodology and results of a data simulation
designed to compare different strategies for controlling confounding in
the context of environmental psychology research. Specifically, the
simulation examines the effect of access to open green spaces
(treatment, \(A_1\)) on happiness (outcome, \(Y_2\)) while addressing
the challenge of unmeasured confounding. The simulation incorporates
baseline measures of exposure and outcome (\(A_0\), \(Y_0\)), baseline
confounders (\(L_0\)), and an unmeasured confounder (\(U\)) to evaluate
the effectiveness of different analytical approaches.

\subsubsection{Methodology}\label{methodology-1}

1.\textbf{Load Libraries \texttt{kableExtra}, \texttt{gtsummary}, and
\texttt{grf}.}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Target}: we simulate data for 10,000 individuals, including
  baseline exposure to green spaces (\(A_0\)), baseline happiness
  (\(Y_0\)), baseline confounders (\(L_0\)), and an unmeasured
  confounder (\(U\)). The simulation uses a logistic model for treatment
  assignment and a linear model for the continuous outcome,
  incorporating interactions to assess how baseline characteristics
  modify the treatment effect.
\item
  \textbf{Set seed and simulate the data distribution}:

  \begin{itemize}
  \tightlist
  \item
    Treatment assignment coefficients: \(\beta_{A0} = 0.25\),
    \(\beta_{Y0} = 0.3\), \(\beta_{L0} = 0.2\), and \(\beta_{U} = 0.1\).
  \item
    Outcome model coefficients: \(\delta_{A1} = 0.3\),
    \(\delta_{Y0} = 0.9\), \(\delta_{A0} = 0.1\), \(\delta_{L0} = 0.3\),
    with an interaction effect (\(\theta_{A0Y0L0} = 0.5\)) indicating
    the combined influence of baseline exposure, outcome, and
    confounders on the follow-up outcome.
  \end{itemize}
\item
  \textbf{Model comparison}:

  \begin{itemize}
  \tightlist
  \item
    \textbf{No control model}: estimates the effect of \(A_1\) on
    \(Y_2\) without controlling for any confounders.
  \item
    \textbf{Standard covariate control model}: controls for baseline
    confounders (\(L_0\)) alongside treatment (\(A_1\)).
  \item
    \textbf{Baseline exposure and outcome model}: extends the standard
    model by including baseline treatment and outcome (\(A_0\), \(Y_0\))
    and their interaction with \(L_0\).
  \end{itemize}
\item
  \textbf{Results}: each model's effectiveness in estimating the true
  treatment effect is assessed by comparing regression outputs. The
  simulation evaluates how well each model addresses the bias introduced
  by unmeasured confounding and the role of baseline characteristics in
  modifying treatment effects.
\item
  \textbf{Presentation}: the results are synthesised in a comparative
  table, formatted using the \texttt{kableExtra} \{Zhu
  (\citeproc{ref-zhu2021KableExtra}{2021}){]} and \texttt{gtsummary}
  packages (\citeproc{ref-gtsummary2021}{Sjoberg \emph{et al.} 2021}),
  highlighting the estimated treatment effects and their statistical
  significance across models.
\end{enumerate}

Overall, we use the simulation to illustrate the importance of
incorporating baseline characteristics and their interactions to
mitigate the influence of unmeasured confounding.

Here is the simulation and modelling code:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(kableExtra)}
\ControlFlowTok{if}\NormalTok{(}\SpecialCharTok{!}\FunctionTok{require}\NormalTok{(kableExtra))\{}\FunctionTok{install.packages}\NormalTok{(}\StringTok{"kableExtra"}\NormalTok{)\} }\CommentTok{\# causal forest}
\ControlFlowTok{if}\NormalTok{(}\SpecialCharTok{!}\FunctionTok{require}\NormalTok{(gtsummary))\{}\FunctionTok{install.packages}\NormalTok{(}\StringTok{"gtsummary"}\NormalTok{)\} }\CommentTok{\# causal forest}
\ControlFlowTok{if}\NormalTok{(}\SpecialCharTok{!}\FunctionTok{require}\NormalTok{(grf))\{}\FunctionTok{install.packages}\NormalTok{(}\StringTok{"grf"}\NormalTok{)\} }\CommentTok{\# causal forest}

\CommentTok{\# r\_texmf()eproducibility}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{) }

\CommentTok{\# set number of observations}
\NormalTok{n }\OtherTok{\textless{}{-}} \DecValTok{10000} 

\CommentTok{\# baseline covariates}
\NormalTok{U }\OtherTok{\textless{}{-}} \FunctionTok{rnorm}\NormalTok{(n) }\CommentTok{\# Unmeasured confounder}
\NormalTok{A\_0 }\OtherTok{\textless{}{-}} \FunctionTok{rbinom}\NormalTok{(n, }\DecValTok{1}\NormalTok{, }\AttributeTok{prob =} \FunctionTok{plogis}\NormalTok{(U)) }\CommentTok{\# Baseline exposure}
\NormalTok{Y\_0 }\OtherTok{\textless{}{-}} \FunctionTok{rnorm}\NormalTok{(n, }\AttributeTok{mean =}\NormalTok{ U, }\AttributeTok{sd =} \DecValTok{1}\NormalTok{) }\CommentTok{\# Baseline outcome}
\NormalTok{L\_0 }\OtherTok{\textless{}{-}} \FunctionTok{rnorm}\NormalTok{(n, }\AttributeTok{mean =}\NormalTok{ U, }\AttributeTok{sd =} \DecValTok{1}\NormalTok{) }\CommentTok{\# Baseline confounders}

\CommentTok{\# coefficients for treatment assignment}
\NormalTok{beta\_A0 }\OtherTok{=} \FloatTok{0.25}
\NormalTok{beta\_Y0 }\OtherTok{=} \FloatTok{0.3}
\NormalTok{beta\_L0 }\OtherTok{=} \FloatTok{0.2}
\NormalTok{beta\_U }\OtherTok{=} \FloatTok{0.1}

\CommentTok{\# simulate treatment assignment}
\NormalTok{A\_1 }\OtherTok{\textless{}{-}} \FunctionTok{rbinom}\NormalTok{(n, }\DecValTok{1}\NormalTok{, }\AttributeTok{prob =} \FunctionTok{plogis}\NormalTok{(}\SpecialCharTok{{-}}\FloatTok{0.5} \SpecialCharTok{+} 
\NormalTok{                                    beta\_A0 }\SpecialCharTok{*}\NormalTok{ A\_0 }\SpecialCharTok{+}
\NormalTok{                                    beta\_Y0 }\SpecialCharTok{*}\NormalTok{ Y\_0 }\SpecialCharTok{+} 
\NormalTok{                                    beta\_L0 }\SpecialCharTok{*}\NormalTok{ L\_0 }\SpecialCharTok{+} 
\NormalTok{                                    beta\_U }\SpecialCharTok{*}\NormalTok{ U))}
\CommentTok{\# coefficients for continuous outcome}
\NormalTok{delta\_A1 }\OtherTok{=} \FloatTok{0.3}
\NormalTok{delta\_Y0 }\OtherTok{=} \FloatTok{0.9}
\NormalTok{delta\_A0 }\OtherTok{=} \FloatTok{0.1}
\NormalTok{delta\_L0 }\OtherTok{=} \FloatTok{0.3}
\NormalTok{theta\_A0Y0L0 }\OtherTok{=} \FloatTok{0.5} \CommentTok{\# Interaction effect between A\_1 and L\_0}
\NormalTok{delta\_U }\OtherTok{=} \FloatTok{0.05}
\CommentTok{\# simulate continuous outcome including interaction}
\NormalTok{Y\_2 }\OtherTok{\textless{}{-}} \FunctionTok{rnorm}\NormalTok{(n,}
             \AttributeTok{mean =} \DecValTok{0} \SpecialCharTok{+}
\NormalTok{               delta\_A1 }\SpecialCharTok{*}\NormalTok{ A\_1 }\SpecialCharTok{+} 
\NormalTok{               delta\_Y0 }\SpecialCharTok{*}\NormalTok{ Y\_0 }\SpecialCharTok{+} 
\NormalTok{               delta\_A0 }\SpecialCharTok{*}\NormalTok{ A\_0 }\SpecialCharTok{+} 
\NormalTok{               delta\_L0 }\SpecialCharTok{*}\NormalTok{ L\_0 }\SpecialCharTok{+} 
\NormalTok{               theta\_A0Y0L0 }\SpecialCharTok{*}\NormalTok{ Y\_0 }\SpecialCharTok{*} 
\NormalTok{               A\_0 }\SpecialCharTok{*}\NormalTok{ L\_0 }\SpecialCharTok{+} 
\NormalTok{               delta\_U }\SpecialCharTok{*}\NormalTok{ U,}
             \AttributeTok{sd =}\NormalTok{ .}\DecValTok{5}\NormalTok{)}
\CommentTok{\# assemble data frame}
\NormalTok{data }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(Y\_2, A\_0, A\_1, L\_0, Y\_0, U)}

\CommentTok{\# model: no control}
\NormalTok{fit\_no\_control }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(Y\_2 }\SpecialCharTok{\textasciitilde{}}\NormalTok{ A\_1, }\AttributeTok{data =}\NormalTok{ data)}

\CommentTok{\# model: standard covariate control}
\NormalTok{fit\_standard }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(Y\_2 }\SpecialCharTok{\textasciitilde{}}\NormalTok{ A\_1 }\SpecialCharTok{+}\NormalTok{ L\_0, }\AttributeTok{data =}\NormalTok{ data)}

\CommentTok{\# model: interaction with baseline confounders, and baseline outcome and exposure}
\NormalTok{fit\_interaction  }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(Y\_2 }\SpecialCharTok{\textasciitilde{}}\NormalTok{ A\_1 }\SpecialCharTok{*}\NormalTok{ (L\_0 }\SpecialCharTok{+}\NormalTok{ A\_0 }\SpecialCharTok{+}\NormalTok{ Y\_0), }\AttributeTok{data =}\NormalTok{ data)}

\CommentTok{\# create gtsummary tables for each regression model}
\NormalTok{tbl\_fit\_no\_control}\OtherTok{\textless{}{-}} \FunctionTok{tbl\_regression}\NormalTok{(fit\_no\_control)  }
\NormalTok{tbl\_fit\_standard }\OtherTok{\textless{}{-}} \FunctionTok{tbl\_regression}\NormalTok{(fit\_standard)}
\NormalTok{tbl\_fit\_interaction }\OtherTok{\textless{}{-}} \FunctionTok{tbl\_regression}\NormalTok{(fit\_interaction)}

\CommentTok{\# get only the treatment variable}
\NormalTok{tbl\_list\_modified }\OtherTok{\textless{}{-}} \FunctionTok{lapply}\NormalTok{(}\FunctionTok{list}\NormalTok{(}
\NormalTok{  tbl\_fit\_no\_control,}
\NormalTok{  tbl\_fit\_standard,}
\NormalTok{  tbl\_fit\_interaction),}
\ControlFlowTok{function}\NormalTok{(tbl) \{}
\NormalTok{  tbl }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{modify\_table\_body}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\NormalTok{ .x }\SpecialCharTok{\%\textgreater{}\%}\NormalTok{ dplyr}\SpecialCharTok{::}\FunctionTok{filter}\NormalTok{(variable }\SpecialCharTok{==} \StringTok{"A\_1"}\NormalTok{))}
\NormalTok{\})}
\CommentTok{\# merge tables}
\NormalTok{table\_comparison }\OtherTok{\textless{}{-}} \FunctionTok{tbl\_merge}\NormalTok{(}
  \AttributeTok{tbls =}\NormalTok{ tbl\_list\_modified,}
  \AttributeTok{tab\_spanner =} \FunctionTok{c}\NormalTok{(}
    \StringTok{"No Control"}\NormalTok{,}
    \StringTok{"Standard"}\NormalTok{,}
    \StringTok{"Interaction"}\NormalTok{)}
\NormalTok{) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{modify\_table\_styling}\NormalTok{(}
    \AttributeTok{column =} \FunctionTok{c}\NormalTok{(p.value\_1, p.value\_2, p.value\_3),}
    \AttributeTok{hide =} \ConstantTok{TRUE}
\NormalTok{  )}
\CommentTok{\# latex table for publication}
\NormalTok{markdown\_table }\OtherTok{\textless{}{-}}
  \FunctionTok{as\_kable\_extra}\NormalTok{(table\_comparison, }\AttributeTok{format =} \StringTok{"latex"}\NormalTok{, }\AttributeTok{booktabs =} \ConstantTok{TRUE}\NormalTok{) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{kable\_styling}\NormalTok{(}\AttributeTok{latex\_options =} \StringTok{"scale\_down"}\NormalTok{)}
\FunctionTok{print}\NormalTok{(markdown\_table)}
\end{Highlighting}
\end{Shaded}

Next, in the following code snippet, we calculate the Average Treatment
Effect (ATE) using simulation-based approaches for two distinct models:
one with standard covariate control and another incorporating
interaction. This approach leverages the \texttt{clarify} package in R,
which facilitates the simulation and interpretation of estimated
coefficients from linear models to derive ATEs under different modelling
assumptions (\citeproc{ref-greifer2023}{Greifer \emph{et al.} 2023}).

First, we use the \texttt{sim} function from the \texttt{clarify}
package to generate simulated coefficient distributions for the standard
model (\texttt{fit\_standard}) and the interaction model
(\texttt{fit\_interaction}). This step is crucial for capturing the
uncertainty in our estimates arising from sampling variability.

Next, we employ each model's \texttt{sim\_ame} function to compute the
average marginal effects (AME), focusing on the treatment variable
(\texttt{A\_1}). The calculation is done under the assumption that all
individuals are treated (i.e., \texttt{A\_1\ ==\ 1}), and we specify the
contrast type as ``RD'' (Risk Difference) to directly obtain the ATE
(Average Treatment Effect). The \texttt{sim\_ame} function simulates the
treatment effect across the distribution of simulated coefficients,
providing a robust estimate of the ATE and its variability.

The summaries of these simulations (\texttt{summary\_sim\_est\_fit\_std}
and \texttt{summary\_sim\_est\_fit\_int}) are then extracted to provide
concise estimates of the ATE along with 95\% confidence intervals (CIs)
for both the standard and interaction models. This step is essential for
understanding the magnitude and precision of the treatment effects
estimated by the models.

Finally, we use the \texttt{glue} package to format these estimates into
a human-readable form, presenting the ATE and its corresponding 95\% CIs
for each model. This presentation facilitates clear communication of the
estimated treatment effects, allowing for direct comparison between the
models and highlighting the effect of including baseline characteristics
and their interactions on estimating the ATE
(\citeproc{ref-hester2022GLUE}{Hester and Bryan 2022}).

This simulation-based approach to estimating the ATE underscores the
importance of considering model complexity and the roles of confounders
and mediators in causal inference analyses. By comparing the ATE
estimates from different models, we can assess the sensitivity of our
causal conclusions to various assumptions and modelling strategies.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#| label: ate{-}sim{-}long}
\CommentTok{\#| tbl{-}cap: "Code for calculating the average treatment effect."}
\CommentTok{\#| echo: false}
\CommentTok{\#| eval: true}

\CommentTok{\# use \textasciigrave{}clarify\textasciigrave{} package to obtain ATE}
\ControlFlowTok{if}\NormalTok{(}\SpecialCharTok{!}\FunctionTok{require}\NormalTok{(clarify))\{}\FunctionTok{install.packages}\NormalTok{(}\StringTok{"clarify"}\NormalTok{)\} }\CommentTok{\# clarify package}

\CommentTok{\# simulate fit 1 ATE}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}
\NormalTok{sim\_coefs\_fit\_no\_control}\OtherTok{\textless{}{-}} \FunctionTok{sim}\NormalTok{(fit\_no\_control)  }
\NormalTok{sim\_coefs\_fit\_std }\OtherTok{\textless{}{-}} \FunctionTok{sim}\NormalTok{(fit\_standard)}
\NormalTok{sim\_coefs\_fit\_int }\OtherTok{\textless{}{-}} \FunctionTok{sim}\NormalTok{(fit\_interaction)}

\CommentTok{\# marginal risk difference ATE, no controls}
\NormalTok{sim\_est\_fit\_no\_control }\OtherTok{\textless{}{-}}
  \FunctionTok{sim\_ame}\NormalTok{(}
\NormalTok{    sim\_coefs\_fit\_no\_control,}
    \AttributeTok{var =} \StringTok{"A\_1"}\NormalTok{,}
    \AttributeTok{subset =}\NormalTok{ A\_1 }\SpecialCharTok{==} \DecValTok{1}\NormalTok{,}
    \AttributeTok{contrast =} \StringTok{"RD"}\NormalTok{,}
    \AttributeTok{verbose =} \ConstantTok{FALSE}
\NormalTok{  )}
\CommentTok{\# marginal risk difference ATE, simulation{-}based: model 1 (L is a confounder)}
\NormalTok{sim\_est\_fit\_std }\OtherTok{\textless{}{-}}
  \FunctionTok{sim\_ame}\NormalTok{(}
\NormalTok{    sim\_coefs\_fit\_std,}
    \AttributeTok{var =} \StringTok{"A\_1"}\NormalTok{,}
    \AttributeTok{subset =}\NormalTok{ A\_1 }\SpecialCharTok{==} \DecValTok{1}\NormalTok{,}
    \AttributeTok{contrast =} \StringTok{"RD"}\NormalTok{,}
    \AttributeTok{verbose =} \ConstantTok{FALSE}
\NormalTok{  )}
\CommentTok{\# marginal risk difference ATE, simulation{-}based: model 2 (L is a mediator)}
\NormalTok{sim\_est\_fit\_int }\OtherTok{\textless{}{-}}
  \FunctionTok{sim\_ame}\NormalTok{(}
\NormalTok{    sim\_coefs\_fit\_int,}
    \AttributeTok{var =} \StringTok{"A\_1"}\NormalTok{,}
    \AttributeTok{subset =}\NormalTok{ A\_1 }\SpecialCharTok{==} \DecValTok{1}\NormalTok{,}
    \AttributeTok{contrast =} \StringTok{"RD"}\NormalTok{,}
    \AttributeTok{verbose =} \ConstantTok{FALSE}
\NormalTok{  )}
\CommentTok{\# obtain summaries}
\NormalTok{summary\_sim\_coefs\_fit\_no\_control }\OtherTok{\textless{}{-}}
  \FunctionTok{summary}\NormalTok{(sim\_est\_fit\_no\_control, }\AttributeTok{null =} \FunctionTok{c}\NormalTok{(}\StringTok{\textasciigrave{}}\AttributeTok{RD}\StringTok{\textasciigrave{}} \OtherTok{=} \DecValTok{0}\NormalTok{))}
\NormalTok{summary\_sim\_est\_fit\_std }\OtherTok{\textless{}{-}}
  \FunctionTok{summary}\NormalTok{(sim\_est\_fit\_std, }\AttributeTok{null =} \FunctionTok{c}\NormalTok{(}\StringTok{\textasciigrave{}}\AttributeTok{RD}\StringTok{\textasciigrave{}} \OtherTok{=} \DecValTok{0}\NormalTok{))}
\NormalTok{summary\_sim\_est\_fit\_int }\OtherTok{\textless{}{-}}
  \FunctionTok{summary}\NormalTok{(sim\_est\_fit\_int, }\AttributeTok{null =} \FunctionTok{c}\NormalTok{(}\StringTok{\textasciigrave{}}\AttributeTok{RD}\StringTok{\textasciigrave{}} \OtherTok{=} \DecValTok{0}\NormalTok{))}

\CommentTok{\# get coefficients for reporting}
\CommentTok{\# ate for fit 1, with 95\% CI}
\NormalTok{ATE\_fit\_no\_control  }\OtherTok{\textless{}{-}}\NormalTok{ glue}\SpecialCharTok{::}\FunctionTok{glue}\NormalTok{(}
  \StringTok{"ATE = \{round(summary\_sim\_coefs\_fit\_no\_control[3, 1], 2)\}, }
\StringTok{  CI = [\{round(summary\_sim\_coefs\_fit\_no\_control[3, 2], 2)\},}
\StringTok{  \{round(summary\_sim\_coefs\_fit\_no\_control[3, 3], 2)\}]"}
\NormalTok{)}
\CommentTok{\# ate for fit 2, with 95\% CI}
\NormalTok{ATE\_fit\_std }\OtherTok{\textless{}{-}}\NormalTok{ glue}\SpecialCharTok{::}\FunctionTok{glue}\NormalTok{(}
  \StringTok{"ATE = \{round(summary\_sim\_est\_fit\_std[3, 1], 2)\}, }
\StringTok{  CI = [\{round(summary\_sim\_est\_fit\_std[3, 2], 2)\},}
\StringTok{  \{round(summary\_sim\_est\_fit\_std[3, 3], 2)\}]"}
\NormalTok{)}
\CommentTok{\# ate for fit 3, with 95\% CI}
\NormalTok{ATE\_fit\_int }\OtherTok{\textless{}{-}}
\NormalTok{  glue}\SpecialCharTok{::}\FunctionTok{glue}\NormalTok{(}
    \StringTok{"ATE = \{round(summary\_sim\_est\_fit\_int[3, 1], 2)\},}
\StringTok{    CI = [\{round(summary\_sim\_est\_fit\_int[3, 2], 2)\},}
\StringTok{    \{round(summary\_sim\_est\_fit\_int[3, 3], 2)\}]"}
\NormalTok{  )}
\CommentTok{\# coefs they used in the manuscript}
\end{Highlighting}
\end{Shaded}

Using the \texttt{clarify} package, we infer the ATE for the standard
model is ATE = 0.86, CI = {[}0.8, 0.92{]}.

Using the \texttt{clarify} package, we infer the ATE for the model that
conditions on the baseline exposure and baseline outcome to be: ATE =
0.29, CI = {[}0.27, 0.31{]}, which is close to the values supplied to
the data-generating mechanism.

\textbf{Take-home message:} The baseline exposure and baseline outcome
are often the most important variables to include for confounding
control. The baseline exposure also allows us to estimate an
incident-exposure effect. For this reason, we should endeavour to obtain
at least three waves of data such that these variables and other
baseline confounders are included at time 0, the exposure is included at
time 1, and the outcome is included at time 2.

\newpage{}

\subsection{Appendix E: Non-parametric Estimation of Average Treatment
Effects Using Causal Forests}\label{appendix-causal-forests}

This appendix provides a practical example of estimating average
treatment effects (ATE) using a non-parametric approach, specifically
applying causal forests. Unlike traditional regression models, causal
forests allow for estimating treatment effects without imposing strict
assumptions about the form of the relationship between treatment,
covariates, and outcomes. This flexibility makes them particularly
useful for analysing complex datasets where the treatment effect may
vary across observations.

\paragraph{Causal Forest Model
Implementation}\label{causal-forest-model-implementation}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Libraries}: the implementation begins with loading the
  necessary R libraries: \texttt{grf} for estimating conditional and
  average treatment effects using causal forests and \texttt{glue} for
  formatting the results for reporting.
\item
  \begin{enumerate}
  \def\labelenumii{\arabic{enumii}.}
  \tightlist
  \item
    \textbf{Data generation}: the code assumes the presence of a data
    frame \texttt{data} generated from the previous code snippet
    containing the variables:
  \end{enumerate}

  \begin{itemize}
  \tightlist
  \item
    \texttt{A\_1}: Treatment indicator.
  \item
    \texttt{L\_0}: A covariate.
  \item
    \texttt{Y\_2}: Outcome of interest.
  \item
    \texttt{A\_0} and \texttt{Y\_0}: Baseline exposure and outcome,
    respectively.
  \end{itemize}

  Treatment (\texttt{W}) and outcome (\texttt{Y}) vectors are extracted
  from \texttt{data} alongside a matrix \texttt{X} that includes
  covariates and baseline characteristics.
\item
  \textbf{Causal Forest model}: a causal forest model is fitted using
  the \texttt{causal\_forest} function from the \texttt{grf} package
  (\citeproc{ref-grf2024}{Tibshirani \emph{et al.} 2024}). This function
  takes the covariate matrix \texttt{X}, the outcome vector \texttt{Y},
  and the treatment vector \texttt{W} as inputs, and it returns a model
  object that can be used for further analysis.
\item
  \textbf{Average Treatment Effect estimation}: the
  \texttt{average\_treatment\_effect} function computes the ATE from the
  fitted causal forest model. This step is crucial as it quantifies the
  overall effect of the treatment across the population, adjusting for
  covariates included in the model.
\item
  \textbf{Reporting}: The estimated ATE and its standard error (se) are
  extracted and formatted for reporting using the \texttt{glue} package
  (\citeproc{ref-hester2022GLUE}{Hester and Bryan 2022}). This
  facilitates clear communication of the results, showing the estimated
  effect size and its uncertainty.
\end{enumerate}

\paragraph{Key Takeaways}\label{key-takeaways}

\begin{itemize}
\item
  \textbf{Flexibility and robustness}: causal forests offer a robust way
  to estimate treatment effects without making parametric solid
  assumptions. This approach is particularly advantageous in settings
  where the treatment effect may vary with covariates or across
  different subpopulations.
\item
  \textbf{ATE estimation}: the model estimates the ATE as the difference
  in expected outcomes between treated and untreated units, averaged
  across the population. This estimate reflects the overall effect of
  the treatment, accounting for the distribution of covariates in the
  sample.
\item
  \textbf{Convergence to the true value}: we find that the estimated ATE
  by the causal forest model converges to the actual value used in the
  data-generating process (assumed to be 0.3). This demonstrates the
  effectiveness of causal forests in uncovering the true treatment
  effect from complex data.
\end{itemize}

This example underscores the utility of semi-parametric and
non-parametric methods, such as causal forests, in causal inference
analyses.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# load causal forest library }
\FunctionTok{library}\NormalTok{(grf) }\CommentTok{\# estimate conditional and average treatment effects}
\FunctionTok{library}\NormalTok{(glue) }\CommentTok{\# reporting }

\CommentTok{\#  \textquotesingle{}data\textquotesingle{} is our data frame with columns \textquotesingle{}A\_1\textquotesingle{} for treatment, \textquotesingle{}L\_0\textquotesingle{} for a covariate, and \textquotesingle{}Y\_2\textquotesingle{} for the outcome}
\CommentTok{\#  we also have the baseline exposure \textquotesingle{}A\_0\textquotesingle{} and \textquotesingle{}Y\_0\textquotesingle{}}
\CommentTok{\#  ensure W (treatment) and Y (outcome) are vectors}
\NormalTok{W }\OtherTok{\textless{}{-}} \FunctionTok{as.matrix}\NormalTok{(data}\SpecialCharTok{$}\NormalTok{A\_1)  }\CommentTok{\# Treatment}
\NormalTok{Y }\OtherTok{\textless{}{-}} \FunctionTok{as.matrix}\NormalTok{(data}\SpecialCharTok{$}\NormalTok{Y\_2)  }\CommentTok{\# Outcome}
\NormalTok{X }\OtherTok{\textless{}{-}} \FunctionTok{as.matrix}\NormalTok{(data[, }\FunctionTok{c}\NormalTok{(}\StringTok{"L\_0"}\NormalTok{, }\StringTok{"A\_0"}\NormalTok{, }\StringTok{"Y\_0"}\NormalTok{)])}

\CommentTok{\# fit causal forest model }
\NormalTok{fit\_causal\_forest }\OtherTok{\textless{}{-}} \FunctionTok{causal\_forest}\NormalTok{(X, Y, W)}

\CommentTok{\# estimate the average treatment effect (ATE)}
\NormalTok{ate }\OtherTok{\textless{}{-}} \FunctionTok{average\_treatment\_effect}\NormalTok{(fit\_causal\_forest)}

\CommentTok{\# make data frame for reporting using "glue\textquotesingle{} }
\NormalTok{ate}\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(ate)}

\CommentTok{\# obtain ate for report}
\NormalTok{ATE\_fit\_causal\_forest }\OtherTok{\textless{}{-}}
\NormalTok{  glue}\SpecialCharTok{::}\FunctionTok{glue}\NormalTok{(}
    \StringTok{"ATE = \{round(ate[1, 1], 2)\}, se = \{round(ate[2, 1], 2)\}"}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

Causal forest estimates the average treatment effect as ATE = 0.3, se =
0.01. This approach converges to the true value supplied to the
generating mechanism of 0.3



\end{document}
