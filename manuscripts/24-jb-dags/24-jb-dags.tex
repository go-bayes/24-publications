% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames,x11names}{xcolor}
%
\documentclass[
  single column]{article}

\usepackage{amsmath,amssymb}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage[]{libertinus}
\ifPDFTeX\else  
    % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\usepackage[top=30mm,left=25mm,heightrounded,headsep=22pt,headheight=11pt,footskip=33pt,ignorehead,ignorefoot]{geometry}
\setlength{\emergencystretch}{3em} % prevent overfull lines
\setcounter{secnumdepth}{-\maxdimen} % remove section numbering
% Make \paragraph and \subparagraph free-standing
\makeatletter
\ifx\paragraph\undefined\else
  \let\oldparagraph\paragraph
  \renewcommand{\paragraph}{
    \@ifstar
      \xxxParagraphStar
      \xxxParagraphNoStar
  }
  \newcommand{\xxxParagraphStar}[1]{\oldparagraph*{#1}\mbox{}}
  \newcommand{\xxxParagraphNoStar}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
  \let\oldsubparagraph\subparagraph
  \renewcommand{\subparagraph}{
    \@ifstar
      \xxxSubParagraphStar
      \xxxSubParagraphNoStar
  }
  \newcommand{\xxxSubParagraphStar}[1]{\oldsubparagraph*{#1}\mbox{}}
  \newcommand{\xxxSubParagraphNoStar}[1]{\oldsubparagraph{#1}\mbox{}}
\fi
\makeatother


\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
% definitions for citeproc citations
\NewDocumentCommand\citeproctext{}{}
\NewDocumentCommand\citeproc{mm}{%
  \begingroup\def\citeproctext{#2}\cite{#1}\endgroup}
\makeatletter
 % allow citations to break across lines
 \let\@cite@ofmt\@firstofone
 % avoid brackets around text for \cite:
 \def\@biblabel#1{}
 \def\@cite#1#2{{#1\if@tempswa , #2\fi}}
\makeatother
\newlength{\cslhangindent}
\setlength{\cslhangindent}{1.5em}
\newlength{\csllabelwidth}
\setlength{\csllabelwidth}{3em}
\newenvironment{CSLReferences}[2] % #1 hanging-indent, #2 entry-spacing
 {\begin{list}{}{%
  \setlength{\itemindent}{0pt}
  \setlength{\leftmargin}{0pt}
  \setlength{\parsep}{0pt}
  % turn on hanging indent if param 1 is 1
  \ifodd #1
   \setlength{\leftmargin}{\cslhangindent}
   \setlength{\itemindent}{-1\cslhangindent}
  \fi
  % set entry spacing
  \setlength{\itemsep}{#2\baselineskip}}}
 {\end{list}}
\usepackage{calc}
\newcommand{\CSLBlock}[1]{\hfill\break\parbox[t]{\linewidth}{\strut\ignorespaces#1\strut}}
\newcommand{\CSLLeftMargin}[1]{\parbox[t]{\csllabelwidth}{\strut#1\strut}}
\newcommand{\CSLRightInline}[1]{\parbox[t]{\linewidth - \csllabelwidth}{\strut#1\strut}}
\newcommand{\CSLIndent}[1]{\hspace{\cslhangindent}#1}

\usepackage{booktabs}
\usepackage{longtable}
\usepackage{array}
\usepackage{multirow}
\usepackage{wrapfig}
\usepackage{float}
\usepackage{colortbl}
\usepackage{pdflscape}
\usepackage{tabu}
\usepackage{threeparttable}
\usepackage{threeparttablex}
\usepackage[normalem]{ulem}
\usepackage{makecell}
\usepackage{xcolor}
\input{/Users/joseph/GIT/latex/latex-for-quarto.tex}
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\AtBeginDocument{%
\ifdefined\contentsname
  \renewcommand*\contentsname{Table of contents}
\else
  \newcommand\contentsname{Table of contents}
\fi
\ifdefined\listfigurename
  \renewcommand*\listfigurename{List of Figures}
\else
  \newcommand\listfigurename{List of Figures}
\fi
\ifdefined\listtablename
  \renewcommand*\listtablename{List of Tables}
\else
  \newcommand\listtablename{List of Tables}
\fi
\ifdefined\figurename
  \renewcommand*\figurename{Figure}
\else
  \newcommand\figurename{Figure}
\fi
\ifdefined\tablename
  \renewcommand*\tablename{Table}
\else
  \newcommand\tablename{Table}
\fi
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Listing}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}
\makeatother
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\@ifpackageloaded{subcaption}{}{\usepackage{subcaption}}
\makeatother
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\usepackage{bookmark}

\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same} % disable monospaced font for URLs
\hypersetup{
  pdftitle={Causal Directed Acyclic Graphs (DAGs): A Practical Guide},
  pdfauthor={Joseph A. Bulbulia},
  colorlinks=true,
  linkcolor={blue},
  filecolor={Maroon},
  citecolor={Blue},
  urlcolor={Blue},
  pdfcreator={LaTeX via pandoc}}

\title{Causal Directed Acyclic Graphs (DAGs): A Practical Guide}

\usepackage{academicons}
\usepackage{xcolor}

  \author{Joseph A. Bulbulia}
            \affil{%
             \small{     Victoria University of Wellington, New Zealand
          ORCID \textcolor[HTML]{A6CE39}{\aiOrcid} ~0000-0002-5861-2056 }
              }
      


\date{2024-05-24}
\begin{document}
\maketitle
\begin{abstract}
Causal inference requires contrasting counterfactual states of the world
under pre-specified interventions. Obtaining counterfactual contrasts
from data, in turn, relies on a framework of explicit assumptions and
careful, multi-step workflows. Causal diagrams are powerful tools for
clarifying whether and how the counterfactual contrasts we seek may be
identified from data. Here, I explain how to use causal directed acyclic
graphs (causal DAGs) to clarify whether and how causal effects may be
identified from observational data. Along the way, I offer practical
tips for reporting and suggestions for avoiding common pitfalls.

\textbf{KEYWORDS}: \emph{Causal Inference}; \emph{Culture};
\emph{DAGs};* \emph{Evolution}; \emph{Human Sciences};
\emph{Longitudinal}.
\end{abstract}

\subsection{Introduction}\label{introduction}

Human research begins with two fundamental questions:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  What do I want to know?
\item
  For which population does this knowledge generalise?
\end{enumerate}

In the human sciences, our questions are typically causal. We aim to
understand the effects of interventions on certain variables. However,
many researchers report non-causal associations, collecting data,
applying complex regressions, and reporting coefficients. We often speak
of covariates as ``predicting'' outcomes. Yet, even when our models
predict well, it remains unclear how these predictions relate to the
scientific questions that sparked our interest. We fail to recognise
that these ``oracles'' lack coherent interpretation.

Some say that association cannot imply causation. However, our
experimental traditions reveal that when interventions are controlled
and randomised, the coefficients we recover from statistical models can
permit causal interpretations.

Despite familiarity with experiments, many researchers struggle to
emulate randomisation and control with non-experimental or
``real-world'' data. Though many use terms like ``control'' and employ
sophisticated adjustment strategies---such as multilevel modelling and
structural equation models---these practices are not systematic. We
often overlook that what we take as control can undermine our ability to
consistently estimate causal effects. Although the term ``crisis'' is
overused, the state of causal inference across many human sciences,
including experimental sciences, has much headroom for improvement.
Moreover, poor experimental designs unintentially weaken causal claims;
causal inferences in experiments is also deserving of greater attention.
Fortuantely, recent decades have seen progress in the the health
sciences, economics, and computer science have markedly improved our
ability to obtain causal inferences in scientific research. This work
demonstrates that obtaining causal inferences from data is feasible but
requires careful, systematic workflows.

Within the workflows of causal inference, causal diagrams---or causal
graphs---are powerful tools for evaluating whether and how causal
effects can be identified from data. My purpose here is to explain where
these tools fit within causal inference workflows and to illustrate
their practical applications. I focus on causal directed acyclic graphs
(causal DAGs) because they are relatively easy to use and optimally
clear for most applications. However, causal directed acyclic graphs can
also be misused. I will also explain common pitfalls and how to avoid
them.

In Part 1, we review the conceptual foundations of causal inference. The
basis of all causal inference lies in counterfactual contrasts. Although
there are different philosophical approaches to counterfactual
reasoning, they are largely similar in practice. The overview here
builds on the Neyman-Rubin potential outcomes framework, extended for
longitudinal data by epidemiologist James Robins. Although this is not
the framework within which causal directed acyclic graphs were
developed, the potential outcomes framework is easier to interpret. (I
discuss Pearl's non-parametric structural equation approach in Appendix
E).

In Part 2, we describe how causal directed acyclic graphs help identify
causal effects. We outline five elementary graphical structures that
encode all causal relations, forming the building blocks of all causal
directed acyclic graphs. We then examine five rules that clarify whether
and how investigators can identify causal effects from data.

In Part 3, we apply causal directed acyclic graphs to practical
problems, showing how repeated measures data collection can solve seven
common identification issues. Timing is critical, but not sufficient
alone. We also use causal diagrams to highlight the limitations of
repeated-measures data collection for identifying causal effects,
tempering enthusiasm for easy solutions. Indeed we will review how many
statistical structural equation models and sophisticated multi-level
models are not well-calibrated for identifying causal effects.

In Part 4, I offer practical suggestions for creating and reporting
causal directed acyclic graphs in scientific research. These graphs
represent investigator assumptions about causal (or structural)
relationships in nature. These relationships cannot typically be derived
from data alone and must be developed with scientific specialists. Where
ambiguity or debate exists, investigators should report multiple causal
diagrams and conduct distinct analyses. I explain how to do this and
walk through the steps.

By understanding and applying causal directed acyclic graphs,
researchers can more effectively identify and communicate causal
relationships, in the service of the causal questions that animate our
scientific interets.

\subsection{Part 1: Causal Inference as Counterfactual Data
Science.}\label{part-1-causal-inference-as-counterfactual-data-science.}

The first step in answering a causal question is to ask it
(\citeproc{ref-hernuxe1n2016}{Hernán \emph{et al.} 2016a}).

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  What do causal quantity to I want to consistently estimate?
\item
  For which population does this knowledge generalise?
\end{enumerate}

Causal diagrams come after we have stated a causal question and the
population of interest, the `target population'. We begin by considering
what is required to state these questions precisely.

\paragraph{1.1.1 The fundamental problem of causal
inference}\label{the-fundamental-problem-of-causal-inference}

To ask a causal question, we must consider the concept of causality
itself. Consider an intervention, \(A\), and its effect, \(Y\). We say
that \(A\) causes \(Y\) if altering \(A\) would lead to a change in
\(Y\) (\citeproc{ref-hume1902}{Hume 1902};
\citeproc{ref-lewis1973}{Lewis 1973}). If altering \(A\) would not
change \(Y\), we say that \(A\) has no causal effect on \(Y\).

In causal inference, we aim to quantitatively contrast the potential
outcomes of \(Y\) in response to different levels of a well-defined
intervention. Commonly, we refer to such interventions as `exposures' or
`treatments;' we refer to the possible effects of interventions as
`potential outcomes.'

Consider a binary treatment variable \(A \in \{0,1\}\). For each unit
\(i\) in the set \(\{1, 2, \ldots, n\}\), when \(A_i\) is set to 0, the
potential outcome under this condition is denoted, \(Y_i(0)\).
Conversely, when \(A_i\) is set to 1, the potential outcome is denoted,
\(Y_i(1)\). We refer to the terms \(Y_i(1)\) and \(Y_i(0)\) as
`potential outcomes' because until realised, the effects of
interventions describe counterfactual states.

Suppose that each unit \(i\) receives either \(A_i = 1\) or \(A_i = 0\).
The corresponding outcomes are realised as \(Y_i|A_i = 1\) or
\(Y_i|A_i = 0\). For now, let us assume that each realised outcome under
that intervention is equivalent to one of the potential outcomes
required for a quantitative causal contrast, such that
\([(Y_i(a)|A_i = a)] = (Y_i|A_i = a)\). Thus when \(A_i = 1\),
\(Y_i(1)|A_i = 1\) is observed. However, when \(A_i = 1\), it follows
that \(Y_i(0)|A_i = 1\) is not observed:

\[
Y_i|A_i = 1 \implies Y_i(0)|A_i = 1~ \text{is counterfactual}
\]

Conversely, if \(A_i = 0\), we may assume the potential outcome
\(Y_i(0)|A_i = 0\)) is observed as \(Y_i|A_i = 0\). However, the
potential outcome \(Y_i(1)|A_i = 0\) is never realised and so not
observed:

\[
Y_i|A_i = 0 \implies Y_i(1)|A_i = 0~ \text{is counterfactual}
\]

We define \(\tau_i\) as the individual causal effect for unit \(i\) and
express the individual causal effect:

\[
\tau_i = Y_i(1) - Y_i(0)
\]

Notice that each unit can only be exposed to only one level of the
exposure \(A_i = a\) at a time. This implies that \(\tau_i\), is not
merely unobserved but inherently \emph{unobservable}.

That individual causal effects cannot be identified from observations is
known as `\emph{the fundamental problem of causal inference}'
(\citeproc{ref-holland1986}{Holland 1986};
\citeproc{ref-rubin1976}{Rubin 1976}).

\paragraph{1.1.2 Causal effects from randomised
experiments}\label{causal-effects-from-randomised-experiments}

Although it is not typically feasible to compute individual causal
effects. under certain assumptions, it may be possible to estimate
\emph{average} treatment effects -- also called `marginal effects.' We
define an average treatment effect (\(ATE\)) as the difference between
the expected or average outcomes under treatment and contrast conditions
for a pre-specified population, typically the population from which an
observed sample is drawn.

Consider a binary treatment, \(A \in \{0,1\}\)

\[
\text{Average Treatment Effect}  = \mathbb{E}[Y(1)] - \mathbb{E}[Y(0)].
\]

This is our prespecified estimand for our target population. A challenge
remains for computing these treatment-group averages, given that
individual causal effects are unobservable. We can frame the problem
framed by referring to the \emph{full data} required to compute this
estimand --- that is in terms of the complete counterfactual dataset
where the missing potential outcomes, inherent in observational data,
were somehow available. The text highlighted in red denotes inherently
missing responses over the joint distribution of the full counterfactual
dataset (also stated by underbraces). We find that for each treatment
condition, half the observations over the joint distribution of the
counterfactual data are inherently unobserved.

\[
\text{Average Treatment Effect} = \left(\underbrace{\underbrace{\mathbb{E}[Y(1)|A = 1]}_{\text{observed for } A = 1} + \underbrace{\textcolor{red}{\mathbb{E}[Y(1)|A = 0]}}_{\textcolor{red}{\text{unobserved for } A = 0}}}_{\text{effect among treated}}\right) - \left(\underbrace{\underbrace{\mathbb{E}[Y(0)|A = 0]}_{\text{observed for } A = 0} + \underbrace{\textcolor{red}{\mathbb{E}[Y(0)|A = 1]}}_{\textcolor{red}{\text{unobserved for } A = 1}}}_{\text{effect among untreated}}\right).
\]

Randomisation allows investigators to recover the treatment group
averages even though treatment groups contain inherently missing
observations. We do not require the joint distribution over the
full-data (i.e.~the counterfactual data) to obtain average treatment
groups. This is because when investigators randomise units into
treatment conditions, there is full adherence, and the sample is
sufficiently large to rule out chance differences in the composition of
the treatment groups to be compared, the distributions of confounders
that could explain differences in the potential outcomes will be
balanced across the conditions. Randomisation under such conditions
rules out explanations for difference in treatment group average except
the treatment. Put differently randomisation implies:

\[
\mathbb{E}[Y(0) | A = 1] = \mathbb{E}[Y(0) | A = 0]
\]

and

\[
\mathbb{E}[Y(1) | A = 1] = \mathbb{E}[Y(1) | A = 0]
\]

We assume, (by causal consistency, see: \(\S 1.2.1\)):

\[ \mathbb{E}[Y(1) | A = 1] = \mathbb{E}[Y| A = 1]\]

and

\[\mathbb{E}[Y(0) | A = 0] = \mathbb{E}[Y| A = 0]\]

It follows that the average treatment effect of the randomised
experiment can be computed:

\[
\text{The Average Treatment Effect} = \widehat{\mathbb{E}}[Y | A = 1] - \widehat{\mathbb{E}}[Y | A = 0].
\]

There are four critical aspects for how ideally randomised experiments
enable the estimation of average treatment effects worth highlighting.

First, the investigators must specify a population for whom they seek to
generalise their results. We refer to this population as the
\emph{target population}. If the study population differs from the
target population in the distribution of covariates that interact with
the treatment, the investigators will have no guarantees their results
will generalise (for discussions of sample/target population mismatch
refer to: Imai \emph{et al.}
(\citeproc{ref-imai2008misunderstandings}{2008}); Westreich \emph{et
al.} (\citeproc{ref-westreich2019target}{2019}); Westreich \emph{et al.}
(\citeproc{ref-westreich2017}{2017}); Pearl and Bareinboim
(\citeproc{ref-pearl2022}{2022}); Bareinboim and Pearl
(\citeproc{ref-bareinboim2013general}{2013}); Stuart \emph{et al.}
(\citeproc{ref-stuart2018generalizability}{2018}); Webster-Clark and
Breskin (\citeproc{ref-webster2021directed}{2021}).)

Second, because the units in the study sample at randomisation may
differ may differ from the units in the study after randomisation,
investigators must be careful to avoid biases that arise from
sample/population mismatch over time. If there is sample attrition or
non-response, the treatment effect investigators obtain for the sample
may differ from the treatment effect in the target population.

Third, a randomised experiment recovers the causal effect of random
treatment assignment, not of the treatment itself, which may differ if
some participants do not adhere to their treatment (even if they remain
in the study). The effect randomised assignment is called the
\emph{intent-to-treat effect}. The effect of perfect adherence is called
the \emph{per protocol effect} (\citeproc{ref-hernan2017per}{Hernán
\emph{et al.} 2017}; \citeproc{ref-lash2020}{Lash \emph{et al.} 2020}).
To obtain the per protocol effect for randomised experiments requires
the application of methods for causal inference in observational
settings.

Fourth, I have presented the average treatment effect on the difference
scale, that is, as a difference in average potential outcomes for the
target population under two distinct levels of treatment. However,
depending on the scientific question at hand, investigators may wish to
estimate causal effects on the risk-ratio scale, the rate-ratio scale
the hazard-ratio scale, or another scale. Where there are interactions
such that treatment effects vary across different strata of the
population, an estimate of the causal effect on the risk difference
scale will differ in at least one stratum to be compared from the
estimate on the risk ratio scale
(\citeproc{ref-greenland2003quantifying}{Greenland 2003}). The
sensitivity of treatment effects in the presence of interactions to the
scale of contrast underscores the importance of pre-specifying a scale
for the causal contrast investigators hope to obtain.

Fifth, investigators may uninentionally spoil randomisation by adjusting
for indicators that might be affected by the treatment, outcome, or
both, by excluding participants using attention checks, by collecting
covariate data that might be affected by the experimental conditions, by
failing to account for non-response and loss-to-follow up, and by
committing any number of other self-inflicted injuries. Unfortunately,
such practices are widespread (\citeproc{ref-montgomery2018}{Montgomery
\emph{et al.} 2018}). Notably causal graphical methods are useful for
describing causal identification in experiments (refer to Hernán
\emph{et al.} (\citeproc{ref-hernan2017per}{2017})), a topic we consider
elsewhere (\citeproc{ref-cite}{\textbf{cite?}})

Setting these considerations aside, understanding how randomisation
obtains the missing counterfactual outcomes that we require to
consistently estimate average treatment effects clarifies the tasks of
causal inference in non-experimental settings
(\citeproc{ref-hernuxe1n2008a}{Hernán \emph{et al.} 2008a};
\citeproc{ref-hernuxe1n2022}{Hernán \emph{et al.} 2022};
\citeproc{ref-hernuxe1n2006}{Hernán and Robins 2006a}).

\subsubsection{1.2 Fundamental Identification
Assumptions}\label{fundamental-identification-assumptions}

There are three fundamental identification assumptions that must be
satisfied to consistently estimate causal effects with data. These
assumptions are typically satisfied in randomised controlled
trials/experiments but not in real-world studies in which randomised
treatment assignment is absent.

\paragraph{1.2.1 Assumption 1: Causal
Consistency}\label{assumption-1-causal-consistency}

We satisfy the causal consistency assumption when, for each unit \(i\)
in the set \(\{1, 2, \ldots, n\}\), the observed outcome corresponds to
one of the specific counterfactual outcomes to be compared such that:

\[
Y_i^{observed}|A_i = 
\begin{cases} 
Y_i(a^*) & \text{if } A_i = a^* \\
Y_i(a) & \text{if } A_i = a
\end{cases}
\]

The causal consistency assumption implies that the observed outcome at a
specific exposure level equates to the counterfactual outcome for that
individual at the observed exposure level. Although it seems
straightforward to equate an individual's observed outcome with their
counterfactual outcome, treatment conditions vary, and treatment
heterogeneity poses considerable challenges for satisfying this
assumption. See: \textbf{Appendix A} for further discussion of how
investigators may obtain statisfy causal consistency in real-world
settings.

\paragraph{1.2.2 Assumption 2:
Positivity}\label{assumption-2-positivity}

We satisfy the positivity assumption if there is a non-zero probability
of receiving each treatment level for every combination of covariates
that occurs in the population. Where \(A\) is the exposure and \(L\) is
a vector of covariates, we say positivity is achieved if:

\[
0 < Pr(A = a | L = l) < 1, \quad \text{for all } a, l \text{ with } Pr(L = l) > 0
\]

There are two types of positivity violation:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Random non-positivity} occurs when an exposure is
  theoretically possible, but specific exposure levels are not
  represented in the data. Notably, random non-positivity is the only
  identifiability assumption verifiable with data.
\item
  \textbf{Deterministic non-positivity} occurs when the exposure is
  implausible by nature. For instance, a hysterectomy in biological
  males would appear biologically implausible.
\end{enumerate}

Satisfying the positivity assumption can present considerable data
challenges (\citeproc{ref-westreich2010}{Westreich and Cole 2010}).
Suppose we had access to extensive panel data that has tracked 20,000
individuals randomly sampled from the target population over three
years. Suppose further that we wanted to estimate a one-year causal
effect of weekly religious service attendance on charitable donations.
We control for baseline attendance to recover an incident exposure
effect estimate. Assume that the natural transition rate from no
religious service attendance to weekly service attendance is low, say
one in a thousand annually. In that case, the effective sample for the
treatment condition dwindles to 20. This example clarifies the problem.
For rare exposures, the data required for valid causal contrasts may be
sparse, even in large datasets. Where the positivity assumption is
violated, causal diagrams will be of limited utility because
observations in the data do not support valid causal inferences.
(\textbf{Appendix B} presents a worked example that illustrates the
difficulty of satisfying this assumption in a setting of a cultural
evolutionary questions.)

\paragraph{1.2.3 Assumption 3: Conditional Exchangeability (or no
unmeasured confounding or conditional
ignorability)}\label{assumption-3-conditional-exchangeability-or-no-unmeasured-confounding-or-conditional-ignorability}

We satisfy the conditional exchangeability assumption if the treatment
groups are conditionally balanced in the variables that could affect the
potential outcomes. In experimental designs, random assignment
facilitates satisfaction of the conditional exchangeability assumption.
In observational studies more effort is required. We must control for
any covariate that could account for observed correlations between \(A\)
and \(Y\) in the absence of a causal effect of \(A\) on \(Y\).

Let \(\coprod\) again denote independence. Let \(L\) denote the set of
covariates necessary to ensure this conditional independence. We satisfy
conditional exchangeability when:

\[
Y(a) \coprod A | L \quad \text{or equivalently} \quad A \coprod Y(a) | L
\]

Assuming conditional exchangeability is satisfied and the other
assumptions required for consistent causal inference also hold, we may
compute the average treatment effect (ATE) on the difference scale:

\[
\text{Average Treatment Effect} = \mathbb{E}[Y(1) | L] - \mathbb{E}[Y(0) | L]
\]

Note that in randomised controlled experiments, exchangeability is
unconditional. Although adjustment by interacting the treatment with
pre-treatment variables may improve efficiency and diminish threats to
randomisation from chance inbalances in measured covariates, it is a
confusion to think of such adjustment as ``control.''

In ``real-world'' observational studies, where measured confounders are
sufficient to ensure conditional exchangeability, we obtain obtain
estimates for the average treatment effect by conditioning on the
densities of measured confounders by treatment group. Where \(A=a\) and
\(A = a^*\) are the treatment levels we seek to contrast:

\[
\widehat{\text{ATE}} =  \sum_l \big( \mathbb{E}[Y(a^*) \mid L] - \mathbb{E}[Y(a) \mid L] \big) \times Pr(L)
\]

Which gives by causal consistency gives us: \[
\widehat{\text{ATE}} =  \sum_l \big( \mathbb{E}[Y \mid A = a^*, L] - \mathbb{E}[Y \mid A = a, L] \big) \times Pr(L)
\]

In the disciplines of cultural evolution, where experimental control is
impractical and we are left with ``real-world'' data, obtaining
consistent causal inferences hinge on the plausibility of satisfying
this `no unmeasured confounding' assumption. In \textbf{Appendix B} I
describe challenges for inferring causal effects from historical data.

However these assumptions occur down stream from the primary functions
of causal directed acyclic graphs, which we now ready to state:

The primary function of a causal directed acyclic graph is to identify
sources of bias that may lead to an association between an exposure and
outcome in the absence of causation. That is, causal directed acycic
graphs visually encode features of a causal order necessary to evaluate
the assumptions of conditional exchangeability -- also called
`no-unmeasured confounding', `ignorability', and -- in the idiom of
causal graphical models -- `d-separation'. Although directed acyclic
graphs can also be useful for addressing broader threats and
opportunities for causal inferences such as sample restriction,
transportability, and measurement error bias, they are designed to
evaluate the assumptions of conditional exchangeability, or equivalently
of non-unmeasured confounding, ignorability, or of d-separation between
the treatments and outcomes.

Perhaps the most frequent error in the deployment of causal graphical
models is using them to encode assumptions about the causal order
pertaining to a study. However, their function is merely to evaluate
those features of the causal order relevant to evaluating whether
balance in the variables that might affect both the treatment and the
outcome can be ensured from measured covariates.

\subsubsection{Summary of Part 1}\label{summary-of-part-1}

Causal data science is not ordinary data science. In causal data
science, the initial step involves formulating a precise causal question
that clearly identifies the exposure, outcome, and population of
interest. We must then satisfy the three fundamental assumptions
required for causal inference, which are implicit in the ideal of a
randomised experiment: causal consistency: ensuring outcomes at a
specific exposure level align with their counterfactual counterparts;
positivity: the existence of a non-zero probability for each exposure
level across all covariate; conditional exchangeability: the absence of
unmeasured confounding, or equivalently the `ignorability' of treatment
assignment, conditional on measured confounders.

\newpage{}

\subsection{Part 2: How Causal Directed Acyclic Graphs Clarify
Identification}\label{part-2-how-causal-directed-acyclic-graphs-clarify-identification}

We introduce causal directed acyclic graphs by describing the meaning of
our symbols.

\subsubsection{2.1 Variable Naming
Conventions}\label{variable-naming-conventions}

\begin{table}

\caption{\label{tbl-terminology}Variable naming conventions}

\centering{

\terminologylocalconventionssimple

}

\end{table}%

\textbf{\(X\)}: Denotes a random variable without reference to its role.

\textbf{\(A\)}: Denotes the ``treatment'' or ``exposure'' - a random
variable.

This is the variable for which we seek to understand the effect of
intervening on it. It is the ``cause.''

\textbf{\(A=a\)}: Denotes a fixed ``treatment'' or ``exposure.''

Random variable \(A\) is set to level \(A=a\).

\textbf{\(Y\)}: Denotes the outcome or response of an intervention.

It is the ``effect.''

\textbf{\(Y(a)\)}: Denotes the counterfactual or potential state of
\(Y\) in response to setting the level of the exposure to a specific
level, \(A=a\).

The outcome \(Y\) as it would be observed when, perhaps contrary to
fact, treatment \(A\) is set to level \(A=a\).

There are different conventions for expressing a potential or
counterfactual outcome, such as \(Y^a\), \(Y_a\).

\textbf{\(L\)}: Denotes a measured confounder or set of confounders.

This set, if conditioned upon, ensures that any differences between the
potential outcomes under different levels of the treatment are the
result of the treatment and not the result of a common cause of the
treatment and the outcome. Mathematically, we write this independence:

\[
Y(a) \coprod A \mid L
\]

\textbf{\(U\)}: Denotes an unmeasured confounder or confounders.

\(U\) is a variable or set of variables that may affect both the
treatment and the outcome, leading to an association in the absence of
causality, even after conditioning on measured covariates:

\[
Y(a) \cancel{\coprod} A \mid L \quad \text{[because of unmeasured } U]
\]

\textbf{\(Z\)}: Denotes a modifier of the treatment effect.

\(Z\) alters the magnitude or direction of the effect of a treatment
\(A\) on an outcome \(Y\).

\textbf{\(M\)}: Denotes a mediator, a variable that transmits the effect
of an exposure (or treatment) \(A\) on an outcome \(Y\).

\textbf{\(\bar{X}\)}: Denotes a sequence of variables, for example, a
sequence of treatments.

\textbf{\(\mathcal{R}\)}: Denotes a randomisation to treatment
condition.

\textbf{\(\mathcal{G}\)}: Denotes a graph, here, a causal directed
acyclic graph.

Note that investigators use a variety of different symbols. As long as
meanings are clear, the symbols we use are arbitrary.

\subsubsection{2.2 Our Conventions for Writing Causal Directed Acyclic
Graphs}\label{our-conventions-for-writing-causal-directed-acyclic-graphs}

The conventions we use to describe components of our causal graphs are
given in Table~\ref{tbl-general}.

\begin{table}

\caption{\label{tbl-general}Nodes, Edges, Conditioning Conventions.}

\centering{

\terminologygeneraldags

}

\end{table}%

\textbf{Node}: a node or vertex represents characteristics or features
of units within a population on a causal diagram -- that is a
``variable.'' In causal directed acyclic graphs, we draw nodes with
respect to the \emph{target population}, which is the population for
whom investigators seek causal inferences
(\citeproc{ref-suzuki2020}{Suzuki \emph{et al.} 2020}). Time-indexed
node: \(X_t\) denotes relative chronology; \(X_{\customphi{t}}\) is our
convention for indicating that timing is assumed, perhaps erroneously.

\textbf{Edge without an Arrow} (\(\association\)): path of association,
causality not asserted.

\textbf{Arrow} (\(\rightarrowNEW\)): denotes causal relationship from
the node at the base of the arrow (a parent) to the node at the tip of
the arrow (a child). We typically refrain from drawing an arrow from
treatment to outcome to avoid asserting a causal path from \(A\) to
\(Y\) because the function of a causal directed acyclic graph is to
evaluate whether causality can be identified for this path.

\textbf{Red Arrow} (\(\rightarrowred\)): path of non-causal association
between the treatment and outcome. Path is associational and may run
against arrows.

\textbf{Dashed Arrow} (\(\rightarrowdotted\)): denotes a true
association between the treatment and outcome that becomes partially
obscured when conditioning on a mediator, assuming \(A\) causes \(Y\).

\textbf{Dashed Red Arrow} (\(\rightarrowdottedred\)): highlights
over-conditioning bias from conditioning on a mediator.

\textbf{Open Blue Arrow} (\(\rightarrowblue\)): highlights effect
modification, which occurs when the levels of the effect of treatment
vary within levels of a covariate. We do not assess the causal effect of
the effect-modifier on the outcome, recognising that it may be
incoherent to consider intervening on the effect-modifier.

\textbf{Boxed Variable} \(\big(\boxed{X}\big)\): conditioning or
adjustment for \(X\).

\textbf{Red-Boxed Variable} \(\big(\boxedred{X}\big)\): highlights the
source of confounding bias from adjustment.

\textbf{Dashed Circle} \(\big( \circledotted{X}\big)\): no adjustment is
made for a variable (implied for unmeasured confounders.)

\textbf{\(\mathbf{\mathcal{R}}\)}
\(\big(\mathcal{R} \rightarrow A\big)\) randomisation into the treatment
condition.

\textbf{Presenting Temporal Order}: Causal directed acyclic graphs must
be --- as truth in advertising implies--- \emph{acyclic.} Directed edges
or arrows define ancestral relations. No descendant node can cause an
ancestor node. Therefore causal diagrams are, by default, sequentially
ordered.

Nevertheless, to make our causal graphs more readable, we adopt the
following conventions:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  The layout of a causal diagram is structured from left to right to
  reflect the assumed sequence of causality as it unfolds.
\item
  We often index our nodes using \(X_t\) to indicate their relative
  timing and chronological order, where \(t\) represents the time point
  or sequence in the timeline of events.
\item
  Where temporal order is uncertain or unknown, we use the notation
  \(X_{\phi t}\) to propose a temporal order that is uncertain.
\end{enumerate}

Typically, the timing of unmeasured confounders is not known, except
that they occur before the treatments of interest; hence, we place
confounders to the left of the treatments and outcomes they are assumed
to affect, but without any time indexing.

Again, both spatial and temporal organisation in a causal directed
acyclic graph are optional. Temporal order is implied by the
relationship of nodes and edges. However, explicitly representing order
in the layout of one's causal graph often makes it easier to evaluate,
and the convention representing uncertainty is useful, particularly when
the data do not ensure the relative timing of the occurrence of the
variable in a causal graph.

More generally, investigators use a variety of conventions to convey
causal structures. For example many will shade a node to denote
conditiong, rather than boxing it. Pearl uses double arrows to denote
unmeasured sources of confounding. There is no functional need to colour
paths, as we have done here -- we only do so highlight features of a
graph relevant to ascertaining identification. The open blue arrow
convention we use do denote effect modification is unconventional -- I
am not aware of others who use it. This convention mearly underscores
that we are not asserting direct causation in the relevant relations. An
arrow together with a brief explanation would have sufficed in place of
the open blue arrow. As long as meanings are clear, the symbols we use
are arbitrary.

Finally, note that all nodes and paths on causal graphs -- including the
absence of nodes and paths -- are asserted. Constructing causal diagrams
requires expert judgement of the scientific system under investigation.
It is a great power that we bestow upon those who construct causal
graphs! However this arrogation of power is unavoidable. Most relations
in a causal diagram must be asserted because the reality is
underdetermined by the data. Nevertheless, investigators should always
bear in mind that with great power comes great responsibility.
Investigators should transparently report the motivations supporting
their decisions about what to include and what not to include. These
decisions should be made independently of that data that have been
collection.

\subsubsection{2.3 Mathematical
Terminology}\label{mathematical-terminology}

The bottom panel of Table~\ref{tbl-general} illustrates the mathematical
notation we use to describe statistical and causal dependencies in the
distributions of variables within a population.

\textbf{Independence \(\coprod\)}: denotes independence. Depending on
the context, the meaning of ``independence'' may be statitical, causal,
or both. For example, \(A \coprod Y(a)\) indicates that the treatment
assignment \(A\) is independent of the potential outcomes \(Y(a)\).

Note that it is the distributions of \textbf{counterfactual outcomes}
that must be independent of the treatments. The distributions of
observed outcomes will depend on treatments wherever the treatments are
causally efficacious.

\textbf{Dependence \(\cancel{\coprod}\)}: denotes dependence. Again,
depending on the context, the meaning of ``independence'' may be
statitical, causal, or both. For example, \(A \cancel{\coprod} Y(a)\)
suggests that the treatment assignment \(A\) is related to the potential
outcomes \(Y(a)\), which could introduce bias into causal estimates.

\textbf{Conditioning \(|\)}: denotes conditioning on a variable. We can
express both conditional independence and conditional dependence:

\begin{itemize}
\tightlist
\item
  \textbf{Conditional Independence (\(A \coprod Y(a) \mid L\)):}
  indicates that once we account for a set of variables \(L\), the
  treatment \(A\) and the potential outcomes \(Y(a)\) are independent.
\item
  \textbf{Conditional Dependence (\(A \cancel{\coprod} Y(a) \mid L\)):}
  indicates that the treatment \(A\) and the potential outcomes \(Y(a)\)
  are not independent after conditioning on \(L\), or perhaps because we
  have conditioned on \(L\).
\end{itemize}

\subsubsection{2.4 How Causal Directed Acyclic Graphs Relate
Observations to
Counterfactuals}\label{how-causal-directed-acyclic-graphs-relate-observations-to-counterfactuals}

\paragraph{Ancestral Relations in Directed Acyclic
Graphs}\label{ancestral-relations-in-directed-acyclic-graphs}

We define the relation of ``parent'' and ``child'' on a directed acyclic
graph as follows:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Node \(A\) is a \textbf{parent} of node \(B\) if there is a directed
  edge from \(A\) to \(B\), denoted \(A \rightarrow B\).
\item
  Node \(B\) is a \textbf{child} of node \(A\) if there is a directed
  edge from \(A\) to \(B\), denoted \(A \rightarrow B\).
\end{enumerate}

It follows that a parent and child are \textbf{adjacent nodes} connected
by a directed edge.

We denote the set of all parents of a node \(B\) as \(\text{Pa}(B)\).

In a directed acyclic graph, the directed edge \(A \rightarrow B\)
indicates a statistical dependency where \(A\) may provide information
about \(B\). In a causal directed acyclic graph, the directed edge
\(A \rightarrow B\) is interpreted as a causal relationship such that
\(A\) is a direct cause of \(B\).

We further define the relations of \textbf{ancestor} and
\textbf{descendant} on a directed acyclic graph as follows:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Node \(A\) is an \textbf{ancestor} of node \(C\) if there exists a
  directed path from \(A\) to \(C\). Formally, \(A\) is an ancestor of
  \(C\) if there exists a sequence of adjacent nodes
  \((A, B_1, B_2, \ldots, B_k, C)\) such that
  \(A \rightarrow B_1 \rightarrow B_2 \rightarrow \cdots \rightarrow B_k \rightarrow C\).
\item
  Node \(C\) is a \textbf{descendant} of node \(A\) if there exists a
  directed path from \(A\) to \(C\). Formally, \(C\) is a descendant of
  \(A\) if there exists a sequence of adjacent nodes
  \((A, B_1, B_2, \ldots, B_k, C)\) such that
  \(A \rightarrow B_1 \rightarrow B_2 \rightarrow \cdots \rightarrow B_k \rightarrow C\).
\end{enumerate}

It follows that a node can have multiple ancestors and descendants.

\paragraph{Markov Factorisation and The Local Markov
Assumption}\label{markov-factorisation-and-the-local-markov-assumption}

Pearl (\citeproc{ref-pearl2009a}{2009}) p 52 asks us to imagine the
following. Suppose we have a distribution \(\mathcal{P}\) defined on n
discrete variables, \(X_1, X_2,  \dots, X_n\). By the chain rule, the
joint distribution for variables \(X_1, X_2, \dots, X_n\) on graph can
be decomposed into the product of \(n\) conditional distributions such
that we may obtain the following factorisation:

\[
\Pr(x_1, \dots, x_n) = \prod_{j=1}^n \Pr(x_j \mid x_1, \dots, x_{j-1})
\]

We translate nodes and edges on a graph into a set of conditional
independences that a graph implies over statistical distributions.

According to \textbf{the local Markov assumption,} given its parents in
a directed acyclic graph, a node is said to be independent of all its
non-descendants. Under this assumption we obtain what Pearl calls
Bayesian network factorisation, such that:

\[
\Pr(x_j \mid x_1, \dots, x_{j-1}) = \Pr(x_j \mid \text{pa}_j)
\]

This factorisation greatly simplifies the calculation of the joint
distributions encoded in the directed acyclic graph (causal or
non-causal) by reducing complex factorisations of the conditional
distributions in \(\mathcal{P}\) to simpler conditional distributions in
the set \(\text{PA}_j\), represented in the structural elements of a
directed acyclic graph (\citeproc{ref-lauritzen1990}{Lauritzen \emph{et
al.} 1990}; \citeproc{ref-pearl1988}{Pearl 1988},
\citeproc{ref-pearl1995}{1995}, \citeproc{ref-pearl2009a}{2009}).

\paragraph{Minimality Assumption}\label{minimality-assumption}

The Minimality assumption combines (a) the local Markov assumption with
(b) the assumption that adjacent nodes on the graph are dependent. This
is needed for causal directed acyclic graphs because the local Markov
assumption permits that adjacent nodes may be independent
(\citeproc{ref-neal2020introduction}{Neal 2020}).

\paragraph{Causal Edges Assumption}\label{causal-edges-assumption}

The causal edges assumption states that every parent is a direct cause
of its children. Given minimalism, the causal edges assumption allows us
to read causal dependence in directed acyclic graphs. In Pearl's
formalism, we use non-parametric structural equations to evaluate causal
assumptions using statistical distributions (refer to Appendix E; Neal
(\citeproc{ref-neal2020introduction}{2020})).

\paragraph{Compatibility Assumption}\label{compatibility-assumption}

The \textbf{compatibility assumption} ensures that the joint
distribution of variables aligns with the conditional independencies
implied by the causal graph. This assumption requires that the
probabilistic model conforms to the graph's structural assumptions.
Demonstrating compatibility directly from data is challenging, as it
involves verifying that all conditional independencies specified by the
causal directed acyclic graph (DAG) are present in the data. Therefore,
we typically assume compatibility rather than empirically proving it
(\citeproc{ref-pearl2009a}{Pearl 2009}).

\paragraph{Faithfulness}\label{faithfulness}

\textbf{Faithfulness} complements Markov Factorisation in causal
diagrams. A causal diagram is considered faithful to a given set of data
if all the conditional independencies present in the data are accurately
depicted in the graph. Conversely, the graph is faithful if every
dependency implied by the graph's structure can be observed in the data
(\citeproc{ref-hernan2024WHATIF}{Hernan and Robins 2024}). This concept
ensures that the graphical representation of relationships between
variables aligns with empirical evidence
(\citeproc{ref-pearl2009a}{Pearl 2009}).

The distinction between \textbf{weak faithfulness} and \textbf{strong
faithfulness} addresses the nature of observed independencies:

\begin{itemize}
\tightlist
\item
  \textbf{Weak faithfulness} allows for the possibility that some
  observed independencies might occur by coincidence, such as through a
  cancellation of effects among multiple causal paths.
\item
  \textbf{Strong faithfulness}, on the other hand, assumes that all
  observed statistical relationships reflect the underlying causal
  structure directly, with no independencies arising purely by chance.
  This stronger assumption is often more pragmatic in real-world
  applications of causal inference, where the complexities of data can
  obscure underlying causal relationships.
\end{itemize}

The faithfulness assumption, whether weak or strong, is not directly
testable from observed data. It is fundamentally a theoretical
assumption about the relationship between the observed data and the
underlying causal structure (\citeproc{ref-pearl2009a}{Pearl 2009}).

\subsubsection{2.5 The d-Separation
Criterion}\label{the-d-separation-criterion}

\textbf{d-Separation}: in a causal diagram, a path is `blocked' or
`d-separated' if a node along it interrupts causation. Two variables are
d-separated if all paths connecting them are blocked, making them
conditionally independent. Conversely, unblocked paths result in
`d-connected' variables, implying potential dependence
(\citeproc{ref-pearl1995}{Pearl 1995}, \citeproc{ref-pearl2009a}{2009}).
(Note that ``d'' stands for ``directed''.)

The rules of d-separation are as follows:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Fork rule} (\(B \leftarrowNEW \boxed{A} \rightarrowNEW C\)):
  \(B\) and \(C\) are independent when conditioning on \(A\)
  (\(B \coprod C \mid A\)).
\item
  \textbf{Chain rule} (\(A \rightarrowNEW \boxed{B} \rightarrowNEW C\)):
  Conditioning on \(B\) blocks the path between \(A\) and \(C\)
  (\(A \coprod C \mid B\)).
\item
  \textbf{Collider rule}
  (\(A \rightarrowNEW \boxed{C} \leftarrowNEW B\)): \(A\) and \(B\) are
  independent until conditioning on \(C\), which introduces dependence
  (\(A \cancel{\coprod} B \mid C\)). Judea Pearl proved these theorems
  in the 1990s (\citeproc{ref-pearl1995}{Pearl 1995},
  \citeproc{ref-pearl2009a}{2009}).
\end{enumerate}

According to these rules:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  An open path (no variables conditioned on) is blocked only if two
  arrows point to the same node: \(A \rightarrowred C \leftarrowred B\).
  The node of common effect (here \(C\)) is called a \emph{collider}.
\item
  Conditioning on a collider does not block a path, such that
  \(A \rightarrowred \boxed{C} \leftarrowred B\) can lead to an
  association between \(A\) with \(B\) in the absence of causation.
\item
  Conditioning on a descendant of a collider does not block a path, such
  that if \(C \rightarrowNEW \boxed{C'}\), then
  \(A \rightarrowred \boxed{C'} \leftarrowred B\) is open.
\item
  If a path does not contain a collider, any variable conditioned along
  the path is blocked, such that
  \(A \rightarrowNEW \boxed{B} \rightarrowNEW C\) blocks the path from
  \(A\) to \(C\). Refer to Pearl (\citeproc{ref-pearl2009a}{2009});
  Hernan and Robins (\citeproc{ref-hernan2023}{2023}), p.~78.
\end{enumerate}

\subsubsection{2.6 The Backdoor Path Criterion and Backdoor Path
Adjustment}\label{the-backdoor-path-criterion-and-backdoor-path-adjustment}

To obtain an unbiased estimate for the causal effect of \(A\) on \(C\),
we need to block all backdoor paths. We do this by conditioning on a set
of covariates \(B\) that is sufficient to close all backdoor paths
linking \(A\) and \(C\). A path is effectively blocked by \(B\) if it
includes at least one non-collider that is a member of \(B\), or if it
does not contain any collider or descendants of a collider.

Pearl (\citeproc{ref-pearl2009a}{2009}) p.173 defines the backdoor path
criterion more generally as follows: a set of variables \(Z\) satisfies
the backdoor criterion relative to variables \(X_i\) and \(X_j\) in a
causal directed acyclic graph \(\mathcal{G}\) if:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  No node in \(Z\) is a descendant of \(X_i\).
\item
  \(Z\) blocks every path between \(X_i\) and \(X_j\) that includes an
  arrow pointing into \(X_i\).
\end{enumerate}

Furthermore, we say that if \(X\) and \(Y\) are two disjoint subsets of
nodes in \(\mathcal{G}\), then \(Z\) meets the backdoor criterion
relative to \((X, Y)\) if it meets the criteria for any pair
\((X_i, X_j)\) where \(X_i \in X\) and \(X_j \in Y\) (Pearl
(\citeproc{ref-pearl2009a}{2009}), p 173).

The name ``backdoor'' refers to condition (2), which requires that only
paths with arrows pointing at \(X_i\) be blocked; these paths can be
viewed as entering \(X_i\) through the back door.

The backdoor criterion uses the rules of d-separation to identify and
block all paths in a causal diagram that could introduce bias between a
treatment (or exposure), \(A\), and an outcome, \(C\), in the absence of
causation (\citeproc{ref-pearl2009a}{Pearl 2009 p. p173}).

Pearl's backdoor path criterion gives rise to Pearl's backdoor path
adjustment theorem: if a set of variables \(Z\) satisfies the backdoor
criterion relative to \((X, Y)\) the causal effect of \(X\) on \(Y\) is
identifiable given the formula:

\[ \Pr(y|\hat{x}) = \sum_z \Pr(y|x,z) \times \Pr(z))\]

Backdoor path adjustment should look familiar. Recall the definition of
conditional exchangeability. We define \(A\) as the treatment (a random
variable) and \(L\) as the set of all measured confounders sufficient to
to ensure exchangability, or equivalently, to ensure ignorable treatment
assigment. Where causal consistency holds, we compute a causal contrast
from data by condition on the distributions of confounders were the
entire population exposed to each treatment condition that we compare:

\[
\widehat{\text{ATE}} =  \sum_l \big( \mathbb{E}[Y \mid A = a^*, L] - \mathbb{E}[Y \mid A = a, L] \big) \times Pr(L)
\] \footnote{note}

\paragraph{2.7 Frontdoor Path Criterion}\label{frontdoor-path-criterion}

To obtain an unbiased estimate for the causal effect of \(A\) on \(C\)
using the frontdoor criterion, we need to identify a set of variables
\(M\) that mediates the effect of \(A\) on \(C\).

Pearl defines the frontdoor criterion more generally as follows: a set
of variables \(B\) satisfies the frontdoor criterion relative to
variables \(A\) and \(C\) in a causal directed acyclic graph
\(\mathcal{G}\) if:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \(B\) is affected by \(A\).
\item
  \(B\) affects \(C\).
\item
  There are no backdoor paths from \(A\) to \(B\).
\item
  All backdoor paths from \(B\) to \(C\) are blocked by conditioning on
  \(A\).
\end{enumerate}

In other words, \(B\) must be an intermediate variable that captures the
entire causal effect of \(A\) on \(C\), with no confounding paths
remaining between \(A\) and \(B\), and any confounding between \(B\) and
\(C\) must be blocked by \(A\).

The frontdoor criterion is less widely used compared to the backdoor
criterion because it requires the identification of an appropriate
mediator that fully captures the causal effect
(\citeproc{ref-pearl2009a}{Pearl 2009}). Here, we state the frontdoor
path criterion for completeness.

\subsubsection{2.8 Comment on Pearl's Do-Calculus versus the Potential
Outcomes
Framework}\label{comment-on-pearls-do-calculus-versus-the-potential-outcomes-framework}

Here, we have developed counterfactual contrasts using the potential
outcomes framework, Pearl develops counterfactual contrasts using
operations on structural functionals, referred to as ``do-calculus''
(Appendix E.) In practice, whether one uses the do-calculus (and the
non-parametric structural equation models it relies on) or the potential
outcomes framework to interpret causal inferences is typically
irrelevant to identification results. However, there are theoretically
interesting debates about edge cases.

In some cases, Pearl's non-parametric structural equation models permit
the identification of contrasts that cannot be falsified under any
experiment (\citeproc{ref-richardson2013}{Richardson and Robins 2013a}).
Because advocates of non-parametric structual equation models treats
causality as primitive, they are less concerned with the requirement for
falsification Dı́az \emph{et al.} (\citeproc{ref-Diaz2023}{2023}). On the
other hand, some advocates of the potential outcomes framework require
falsifiability Shpitser and Tchetgen
(\citeproc{ref-shpitser2016causal}{2016}). Conversely, there are edge
cases where the potential outcomes framework achieves identification
while the do-calculus does not (\citeproc{ref-richardson2013}{Richardson
and Robins 2013a}).

I have presented the potential outcomes framework because it is easier
to interpret. Moreover, one does not need to be a verificationist to
adopt it. For most practical purposes, the two frameworks are equivalent
in terms of their utility for causal inference. Furthermore, readers
should be aware that there are causal diagrams called ``Single World
Intervention Graphs'' which enable investigators to represent
conditional independences of potential outcomes on graphs
(\citeproc{ref-richardson2013swigsprimer}{Richardson and Robins 2013b}),
which can be useful.

\#\#\# 2.9 Five Elementary Structures of Causality

\begin{table}

\caption{\label{tbl-fiveelementary}The five elementary structures of
causality from which all causal directed acyclic graphs can be built.}

\centering{

\terminologydirectedgraph

}

\end{table}%

Table~\ref{tbl-fiveelementary} presents five elementary structures of
causality from which all causal directed acyclic graphs are built.

\paragraph{Causal Relations With Two
Variables}\label{causal-relations-with-two-variables}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Causality Absent:} There is no causal effect between variables
  \(A\) and \(B\). They do not influence each other, denoted as
  \(A \coprod B\), indicating they are statistically independent.
\item
  \textbf{Causality:} Variable \(A\) causally affects variable \(B\).
  This relationship suggests an association between them, denoted as
  \(A \cancel{\coprod} B\), indicating they are statistically dependent.
\end{enumerate}

\paragraph{Causal Relations with Three
Variables}\label{causal-relations-with-three-variables}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  \textbf{Fork Relation:} Variable \(A\) causally affects both \(B\) and
  \(C\). Variables \(B\) and \(C\) are conditionally independent given
  \(A\), denoted as \(B \coprod C \mid A\). This structure implies that
  knowing \(A\) removes any association between \(B\) and \(C\) due to
  their common cause.
\item
  \textbf{Chain Relation:} A causal chain exists where \(C\) is affected
  by \(B\), which in turn is affected by \(A\). Variables \(A\) and
  \(C\) are conditionally independent given \(B\), denoted as
  \(A \coprod C \mid B\). This indicates that \(B\) mediates the effect
  of \(A\) on \(C\), and knowing \(B\) breaks the association between
  \(A\) and \(C\).
\item
  \textbf{Collider Relation:} Variable \(C\) is affected by both \(A\)
  and \(B\), which are independent. However, conditioning on \(C\)
  induces an association between \(A\) and \(B\), denoted as
  \(A \cancel{\coprod} B \mid C\). This structure is important because
  it suggests that \(A\) and \(B\), while initially independent, become
  associated when we account for their common effect \(C\).
\end{enumerate}

Understanding the basic relationships between two variables allows us to
build upon these to create more complex relationships. These
elementalary structures can be assembled in different combinations to
clarify the causal relationships that are are presented in a causal
directed acyclic graph. Such clarity is crucial for ensuring whether
confounders may be balanced across treatment groups to be compared,
conditional on measured co-variates, so that \(Y(a) \coprod A \mid L\).

\newpage{}

\subsubsection{2.10 Five Elementary Rules for
Identification}\label{five-elementary-rules-for-identification}

Table~\ref{tbl-terminologyconfounders} describe five elementary rules of
confounding control:

\begin{table}

\caption{\label{tbl-terminologyconfounders}Five elementary rules for
Causal Identifiation.}

\centering{

\terminologyelconfounders

}

\end{table}%

There are no shortcuts to reasoning about causality. Each causal
question must be asked in the context of a specific scientific question,
and each causal graph must be build under the best lights of domain
expertise. However, the following five elementary rules for confounding
control are implied by the theorems that underpin casuald directed
acyclic graphs. They may useful start for evaluating the prospects for
causal identification across a broad range of settings.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Ensure That Treatments Precedes Outcomes}: this rule is a
  logical consequence of our assumption that causality follows the arrow
  of time, and that a causal directed acyclic graph is faithful to this
  ordering. However, the assumption that treatments precede outcomes may
  be easily violated where investigators cannot ensure the relative
  timing of events from there data.
\end{enumerate}

Note that this assumption does raise concerns in settings where past
outcomes may not affect future treatments. Indeed, often an effective
strategy for confounding control in such settings is to condition on
past outcomes, and where relevant, one past treatments as well. For
example, if we wish to identify the cuasal effect of \(A_1\) on \(Y_2\),
if repeated-measures time series data are available, it may be useful to
condition such that \(\boxed{A_{-1}} \to \boxed{Y_0} \to A_1 ~~ Y_2\).
Critically, the relations of variables must be arranged sequentially
without cycles.

Note further that to estimate a causal effect of \(Y\) on \(A\) we would
focus on: \(\boxed{Y_{-1}} \to \boxed{A_0} \to Y_1 ~~ A_2\). Departing
from convention, here \(Y\) denotes the treatment and \(A\) denotes the
outcome. Graphs must be acyclic. Most processes in nature include
feedback loops. There is no contradition as long as we represent these
loops as sequential events.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\item
  \textbf{Condition on Common Causes or Their Proxies}: This rule
  applies to settings in which the treatment \(A\) and the outcome \(Y\)
  share common causes. By conditioning on these common causes, we block
  the open backdoor paths that could introduce bias into our causal
  estimates. Controlling for these common causes (or their proxies)
  helps to isolate the specific effect of \(A\) on \(Y\). Note that we
  do not draw a path from \(A \to Y\) in this context because it
  represents an interventional distribution. In a causal directed
  acyclic graph, conditioning does not occur on interventional
  distributions. We do not box \(A\) and \(Y\).
\item
  \textbf{Do Not Condition on a Mediator when estimating total effects}:
  this rule applies to settings in which the variable \(L\) is a
  mediator of \(A \to Y\). Recall Pearl's backdoor path criterion
  require that we do not condition on a decendant of the treatment.
  Here, conditioning on a \(L\)\$ violates the backdoor path criterion,
  risking bias for a total causal effect estimate. If we are interested
  in total effect estimates, we must not condition on a mediator. Note
  we draw the path from \(A \to Y\) to underscore that this specific
  overconditioning threat occurs in in the presence of a true treatment
  effect. As we consider below, over-conditioning bias can operate in
  the absence of a true-treatment effect. This is important because
  condiitoning on a mediator might create associations in the absence of
  causation. In many settings, ensuring accuracy in the relative timing
  of events in our data will prevent the self-inflicted injury of
  conditioning on a common effect of the treatment.
\item
  \textbf{Do Not Condition on a Collider}: this rule applies to settings
  in which we \(L\) is a common effect of \(A\) and \(Y\). Conditioning
  on a collider may invoke a spurious association. Again the backdoor
  path criterion require that we do not condition on a decendant of the
  treatment. We will not be tempted to condition on \(L\) if we knew
  that it was an effect of \(A\). In many settings, ensuring accuracy in
  the relative timing of events in our data will prevent the
  self-inflicted injury of conditioning on a common effect of the
  treatment and outcome.
\item
  \textbf{Proxy Rule: Conditioning on a Descendent Is Akin to
  Conditioning on Its Parent}: this rule applies to settings in which we
  \(L’\) is an effect from another variable \(L\). The graph considers
  when \(L’\) is downstream of a collider. Here again, in many settings,
  ensuring accuracy in the relative timing of events in our data will
  prevent the self-inflicted injury of conditioning on a common effect
  of the treatment and outcome.
\end{enumerate}

\newpage{}

\subsubsection{Summary Part 2}\label{summary-part-2}

We use causal directed acyclic graphs to represent and evalaute
structural sources of bias. We do not use these causal graphs to
represent the the entirity of the causal system in which we are
interested, but rather only those features necessary to evaluate
conditional exchangeabilty, or equivalently to evaluate d-separation.
Moreover, causal directed acyclic graphs should not be confused with the
structural equation models employed in the statistical structural
equation modelling traditions. Although Pearl's formalism is built upon
``Non-Parametric Structural Equation Models'', the term ``Structural
Equation Model'' is a false cognate. Causal graphs are structural
models, not statistical models. We create causal graphs before we embark
on statistical modelling. Their purpose is to clarify how to write
statistical models by eludicating which variables we must include in our
statistical models and equally important, which variables we must
exclude from our statistical models to avoid invalidating our causal
inferences. All causal graph are grounded in our assumptions about the
structures of causation. Although it is sometimes possible to use causal
diagrams for causal discovery, their primary (and original) use is to
evaluate the implications of assumptions.

This distinction between structural and statistical models is
fundamental because absent clearly defined causal contrasts and
carefully evaluated assumptions about structural sources of bias, the
statistical structural equation modelling tradition offers no guarantees
that the coefficients investigators recover are interpretable.
Misunderstanding this difference between structural and statistical
models has led to considerable confusion across the human sciences
(\citeproc{ref-bulbulia2022}{Bulbulia 2022}.;
\citeproc{ref-vanderweele2015}{VanderWeele 2015};
\citeproc{ref-vanderweele2022}{VanderWeele 2022};
\citeproc{ref-vanderweele2022b}{VanderWeele and Vansteelandt 2022})

\subsection{Part 3. How Causal Directed Acyclic Graphs Clarify The
Importance of Timing of Events Recorded in
Data}\label{part-3.-how-causal-directed-acyclic-graphs-clarify-the-importance-of-timing-of-events-recorded-in-data}

\begin{table}

\caption{\label{tbl-elementary-chronological-hyg}}

\centering{

\captionsetup{labelsep=none}

\terminologychronologicalhygeine

}

\end{table}%

As hinted at in the previous section, the five elementary rules of
confounding control reveal the importance of ensuring accurate timing in
the occurance of the variables whose structural features a causal
directed acyclic graph encodes. We begin by consider seven examples of
confounding problems resolved when such accuracy is ensured.

The first seven case-studies illustrate the focus that causal directed
acyclic graphs bring to fundmental imperative to ensure accurate timing
in the chronology of events recorded in data. These illustrations refer
to causal graph in Table~\ref{tbl-elementary-chronological-hyg}

\subsubsection{3.1 Reverse Causation}\label{reverse-causation}

Table~\ref{tbl-elementary-chronological-hyg} \(\mathcal{G} 3.1\)
illustrates bias from reverse causation. Suppose we were interested in
the causal effect of marriage on well-being. Suppose we observe married
people are happier than unmarried people. We would erroneously infer
that marriage causes happiness of happiness causes marriage (refer to
McElreath (\citeproc{ref-mcelreath2020}{2020})).

Table~\ref{tbl-elementary-chronological-hyg} \(\mathcal{G} 3.2\)
clarifies a response. Ensure that the treatment is observed before the
outcome is observed. Note further that the treatment, in this case, is
not clearly specified because ``marriage'' is unclear. There are least
four causal contrasts we might consider when thinking of ``marriage'',
namely:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \(Y(0, 0)\): The potential outcome when there is no marriage.
\item
  \(Y(0, 1)\): The potential outcome when there is marriage.
\item
  \(Y(1, 0)\): The potential outcome when there is divorce.
\item
  \(Y(1, 1)\): The potential outcome from marriage prevalance.
\end{enumerate}

Each of these outcomes allows for a specific contrast. There are
\(\binom{4}{2}\) unique contrasts? Which of the six unique contrasts do
we wish to contrast? Not only is it important to order our data in time,
`what is the causal effect of marriage on happiness' is ambiguous. We
must stated a causal contrast of interested in, given our substantive
research interests. This statement requires a sequences of exposures to
be contrasted, and a population over whom causal inferences are meant to
generalise.

\subsubsection{3.2 Confounding by Common
Cause}\label{confounding-by-common-cause}

Table~\ref{tbl-elementary-chronological-hyg} \(\mathcal{G} 3.2\)
illustrates confounding by common cause. Suppose there is a common
cause, \(L\), of the exposure, \(A\), and outcome, \(Y\). In this
setting, \(L\) may create a statistical association between \(A\) and
\(Y\), implying causation in its absence. Most human scientists will be
familiar of the threat to inference in this setting: a ``third
variable'' leads to association without causation.

Consider an example where smoking, \(L\), is a common cause of both
yellow fingers, \(A\), and cancer, \(Y\). Here, \(A\) and \(Y\) may show
an association without causation. If investigators were to scrub the
hands of smokers, this would not affect cancer rates.

Table~\ref{tbl-elementary-chronological-hyg} \(\mathcal{G} 3.2\)
clarifies a response. Condition on the common cause , smoking. Within
strata of smokers, non-smokers, there will be no association between
yellow fingers and cancer.

\subsubsection{3.3 Mediator Bias}\label{mediator-bias}

Table~\ref{tbl-elementary-chronological-hyg} \(\mathcal{G} 3.1\)
illustrates mediator bias. Conditioning on the effect of a treatment in
this graph partially blocks the flow of information from treatment to
outcome, biasing the total effect estimate.

Suppose investigators are interested in whether cultural `beliefs in big
Gods' \(A\) affect social complexity \(Y\). Suppose that `economic
trade', \(L\) is both a common cause of the treatment and outcome. To
address confounding by common cause we must condition on economic trade.
However, timing matters. If we condition on measurements that reflect
economic trade after the emergence of beliefs in big Gods, we may bias
our total effect estimate.

Table~\ref{tbl-elementary-chronological-hyg} \(\mathcal{G} 3.2\)
clarifies a response. Ensure that measurements of economic trade are
obtained for cultural histories before big-Gods arise. Do not condition
on post-treatment instances of economic trade.

\subsubsection{3.4 Collider Bias}\label{collider-bias}

Table~\ref{tbl-elementary-chronological-hyg} \(\mathcal{G} 4.1\)
illustrates collider bias. Imagine a randomised experiment investigating
the effects of different settings on individuals' self-rated health. In
this study, participants are assigned to either civic settings (e.g.,
community centres) or religious settings (e.g., places of worship). The
exposure of interest, \(A\), is the type of setting, and the outcome,
\(Y\), is self-rated health. Suppose there is no effect of setting on
self-rated health. However, suppose both setting and rated health
independently influence a third variable: cooperativeness. Specifically,
imagine religious settings encourage cooperative behaviour, and at the
same time, individuals with better self-rated health are more likely to
engage cooperatively. Now suppose the investigators decide to condition
on cooperativeness, which in reality is the common effect of an \(A\),
and the outcome \(Y\). Their rational might be to study the effects of
setting on health among those who are more cooperative, or perhaps to
`control for' cooperation in the health effects of religious setting. By
introducing such `control' the investigators would inadvertently
introduce collider bias, because the control variable is a common effect
of the exposure and the outcome.

Table~\ref{tbl-elementary-chronological-hyg} \(\mathcal{G} 4.2\)
clarifies a response. If the worry is that cooperativeness is a
confounder, ensure that cooperativeness is measured before the intiation
of exposures to religious settings. Note that in experimental settings
investigators do not have this worry, assuming randomisation succeeds
and samples are large. However, conditioning on a variable that is
associated with the outcome may improve estimation efficiency, and
safegaurds against random imbalance arising from sampling variability.

\subsubsection{3.5 Collider Proxy Bias}\label{collider-proxy-bias}

Table~\ref{tbl-elementary-chronological-hyg} \(\mathcal{G} 5.1\)
illustrates bias from conditioning on the proxy of a collider. Consider
again the scenario described in \(\sec 3.4\), however in place of
controlling for cooperativeness investigators control for charitable
donations, a proxy for cooperativeness. Here, because the control
variable is a decendant of a collider, conditioning on the proxy is akin
to conditioning on the the collider.

Table~\ref{tbl-elementary-chronological-hyg} \(\mathcal{G} 5.2\)
clarifies a response. Do not condition on charitable donations, an
effect of treatment.

\subsubsection{3.6 Post-Treatment Collider Stratification
Bias}\label{post-treatment-collider-stratification-bias}

Table~\ref{tbl-elementary-chronological-hyg} \(\mathcal{G} 6.1\)
illustrates post-treatment collider stratification bias. Consider a gain
an experiment investigating the effect of religious service on
self-rated health. Suppose we measure ``religiosity'' after the
experiment, although with other demographic data. Suppose further that
religious setting affects religiosity, as does an unmeasured confounder,
such as child-hood deprivation. Suppose that childhood deprivation
affects self-reported health. Although our experiment ensured
randomisation of the treatment, and therefore ensured there
no-unmeasured common cause of the treatment and outcome, conditioning on
the post-treatment variable ``religiosity'' opens a back-door path from
the treatment to the outcome. This path is
\(A_0 \association L_1 \association U \association Y_2\). We introduced
confounding into our randomised experiment.

Table~\ref{tbl-elementary-chronological-hyg} \(\mathcal{G} 6.2\)
clarifies a response. Do not condition on a variable that the treatment
may effect. (refer to Cole \emph{et al.} (\citeproc{ref-cole2010}{2010})
for theoretical examples; refer to Montgomery \emph{et al.}
(\citeproc{ref-montgomery2018}{2018}) for evidence of widespread
prevalance of post-treatment adjustment in political science
experiments).

\subsubsection{3.7 Conditioning on Past Treatments and Past Outcomes to
Control for Unmeasured
Confounders}\label{conditioning-on-past-treatments-and-past-outcomes-to-control-for-unmeasured-confounders}

Table~\ref{tbl-elementary-chronological-hyg} \(\mathcal{G} 7.1\)
illustrates the threat of unmeasured confounding. In ``real world''
studies this threat is ubiquitous.

Table~\ref{tbl-elementary-chronological-hyg} \(\mathcal{G} 7.2\)
clarifies a response. With at least three repeated measurements,
investigators may greatly reduce unmeasured confoudning by controlling
for past-measurements of the treatment as well as past-measurements of
the outcome. With such control, any unmeasured confounder would need to
be orthogonal to its effects at baseline (refer to VanderWeele \emph{et
al.} (\citeproc{ref-vanderweele2020}{2020})). Moreover, controlling for
past treatments allows investigators to estimate an \textbf{incident
exposure}, effect over \textbf{a prevelence expsoure effect.} The
prevalence exposure effect describes the effect of current or ongoing
exposures on outcomes. This effects risks pointing to erroneous
conclusions. The incident exposure targets initiation into treatment,
which is typically the effect we obtain from experiments to obtain the
incident exposure effect, we generally require that events in the data
can be accurately classified into at least three relative time intervals
(refer to Hernán \emph{et al.} (\citeproc{ref-hernuxe1n2016}{2016a});
Danaei \emph{et al.} (\citeproc{ref-danaei2012}{2012}); VanderWeele
\emph{et al.} (\citeproc{ref-vanderweele2020}{2020}); Bulbulia
(\citeproc{ref-bulbulia2022}{2022}))

\subsection{3.8. Summary Part 3}\label{summary-part-3}

Pearl's backdoor-adjustment theorm implies that the treatment, outcome,
and confounders are accuratedly recorded in time. Although timing is not
sufficient for obtaining valid inferences, we have considered that the
ability to order events in time is necessary for obtaining valid
inferences. Indeed, inattention to timing may invalidate experimental
inferences, as when investigator collect data on ``control'' variables
after randomisation into treatment.

\newpage{}

\subsection{Part 4 How Causal Directed Acyclic Graphs Clarify The
Insufficiency of the Timing of Events Recorded in
Data}\label{part-4-how-causal-directed-acyclic-graphs-clarify-the-insufficiency-of-the-timing-of-events-recorded-in-data}

We next present a series of illustrations that clarify ordering
variables in time is insufficient insurance against confounding biases.
Time is on your side but time is not enough. Again we read graphs by
rows from left to right, such that \(\mathcal{G} 1.1\) is the left-most
graph in a row, followed by \(\mathcal{G} 1.2 \ldots\). All graphs in
Part 4 refer to Table~\ref{tbl-chronology-notenough}.

\begin{table}

\caption{\label{tbl-chronology-notenough}Common confounding scenarios in
which chronology is not enough.}

\centering{

\terminologychronologicalhygeineNOTENOUGH

}

\end{table}%

\subsubsection{4.1. M-bias}\label{m-bias}

Table~\ref{tbl-chronology-notenough} \(\mathcal{G} 1.1\) illustrates the
threat of over-conditioning in pre-treatment variables. Suppose we hope
to estimate the effect of religious service attendance on charitable
donations. We obtain time-series data and include a rich set of
co-variates, including baseline measures of religious service and of
charity. Suppose there is no treatment effect. Suppose further that we
condition on measures of loyality, yet loyality neigher affects
religious service attendance or charitable giving. Measures of loyality
are included in the confounder set \(L\). However, imagine that loyalty
is affected by two unmeasured confounders. Imagine that one's childhood
upbrining (an unmeasured variable) affect both loyality and inclinations
to religious service, but not charitable giving. \(U_A\) denotes this
unmeasured confounder. Furthermore, suppose that wealth affect both
loyality and charitable giving, but not religious service. \(U_Y\)
denotes this unmeasured confounder. In this setting, because loyality is
a collider of the unmeasured confounders, conditioning on loyality opens
a path between treatment and outcome. This path is
\(A\association U_A \association U_Y \association Y\).

Table~\ref{tbl-chronology-notenough} \(\mathcal{G} 1.2\) clarifies a
response. If we are confident of that \(\mathcal{G} 1.1\) describes the
strucutral features of confounding, we should not condition on loyality.

\subsubsection{4.2 M-bias with where the pre-treatment collider is a
confounder}\label{m-bias-with-where-the-pre-treatment-collider-is-a-confounder}

Table~\ref{tbl-chronology-notenough} \(\mathcal{G} 2.1\) illustrates the
threat of incorrigible confounding. Imagine the scenario in
\(\mathcal{G} 1.1\) and \(\mathcal{G} 1.1\) but with one change.
Loyality is a indeed a common cause of religious service attendance, the
treatment, and charitable giving, the outcome. If we do not condition on
loyality we have unmeasured confounding. This is bad. If we condition on
loyality, as we have just considered, we also have unmeasured
confounding. This is also bad.

Table~\ref{tbl-chronology-notenough} \(\mathcal{G} 2.2\) clarifies a
response. Suppose that although we have not measured wealth we have
measured a surrogate of wealth, say neighbourhood deprivation.
Conditioning on this surrogate is akin to conditioning on the unmeasured
confounder; we should adjust for neighbourhood deprivation.

\subsubsection{4.3 Opportunities for post-treatment conditioning for
confounder
control}\label{opportunities-for-post-treatment-conditioning-for-confounder-control}

Table~\ref{tbl-chronology-notenough} \(\mathcal{G} 3.1\) illustrates the
threat, once again, of unmeasured confounder. Suppose we are interested
in whether curiosity affects educational attainment. The effect might be
unclear. Curiosity might increase attention but it might also increase
distraction. Consider an unmeasured genetic factor \(U\) that influences
both an curiosity and educational attainment, say anxiety. Suppose we do
not have early childhood measures of anxiety in our dataset. We have
unmeasured confounding. This is bad.

Table~\ref{tbl-chronology-notenough} \(\mathcal{G} 3.2\) clarifies a
response. Suppose \(U\) were also to affect melanin production in hair
follicles. If gray hair could be an effect of curiosity along the path
to educational attainment, and could not be an effect of educational
attainment, we could diminish unmeasured confounding by adjusting for
grey hair in adulthood. This example illustrates how conditioning on a
variable that occurs after the treatment has occurred, or even after the
outcome has completed, may prove useful for confounding control. When
considering adjustment strategies it is sometimes useful to consider
adjustment on post-treatment confounders.

\subsubsection{4.4 Residual Confounding After Conditioning on Past
Treatments and Past
Outcomes}\label{residual-confounding-after-conditioning-on-past-treatments-and-past-outcomes}

Table~\ref{tbl-chronology-notenough} \(\mathcal{G} 4.1\) illustrates the
threat of confounding even after adjusting for baseline measures of the
treatment and of the outcome. Imagine that childhood deprivation, and
unmeasured variable, affects both religious service attendance and and
charitable giving. Despite adjusting for religious status and charitable
giving at baseline, childhood deprivation might affect the indidence of
change in one or both variables inducing a longitudinal association
between religious service and charitable giving in the absence of a
causal association.

Table~\ref{tbl-chronology-notenough} \(\mathcal{G} 4.2\) clarifies a
response. This response pertains to all ``real-world'' observational
studies. Preform sensitivity analyses (refer to Linden \emph{et al.}
(\citeproc{ref-linden2020EVALUE}{2020})); seek negative controls(refer
to Hernan and Robins (\citeproc{ref-hernan2024WHATIF}{2024})).

\subsubsection{4.5 Intermediary Confounding in Causal
Mediation}\label{intermediary-confounding-in-causal-mediation}

Table~\ref{tbl-chronology-notenough} \(\mathcal{G} 5\) illustrates the
threat of treatment cconfounding in causal mediation. Imagine that the
treatment is randomised; there is no exposure outcome confounding. Nor
is there exposure mediator confounding. \(\mathcal{R} \to A\) ensures
that backdoor paths from the treatment to the outcome are closed.
Despite randomisation we may obtain biased results because the mediator
is not randomised. Suppose we are interested in the whether the effects
of COVID-19 lockdowns on psychological distress were mediated by levels
of satisfaction with the government. Suppose that assignment to COVID-19
lockdowns was random, and that time series data taken before the
COVID-19 provid comparable population level contrasts. Despite random
assignment to treatment, asssume that there are variables that may
affect both satisfaction with the government and psychological distress.
For example, job security or relationship satisfaction might plausible
function as common causes of the mediator (government satisfaction) and
the outcome (psychological distress). To obtain valid inference for the
mediator outcome path we must control for these common causes.

Table~\ref{tbl-chronology-notenough} \(\mathcal{G} 5\) reveals the
difficulty for estimation that decomposes the total effect of COVID-19
on Psychological Distress into the direct effect of COVID-19 that is not
mediated by satisfication with the goverment and the indirect effect
that is mediated. Let us assume that confounders of the mediator outcome
path are themselves potentially affected by the treatment. In this
example, imagine that COVID-19 lockdowns affect relationship
satisfaction because couples are trapped in captivity, so to speak.
Imagine further that COVID-19 lockdowns affect job-security. This is
reasonable if one owns a street facing business. If we adjust for these
intermediatory variables along the path between the treatment and
outcome we will partialy block the treatment mediator path. This means
that we will not be able to obtain an natural indirect effect estimate
for that decomposes the effect of the treatment into that part that goes
through intermediary path \(A \to V \to M \to Y\) and that part that
goes through the mediated path independently of \(V\), namely
\(A \to V \to M \to Y\), at least not without additional assumptions and
methods to disentangle seperable effects or that permute the estimand
(refer to Dı́az \emph{et al.} (\citeproc{ref-Diaz2023}{2023}); Shpitser
\emph{et al.} (\citeproc{ref-shpitser2022multivariate}{2022}); Stensrud
\emph{et al.} (\citeproc{ref-stensrud2023conditional}{2023})). However
it may be possible to estimate controlled direct effects -- that is,
direct effects when the mediator is fixed to different levels (refer to
Greenland \emph{et al.} (\citeproc{ref-greenland1999}{1999});
VanderWeele (\citeproc{ref-vanderweele2015}{2015}))

\subsubsection{4.6. Treatment Confounder Feedback in Sequential
Treatments}\label{treatment-confounder-feedback-in-sequential-treatments}

Table~\ref{tbl-chronology-notenough} \(\mathcal{G} 6\) illustrates the
threat of treatment confounder feedback in sequential treatment regimes.
Suppose we are interested in whether beliefs in big gods affect social
complexity, suppose that beliefs in big gods affect economic trade, and
that economic trade may affect beliefs in big gods and social
complexity. Suppose the historical record is fragmented such that there
are unmeasured variables that affect both trade and social complexity.
Even if these unmeasured variables do not affect the treatment,
conditioning on the \(L\) a confounder and sequential treatment opens a
backdoor path \(A \association L \association U \association Y\). We
have confounding.

Table~\ref{tbl-chronology-notenough} \(\mathcal{G} 6\) reveals the
difficulty of sequentially estimating causal effects. To estimate an
effect requires special estimators, under the assumption of sequential
randomisation for fixed treatments, and the assumption of strong
sequential randomisation for time-varying treatments -- that is, for
treatments whose levels depend on past treatments and confounders (refer
to Robins (\citeproc{ref-robins1986}{1986}); Hernán \emph{et al.}
(\citeproc{ref-hernan2004STRUCTURAL}{2004}); Van Der Laan and Rose
(\citeproc{ref-vanderlaan2011}{2011}); Van Der Laan and Rose
(\citeproc{ref-vanderlaan2018}{2018}); Haneuse and Rotnitzky
(\citeproc{ref-haneuse2013estimation}{2013}); Young \emph{et al.}
(\citeproc{ref-young2014identification}{2014}); Rotnitzky \emph{et al.}
(\citeproc{ref-rotnitzky2017multiply}{2017}); Richardson and Robins
(\citeproc{ref-richardson2013}{2013a}); Dı́az \emph{et al.}
(\citeproc{ref-diaz2021nonparametric}{2021}); Williams and Díaz
(\citeproc{ref-williams2021}{2021}); Hoffman \emph{et al.}
(\citeproc{ref-hoffman2023}{2023})).

Importantly, we have we have six potential contrasts for the two
sequential treatments: beliefs in big gods at both time points vs
beliefs in big gods at neither time point; beliefs in big gods first,
then lost vs never believe in in big gods at both

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.1333}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.5467}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3200}}@{}}
\caption{Table outlines four fixed treatment regimens and six causal
contrasts in time-series data where exposure
varies.}\label{tbl-regimens}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Type
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Description
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Counterfactual Outcome
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Type
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Description
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Counterfactual Outcome
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Regime & Always believe in big gods & \(Y(1,1)\) \\
Regime & Never believe in big gods & \(Y(0,0)\) \\
Regime & Believe once first, then scepticism & \(Y(1,0)\) \\
Regime & Start with scepticism, then believe & \(Y(0,1)\) \\
Contrast & Always believe vs.~Never believe & \(E[Y(1,1) - Y(0,0)]\) \\
Contrast & Always believe vs.~Treat once first &
\(E[Y(1,1) - Y(1,0)]\) \\
Contrast & Always believe vs.~Treat once second &
\(E[Y(1,1) - Y(0,1)]\) \\
Contrast & Never believe vs.~Treat once first &
\(E[Y(0,0) - Y(1,0)]\) \\
Contrast & Never believe vs.~Treat once second &
\(E[Y(0,0) - Y(0,1)]\) \\
Contrast & Believe once first vs.~Believe once second &
\(E[Y(1,0) - Y(0,1)]\) \\
\end{longtable}

We can compute six causal contrasts for these four fixed regimens, as
shown in Table~\ref{tbl-regimens}.

A limitation of directed acyclic causal diagrams is that we do not
project factorisations of the counterfactual contrasts onto the graphs
themselves. To evaluate counterfactual identification, it can be helpful
to use Single World Intervention Graphs (refer to Robins and Richardson
(\citeproc{ref-robins2010alternative}{2010}); Richardson and Robins
(\citeproc{ref-richardson2013swigsprimer}{2013b}); Richardson and Robins
(\citeproc{ref-richardson2023potential}{2023})).

\subsubsection{4.7 Collider Stratification Bias in Sequential
Treatments}\label{collider-stratification-bias-in-sequential-treatments}

Table~\ref{tbl-chronology-notenough} \(\mathcal{G} 7\) illustrates the
threat of counfounding bias in sequential treatments even without
treatment confounder feedbak. Assume the setting as \(\mathcal{G} 6.1\)
with two differences. First, assume that the treatment, beliefs in big
gods, does not affect trade networks. However assume that an umeasured
confounder affects both the beliefs in big gods and the confounder,
trade networks. Such a confounder might openess to outsiders, a feature
of ancient cultures for which no clear measures are available. We need
not imagine that a treatment affect future states of confounders for
time-varying confounding. It would be sufficient to induce bias for an
unmeasured confounder to affect the treatment and the confounder, in the
presence of another confounder that affects both the confounder and the
outcome.

Table~\ref{tbl-chronology-notenough} \(\mathcal{G} 7\) reveals the
challenges of sequentially estimating causal effects. Yet again, to
estimate causal effects here requires requires special estimators, under
the assumption of sequential randomisation for fixed treatments, and the
assumption of strong sequential randomisation for time-varying
treatments (refer to Robins (\citeproc{ref-robins1986}{1986}); Hernán
\emph{et al.} (\citeproc{ref-hernan2004STRUCTURAL}{2004}); Van Der Laan
and Rose (\citeproc{ref-vanderlaan2011}{2011}); Van Der Laan and Rose
(\citeproc{ref-vanderlaan2018}{2018}); Haneuse and Rotnitzky
(\citeproc{ref-haneuse2013estimation}{2013}); Young \emph{et al.}
(\citeproc{ref-young2014identification}{2014}); Rotnitzky \emph{et al.}
(\citeproc{ref-rotnitzky2017multiply}{2017}); Richardson and Robins
(\citeproc{ref-richardson2013}{2013a}); Dı́az \emph{et al.}
(\citeproc{ref-diaz2021nonparametric}{2021}); Williams and Díaz
(\citeproc{ref-williams2021}{2021}); Hoffman \emph{et al.}
(\citeproc{ref-hoffman2023}{2023})). We note again that a specific
causal contrast must be stated, and we must ask, for which cultures do
causal effects generalise.

Readers should be aware that merely applying currently popular tools of
time-series data anlysis -- multi-level models and structural equation
models -- will not overcome the threats of confounding in sequential
treatments. Applying models to data will not recover consistent causal
effect estimates. Again, space contrainsts prevent us from discussing
statistical estimands and estimation.

\subsubsection{4.8 Summary Part 4.}\label{summary-part-4.}

Directed acyclic graphs reveal that ensuring the timing of events in
one's data does not ensure identification. Indeed when discussing
mediation we considered that certain mediated effects cannot be
identified by any data. Yet across the human sciences we readily apply
statistical models to data and interpret the outputs of these models as
meaningful. If the reader takes nothing else from this article, it is
that causal diagrams reveal that standard approaches, no matter how
sophisticated and considered, are hazardous.

\subsection{Part 5. Creating Causal Diagrams: Pitfalls and Tips
\{sec-how-to-create-causal-diagrams\}}\label{part-5.-creating-causal-diagrams-pitfalls-and-tips-sec-how-to-create-causal-diagrams}

The primary interest of causal diagrams is to address
\textbf{identification problems}. Pearl's backdoor adjustment theorem
proves that if we adopt and adjustment set such that \(A\) and \(Y\) are
d-separated, and furthermore do not condition on a variable along the
path from \(A\) to \(Y\), association is causation.

Here is how investigators may construct safe and effective directed
acyclic graphs.

\paragraph{Step 1. Clarify The Causal Question and Target
Population.}\label{step-1.-clarify-the-causal-question-and-target-population.}

An identification strategy is relative to the question at hand. The
adjustment criteria for estimating an effect of \(A\) on \(Y\) will
generally differ from the adjustment criteria for estimating an effect
of \(Y\) on \(A\), for example. Before attempting to draw any causal
diagram, state the problem your diagram addresses and the population to
whom itapplies. Consider further that in adopting a specific
identification strategy for a treatment or set of treatments, the
coefficients we obtain for the other variables in the model will often
be biased causal effect estimates for those variables. Moroever the
coefficients we obtain from statistical models we develop to estimate
causal effects will typically not have a marginal interpretation unless
we employ marginal structural models (refer to Cole and Hernán
(\citeproc{ref-cole2008}{2008}); VanderWeele
(\citeproc{ref-vanderweele2009a}{2009b})). This structural implication
has wide-ranging implications for scientific reporting. For example, if
regression coefficients are reported at all, they should be reported
with clearly labelled warnings against intretating them as having any
meaning or interpretation (refer to Westreich and Greenland
(\citeproc{ref-westreich2013}{2013}); McElreath
(\citeproc{ref-mcelreath2020}{2020}); Bulbulia
(\citeproc{ref-bulbulia2023}{2023}).) Powerful machine learning
algorithims treat these parameters as nuisance, and in many cases
coefficients cannot be obtained. Referees of human science journals will
need to be alerted to this fact, and retrained.

\paragraph{Step 2. Consider Whether the Three Fundamental Assumptions
for Causal Inference May be
Satisfied.}\label{step-2.-consider-whether-the-three-fundamental-assumptions-for-causal-inference-may-be-satisfied.}

Merely because we are in possession of data, even if the data are richly
detailed time-series data, does not entail our causal questions will
find answers. Along with identification, we must also consider the
causal consistency and positivity assumptions.

\paragraph{Step 3. Clarify the meanings symbols and
conventions}\label{step-3.-clarify-the-meanings-symbols-and-conventions}

It is fair to say that the state of terminology in causal inference is a
dog's breakfast. Meanings and conventions vary not only for terminology
but also for causal graphs. For example, where as we have denoted
unmeasured confounders using the variable \(U\), those who follow Pearl
will often draw a bi-directional arrow. Although investigators will have
their preferences, there is generally little substantive interest in
one's conventions, only that they are made clear, frequently repeated
(as I have done for each graph table), and applied correctly.

Although we have covered too many graphs to adopt a strategy in which
the contents of each graph are stated in words, doing so can aid
understanding in those with visual impairments who lack translational
technology.

\paragraph{Step 4. Include all common causes of the exposure and
outcome}\label{step-4.-include-all-common-causes-of-the-exposure-and-outcome}

Once we have stated our causal question, we are ready to crete a draft
of our causal graph. This graph should incorporate the most recent
common causes (parents) of both the treatment and the outcome, or where
measures are not available, measures for proxies that are available.

Where possible, aggregate functionally similar common causes into a
single variable notation. For example, include all functionally similar
demogrphic variables in \(L_0\).

Recall that a causal direct acyclic graph \emph{asserts} structural
assummptions. Merely because one has become expert in crafting causal
diagrams does not ensure that one will be expert in enconding plausible
structural assumptions. The processes of creating an revising them
should be detailed in published research, typically in supplements.

\paragraph{Step 4. Consider Potential Unmeasured
Confounders}\label{step-4.-consider-potential-unmeasured-confounders}

We leverage domain expertise not merely to clarify measured sources of
counfounding but also -- and perhaps most importantly -- to clarify
possible unmeasured confounding. We should include these in our causal
diagrams.

Note that because we cannot gaurd against all unmeasured confounding we
will always need to perform sensitivity analyses and consider negative
control treatments, negative control outcomes, instrumental variables,
and other strategies for improving the clarity for our causal
inferences, and their limitations.

\paragraph{Step 5. Ensure the causal directed acyclic graph is
acyclic.}\label{step-5.-ensure-the-causal-directed-acyclic-graph-is-acyclic.}

Although not strictly necessary, it may be useful to annotate the
temporal sequence of events using subscripts (e.g., \(L_0\), \(A_1\),
\(Y_2\)), as we have done here. Moreover, spatially ordering your
directed acylic to reflect the progression of causality in time - either
left-to-right or top-to-bottom -- will often enhances the graphs
comprehensibility. Again, although establishing temporal ordering is not
sufficient for addressing identification problems it is necessary.

\paragraph{Step 6. Represent paths structurally, not
parametrically}\label{step-6.-represent-paths-structurally-not-parametrically}

Whether a path is linear is unimportant for causal identification -- and
remember causal diagrams are tools for causal identification. Focus on
whether paths exist, not their functional form (linear, non-linear,
etc.).

Consider a subway map of Paris. We do not include all the streets on
this map, all notworthy sites, a detailed overview of the holdings by
room in the Louvre. We use other maps for these purposes. Almost every
detail about assumed reality must be left out of a causal diagram if it
is to be useful for causal identification.

Parametric descriptions are simply not not relevant for evaluating bias
in a causal diagram.

\paragraph{Step 7. Minimise paths to those necessary for addressing an
identification
problem}\label{step-7.-minimise-paths-to-those-necessary-for-addressing-an-identification-problem}

Reduce clutter; only include paths critical for a specific question
(e.g., backdoor paths, mediators). For example in
Table~\ref{tbl-chronology-notenough} \(\mathcal{G} 6\) and
Table~\ref{tbl-chronology-notenough} \(\mathcal{G} 7\) I did not draw
arrows form the first treatment to the second treatment. Although I
assume that such arrow exist drawing them was not, in these examples,
relevant to evaluating the identification problem at hand.

\paragraph{Step 8. When Temporal Order is Unknown Explicitly Represent
This Uncertainty On Your Causal
Diagram}\label{step-8.-when-temporal-order-is-unknown-explicitly-represent-this-uncertainty-on-your-causal-diagram}

In many settings the relevant timing of events cannot be ascertained
with confidence. Here we have adopted a convention of indexing nodes
with uncertain timing using \(X_{\phi t}\) notation. There is no widely
adopted convention for representing uncertainty in timing. We are only
bound to be clear.

\paragraph{Step 9. Even When Structural Features are Uncertain Certain
Create, Report, and Deploy Multiple
Graphs}\label{step-9.-even-when-structural-features-are-uncertain-certain-create-report-and-deploy-multiple-graphs}

Causal inference turns on assumptions. Experts might disagree. Where the
structure of reality encoded in a causal graph is uncertain or debated
investigators should produce multiple causal diagrams that reflect
uncertainties and debates.

By stating different assumptions and adopting multiple modelling
strategies that accord to these different assumptions need not be
confusing. For example, we might discover that our causal conclusions
are robust to differences in structural assuptions. Even where the
implications of different structural assumptions lead to opposing causal
inferences, such knowledge might better direct future data collection to
eventually settle such differences. The overarching requirement of
causal inference, as with other areas of science, is to truthfully
advance empirical understanding. Assertions are poor substitutes for
honesty. Rather than asserting one causal directed graph investigators
might want to follow the implications of several

\paragraph{Step 10. Use Automated Identification Algorthims With
Care}\label{step-10.-use-automated-identification-algorthims-with-care}

Automated software can assist with identification tasks, which amount to
factorising typically complex conditional independencies. However,
automated software may not converge on identifying the optimal set of
confounders in the presence of intractable confounding.

Consider Tyler Vanderweele's \textbf{modified disjunctive cause
criterion}. VanderWeele (\citeproc{ref-vanderweele2019}{2019})
recommends obtaining a maximally efficient adjustment which he calls a
`confounder set' A member of this set is any set of variables that can
reduce or remove a structural sources of bias. The strategy is as
follows:

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\tightlist
\item
  Control for any variable that causes the exposure, the outcome, or
  both.
\item
  Control for any proxy for an unmeasured variable that is a shared
  cause of both the exposure and outcome.
\item
  Define an instrumental variable as a variable associated with the
  exposure but does not influence the outcome independently, except
  through the exposure. Exclude any instrumental variable that is not a
  proxy for an unmeasured confounder from the confounder set
  (\citeproc{ref-vanderweele2019}{VanderWeele 2019}).
\end{enumerate}

I do not think we can do better than Vanderweele's modified disjunctive
cause criterion when selecting an optimal confounder set. However this
set might not, and likely will not remove all structural sources of
confounding bias in most observational settings. As such, an automated
algorithm might reject it. However, we might think that such rejection
is unwise, for example, because where assignment to treatment has not
been randomised we should almost always draw relations of unmeasured
confoudning on our causal graphs. Rejecting causal inferences in
observational settings entirely would be unwise because there are many
examples in which they closely approximate randomised control trials
(refer to Hernán \emph{et al.} (\citeproc{ref-hernan2016}{2016b});
Hernán and Robins (\citeproc{ref-hernan2006estimating}{2006b}); Hernán
\emph{et al.}
(\citeproc{ref-hernan2008aObservationalStudiesAnalysedLike}{2008b}))

For example, return to Table~\ref{tbl-chronology-notenough}
\(\mathcal{G} 2.1\) we encountered intractable confounding. What if
there were no proxy for an unmeasured confounder? Should we condition on
the measured confounder and induce M-bias or leave the backdoor path
from the measured confounder open. Or should we simply not attempt
causal inferences? The answer will depend on assumptions about the
relative strength of confounding in the graph. In place of a generic
strategy we require subject-specialist expertise.

\subsection{Summary}\label{summary}

Accurate timing in the sequence of events encoded in data is
insufficient for identification. Causal directed acyclic graphs show
this.

\subsection{Conclusions}\label{conclusions}

\paragraph{Limitations}\label{limitations}

First, I have focused on the application of causal directed acyclic
graphs to evaluating confounding bias. However, these causal graphs can
also be extended to evaluate measurement-error biases and some features
of sampling restriction bias (often called `selection biases') (refer to
Hernán (\citeproc{ref-hernan2017SELECTIONWITHOUTCOLLIDER}{2017}); Liu
\emph{et al.} (\citeproc{ref-liu2023application}{2023}); Hernan and
Robins (\citeproc{ref-hernan2024WHATIF}{2024}); Hernán and Cole
(\citeproc{ref-hernan2009MEASUREMENT}{2009}); VanderWeele and Hernán
(\citeproc{ref-vanderweele2012MEASUREMENT}{2012})). Valid causal
inferences demand addressing all structural sources of bias. I have not
aimed for complete coverage, but I hope this work stimulates curiosity.

Second, although causal directed acyclic graphs are powerful tools for
addressing identification problems, they are not the only graphical
tools researchers use to investigate causality. For example, Robins
(\citeproc{ref-robins1986}{1986}) developed the ``finest fully
randomized causally interpreted structured tree graph (FFRCISTG),''
which has been more recently revived and simplified in Single World
Intervention Graphs (refer to Richardson and Robins
(\citeproc{ref-richardson2013swigsprimer}{2013b})). Such graphs
explicitly factorise counterfactual states, which can be helpful for
identification in complex longitudinal settings. Moreover, for some, the
representation of counterfactual states on a graph is more satisfying,
as it allows inspection of the conditional independences of expectations
over \(Y(a^*)\) and \(Y(a)\) separately on a graph.

Third, we have not discussed statistical estimands, statistical
estimation, and the interpretations and reporting of causal inferences,
all of which come downstream of causal graphs in the workflows of causal
inference. Rapid developments in machine learning present applied
researchers with new tools for handling model misspecification (Van Der
Laan and Rose (\citeproc{ref-vanderlaan2018}{2018}); Laan and Gruber
(\citeproc{ref-van2012targeted}{2012}); Dı́az \emph{et al.}
(\citeproc{ref-diaz2021nonparametric}{2021}); Williams and Díaz
(\citeproc{ref-williams2021}{2021}); Hoffman \emph{et al.}
(\citeproc{ref-hoffman2023}{2023})) and for assessing treatment effect
heterogeneity (Athey \emph{et al.} (\citeproc{ref-athey2019}{2019});
Athey and Wager (\citeproc{ref-athey2021}{2021}); Wager and Athey
(\citeproc{ref-wager2018}{2018}); Vansteelandt and Dukes
(\citeproc{ref-vansteelandt2022a}{2022})). Those interested in workflows
within the human sciences might consider VanderWeele \emph{et al.}
(\citeproc{ref-vanderweele2020}{2020}); the workflows in my research
group may be found here: Bulbulia
(\citeproc{ref-bulbulia2024PRACTICAL}{2024}). However, I expect all such
workflows will evolve considerably in the near future.

Nevertheless, after precisely stating our causal question, the most
difficult and important challenge will be considering whether and how it
might be identified in data. I believe the ``statistical models first''
approach that is routinely applied in most human sciences is soon
ending. This approach has been attractive because it is relatively easy
to implement -- the methods do not require extensive training -- and
because the application of statistical models to data appears rigorous.
However, if the coefficients we recover from these methods have meaning,
this is typically accidental. It is not merely that the coefficients are
uninformative about what works and why; absent a causal framework, they
do not have any meaning (\citeproc{ref-ogburn2021}{Ogburn and Shpitser
2021}).

\subsubsection{On the Priority of
Assumptions}\label{on-the-priority-of-assumptions}

We might wonder, ``if not from the data, where do our assumptions about
causality come from?'' We have said that they must come from expert
knowledge. This reliance on expert knowledge might seem counterintuitive
for building scientific knowledge---shouldn't we use data to build
knowledge, not the other way around? Isn't scientific history a record
of expert opinions being undone?

The Austrian philosopher Otto Neurath famously described scientific
progress using the metaphor of a ship that must be rebuilt at sea:

\begin{quote}
\ldots{} every statement about any happening is saturated with
hypotheses of all sorts and that these in the end are derived from our
whole world-view. We are like sailors who on the open sea must
reconstruct their ship but are never able to start afresh from the
bottom. Where a beam is taken away a new one must at once be put there,
and for this the rest of the ship is used as support. In this way, by
using the old beams and driftwood, the ship can be shaped entirely anew,
but only by gradual reconstruction. (\citeproc{ref-neurath1973}{Neurath
1973 p. 199})
\end{quote}

This quotation emphasises the iterative process that accumulates
scientific knowledge; new insights are cast from the foundation of
existing knowledge.

Causal diagrams are at home in Neurath's boat. The tradition of science
that believes knowledge develops solely from the results of statistical
tests applied to data should be resisted. The data have never fully
contained the answers we seek. When reconstructing science, we have
always relied on assumptions. Causal graphs enable us to make these
assumptions explicit and to understand what we obtain on the basis of
them.

\newpage{}

\subsection{Funding}\label{funding}

This work is supported by a grant from the Templeton Religion Trust
(TRT0418) and RSNZ Marsden 3721245, 20-UOA-123; RSNZ 19-UOO-090. I also
received support from the Max Planck Institute for the Science of Human
History. The Funders had no role in preparing the manuscript or the
decision to publish it.

\subsection{Acknowledgements}\label{acknowledgements}

I am grateful to Dr Inkuk Kim for checking previous versions of this
manuscript and offering feedback.

I am also grateful to two anonymous reviewers and the editor, Charles
Efferson, of \emph{Evolutionary Human Sciences} for their constructive
feedback that improved this manuscript.

Any remaining errors are my own.

\newpage{}

\subsection{Appendix A: Glossary}\label{appendix-a-glossary}

\begin{table}

\caption{\label{tbl-experiments}Glossary}

\centering{

\glossaryTerms

}

\end{table}%

\subsection{Appendix B:}\label{appendix-b}

\subsection{Examples of common causal
questions}\label{examples-of-common-causal-questions}

\begin{table}

\caption{\label{tbl-common-interests}Common causal questions}

\centering{

\terminologycommoncausalinterests

}

\end{table}%

\subsection{Appendix C: Causal Inference in History: The Difficulty In
Satisfying the Three Fundamental
Assumptions}\label{appendix-c-causal-inference-in-history-the-difficulty-in-satisfying-the-three-fundamental-assumptions}

Consider the Protestant Reformation of the 16\(^{th}\) century, which
initiated religious change throughout much of Europe. Historians have
argued that Protestantism caused social, cultural, and economic changes
in those societies where it took hold; see: Weber
(\citeproc{ref-weber1905}{1905}); Weber
(\citeproc{ref-weber1993}{1993}); Swanson
(\citeproc{ref-swanson1967}{1967}); Swanson
(\citeproc{ref-swanson1971}{1971}); Basten and Betz
(\citeproc{ref-basten2013}{2013}), and for an overview see: Becker
\emph{et al.} (\citeproc{ref-becker2016}{2016}).

Suppose we were interested in estimating the `Average Treatment Effect'
of the Protestant Reformation. Let \(A = a^*\) denote the adoption of
Protestantism. We compare this effect with that of remaining Catholic,
represented as \(A = a\). We assume that both the concepts of `adopting
Protestantism' and of `economic development' are well-defined (e.g.~GDP
+1 century after a country has a Protestant majority contrasted with
remaining Catholic). The causal effect for any individual country is
\(Y_i(a^*) - Y_i(a)\). Although we cannot identify this effect, if the
basic assumptions of causal inference are met, we can estimate the
average or marginal effect conditioning the confounding effects of \(L\)
gives us,

\[ATE_{\textnormal{economic~development}} = \mathbb{E}[Y(\textnormal{Became~Protestant}|L) - Y(\textnormal{Remained~Catholic}|L)]\]

When asking causal questions about the economic effect of adopting
Protestantism versus remaining Catholic, there are several challenges
that arise in relation to the three fundamental assumptions required for
causal inference.

\textbf{Causal Consistency}: requires the outcome under each level of
exposure is well-defined. In this context, defining what `adopting
Protestantism' and `remaining Catholic' mean may present challenges. The
practices and beliefs associated with each religion might vary
significantly across countries and time periods, and it may be difficult
to create a consistent, well-defined exposure. Furthermore, the outcome
- economic development - may also be challenging to measure consistently
across different countries and time periods.

There is undoubtedly considerable heterogeneity in the `Protestant
exposure.' In England, Protestantism was closely tied to the monarchy
(\citeproc{ref-collinson2003}{Collinson 2003}). In Germany, Martin
Luther's teachings emphasised individual faith in scripture, which, it
has been claimed, supported economic development by promoting literacy
(\citeproc{ref-gawthrop1984}{Gawthrop and Strauss 1984}). In England,
King Henry VIII abolished Catholicism
(\citeproc{ref-collinson2003}{Collinson 2003}). The Reformation, then,
occurred differently in different places. The exposure needs to be
better-defined.

There is also ample scope for interference: 16th century societies were
interconnected through trade, diplomacy, and warfare. Thus, the
religious decisions of one society were unlikely to have been
independent from those of other societies.

\textbf{Exchangeability}: requires that given the confounders, the
potential outcomes are independent of the treatment assignment. It might
be difficult to account for all possible confounders in this context.
For example, historical, political, social, and geographical factors
could influence both a country's religious affiliations and its economic
development.

\textbf{Positivity}: requires that there is a non-zero probability of
every level of exposure for every strata of confounders. If we consider
various confounding factors such as geographical location, historical
events, or political circumstances, some countries might only ever have
the possibility of either remaining Catholic or becoming Protestant, but
not both. For example, it is unclear under which conditions 16th century
Spain could have been randomly assigned to Protestantism
(\citeproc{ref-nalle1987}{Nalle 1987}).

Perhaps a more credible measure of effect in the region of our interests
is the Average Treatment Effect in the Treated (ATT) expressed:

\[ATT_{\textnormal{economic~development}} = \mathbb{E}[(Y(a*)- Y(a)|A = a*,L]\]

Where \(Y(a*)\) represents the potential outcome if treated. \(Y(a)\)
represents the potential outcome if not treated. The expectation is
taken over the distribution of the treated units (i.e., those for whom
\(A = a*\)). \(L\) is a set of covariates on which we condition to
ensure that the potential outcomes \(Y(a*)\) and \(Y(a)\) are
independent of the treatment assignment \(A\), given \(L\). This
accounts for any confounding factors that might bias the estimate of the
treatment effect.

Here, the ATT defines the expected difference in economic success for
cultures that became Protestant compared with the expected economic
success if those cultures had not become Protestant, conditional on
measured confounders \(L\), among the exposed (\(A = a^*\)). To estimate
this contrast, our models would need to match Protestant cultures with
comparable Catholic cultures effectively. By estimating the ATT, we
would avoid the assumption of non-deterministic positivity for the
untreated. However, whether matching is conceptually plausible remains
debatable. Ostensibly, it would seem that assigning a religion to a
culture is not as easy as administering a pill
(\citeproc{ref-watts2018}{Watts \emph{et al.} 2018}).

\newpage{}

\subsection{Apendix C:}\label{apendix-c}

\subsubsection{Appendix A: Causal Consistency Under Multiple Versions of
Treatment}\label{appendix-a-causal-consistency-under-multiple-versions-of-treatment}

To better understand how the causal consistency assumption might fail,
consider a question that has been discussed in the evolutionary human
science literature about whether a society's beliefs in big Gods affects
its development of social complexity (\citeproc{ref-beheim2021}{Beheim
\emph{et al.} 2021}; \citeproc{ref-johnson2015}{Johnson 2015};
\citeproc{ref-norenzayan2016}{Norenzayan \emph{et al.} 2016};
\citeproc{ref-sheehan2022}{Sheehan \emph{et al.} 2022};
\citeproc{ref-slingerland2020coding}{Slingerland \emph{et al.} 2020};
\citeproc{ref-watts2015}{Watts \emph{et al.} 2015};
\citeproc{ref-whitehouse2023}{Whitehouse \emph{et al.} 2023}).
Historians and anthropologists report that such beliefs vary over time
and across cultures in intensity, interpretations, institutional
management, and rituals (\citeproc{ref-bulbuliaj.2013}{Bulbulia, J.
\emph{et al.} 2013}; \citeproc{ref-decoulanges1903}{De Coulanges 1903};
\citeproc{ref-geertz2013}{Geertz \emph{et al.} 2013};
\citeproc{ref-wheatley1971}{Wheatley 1971}). Knowing nothing else, we
might expect that variation in content and settings could influence
social complexity. Moreover, the treatments realised in one society
might affect the treatments realised in other societies, that is, there
might be \emph{spill-over} effects in the exposures (`treatments') to be
compared (\citeproc{ref-murray2021a}{Murray \emph{et al.} 2021};
\citeproc{ref-shiba2023uncovering}{Shiba \emph{et al.} 2023}).

The theory of causal inference under multiple versions of treatment,
developed by VanderWeele and Hernán, formally addresses this challenge
of treatment-effect heterogeneity
(\citeproc{ref-vanderweele2009}{VanderWeele 2009a},
\citeproc{ref-vanderweele2018}{2018};
\citeproc{ref-vanderweele2013}{VanderWeele and Hernan 2013}). The
authors proved that if the treatment variations, \(K\), are
conditionally independent of the potential outcomes, \(Y(k)\), given
covariates \(L\), then conditioning on \(L\) allows us to consistently
estimate causal effects over the heterogeneous treatments
(\citeproc{ref-vanderweele2009}{VanderWeele 2009a}).

Where \(\coprod\) denotes independence, we may assume causal consistency
where the interventions to be compared are independent of their
potential outcomes, conditional on covariates, \(L\):

\[
K \coprod Y(k) | L
\]

That is, according to the theory of causal inference under multiple
versions of treatment, we may think of \(K\) as a `coarsened indicator'
for \(A\).

Notice that the theory allows us to clarify what we require to estimate
causal effects in the presence of interference or `spill-over', which we
may consider to be a special case of treatment-effect heterogeneity. To
handle interference, we say the potential outcome of each unit
\(i \neq j\) must be independent of its own treatment received as well
as of the treatment that all other units received on \(j \neq i\),
conditional on measured covariates \(L\):

\[
Y_i(k) \coprod K_i, K_j | L, \quad \forall i, \forall j \neq i
\]

Although the theory of causal inference under multiple versions of
treatment provides a formal solution to the problem of treatment-effect
heterogeneity, computing and interpreting causal effect estimates under
this theory can be challenging.

Consider the question of whether a reduction in Body Mass Index (BMI)
affects health (\citeproc{ref-hernuxe1n2008}{Hernán and Taubman 2008}).
Weight loss can occur through various methods, each with different
health implications. Specific methods, such as regular exercise or a
calorie-reduced diet, benefit health. However, weight loss might result
from adverse conditions such as infectious diseases, cancers,
depression, famine, or accidental amputations, which we may suppose are
generally not beneficial to health, at least not in the same way as,
say, reducing weight by increasing physical activity. Hence, even if
causal effects of `weight loss' could be consistently estimated when
adjusting for covariates \(L\) in these settings, we might be uncertain
about how to interpret the effect we are consistently estimating. This
uncertainty highlights the need for precise and well-defined causal
questions. For example, rather than stating the intervention vaguely as
`weight loss', we could state the intervention clearly and specifically,
say, `weight loss achieved through aerobic exercise over at least five
years, compared with no weight loss.' This specificity in the definition
of the exposure, along with comparable specificity in the statement of
the outcomes helps to ensure that the causal estimates we obtain are not
merely unbiased but also interpretable; for discussion see: Hernán
\emph{et al.} (\citeproc{ref-hernuxe1n2022}{2022}); Murray \emph{et al.}
(\citeproc{ref-murray2021a}{2021}); Hernán and Taubman
(\citeproc{ref-hernuxe1n2008}{2008}).

Beyond uncertainties for the interpretation of heterogeneous treatment
effect estimates, there is, as just mentioned, the additional
consideration that we cannot fully verify from data whether the measured
covariates \(L\) suffice to render the multiple versions of treatment
independent of the counterfactual outcomes. This problem is acute when
there is \emph{interference}, which occurs when treatment effects are
relative to the density and distribution of treatment effects in a
population. Scope for interference will often make it difficult to
warrant the assumption that the potential outcomes are independent of
the many versions of treatment that have been realised, dependently, on
the administration of previous versions of treatments across the
population (\citeproc{ref-bulbulia2023a}{Bulbulia \emph{et al.} 2023};
\citeproc{ref-ogburn2022}{Ogburn \emph{et al.} 2022};
\citeproc{ref-vanderweele2013}{VanderWeele and Hernan 2013}).

In short, although the theory of causal inference under multiple
versions of treatment provides a formal solution for consistent causal
effect estimation in observational settings, \emph{treatment
heterogeneity} remains a practical threat. Generally, we should assume
that causal consistency is unrealistic unless proven innocent.

For now, we note that the causal consistency assumption provides a
theoretical starting point for recovering the missing counterfactuals
required for computing causal contrasts. It identifies half of these
missing counterfactuals directly from observed data. The concept of
conditional exchangeability, which we explore next, offers a means for
recovering the remaining half.

\newpage{}

\subsection{Appendix E}\label{appendix-e}

Pearl's Do-Calculus and Structural Equation Models

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Setting}: We begin by defining the sequence of variables in
  our model: \[
  S_i= (W, Y_0, L_1, A_1, L_2, A_2, ..., L_\tau, A_\tau, Y_{\tau}) \sim \mathbf{P}
  \] where \(S_i\) is a sample from the distribution \(\mathbf{P}\) and
  includes baseline covariates \(W\), intermediate outcomes \(L_t\),
  treatments \(A_t\), and final outcomes \(Y_{\tau}\) over time periods
  \(t = 1, 2, \ldots, \tau\).
\item
  \textbf{Defining the outcome}: We define the final outcome:

  \[
  Y = A_{\tau + 1}
  \]
\item
  \textbf{History of variables}: Define the history of all variables up
  to treatment \(A_t\) as: \[
  H_t = (\bar{A}_{t-1}, \bar{L}_t)
  \] Here, \(\bar{A}_{t-1}\) represents the history of treatments up to
  time \(t-1\), and \(\bar{L}_t\) represents the history of intermediate
  outcomes up to time \(t\).
\item
  \textbf{Exogenous variables}: Define the vector of exogenous
  variables: \[
  U = (U_{L,t}, U_{A,t}, U_{Y}: t \in \{1 \dots \tau\})
  \] where \(U\) describes the set of exogenous variables affecting
  \(L_t\), \(A_t\), and \(Y\).
\item
  \textbf{Deterministic functions}: Assume the following deterministic
  functions for the intermediate outcomes, treatments, and final
  outcome:

  For intermediate outcomes: \[
  L_t = f_{L_t}(A_{t-1}, H_{t-1}, U_{L,t})
  \]

  For treatments: \[
  A_t = f_{A_t}(H_t, U_{A,t})
  \]

  For the final outcome: \[
  Y = f_{Y}(A_{\tau}, H_{\tau}, U_{Y})
  \]
\item
  \textbf{Hypothetical Interventions (LMTPs)}: Local Marginal Treatment
  Policies (LMTPs) are defined as hypothetical interventions. We replace
  the deterministic function for treatments: \[
  A_t = f_{A_t}(H_t, U_{A,t})
  \] with an intervention: \[
  A^\mathbf{d}_t
  \]
\item
  \textbf{Counterfactual Variables}: This intervention produces
  counterfactual variables. For intermediate outcomes: \[
  L_t(\bar{A}^\mathbf{d}_{t-1}) = f_{L_t}(A^\mathbf{d}_{t-1}, H_{t-1}(\bar{A}^\mathbf{d}_{t-2}), U_{L,t})
  \]

  For treatments, the counterfactual variable
  \(A_t(\bar{A}^\mathbf{d}_{t-1})\) is defined as the natural value of
  the treatment, i.e., the value of the treatment that would have been
  observed at time \(t\) under the intervention history leading up to it
  at \(t-1\), and then discontinued: \[
  A_t(\bar{A}^\mathbf{d}_{t-1}) = f_{A_t}(H_t(\bar{A}^\mathbf{d}_{t-1}), H_{t-1}(\bar{A}^\mathbf{d}_{t-2}), U_{L,t})
  \]
\item
  \textbf{Full Intervention}: When all variables are intervened on, the
  counterfactual final outcome is: \[
  Y(\bar{A}^\mathbf{d}) = f_Y(A^\mathbf{d}_\tau, H_\tau(\bar{A}^\mathbf{d}_{\tau-1}), U_{Y})
  \]
\end{enumerate}

\newpage{}

\phantomsection\label{refs}
\begin{CSLReferences}{1}{0}
\bibitem[\citeproctext]{ref-athey2019}
Athey, S, Tibshirani, J, and Wager, S (2019) Generalized random forests.
\emph{The Annals of Statistics}, \textbf{47}(2), 1148--1178.
doi:\href{https://doi.org/10.1214/18-AOS1709}{10.1214/18-AOS1709}.

\bibitem[\citeproctext]{ref-athey2021}
Athey, S, and Wager, S (2021) Policy Learning With Observational Data.
\emph{Econometrica}, \textbf{89}(1), 133--161.
doi:\href{https://doi.org/10.3982/ECTA15732}{10.3982/ECTA15732}.

\bibitem[\citeproctext]{ref-bareinboim2013general}
Bareinboim, E, and Pearl, J (2013) A general algorithm for deciding
transportability of experimental results. \emph{Journal of Causal
Inference}, \textbf{1}(1), 107--134.

\bibitem[\citeproctext]{ref-basten2013}
Basten, C, and Betz, F (2013) Beyond work ethic: Religion, individual,
and political preferences. \emph{American Economic Journal: Economic
Policy}, \textbf{5}(3), 67--91.
doi:\href{https://doi.org/10.1257/pol.5.3.67}{10.1257/pol.5.3.67}.

\bibitem[\citeproctext]{ref-becker2016}
Becker, SO, Pfaff, S, and Rubin, J (2016) Causes and consequences of the
protestant reformation. \emph{Explorations in Economic History},
\textbf{62}, 1--25.

\bibitem[\citeproctext]{ref-beheim2021}
Beheim, B, Atkinson, QD, Bulbulia, J, \ldots{} Willard, AK (2021)
Treatment of missing data determined conclusions regarding moralizing
gods. \emph{Nature}, \textbf{595}(7866), E29--E34.
doi:\href{https://doi.org/10.1038/s41586-021-03655-4}{10.1038/s41586-021-03655-4}.

\bibitem[\citeproctext]{ref-bulbulia2024PRACTICAL}
Bulbulia, J (2024) A practical guide to causal inference in three-wave
panel studies. \emph{PsyArXiv Preprints}.
doi:\href{https://doi.org/10.31234/osf.io/uyg3d}{10.31234/osf.io/uyg3d}.

\bibitem[\citeproctext]{ref-bulbulia2022}
Bulbulia, JA (2022) A workflow for causal inference in cross-cultural
psychology. \emph{Religion, Brain \& Behavior}, \textbf{0}(0), 1--16.
doi:\href{https://doi.org/10.1080/2153599X.2022.2070245}{10.1080/2153599X.2022.2070245}.

\bibitem[\citeproctext]{ref-bulbulia2023}
Bulbulia, JA (2023) Causal diagrams (directed acyclic graphs): A
practical guide.

\bibitem[\citeproctext]{ref-bulbulia2023a}
Bulbulia, JA, Afzali, MU, Yogeeswaran, K, and Sibley, CG (2023)
Long-term causal effects of far-right terrorism in {N}ew {Z}ealand.
\emph{PNAS Nexus}, \textbf{2}(8), pgad242.

\bibitem[\citeproctext]{ref-bulbuliaj.2013}
Bulbulia, J., Geertz, AW, Atkinson, QD, \ldots{} Wilson, DS (2013) The
cultural evolution of religion. In P. J. Richerson and M. Christiansen,
eds., Cambridge, MA: MIT press, 381--404.

\bibitem[\citeproctext]{ref-cole2008}
Cole, SR, and Hernán, MA (2008) Constructing inverse probability weights
for marginal structural models. \emph{American Journal of Epidemiology},
\textbf{168}(6), 656--664.

\bibitem[\citeproctext]{ref-cole2010}
Cole, SR, Platt, RW, Schisterman, EF, \ldots{} Poole, C (2010)
Illustrating bias due to conditioning on a collider. \emph{International
Journal of Epidemiology}, \textbf{39}(2), 417--420.
doi:\href{https://doi.org/10.1093/ije/dyp334}{10.1093/ije/dyp334}.

\bibitem[\citeproctext]{ref-collinson2003}
Collinson, P (2003) \emph{The reformation: A history}, Weidenfeld;
Nicholson; London, England.

\bibitem[\citeproctext]{ref-danaei2012}
Danaei, G, Tavakkoli, M, and Hernán, MA (2012) Bias in observational
studies of prevalent users: lessons for comparative effectiveness
research from a meta-analysis of statins. \emph{American Journal of
Epidemiology}, \textbf{175}(4), 250--262.
doi:\href{https://doi.org/10.1093/aje/kwr301}{10.1093/aje/kwr301}.

\bibitem[\citeproctext]{ref-decoulanges1903}
De Coulanges, F (1903) \emph{La cité antique: Étude sur le culte, le
droit, les institutions de la grèce et de rome}, Hachette.

\bibitem[\citeproctext]{ref-diaz2021nonparametric}
Dı́az, I, Hejazi, NS, Rudolph, KE, and Der Laan, MJ van (2021)
Nonparametric efficient causal mediation with intermediate confounders.
\emph{Biometrika}, \textbf{108}(3), 627--641.

\bibitem[\citeproctext]{ref-Diaz2023}
Dı́az, I, Williams, N, and Rudolph, KE (2023) \emph{Journal of Causal
Inference}, \textbf{11}(1), 20220077.
doi:\href{https://doi.org/doi:10.1515/jci-2022-0077}{doi:10.1515/jci-2022-0077}.

\bibitem[\citeproctext]{ref-gawthrop1984}
Gawthrop, R, and Strauss, G (1984) Protestantism and literacy in early
modern germany. \emph{Past \& Present}, (104), 31--55.

\bibitem[\citeproctext]{ref-geertz2013}
Geertz, AW, Atkinson, QD, Cohen, E, \ldots{} Wilson, DS (2013) The
cultural evolution of religion. In P. J. Richerson and M. Christiansen,
eds., Cambridge, MA: MIT press, 381--404.

\bibitem[\citeproctext]{ref-greenland2003quantifying}
Greenland, S (2003) Quantifying biases in causal models: Classical
confounding vs collider-stratification bias. \emph{Epidemiology},
300--306.

\bibitem[\citeproctext]{ref-greenland1999}
Greenland, S, Pearl, J, and Robins, JM (1999) Causal diagrams for
epidemiologic research. \emph{Epidemiology (Cambridge, Mass.)},
\textbf{10}(1), 37--48.

\bibitem[\citeproctext]{ref-haneuse2013estimation}
Haneuse, S, and Rotnitzky, A (2013) Estimation of the effect of
interventions that modify the received treatment. \emph{Statistics in
Medicine}, \textbf{32}(30), 5260--5277.

\bibitem[\citeproctext]{ref-hernan2023}
Hernan, MA, and Robins, JM (2023) \emph{Causal inference}, Taylor \&
Francis. Retrieved from
\url{https://books.google.co.nz/books?id=/_KnHIAAACAAJ}

\bibitem[\citeproctext]{ref-hernan2024WHATIF}
Hernan, MA, and Robins, JM (2024) \emph{Causal inference: What if?},
Taylor \& Francis. Retrieved from
\url{https://www.hsph.harvard.edu/miguel-hernan/causal-inference-book/}

\bibitem[\citeproctext]{ref-hernan2017SELECTIONWITHOUTCOLLIDER}
Hernán, MA (2017) Invited commentary: Selection bias without colliders
\textbar{} american journal of epidemiology \textbar{} oxford academic.
\emph{American Journal of Epidemiology}, \textbf{185}(11), 1048--1050.
Retrieved from \url{https://doi.org/10.1093/aje/kwx077}

\bibitem[\citeproctext]{ref-hernan2008aObservationalStudiesAnalysedLike}
Hernán, MA, Alonso, A, Logan, R, \ldots{} Robins, JM (2008b)
Observational studies analyzed like randomized experiments: An
application to postmenopausal hormone therapy and coronary heart
disease. \emph{Epidemiology}, \textbf{19}(6), 766.
doi:\href{https://doi.org/10.1097/EDE.0b013e3181875e61}{10.1097/EDE.0b013e3181875e61}.

\bibitem[\citeproctext]{ref-hernuxe1n2008a}
Hernán, MA, Alonso, A, Logan, R, \ldots{} Robins, JM (2008a)
Observational studies analyzed like randomized experiments: An
application to postmenopausal hormone therapy and coronary heart
disease. \emph{Epidemiology}, \textbf{19}(6), 766.
doi:\href{https://doi.org/10.1097/EDE.0b013e3181875e61}{10.1097/EDE.0b013e3181875e61}.

\bibitem[\citeproctext]{ref-hernan2009MEASUREMENT}
Hernán, MA, and Cole, SR (2009) Invited commentary: Causal diagrams and
measurement bias. \emph{American Journal of Epidemiology},
\textbf{170}(8), 959--962.
doi:\href{https://doi.org/10.1093/aje/kwp293}{10.1093/aje/kwp293}.

\bibitem[\citeproctext]{ref-hernan2004STRUCTURAL}
Hernán, MA, Hernández-Díaz, S, and Robins, JM (2004) A structural
approach to selection bias. \emph{Epidemiology}, \textbf{15}(5),
615--625. Retrieved from \url{https://www.jstor.org/stable/20485961}

\bibitem[\citeproctext]{ref-hernuxe1n2006}
Hernán, MA, and Robins, JM (2006a) Estimating causal effects from
epidemiological data. \emph{Journal of Epidemiology \& Community
Health}, \textbf{60}(7), 578--586.
doi:\href{https://doi.org/10.1136/jech.2004.029496}{10.1136/jech.2004.029496}.

\bibitem[\citeproctext]{ref-hernan2006estimating}
Hernán, MA, and Robins, JM (2006b) Estimating causal effects from
epidemiological data. \emph{Journal of Epidemiology \& Community
Health}, \textbf{60}(7), 578--586.
doi:\href{https://doi.org/10.1136/jech.2004.029496}{10.1136/jech.2004.029496}.

\bibitem[\citeproctext]{ref-hernan2017per}
Hernán, MA, Robins, JM, et al. (2017) Per-protocol analyses of pragmatic
trials. \emph{N Engl J Med}, \textbf{377}(14), 1391--1398.

\bibitem[\citeproctext]{ref-hernuxe1n2016}
Hernán, MA, Sauer, BC, Hernández-Díaz, S, Platt, R, and Shrier, I
(2016a) Specifying a target trial prevents immortal time bias and other
self-inflicted injuries in observational analyses. \emph{Journal of
Clinical Epidemiology}, \textbf{79}, 70--75.

\bibitem[\citeproctext]{ref-hernan2016}
Hernán, MA, Sauer, BC, Hernández-Díaz, S, Platt, R, and Shrier, I
(2016b) Specifying a target trial prevents immortal time bias and other
self-inflicted injuries in observational analyses. \emph{Journal of
Clinical Epidemiology}, \textbf{79}, 70--75.

\bibitem[\citeproctext]{ref-hernuxe1n2008}
Hernán, MA, and Taubman, SL (2008) Does obesity shorten life? The
importance of well-defined interventions to answer causal questions.
\emph{International Journal of Obesity (2005)}, \textbf{32 Suppl 3},
S8--14.
doi:\href{https://doi.org/10.1038/ijo.2008.82}{10.1038/ijo.2008.82}.

\bibitem[\citeproctext]{ref-hernuxe1n2022}
Hernán, MA, Wang, W, and Leaf, DE (2022) Target trial emulation: A
framework for causal inference from observational data. \emph{JAMA},
\textbf{328}(24), 2446--2447.
doi:\href{https://doi.org/10.1001/jama.2022.21383}{10.1001/jama.2022.21383}.

\bibitem[\citeproctext]{ref-hoffman2023}
Hoffman, KL, Salazar-Barreto, D, Rudolph, KE, and Díaz, I (2023)
Introducing longitudinal modified treatment policies: A unified
framework for studying complex exposures.
doi:\href{https://doi.org/10.48550/arXiv.2304.09460}{10.48550/arXiv.2304.09460}.

\bibitem[\citeproctext]{ref-holland1986}
Holland, PW (1986) Statistics and causal inference. \emph{Journal of the
American Statistical Association}, \textbf{81}(396), 945--960.

\bibitem[\citeproctext]{ref-hume1902}
Hume, D (1902) \emph{Enquiries Concerning the Human Understanding: And
Concerning the Principles of Morals}, Clarendon Press.

\bibitem[\citeproctext]{ref-imai2008misunderstandings}
Imai, K, King, G, and Stuart, EA (2008) Misunderstandings between
experimentalists and observationalists about causal inference.
\emph{Journal of the Royal Statistical Society Series A: Statistics in
Society}, \textbf{171}(2), 481--502.

\bibitem[\citeproctext]{ref-johnson2015}
Johnson, DD (2015) Big gods, small wonder: Supernatural punishment
strikes back. \emph{Religion, Brain \& Behavior}, \textbf{5}(4),
290--298.

\bibitem[\citeproctext]{ref-van2012targeted}
Laan, MJ van der, and Gruber, S (2012) Targeted minimum loss based
estimation of causal effects of multiple time point interventions.
\emph{The International Journal of Biostatistics}, \textbf{8}(1).

\bibitem[\citeproctext]{ref-lash2020}
Lash, TL, Rothman, KJ, VanderWeele, TJ, and Haneuse, S (2020)
\emph{Modern epidemiology}, Wolters Kluwer. Retrieved from
\url{https://books.google.co.nz/books?id=SiTSnQEACAAJ}

\bibitem[\citeproctext]{ref-lauritzen1990}
Lauritzen, SL, Dawid, AP, Larsen, BN, and Leimer, H-G (1990)
Independence properties of directed {M}arkov fields. \emph{Networks},
\textbf{20}(5), 491--505.

\bibitem[\citeproctext]{ref-lewis1973}
Lewis, D (1973) Causation. \emph{The Journal of Philosophy},
\textbf{70}(17), 556--567.
doi:\href{https://doi.org/10.2307/2025310}{10.2307/2025310}.

\bibitem[\citeproctext]{ref-linden2020EVALUE}
Linden, A, Mathur, MB, and VanderWeele, TJ (2020) Conducting sensitivity
analysis for unmeasured confounding in observational studies using
e-values: The evalue package. \emph{The Stata Journal}, \textbf{20}(1),
162--175.

\bibitem[\citeproctext]{ref-liu2023application}
Liu, Y, Schnitzer, ME, Herrera, R, Dı́az, I, O'Loughlin, J, and
Sylvestre, M-P (2023) The application of target trials with longitudinal
targeted maximum likelihood estimation to assess the effect of alcohol
consumption in adolescence on depressive symptoms in adulthood.
\emph{American Journal of Epidemiology}, kwad241.

\bibitem[\citeproctext]{ref-mcelreath2020}
McElreath, R (2020) \emph{Statistical rethinking: A {B}ayesian course
with examples in r and stan}, CRC press.

\bibitem[\citeproctext]{ref-montgomery2018}
Montgomery, JM, Nyhan, B, and Torres, M (2018) How conditioning on
posttreatment variables can ruin your experiment and what to do about
It. \emph{American Journal of Political Science}, \textbf{62}(3),
760--775.
doi:\href{https://doi.org/10.1111/ajps.12357}{10.1111/ajps.12357}.

\bibitem[\citeproctext]{ref-murray2021a}
Murray, EJ, Marshall, BDL, and Buchanan, AL (2021) Emulating target
trials to improve causal inference from agent-based models.
\emph{American Journal of Epidemiology}, \textbf{190}(8), 1652--1658.
doi:\href{https://doi.org/10.1093/aje/kwab040}{10.1093/aje/kwab040}.

\bibitem[\citeproctext]{ref-nalle1987}
Nalle, ST (1987) Inquisitors, priests, and the people during the
catholic reformation in spain. \emph{The Sixteenth Century Journal},
557--587.

\bibitem[\citeproctext]{ref-neal2020introduction}
Neal, B (2020) Introduction to causal inference from a machine learning
perspective. \emph{Course Lecture Notes (Draft)}. Retrieved from
\url{https://www.bradyneal.com/Introduction_to_Causal_Inference-Dec17_2020-Neal.pdf}

\bibitem[\citeproctext]{ref-neurath1973}
Neurath, O (1973) Anti-spengler. In M. Neurath and R. S. Cohen, eds.,
\emph{Empiricism and sociology}, Dordrecht: Springer Netherlands,
158--213.
doi:\href{https://doi.org/10.1007/978-94-010-2525-6_6}{10.1007/978-94-010-2525-6\_6}.

\bibitem[\citeproctext]{ref-norenzayan2016}
Norenzayan, A, Shariff, AF, Gervais, WM, \ldots{} Henrich, J (2016) The
cultural evolution of prosocial religions. \emph{Behavioral and Brain
Sciences}, \textbf{39}, e1.
doi:\href{https://doi.org/10.1017/S0140525X14001356}{10.1017/S0140525X14001356}.

\bibitem[\citeproctext]{ref-ogburn2021}
Ogburn, EL, and Shpitser, I (2021) Causal modelling: The two cultures.
\emph{Observational Studies}, \textbf{7}(1), 179--183.
doi:\href{https://doi.org/10.1353/obs.2021.0006}{10.1353/obs.2021.0006}.

\bibitem[\citeproctext]{ref-ogburn2022}
Ogburn, EL, Sofrygin, O, Díaz, I, and Laan, MJ van der (2022) Causal
inference for social network data. \emph{Journal of the American
Statistical Association}, \textbf{0}(0), 1--15.
doi:\href{https://doi.org/10.1080/01621459.2022.2131557}{10.1080/01621459.2022.2131557}.

\bibitem[\citeproctext]{ref-pearl1988}
Pearl, J (1988) \emph{Probabilistic reasoning in intelligent systems:
Networks of plausible inference}, Morgan kaufmann.

\bibitem[\citeproctext]{ref-pearl1995}
Pearl, J (1995) Causal diagrams for empirical research.
\emph{Biometrika}, \textbf{82}(4), 669--688.

\bibitem[\citeproctext]{ref-pearl2009a}
Pearl, J (2009) \emph{Causality}, Cambridge University Press.

\bibitem[\citeproctext]{ref-pearl2022}
Pearl, J, and Bareinboim, E (2022) External validity: From do-calculus
to transportability across populations. In, 1st edn, Vol. 36, New York,
NY, USA: Association for Computing Machinery, 451--482. Retrieved from
\url{https://doi.org/10.1145/3501714.3501741}

\bibitem[\citeproctext]{ref-richardson2013}
Richardson, TS, and Robins, JM (2013a) Single world intervention graphs:
A primer. In, Citeseer.

\bibitem[\citeproctext]{ref-richardson2013swigsprimer}
Richardson, TS, and Robins, JM (2013b) Single world intervention graphs:
A primer. In \emph{Second UAI workshop on causal structure learning,
{B}ellevue, {W}ashington}, Citeseer. Retrieved from
\url{https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=07bbcb458109d2663acc0d098e8913892389a2a7}

\bibitem[\citeproctext]{ref-richardson2023potential}
Richardson, TS, and Robins, JM (2023) Potential outcome and decision
theoretic foundations for statistical causality. \emph{Journal of Causal
Inference}, \textbf{11}(1), 20220012.

\bibitem[\citeproctext]{ref-robins1986}
Robins, J (1986) A new approach to causal inference in mortality studies
with a sustained exposure period---application to control of the healthy
worker survivor effect. \emph{Mathematical Modelling}, \textbf{7}(9-12),
1393--1512.

\bibitem[\citeproctext]{ref-robins2010alternative}
Robins, JM, and Richardson, TS (2010) Alternative graphical causal
models and the identification of direct effects. \emph{Causality and
Psychopathology: Finding the Determinants of Disorders and Their Cures},
\textbf{84}, 103--158.

\bibitem[\citeproctext]{ref-rotnitzky2017multiply}
Rotnitzky, A, Robins, J, and Babino, L (2017) On the multiply robust
estimation of the mean of the g-functional. Retrieved from
\url{https://arxiv.org/abs/1705.08582}

\bibitem[\citeproctext]{ref-rubin1976}
Rubin, DB (1976) Inference and missing data. \emph{Biometrika},
\textbf{63}(3), 581--592.
doi:\href{https://doi.org/10.1093/biomet/63.3.581}{10.1093/biomet/63.3.581}.

\bibitem[\citeproctext]{ref-rudolph2024mediation}
Rudolph, KE, Williams, NT, and Diaz, I (2024) {Practical causal
mediation analysis: extending nonparametric estimators to accommodate
multiple mediators and multiple intermediate confounders}.
\emph{Biostatistics}, kxae012.
doi:\href{https://doi.org/10.1093/biostatistics/kxae012}{10.1093/biostatistics/kxae012}.

\bibitem[\citeproctext]{ref-sheehan2022}
Sheehan, O, Watts, J, Gray, RD, \ldots{} Atkinson, QD (2022) Coevolution
of religious and political authority in austronesian societies.
\emph{Nature Human Behaviour}.
doi:\href{https://doi.org/10.1038/s41562-022-01471-y}{10.1038/s41562-022-01471-y}.

\bibitem[\citeproctext]{ref-shiba2023uncovering}
Shiba, K, Daoud, A, Hikichi, H, \ldots{} Kawachi, I (2023) Uncovering
heterogeneous associations between disaster-related trauma and
subsequent functional limitations: A machine-learning approach.
\emph{American Journal of Epidemiology}, \textbf{192}(2), 217--229.

\bibitem[\citeproctext]{ref-shpitser2022multivariate}
Shpitser, I, Richardson, TS, and Robins, JM (2022) Multivariate
counterfactual systems and causal graphical models. In
\emph{Probabilistic and causal inference: The works of {J}udea {P}earl},
813--852.

\bibitem[\citeproctext]{ref-shpitser2016causal}
Shpitser, I, and Tchetgen, ET (2016) Causal inference with a graphical
hierarchy of interventions. \emph{Annals of Statistics}, \textbf{44}(6),
2433.

\bibitem[\citeproctext]{ref-slingerland2020coding}
Slingerland, E, Atkinson, QD, Ember, CR, \ldots{} Gray, RD (2020) Coding
culture: Challenges and recommendations for comparative cultural
databases. \emph{Evolutionary Human Sciences}, \textbf{2}, e29.

\bibitem[\citeproctext]{ref-stensrud2023conditional}
Stensrud, MJ, Robins, JM, Sarvet, A, Tchetgen Tchetgen, EJ, and Young,
JG (2023) Conditional separable effects. \emph{Journal of the American
Statistical Association}, \textbf{118}(544), 2671--2683.

\bibitem[\citeproctext]{ref-stuart2018generalizability}
Stuart, EA, Ackerman, B, and Westreich, D (2018) Generalizability of
randomized trial results to target populations: Design and analysis
possibilities. \emph{Research on Social Work Practice}, \textbf{28}(5),
532--537.

\bibitem[\citeproctext]{ref-suzuki2020}
Suzuki, E, Shinozaki, T, and Yamamoto, E (2020) Causal Diagrams:
Pitfalls and Tips. \emph{Journal of Epidemiology}, \textbf{30}(4),
153--162.
doi:\href{https://doi.org/10.2188/jea.JE20190192}{10.2188/jea.JE20190192}.

\bibitem[\citeproctext]{ref-swanson1967}
Swanson, GE (1967) Religion and regime: A sociological account of the
{R}eformation.

\bibitem[\citeproctext]{ref-swanson1971}
Swanson, GE (1971) Interpreting the reformation. \emph{The Journal of
Interdisciplinary History}, \textbf{1}(3), 419--446. Retrieved from
\url{http://www.jstor.org/stable/202620}

\bibitem[\citeproctext]{ref-vanderlaan2011}
Van Der Laan, MJ, and Rose, S (2011) \emph{Targeted Learning: Causal
Inference for Observational and Experimental Data}, New York, NY:
Springer. Retrieved from
\url{https://link.springer.com/10.1007/978-1-4419-9782-1}

\bibitem[\citeproctext]{ref-vanderlaan2018}
Van Der Laan, MJ, and Rose, S (2018) \emph{Targeted Learning in Data
Science: Causal Inference for Complex Longitudinal Studies}, Cham:
Springer International Publishing. Retrieved from
\url{http://link.springer.com/10.1007/978-3-319-65304-4}

\bibitem[\citeproctext]{ref-vanderweele2009}
VanderWeele, TJ (2009a) Concerning the consistency assumption in causal
inference. \emph{Epidemiology}, \textbf{20}(6), 880.
doi:\href{https://doi.org/10.1097/EDE.0b013e3181bd5638}{10.1097/EDE.0b013e3181bd5638}.

\bibitem[\citeproctext]{ref-vanderweele2009a}
VanderWeele, TJ (2009b) Marginal structural models for the estimation of
direct and indirect effects. \emph{Epidemiology}, 18--26.

\bibitem[\citeproctext]{ref-vanderweele2015}
VanderWeele, TJ (2015) \emph{Explanation in causal inference: Methods
for mediation and interaction}, Oxford University Press.

\bibitem[\citeproctext]{ref-vanderweele2018}
VanderWeele, TJ (2018) On well-defined hypothetical interventions in the
potential outcomes framework. \emph{Epidemiology}, \textbf{29}(4), e24.
doi:\href{https://doi.org/10.1097/EDE.0000000000000823}{10.1097/EDE.0000000000000823}.

\bibitem[\citeproctext]{ref-vanderweele2019}
VanderWeele, TJ (2019) Principles of confounder selection.
\emph{European Journal of Epidemiology}, \textbf{34}(3), 211--219.

\bibitem[\citeproctext]{ref-vanderweele2022}
VanderWeele, TJ (2022) Constructed measures and causal inference:
Towards a new model of measurement for psychosocial constructs.
\emph{Epidemiology}, \textbf{33}(1), 141.
doi:\href{https://doi.org/10.1097/EDE.0000000000001434}{10.1097/EDE.0000000000001434}.

\bibitem[\citeproctext]{ref-vanderweele2013}
VanderWeele, TJ, and Hernan, MA (2013) Causal inference under multiple
versions of treatment. \emph{Journal of Causal Inference},
\textbf{1}(1), 1--20.

\bibitem[\citeproctext]{ref-vanderweele2012MEASUREMENT}
VanderWeele, TJ, and Hernán, MA (2012) Results on differential and
dependent measurement error of the exposure and the outcome using signed
directed acyclic graphs. \emph{American Journal of Epidemiology},
\textbf{175}(12), 1303--1310.
doi:\href{https://doi.org/10.1093/aje/kwr458}{10.1093/aje/kwr458}.

\bibitem[\citeproctext]{ref-vanderweele2020}
VanderWeele, TJ, Mathur, MB, and Chen, Y (2020) Outcome-wide
longitudinal designs for causal inference: A new template for empirical
studies. \emph{Statistical Science}, \textbf{35}(3), 437--466.

\bibitem[\citeproctext]{ref-vanderweele2022b}
VanderWeele, TJ, and Vansteelandt, S (2022) A statistical test to reject
the structural interpretation of a latent factor model. \emph{Journal of
the Royal Statistical Society Series B: Statistical Methodology},
\textbf{84}(5), 2032--2054.

\bibitem[\citeproctext]{ref-vansteelandt2022a}
Vansteelandt, S, and Dukes, O (2022) Assumption-lean inference for
generalised linear model parameters. \emph{Journal of the Royal
Statistical Society Series B: Statistical Methodology}, \textbf{84}(3),
657--685.

\bibitem[\citeproctext]{ref-wager2018}
Wager, S, and Athey, S (2018) Estimation and inference of heterogeneous
treatment effects using random forests. \emph{Journal of the American
Statistical Association}, \textbf{113}(523), 1228--1242.
doi:\href{https://doi.org/10.1080/01621459.2017.1319839}{10.1080/01621459.2017.1319839}.

\bibitem[\citeproctext]{ref-watts2015}
Watts, J, Greenhill, SJ, Atkinson, QD, Currie, TE, Bulbulia, J, and
Gray, RD (2015) \emph{Broad supernatural punishment but not moralizing
high gods precede the evolution of political complexity in
{A}ustronesia} \emph{Proceedings of the Royal Society B: Biological
Sciences}, Vol. 282, The Royal Society, 20142556.

\bibitem[\citeproctext]{ref-watts2018}
Watts, J, Sheehan, O, Bulbulia, Joseph A, Gray, RD, and Atkinson, QD
(2018) Christianity spread faster in small, politically structured
societies. \emph{Nature Human Behaviour}, \textbf{2}(8), 559--564.
doi:\href{https://doi.org/gdvnjn}{gdvnjn}.

\bibitem[\citeproctext]{ref-weber1905}
Weber, M (1905) \emph{The protestant ethic and the spirit of capitalism:
And other writings}, Penguin.

\bibitem[\citeproctext]{ref-weber1993}
Weber, M (1993) \emph{The sociology of religion}, Beacon Press.

\bibitem[\citeproctext]{ref-webster2021directed}
Webster-Clark, M, and Breskin, A (2021) Directed acyclic graphs, effect
measure modification, and generalizability. \emph{American Journal of
Epidemiology}, \textbf{190}(2), 322--327.

\bibitem[\citeproctext]{ref-westreich2010}
Westreich, D, and Cole, SR (2010) Invited commentary: positivity in
practice. \emph{American Journal of Epidemiology}, \textbf{171}(6).
doi:\href{https://doi.org/10.1093/aje/kwp436}{10.1093/aje/kwp436}.

\bibitem[\citeproctext]{ref-westreich2019target}
Westreich, D, Edwards, JK, Lesko, CR, Cole, SR, and Stuart, EA (2019)
Target validity and the hierarchy of study designs. \emph{American
Journal of Epidemiology}, \textbf{188}(2), 438--443.

\bibitem[\citeproctext]{ref-westreich2017}
Westreich, D, Edwards, JK, Lesko, CR, Stuart, E, and Cole, SR (2017)
Transportability of trial results using inverse odds of sampling
weights. \emph{American Journal of Epidemiology}, \textbf{186}(8),
1010--1014.
doi:\href{https://doi.org/10.1093/aje/kwx164}{10.1093/aje/kwx164}.

\bibitem[\citeproctext]{ref-westreich2013}
Westreich, D, and Greenland, S (2013) The table 2 fallacy: Presenting
and interpreting confounder and modifier coefficients. \emph{American
Journal of Epidemiology}, \textbf{177}(4), 292--298.

\bibitem[\citeproctext]{ref-wheatley1971}
Wheatley, P (1971) \emph{The pivot of the four quarters : A preliminary
enquiry into the origins and character of the ancient chinese city},
Edinburgh University Press. Retrieved from
\url{https://cir.nii.ac.jp/crid/1130000795717727104}

\bibitem[\citeproctext]{ref-whitehouse2023}
Whitehouse, H, Francois, P, Savage, PE, \ldots{} Turchin, P (2023)
Testing the big gods hypothesis with global historical data: A review
and retake. \emph{Religion, Brain \& Behavior}, \textbf{13}(2),
124--166.

\bibitem[\citeproctext]{ref-williams2021}
Williams, NT, and Díaz, I (2021) \emph{{l}mtp: Non-parametric causal
effects of feasible interventions based on modified treatment policies}.
doi:\href{https://doi.org/10.5281/zenodo.3874931}{10.5281/zenodo.3874931}.

\bibitem[\citeproctext]{ref-young2014identification}
Young, JG, Hernán, MA, and Robins, JM (2014) Identification, estimation
and approximation of risk under interventions that depend on the natural
value of treatment using observational data. \emph{Epidemiologic
Methods}, \textbf{3}(1), 1--19.

\end{CSLReferences}



\end{document}
