% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames,x11names}{xcolor}
%
\documentclass[
  singlecolumn]{article}

\usepackage{amsmath,amssymb}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage[]{libertinus}
\ifPDFTeX\else  
    % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\usepackage[top=30mm,left=20mm,heightrounded]{geometry}
\setlength{\emergencystretch}{3em} % prevent overfull lines
\setcounter{secnumdepth}{-\maxdimen} % remove section numbering
% Make \paragraph and \subparagraph free-standing
\makeatletter
\ifx\paragraph\undefined\else
  \let\oldparagraph\paragraph
  \renewcommand{\paragraph}{
    \@ifstar
      \xxxParagraphStar
      \xxxParagraphNoStar
  }
  \newcommand{\xxxParagraphStar}[1]{\oldparagraph*{#1}\mbox{}}
  \newcommand{\xxxParagraphNoStar}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
  \let\oldsubparagraph\subparagraph
  \renewcommand{\subparagraph}{
    \@ifstar
      \xxxSubParagraphStar
      \xxxSubParagraphNoStar
  }
  \newcommand{\xxxSubParagraphStar}[1]{\oldsubparagraph*{#1}\mbox{}}
  \newcommand{\xxxSubParagraphNoStar}[1]{\oldsubparagraph{#1}\mbox{}}
\fi
\makeatother


\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
% definitions for citeproc citations
\NewDocumentCommand\citeproctext{}{}
\NewDocumentCommand\citeproc{mm}{%
  \begingroup\def\citeproctext{#2}\cite{#1}\endgroup}
\makeatletter
 % allow citations to break across lines
 \let\@cite@ofmt\@firstofone
 % avoid brackets around text for \cite:
 \def\@biblabel#1{}
 \def\@cite#1#2{{#1\if@tempswa , #2\fi}}
\makeatother
\newlength{\cslhangindent}
\setlength{\cslhangindent}{1.5em}
\newlength{\csllabelwidth}
\setlength{\csllabelwidth}{3em}
\newenvironment{CSLReferences}[2] % #1 hanging-indent, #2 entry-spacing
 {\begin{list}{}{%
  \setlength{\itemindent}{0pt}
  \setlength{\leftmargin}{0pt}
  \setlength{\parsep}{0pt}
  % turn on hanging indent if param 1 is 1
  \ifodd #1
   \setlength{\leftmargin}{\cslhangindent}
   \setlength{\itemindent}{-1\cslhangindent}
  \fi
  % set entry spacing
  \setlength{\itemsep}{#2\baselineskip}}}
 {\end{list}}
\usepackage{calc}
\newcommand{\CSLBlock}[1]{\hfill\break\parbox[t]{\linewidth}{\strut\ignorespaces#1\strut}}
\newcommand{\CSLLeftMargin}[1]{\parbox[t]{\csllabelwidth}{\strut#1\strut}}
\newcommand{\CSLRightInline}[1]{\parbox[t]{\linewidth - \csllabelwidth}{\strut#1\strut}}
\newcommand{\CSLIndent}[1]{\hspace{\cslhangindent}#1}

\input{/Users/joseph/GIT/latex/latex-for-quarto.tex}
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\AtBeginDocument{%
\ifdefined\contentsname
  \renewcommand*\contentsname{Table of contents}
\else
  \newcommand\contentsname{Table of contents}
\fi
\ifdefined\listfigurename
  \renewcommand*\listfigurename{List of Figures}
\else
  \newcommand\listfigurename{List of Figures}
\fi
\ifdefined\listtablename
  \renewcommand*\listtablename{List of Tables}
\else
  \newcommand\listtablename{List of Tables}
\fi
\ifdefined\figurename
  \renewcommand*\figurename{Figure}
\else
  \newcommand\figurename{Figure}
\fi
\ifdefined\tablename
  \renewcommand*\tablename{Table}
\else
  \newcommand\tablename{Table}
\fi
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Listing}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}
\makeatother
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\@ifpackageloaded{subcaption}{}{\usepackage{subcaption}}
\makeatother
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\usepackage{bookmark}

\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same} % disable monospaced font for URLs
\hypersetup{
  pdftitle={An Invitation to Causal Inference in Environmental Psychology},
  pdfkeywords={DAGS, Causal
Inference, Confounding, Environmental, Longitudinal, Psychology},
  colorlinks=true,
  linkcolor={blue},
  filecolor={Maroon},
  citecolor={Blue},
  urlcolor={Blue},
  pdfcreator={LaTeX via pandoc}}

\title{An Invitation to Causal Inference in Environmental Psychology}

\usepackage{academicons}
\usepackage{xcolor}

  \author{Joseph A Bulbulia}
            \affil{%
             \small{     Victoria University of Wellington, New Zealand,
School of Psychology, Centre for Applied Cross-Cultural Research
          ORCID \textcolor[HTML]{A6CE39}{\aiOrcid} ~0000-0002-5861-2056 }
              }
      \usepackage{academicons}
\usepackage{xcolor}

  \author{Donald W Hine}
            \affil{%
             \small{     University of Canterbury, School of Psychology,
Speech and Hearing
          ORCID \textcolor[HTML]{A6CE39}{\aiOrcid} ~0000-0002-3905-7026 }
              }
      


\date{2024-10-07}
\begin{document}
\maketitle
\begin{abstract}
This chapter introduces causal inference within environmental
psychology, underscoring its fundamental differences from traditional
statistical analysis. The content is organised into four main sections:
1. \textbf{Non-technical introduction}: stating a causal question
requires defining pre-specified contrasts between interventions
experienced by an entire population. Because each individual can
experience only one intervention, causal effects must be estimated using
assumptions. We build intuitions for these assumptions by clarifying
their satisfaction in randomised controlled experiments. 2.
\textbf{Causal Directed Acyclic Graphs (DAGs) tutorial}. Causal DAGs are
potent tools for clarifying whether and how causal effects may be
identified from data. We explain how they work. 3. \textbf{Practical
examples}: we apply causal DAGs to common scenarios in observational
environmental psychology. 4. \textbf{Guidelines}: the main aim of this
chapter is to motivate broader adoption of causal workflows so that
environmental psychologists may systematically address causal questions
that animate the interests but which presently remain elusive.
\end{abstract}

Psychological scientists are taught that ``correlation does not imply
causation.'' By ``correlation,'' we refer to statistical measures of
association between variables. Most statistical techniques ---- from
t-tests to structural equation models -- estimate associations from
data. Although we know that measuring associations does not imply
causation, in observational settings, we often persist in reporting
statistical associations as if they are meaningful and, perhaps, even as
tentative evidence for causation. The purpose of this chapter is to
clarify why such reporting is confused and misleading, and to guide you
toward better practices.

What do we mean by ``causation?'' Causation has been a topic of
extensive interest and debate in philosophy
(\citeproc{ref-lewis1973}{Lewis 1973}). Here, we narrow our focus. We
consider the assumptions under which it is possible to estimate causal
effects from data and how to estimate them. In causal-effect estimation,
or ``causal inference,'' investigators seek to quantify the average
differences across a specified population or sub-population (the
``target population'') that interventions would produce on well-defined
outcomes. This requires comparing at least two states of the world: one
where the population experiences a treatment and another where they do
not or experience a different level of treatment. While the concept of
causation in causal inference is more narrowly defined than causation
itself, it draws intellectual inspiration from David Hume, who, in his
\emph{Enquiries Concerning Human Understanding} (1751), characterises
the cause-effect relationship as follows:

\begin{quote}
``If the first object had not been, \emph{the second never would have
existed}.'' (\citeproc{ref-hume1902}{Hume 1902}) (emphasis added).
\end{quote}

This conceptualisation aligns closely with the counterfactual approach
in causal inference, which considers what would have happened to an
outcome in both the presence and the absence of a treatment.

Hume's definition relies on counterfactual thinking---specifically,
comparing two mutually exclusive states of the world: one where an event
occurs and one where it does not. For Hume, assessing causation requires
not just observing events as they happen but also considering how the
world might have differed had those events not occurred. Such
comparisons, where we consider scenarios where treatments did not take
place, are known as ``counterfactual contrasts;'' such contrasts are
fundamental to causal-effect estimation. In modern causal inference,
these contrasts are formalised within the potential outcomes framework,
which estimates the average difference in outcomes between treated and
control conditions had the entire population of interest been treated at
different levels of the intervention.

Importantly, although statistically evaluating associations from data is
essential for estimating average treatment effects, causation estimation
cannot be derived from the study of associations alone. Every
realisation in the data provides information about only one of the two
potential outcomes necessary for a causal contrast. We may only obtain
causal contrasts under assumptions that are not testable from the data.
Moreover, a careful and systematic workflow is required. By the end of
this chapter, you will understand why, without such a workflow, common
analytic techniques---such as linear regression, correlation, and
structural equation modelling---lack causal interpretations and may
mislead investigators.

\hyperref[section-part1]{\textbf{Part 1}} introduces the counterfactual
framework of causal inference, focusing on the three fundamental
assumptions necessary for estimating average causal effects. We build
intuition for these concepts by considering randomised controlled
trials, where these assumptions are met through enforced randomisation
(\citeproc{ref-hernan2017per}{HernÃ¡n and Robins 2017};
\citeproc{ref-robins2008estimation}{Robins and Hernan 2008};
\citeproc{ref-westreich2012berkson}{Westreich 2012};
\citeproc{ref-westreich2015}{Westreich \emph{et al.} 2015}).

\hyperref[section-part2]{\textbf{Part 2}} introduces causal Directed
Acyclic Graphs (DAGs), powerful tools for visualising and addressing the
assumption of conditional exchangeability (also known as the ``no
unmeasured confounders'' assumption). Here, we introduce Judea Pearl's
rules of d-separation (\citeproc{ref-pearl1995}{Pearl 1995}), which
allow investigators to identify appropriate variables to adjust for
confounding. Although most psychological scientists are aware that
regression adjustment is commonly used to control confounding, they may
not be fully aware of the formal criteria required to select appropriate
adjustment variables. However, understanding the formal criteria is
essential because over-adjustment may introduce bias or reduce
statistical power, leading to misleading conclusions.

\hyperref[section-part3]{\textbf{Part 3}} provides seven practical
examples where causal diagrams address real-world causal questions.
These examples clarify three objectives: (1) constructing causal
diagrams that accurately represent hypothesised relationships between
variables; (2) identifying and evaluating the assumptions required to
estimate causal effects from observational data, ensuring they align
with the underlying causal structure; and (3) using DAGs to guide the
estimation of causal effects, from identifying necessary adjustment sets
to applying appropriate statistical methods for valid inference. These
examples serve as practical guides for translating causal questions into
causal identification models, clarifying the assumptions needed to
estimate causal relationships from data. It is only after these
identification assumptions are stated that that we can develop
statistical estimators and perform statistical analysis
(\citeproc{ref-vansteelandt2012}{Vansteelandt \emph{et al.} 2012};
\citeproc{ref-wager2018}{Wager and Athey 2018})

\hyperref[section-part4]{\textbf{Part 4}} offers practical guidelines
for environmental psychologists aiming to infer causal effects from
observational data. Given that assumptions about causal relationships
are often uncertain or subject to debate, we recommend developing
multiple causal diagrams and reporting their corresponding analyses to
evaluate different plausible causal pathways. This approach enhances
transparency in how causal relationships are inferred.

We conclude by suggesting further readings and resources for those
interested in learning more about causal inference.

\subsection{Part 1: An Overview of the Counterfactual Framework for
Causal Inference}\label{section-part1}

\subsubsection{The Origins of Causal
Inference}\label{the-origins-of-causal-inference}

Causal inference began in the early 20\(^{th}\) century with Jerzy
Neyman's invention of the potential outcomes framework, initially
developed for agricultural experiments. Neyman realised that
understanding the causal effect of an experimental treatment required
comparing potential outcomes under different treatment conditions, even
though only one outcome could be observed for each unit
(\citeproc{ref-neyman1923}{Splawa-Neyman 1990 (orig. 1923)}). This
framework laid the foundation for modern causal inference.

Donald Rubin extended this approach to observational settings into what
is now known as the Neyman-Rubin Causal Model
(\citeproc{ref-holland1986}{Holland 1986};
\citeproc{ref-rubin1976}{Rubin 1976}). James Robins advanced causal
inference by introducing the mathematical and conceptual framework for
understanding causal effects in settings where there are two or more
sequential treatments over time (\citeproc{ref-robins1986}{Robins
1986}). Robins and his colleagues also developed statistical tools such
as marginal structural models and structural nested models to enable
quantitative assessment of time-varying treatments and time-varying
confounding (\citeproc{ref-hernan2024WHATIF}{Hernan and Robins 2024}).
The computer scientist Judea Pearl developed causal Directed Acyclic
Graphs (DAGs), and proved that these graphical models may be applied to
evaluate one of the three fundamental assumptions of causal inference,
namely ``no unmeasured confounding'', also known as ``conditional
exchangeability'', ``selection on observables'' and ``ignorability'', or
in Pearl's formalism, ``d-separation'' or equivalently, ``no open
backdoor path''(\citeproc{ref-pearl2009}{Pearl 2009a};
\citeproc{ref-pearl1995a}{Pearl and Robins 1995}). The jargon that has
evolved in causal inference is not restricted to the concept of no
unmeasured confounding. In \hyperref[section-part2]{\textbf{Part 2}} we
encounter the concept of ``causal identification,'' which is unrelated
to the concept of ``statistical identification.''

Although terminology differs across the subfields of causal inference,
the mathematical basis of causal inference evolved nearly independently
in the fields of biostatistics, economics, and computer science. This
shared foundation, anchored in proofs, has enabled a remarkable
agreement across disciplinary lines despite terminological differences.
In every approach to causal inference, a causal effect is conceptualised
as a contrast between two states of the world, only one of which may be
observed on any individual -- a ``counterfactual contrast'' also known
as a ``causal estimand'' (\citeproc{ref-hernan2024WHATIF}{Hernan and
Robins 2024}). Notably, prior to intervention, these scenarios are
purely hypothetical. Post-intervention, only one scenario is actualised
for each realised treatment, leaving the alternative as a non-observed
counterfactual. For any individual unit to be treated, that only one of
the two possible outcomes is realised underscores a critical property of
causality: causality is \textbf{not directly observable}
(\citeproc{ref-hume1902}{Hume 1902}). Causal inference, therefore, can
only quantify causal effects by combining data with counterfactual
simulation (\citeproc{ref-bulbulia2023a}{Bulbulia \emph{et al.} 2023};
\citeproc{ref-edwards2015}{Edwards \emph{et al.} 2015}). The concept of
a counterfactual data science -- may sound strange. However, anyone who
has encountered a randomised experiment has encountered counterfactual
data science. Before building intuitions for causal inference from the
familiar example of experiments, let's first build intuitions for the
idea that causal quantities are never directly observed.

\subsection{The Fundamental Problem of Causal Inference: Counterfactual
Comparisons in Environmental
Psychology}\label{the-fundamental-problem-of-causal-inference-counterfactual-comparisons-in-environmental-psychology}

Imagine you are faced with a significant life decision: enrolling in a
graduate programme in environmental psychology in New Zealand or
accepting a job offer from a leading renewable energy company. This
choice will shape your future, influencing your lifestyle, income, and
social network. Which option is best for you?

The challenge is that, once you choose one path, you cannot observe how
your life would have unfolded on the other. If you go to graduate
school, you will experience that outcome, but the outcome of taking the
job remains unknown---and vice versa. This is \textbf{the fundamental
problem of causal inference}: we can never observe both potential
outcomes for the same individual, so the path not taken remains an
unobservable ``what if?'' --- a counterfactual that cannot be measured
(\citeproc{ref-holland1986}{Holland 1986}).

Again, counterfactual comparisons lie at the heart of causal inference,
as they involve contrasting what actually happened with what would have
happened under a different scenario. In environmental psychology,
computing counterfactual contrasts is essential for understanding the
effects of psychological and behavioural interventions on well-defined
outcomes. However, the full data required to make such contrasts are
inevitably partially missing and can only be recovered by assumptions
(\citeproc{ref-edwards2015}{Edwards \emph{et al.} 2015};
\citeproc{ref-westreich2015}{Westreich \emph{et al.} 2015}).

\subsubsection{Causal Inference in Experiments: The Problem of Missing
Counterfactuals}\label{causal-inference-in-experiments-the-problem-of-missing-counterfactuals}

Consider a question relevant to environmental psychologists: What is the
causal effect of access to green spaces on subjective happiness? Denote
happiness by \(Y\), where \(Y_i\) represents the happiness of individual
\(i\). Assume that ``subjective happiness'' is a coherent concept. For
now, assume that errors in its measurement are not systematically linked
to access to green spaces. Suppose further that ``ample access to green
space'' is represented as a binary variable: \(A = 1\) for ``ample
access'' and \(A = 0\) for ``lack of ample access.'' These conditions
are mutually exclusive. While we simplify the treatment to a binary
variable, the concepts apply to more complex or continuous treatments.
Estimating causal effects always requires a contrast between
well-defined treatment conditions. Importantly, defining clear causal
questions is essential but often neglected in psychological science
outside experimental work.

Imagine our aim is to compare potential outcomes under different
treatment conditions. Specifically, we contrast the happiness of
individuals with access to green space (\(A = 1\)) against those without
(\(A = 0\)). The target population for this contrast should be
explicit---for example, all New Zealand residents in 2024.

A clear causal question, framed as a counterfactual contrast---also
known as a ``causal estimand''---might be:

\begin{quote}
\emph{``Among New Zealand residents, does access to abundant green space
increase self-perceived happiness compared to environments without such
spaces?''}
\end{quote}

Now, imagine---hypothetically and ethically---that we could randomise
individuals to high or low green space access. Notice that even if such
an experimental setup were possible, causal inference would face missing
data in the potential outcomes. For each person assigned to treatment,
only one potential outcome is observed, depending on the treatment they
receive. The outcome they would have experienced under the alternative
treatment is unobserved -- it remains counterfactual. Such missingness
is the fundamental problem of causal inference, raised to the level of
treatment groups. We only observe \(Y_i(1)\) for individuals with
\(A_i = 1\) and \(Y_i(0)\) for individuals with \(A_i = 0\). The other
potential outcome for each individual remains unobserved. However,
although this missingness in the ``full data'' poses a challenge at the
individual level, we can estimate the average treatment effect within
the sampled population without needing to obtain individual-level causal
effects.

Although individual causal effects are generally not recoverable from
data, we can recover causal effect estimates by changing our causal
question. For example, although randomised controlled experiments do not
solve the fundamental problem of causal inference at the level of
individuals, they may obtain consistent causal effect estimates for
average treatment effects at the level of populations. Randomised
experiments solve the missing data problem at the heart of causal
inference by satisfying three fundamental assumptions required for
obtaining average treatment effects. These are the (1) Conditional
Exchangeability Assumption, (2) The Causal Consistency Assumption, and
(3) The Positivity Assumption. We call an ``observational study'' one in
which the data have not been obtained by randomised treatment assignment
and controlled administration of treatment. We then consider how, when
observational studies satisfy the three fundamental assumptions of
causal inference, investigators may obtain consistent causal effect
estimates for average treatment effects.

Before proceeding, we offer the following clarification: the concept of
within-subject effects as understood in traditional statistical analyses
does not directly apply in causal inference and may even appear
incoherent within this framework. In conventional analyses,
within-subject effects refer to changes observed within the same
individual across different conditions or over time, using repeated
measures to control for individual-specific variability. However, causal
inference is fundamentally concerned with estimating causal effects
based on hypothetical interventions and potential outcomes---scenarios
that cannot be simultaneously realized for a single individual. Since an
individual cannot both receive and not receive a treatment at the same
time, we cannot observe both potential outcomes required to define the
causal effect at the individual level. Instead, causal inference relies
on comparisons of population averages under distinct treatments or
treatment regimes. Therefore, what is often termed a 'within-subject
effect'' is more accurately understood as a within-subject research
design in the context of causal inference. In causal inference, there is
no coherent distinction for a within-person/between-person causal effect
(\citeproc{ref-rohrer2023withinbetween}{Rohrer and Murayama 2023})

\paragraph{Assumption 1: Conditional
Exchangeability}\label{assumption-1-conditional-exchangeability}

First, we define the expected value of a treatment \(A=a\) as the sum of
individual counterfactual (or equivalently) ``potential'' outcomes for
individuals within a specified population:

\[
\mathbb{E}[Y(a)] \equiv \frac{1}{n} \sum_{i=1}^n Y_i(a)
\]

Note that in causal inference, we assume these potential outcomes to be
real, even if determined stochastically.

We next define the average treatment effect (ATE) as a contrast between
the averages of the potential outcomes in each treatment condition:

\[
\text{ATE} = \mathbb{E}[Y(1)] - \mathbb{E}[Y(0)]
\]

Suppose that the individuals in a sample are representative of the
population of interest---the target population. In this setting,
randomisation ensures there is no common cause of the treatment and the
potential outcomes that would be observed under treatment. That is, if
treatment assignment is determined by chance, any difference in the
average response within treatment groups is best explained by the
treatment itself.

Mathematically, we express the absence of a common cause of treatment
assignment and the potential outcomes under treatment:

\[
Y(a) \coprod A
\]

This notation means that the potential outcomes \(Y(a)\) are independent
of (\(\coprod\)) the treatment assignment \(A\). Importantly, the
observed outcomes need not be independent of treatment. Note that we do
not require that exposure is independent of the actual outcomes under
exposure -- \(Y|A\) --, which would be equivalent to requiring there can
be no causal effects for treatment! Randomisation achieves unconditional
independence, allowing us to estimate average causal effects because
treatments remain independent of potential outcomes under treatment. Put
differently, randomisation ensures that average treatment differences
(or equivalently, differences in treatment averages) are the result of
the treatments themselves rather than the effects of another variable.

Importantly, we can relax the requirement for unconditional independence
and allow randomisation to occur conditional on certain measurable
features of the sampled population. For example, suppose we randomise
treatment within different age cohorts such that older individuals have
a greater probability of receiving the treatment than younger
individuals. Assume that the treatment effect is constant across all
ages. In this scenario, we would expect to see higher average treatment
effects in older cohorts simply because a larger proportion of older
individuals receive the treatment.

When randomisation occurs conditional on a measurable feature such as
age, and the treatment effect is constant, differences in treatment
probabilities across groups can lead to variations in observed average
treatment effects. However, if we adjust for this difference in the
probability of receiving treatment, or simply compare treatment effects
within the different age strata, the bias from differential treatment
group assignment disappears.

We use the symbol \(L\) to denote measured variables that might be
common causes of the treatment (\(A\)) and the treatment effect
(\(Y(a)\)). We write that the potential or counterfactual outcome
\(Y(a)\) is independent of \(A\), conditional on \(L\) as follows:

This assumption means that, within levels of \(L\), the treatment
assignment is as good as random. In experiments, randomisation ensures
unconditional exchangeability (\(Y(a) \coprod  A\)). However, to compute
causal effects from data in which the treatments are not randomised, we
must believe that within levels of measured covariates \(L\), the
treatment is as good as random. Practically, this involves measuring all
common causes of \(A\) and \(Y\).

\subparagraph{Challenges in Observational
Settings}\label{challenges-in-observational-settings}

In observational studies, achieving conditional exchangeability is
challenging because treatment assignment is not controlled. Individuals
with access to green spaces may differ from those without in various
ways. We have noted that income might be a common cause of both access
to green space and happiness. However, there are other common causes,
such as age, health status, and past happiness. Different age groups may
have varying access to green spaces and different happiness baselines.
Healthier individuals might choose to live near green spaces and also
report higher happiness. Furthermore, happiness might cause people to
seek green spaces, and happiness in one's past might be a cause of
happiness in one's future.

Merely observing statistical associations between access to green space
and happiness does not itself resolve the causal question of whether
intervening on access across a population would affect happiness levels,
and if so, in which direction and by how much.

\paragraph{Assumption 2: Causal
Consistency}\label{assumption-2-causal-consistency}

Causal consistency requires that the potential outcome under the
treatment actually received equals the observed outcome:

\[
Y_i = Y_i(a) \quad \text{if } A_i = a
\]

Note that in causal inference, we compare potential outcomes under at
least two different treatments, say, \(Y(a = 1)\) and \(Y(a = 0)\) (in
causal inference, we use a lowercase variable to note that the random
variable \(A\) is fixed to a certain level (\(A = a\)). To compute
causal contrasts, the counterfactual or potential outcomes under
treatment must be observable. The causal consistency assumption allows
investigators to link counterfactual or potential outcomes to observed
outcomes. It might seem obvious that if one receives a treatment, we can
say that the observation of the outcome following treatment is no longer
counterfactual---it is actual. However, for the causal consistency
assumption to hold across a population, we must assume that the
treatments are well-defined and consistently administered. Thus, the
causal consistency assumption is a two-edged sword. On the one hand, we
may use it---if other assumptions are satisfied---to compute causal
contrasts. Simply put, causal consistency puts the factual into
counterfactual.

In controlled experiments, we may typically take causal consistency for
granted. The investigators administer consistent treatments. However, in
observational settings, no such control is ensured. For example, one
reason it has been difficult to investigate the causal effects of weight
loss from massive observational medical datasets is that there are many
ways to lose weight---eating less and healthfully and exercising causes
people to lose weight. However, one may also lose weight from smoking,
psychological distress, stomach stapling, amputation, cancer, and
famine. The mechanisms of the latter forms of weight loss are unhealthy.
Thus, stating a causal contrast, for example, as the expected difference
in all-cause mortality after five successive years of weight loss, is an
invitation for confusion. The treatments that lead to weight loss in
medical data are not comparable across all cases (refer to VanderWeele
and Hernan (\citeproc{ref-vanderweele2013}{2013}), Bulbulia
(\citeproc{ref-bulbulia2023}{2024b})).

\subparagraph{Challenges in Observational
Settings}\label{challenges-in-observational-settings-1}

Similarly, in observational environmental psychology, the treatments of
interest may not be standardised. For green space access, we might worry
about variability in the green spaces we have measured: not all green
spaces are equal---differences in size, quality, and amenities can
affect outcomes. Moreover, there is likely considerable variability in
exposure levels; the amount of time that individuals spend in green
spaces can vary widely. These variations can violate causal consistency
because the ``treatment'' is not the same across individuals on which
treatments are to be compared. Supplemental materials S-Appendix-B
address the causal consistency assumption in detail.

\paragraph{Assumption 3: Positivity}\label{assumption-3-positivity}

Positivity requires that every individual has a non-zero probability of
receiving each level of the treatment, given their covariates \(L\):

\[
P(A = a \mid L = l) > 0 \quad \text{for all } a, l
\]

This assumption ensures that we have data to compare treatment effects
across all levels of \(L\). For example, suppose we are interested in
the causal effects of vasectomy on happiness. It would not make sense to
include biological females in this study because biological females
cannot have vasectomies (\citeproc{ref-hernan2023}{Hernan and Robins
2023}; \citeproc{ref-westreich2010}{Westreich and Cole 2010}).

\subparagraph{Challenges in Observational
Settings}\label{challenges-in-observational-settings-2}

In practice, some individuals may have zero probability of receiving
certain treatments. For example, some regions may lack green spaces
entirely owing to geographic constraints. Economic barriers may mean
that low-income individuals do not have access to areas with ample green
spaces. Policy restrictions, such as zoning laws, may also prevent
individuals from accessing specific environments.

\subsubsection{Summary}\label{summary}

Causal inference requires statistical inference; however, statistics
comes near the end of a workflow that begins by stating a causal
quantity with reference to contrasts that would be obtained if we had
the full data for the population of interest treated at different levels
of the intervention (\citeproc{ref-ogburn2021}{Ogburn and Shpitser
2021}). We then seek to understand how these contrasts may be estimated
from the data we have.

Standard practices in statistical modelling, such as regression
adjustment and structural equation modelling, frequently fall short in
estimating causal effects because our models do not attempt to state
well defined treatments to be compared, and do not appropriately account
for the causal structures that may bias treatment-outcome associations
in the data.

In the next section, we introduce causal directed acyclic graphs (causal
DAGs) -- intuitive graphical tools that allow investigators to evaluate
the complex conditional dependencies required to assess the conditional
exchangeability assumption of ``no unmeasured confounders'' by
inspecting relationships on a graph according to a simple algorithm know
as the rules of d-separation (\citeproc{ref-pearl1995}{Pearl 1995},
\citeproc{ref-pearl2009a}{2009b}). We shall discover a second use:
Causal DAGs demonstrate how common practices in statistical modelling
such as over-adjustment in regression and mediation analysis in
structural equation modelling, may inadvertently introduce bias through
inappropriate variable adjustment. Such over-adjustment can distort and
mask true causal associations. In causal inference, we call non-causal
associations ``spurious'' because the association lacks any coherent
interpretation. It is unclear how to interpret a non-causal association
because it may indicate no causal association, a causal association with
a different magnitude, or a causal association in the opposite
direction. By making the causal structure explicit and by carefully
considering which variables to adjust for, causal DAGs may help
environmental psychologists better estimate causal effects in
environmental psychology.

\subsection{Part 2: An Introduction to Causal
Diagrams}\label{section-part2}

We introduce causal DAGs by defining essential terminology. Refer to
\hyperref[appendix-a]{\textbf{Appendix A}} for a detailed glossary.

\subsubsection{Elements of Causal
Diagrams}\label{elements-of-causal-diagrams}

A causal directed acyclic graph is a diagram that clarifies whether and
how the assumption of conditional exchangeability (no unmeasured
confounders) may be satisfied from data. A causal DAG is constructed
with the aim of evaluating this assumption for a specific treatment
variable (or set of treatment variables) and a specific outcome (or set
of outcomes). When there are multiple treatments, the assumptions
required for causal identification become increasingly complex. Here, we
restrict our focus to the setting in which there is one treatment
variable. The difficulty of ensuring identification for multiple
treatments will become evident from this restricted setting.

It is important to note at the outset that in a causal DAG, we do not
attempt to represent all causal relationships within a system, but only
those that are relevant to the causal question of interest. This
requires a conceptual shift from the use of graphs in the structural
equation modelling (SEM) tradition, where the goal is to represent a
system of statistical relationships obtained from data. Instead, causal
DAGs focus on the minimal set of relationships necessary to address the
causal question and assess whether pre-specified causal contrasts can be
identified from data.

\paragraph{\texorpdfstring{1. \textbf{Nodes}}{1. Nodes}}\label{nodes}

Nodes represent variables --- or states of the world relevant to our
research question.

\paragraph{\texorpdfstring{2. \textbf{Arrows
(Edges)}}{2. Arrows (Edges)}}\label{arrows-edges}

Arrows indicate the direction and presence of causal relationships
between nodes. They represent the assumed flow of causal influence from
a ``parent'' (originating variable) to a ``child'' (receiving variable).
For example, an arrow from \(A \rightarrow Y\) suggests that \(A\)
causally influences \(Y\).

The goal of a causal DAG is to determine whether consistent causal
effect estimates can be obtained from data by inspecting the
relationships within the diagram.

\paragraph{\texorpdfstring{3.
\textbf{Conditioning}}{3. Conditioning}}\label{conditioning}

Recall that \(Y(a) \coprod A \mid L\) is the conditional exchangeability
assumption---or the assumption of ``no unmeasured confounders.'' To
infer causal effects, we must ensure that, conditional on To infer
causal effects we must ensure that conditional on \(L\), treatment
assignment is ``as good as random.'' That is, given \(L\), the treatment
assignment is essentially random, and any observed association between
\(A\) and \(Y\) can be interpreted causally.

We denote variables that we ``control for,'' ``condition on,'' or
``adjust for'' by enclosing them in a box: \(\boxed{L}\).

\subsubsection{The Rules of
d-Separation}\label{the-rules-of-d-separation}

In causal diagrams, d-separation is a graphical criterion used to
determine whether a set of variables is independent of another set,
given a third set, based on the structure of the graph. A path between
two variables is considered blocked or d-separated if, given the
variables we condition on, the path does not transmit any association
between the variables. If all paths between two variables are blocked,
the variables are d-separated and are conditionally independent.
Conversely, if any path is unblocked (d-connected), the variables may be
dependent (\citeproc{ref-pearl1995}{Pearl 1995},
\citeproc{ref-pearl2009a}{2009b}). Note that the ``d'' in d-separation
stands for ``directional.''

We next introduce the rules of d-separation, which help us identify
confounders and develop strategies for valid causal inference from
statistical associations (\citeproc{ref-pearl1995}{Pearl 1995},
\citeproc{ref-pearl2009a}{2009b}).

\paragraph{Blocked Paths and
d-Separation}\label{blocked-paths-and-d-separation}

A path is ``blocked'' when a node on it prevents causal influence from
passing between variables. We say that two variables \(A\) and \(B\) are
d-separated when all paths between them are blocked, which we denote as
\(A \coprod B\). D-separation implies that there is no statistical
association between these variables, given the conditioning set. It
helps us identify which variables need to be controlled to ensure valid
causal inference.

\paragraph{Open Paths}\label{open-paths}

An path is ``open'' if at least one route between variables remains
unblocked, allowing the transmission of association---even without
direct causation (\(A \cancel\coprod B\)). Open paths imply that there
is still potential for information, influence, or confounding to flow
between \(A\) and \(B\), which can introduce bias into causal estimates
if left uncontrolled.

\paragraph{Pearl's Backdoor Path
Criterion}\label{pearls-backdoor-path-criterion}

Pearl's backdoor path criterion is a rule used to identify which
variables must be controlled to obtain an unbiased estimate of the
causal effect of one variable on another. A set of variables \(A\)
satisfies the backdoor criterion for estimating the causal effect of
\(B\) on \(C\) if:

\begin{enumerate}
\def\labelenumi{(\arabic{enumi})}
\tightlist
\item
  No variable in \(A\) is a descendant of \(B\).
\item
  \(A\) blocks every backdoor path between \(B\) and \(C\).
\end{enumerate}

A backdoor path is any path between \(B\) and \(C\) that starts with an
arrow pointing into \(B\), and ends with an arrow pointing out of \(C\)
(here we ignore the direction of the arrows). These paths allow for bias
such that the statistical association between \(B\) and \(C\) does not
reflect a causal association. By blocking all backdoor paths,
conditioning on \(\boxed{A}\) removes spurious associations between
\(B\) and \(C\), allowing for an unbiased estimate of the causal effect.

\subsubsection{The Five Elementary Graphical Structures of Causality and
Five Rules for Confounding Control}\label{sec-five-elementary}

To uncover causal insights from statistical relationships, we must
understand five basic graphical structures and the corresponding rules
for confounding control. These structures help identify how variables in
a system are connected and how to use those connections to avoid bias
when estimating causal effects. Remember that balancing confounders
across treatments requires ensuring statistical independence between
potential outcomes and treatment within groups defined by measured
covariates.

\paragraph{Absence of Causality: Two Variables with No
Arrows}\label{absence-of-causality-two-variables-with-no-arrows}

When no arrows connect \(A\) and \(B\), we assume they do not share a
causal relationship and are statistically independent. This means that
knowing something about \(A\) does not give us any information about
\(B\) and vice versa. Graphically, we represent this relationship as:

\[
\xorxALARGE
\]

\paragraph{Causal Structure 1: Direct Causation Between Two
Variables}\label{causal-structure-1-direct-causation-between-two-variables}

A causal arrow (\(A \to B\)) signifies that changes in \(A\) directly
cause changes in \(B\), creating statistical dependence between them.
This direct causal link is graphically depicted as:

\[
\xtoxALARGE
\]

\paragraph{\texorpdfstring{\textbf{Rule 1: Ensure That the Treatment
Precedes the
Outcome}}{Rule 1: Ensure That the Treatment Precedes the Outcome}}\label{sec-four-rules}

Causality follows the arrow of time; an outcome occurs after the
intervention that causes it. Therefore, the treatment (\(A\)) must occur
before the outcome (\(B\)) in order to claim causality. This underscores
the importance of temporal precedence in causal inference. Where the
relative timing of events is unknown---as it often is in observational
data---we cannot ensure that the treatment precedes the outcome unless
we make additional assumptions. Such assumptions may be implausible or
untestable. Where this is the case, we cannot rule out the possibility
that the outcome precedes the treatment.

\subparagraph{Motivating Example}\label{motivating-example}

Suppose we find an association between conservation behaviours and
happiness. We might infer that conservation behaviours cause happiness.
However, it is also possible that happy people are simply more likely to
engage in conservation behaviours. If we do not have longitudinal data
that shows treatment preceding the outcome, we cannot rule out this
alternative explanation.

\paragraph{Causal Structure 2: The Fork Structure---Common Cause
Scenario}\label{causal-structure-2-the-fork-structurecommon-cause-scenario}

The fork structure occurs when a variable \(A\) influences both \(B\)
and \(C\) (\(A \rightarrow B\) and \(A \rightarrow C\)). This makes
\(A\) a common cause of \(B\) and \(C\). Graphically:

\[
\forkLARGE
\]

\paragraph{Rule 2: Condition on the Common Cause to Block
Confounding}\label{rule-2-condition-on-the-common-cause-to-block-confounding}

To estimate the causal effect between \(B\) and \(C\), we need to adjust
for the common cause \(A\) (indicated by \(\boxed{A}\)). This adjustment
blocks the confounding path and removes any non-causal association
between \(B\) and \(C\).

\subparagraph{Motivating Example}\label{motivating-example-1}

Suppose data reveals that areas with higher rates of bicycle commuting
also have lower levels of psychological distress. Does bicycle commuting
directly reduce distress? Not necessarily. Sunshine hours (\(A\)) could
be a common cause:

\begin{itemize}
\tightlist
\item
  Sunshine (\(A\)) encourages bicycle commuting (\(B\)).
\item
  Sunshine (\(A\)) reduces psychological distress (\(C\)).
\end{itemize}

By adjusting for sunshine (\(\boxed{A}\)), the apparent link between
bicycle commuting (\(B\)) and psychological distress (\(C\)) disappears.
This is an example of a spurious association -- one that that does not
arise from a direct causal relationship between \(B\) and \(C\) but
rather from their common cause, \(A\)

\paragraph{Causal Structure 3: The Chain Structure---A
Mediator}\label{causal-structure-3-the-chain-structurea-mediator}

The chain structure (\(A \rightarrow B \rightarrow C\)) illustrates a
scenario where \(A\) affects \(B\), and \(B\) in turn affects \(C\). In
this case, \(B\) acts as a mediator in the relationship between \(A\)
and \(C\). Graphically:

\[
\chainLARGE
\]

\paragraph{Rule 3: Avoid Conditioning on the Mediator for Total
Effect}\label{rule-3-avoid-conditioning-on-the-mediator-for-total-effect}

If we are interested in estimating the total causal effect of \(A\) on
\(C\), we should not condition on the mediator \(B\). Conditioning on
\(B\) would \textbf{block} the path from \(A\) to \(C\), making \(A\)
and \(C\) appear independent, even though there is a causal
relationship.

Many psychological scientists are taught to adjust for a rich array of
confounders without attending to hazards of over-adjustment. Consider
the following example of how over-adjustment can distort causal
pathways, potentially masking true relationships.

\subparagraph{Motivating Example}\label{motivating-example-2}

Suppose we examine whether renovating green spaces (\(A\)) in urban
areas reduces crime rates (\(C\)). The renovation improves community
engagement (\(B\)), which then reduces crime. If we adjust for community
engagement, it might look like the green space renovation has no effect
on crime, when in fact it does, but indirectly through community
engagement. This example illustrates the risks of
\textbf{over-conditioning bias.} Let us denote the relative timing of
events using subscripts. Here, we assume that \(A_1 \to B_2 \to C_3\).
It is clear from the rules of d-separation that
\(A_{1} \rightarrow \boxed{B_2} \rightarrow C_{3}\) (conditioning on
\(B_2\)) blocks the path from \(A_1\) to \(C_3\). However, what if
\(B_0 \to A_1\) and \(B_0 \to C_3\). The rules of d-separation tell us
that to obtain an unbiased causal effect estimate of \(A_1 \to C_3\) we
must condition on \(B_0\). With cross-sectional data we cannot typically
ensure the relative timing of confounders, treatments, and outcomes. For
this reason, we should collect time-series data to ensure that the
relative timing of events assumed in our model is appropriate.

In some cases, investigators may want to distinguish between direct and
indirect effects, as is done in causal mediation analysis. However,
performing causal mediation analysis requires making additional
assumptions about the relationships among variables and about unmeasured
confounding. These assumptions are often difficult to verify. Causal
mediation analysis goes beyond the scope of our current discussion,
however interested readers may refer to VanderWeele
(\citeproc{ref-vanderweele2015}{2015}); Bulbulia
(\citeproc{ref-bulbulia2024swigstime}{2024c}).

\paragraph{Causal Structure 4: The Collider Structure---A Common
Effect}\label{causal-structure-4-the-collider-structurea-common-effect}

The collider structure (\(A \to C\), \(B \to C\)) occurs when two
variables \(A\) and \(B\) independently cause a common effect \(C\).
Initially, \(A\) and \(B\) are independent. However, conditioning on the
collider \(C\) introduces a non-causal or ``spurious'' association
between \(A\) and \(B\). Graphically:

\[
\immoralityLARGE
\]

This gives us two rules:

\paragraph{Rule 4: Do Not Condition on a
Collider}\label{rule-4-do-not-condition-on-a-collider}

When assessing the causal effect of \(A\) on \(B\), avoid conditioning
on a collider (\(C\)) or its descendants, as it may introduce a
misleading association between \(A\) and \(B\).

\paragraph{Rule 5: Do Not Condition on Descendants of
Colliders}\label{rule-5-do-not-condition-on-descendants-of-colliders}

Because causation induces association, conditioning on a descendant is
akin to conditioning on its parent. As such a descendant functions as a
\emph{proxy} for its parent. We should thus avoid conditioning on a
descendant of a collider (or mediator).

\subparagraph{Motivating Example}\label{motivating-example-3}

Suppose we want to assess if access to green spaces (\(A\)) makes people
happier (\(B\)). If we condition on health status (\(C\)), which is
affected by both \(A\) and \(B\), we may introduce a spurious
association between \(A\) and \(B\). This is because conditioning on
\(C\) creates an artificial link between \(A\) and \(B\), even if no
direct causal relationship exists. Specifically, among unhealthy
individuals (low \(C\)), those with high green space access (\(A\)) may
appear less happy (\(B\)). When we stratify by \(C\), a negative
association emerges, even if \(A\) and \(B\) are not causally linked.
Likewise, among healthy individuals (high \(C\)), those with low green
space access (\(A\)) may be happier (\(B\)). Stratifying by \(C\) would
suggest a negative association between green space access and happiness
in the absence of any causal relationship.

This example again illustrates the risks of \textbf{over-conditioning
bias.} By ensuring that confounders precede both the treatment and the
outcome, we can often avoid collider bias -- often, but not always.
Below we consider how adjusting for pre-treatment confounders may induce
collider bias. Elsewhere, we consider how conditioning on post-treatment
proxies for pre-treatment confounders (\(\L^prime\)) may be sensible
strategy for confounding control (refer to Bulbulia
(\citeproc{ref-bulbulia2023}{2024b})). For now, we simply note that
collider bias is a type of over-conditioning bias, one whose threat to
valid causal inference can be understood by referring to elementary
relationships assumed in a causal DAG.

\subsubsection{Complex Causal Relationships Are Composed of Basic
Structures That We
Assume}\label{complex-causal-relationships-are-composed-of-basic-structures-that-we-assume}

All forms of confounding bias stem from combinations of the basic causal
structures outlined above (absence of cause, causality, forks, chains,
and colliders). Understanding these elements allows us to identify
potential confounders based on the assumptions encoded in a causal
diagram.

Causal directed acyclic graphs (DAGs) provide a framework for
structuring assumptions about causal relationships. They cannot prove
causation on their own, as multiple diagrams may be consistent with the
data. The strength of causal DAGs lies in helping researchers understand
the implications of their assumptions. Therefore, causal DAGs should be
created in collaboration with domain experts, and when experts disagree,
multiple causal diagrams should be proposed to reflect differing
assumptions.

\subsection{Part 3. How to Use Causal Diagrams for Causal Identification
Tasks- Worked Examples}\label{section-part3}

\subsubsection{Notation}\label{notation}

Causal diagrams use specific symbols to represent elements essential in
causal inference (\citeproc{ref-greenland1999}{Greenland \emph{et al.}
1999}; \citeproc{ref-pearl1995}{Pearl 1995},
\citeproc{ref-pearl2009}{2009a}). We review our notation.

\begin{itemize}
\tightlist
\item
  \textbf{Treatment (\(A\))}: an intervention or exposure, such as
  giving people access to green space.
\item
  \textbf{Outcome (\(Y\))}: the variable of interest affected by the
  treatment, such as subjective well-being. Recall that \(Y(a)\)
  represents the effect on \(Y\) when \(A\) is set to a specific value,
  \(a\).
\item
  \textbf{Measured Confounders (\(L\))}: variables that may influence
  both the treatment and outcome, such as age, gender, or income.
\item
  \textbf{Measured Descendants of Confounders Not Affected By Treatment
  or Outcome: (\(L^\prime\))}: we shall see that by conditioning on
  measured descendants of confounders not affected by the treatment, we
  may adjust for the effects of confounders. That is, descendants of
  confounders function as surrogates for the confounders.
\item
  \textbf{Unmeasured Confounders (\(U\))}: variables not directly
  observed but potentially influence the treatment and outcome, leading
  to a non-causal or ``spurious'' association between treatment and
  outcome. Table~\ref{tbl-04} provides seven worked examples that put
  causal diagrams to work. Our example will focus on the question of
  whether access to green space affects happiness and approach this
  question by focusing on how different assumptions about (i) the
  structure of the world and (ii) the observational data that have been
  collected may affect strategies for confounding control and the
  confidence in our results. Each example refers to a row in the table.
\end{itemize}

\begin{table}

\caption{\label{tbl-04}Putting causal directed acyclic graphs (DAGs) to
work.}

\centering{

\terminologyelconfoundersLONG

}

\end{table}%

\subsubsection{1. Confounding by a Common
Cause}\label{confounding-by-a-common-cause}

Table~\ref{tbl-04} Row 1 describes the confounding problem of a common
cause. We encountered this problem in \textbf{Part 2}(\#section-part2).
Such confounding arises when there is a variable or set of variables,
denoted by \(L\), that influence both the exposure, denoted by \(A\),
and the outcome, denoted by \(Y\) Because \(L\) is a common cause of
both \(A\) and \(Y\), \(L\) may create a statistical association between
\(A\) and \(Y\) that does not reflect a causal association.

For instance, in the context of green spaces, consider people who live
closer to green spaces (exposure \(A\)) and their experience of improved
happiness (outcome \(Y\)). A common cause might be socioeconomic status
\(L\). Individuals with higher socioeconomic status might have the
financial capacity to afford housing near green spaces and
simultaneously afford better healthcare and lifestyle choices,
contributing to greater happiness. Thus, although the data may show a
statistical association between living closer to green spaces \(A\) and
greater happiness \(Y\), this association might not reflect a direct
causal relationship owing to confounding by socioeconomic status \(L\).

Addressing confounding by a common cause involves adjusting for the
confounder in one's statistical model. We may adjust through regression,
or more complicated methods, such as the inverse probability of
treatment weighting, marginal structural models, and others see Hernan
and Robins (\citeproc{ref-hernan2024WHATIF}{2024}). Such adjustment
effectively closes the backdoor path from the exposure to the outcome.
Equivalently, conditioning on \(L\) leads to d-separation of \(A\) and
\(Y\) because \(\boxed{L}\) closes the backdoor path from \(A\) to
\(Y\).

Table~\ref{tbl-04} Row 1, Column 3, emphasises that a confounder by
common cause must precede both the exposure and the outcome. Although in
cross-sectional data it is sometimes clear whether a confounder precedes
the treatment (e.g., a person's country of birth reported in
cross-sectional data) often the relative timing of events is unclear. We
must draw several causal diagrams to consider the implications of
different assumptions. Below we return to idea that investigators should
draw multiple causal DAGs and perform multiple analyses. Next, we
consider scenarios in which investigators condition on effects of the
treatment.

\subsubsection{2. Mediator Bias}\label{mediator-bias}

Table~\ref{tbl-04} Row 1 presents a problem of mediator bias. Consider
again whether proximity to green spaces, \(A\), affects happiness,
\(Y\). Suppose that physical activity is a mediator, \(L\).

Imagine living close to green spaces (\(A\)) increases physical activity
(\(L\)), which then improves happiness (\(Y\)). If we treat physical
activity (\(L\)) as a confounder and adjust for it, we will bias our
estimate of the total effect of green space proximity (\(A\)) on
happiness (\(Y\)). This bias happens because adjusting for \(L\) breaks
the link between \(A\) and \(Y\). This is called mediator bias. Notably,
experiments are not immune to such over-conditioning biases. For example
Montgomery \emph{et al.} (\citeproc{ref-montgomery2018}{2018}) found
that over half of all published research in political science
experiments were guilty of conditioning on a post-treatment variable
(see also Bulbulia (\citeproc{ref-bulbulia_2024_experiments}{2024e})).

To avoid mediator bias when estimating a total causal effect, we should
never condition on a mediator. The surest way to prevent this problem is
to ensure that data collection for covariates we assume to be
confounders \(L\) occurs before data collection of the treatment \(A\),
which in turn occurs before data collection of the outcome \(Y\)
(\citeproc{ref-vanderweele2020}{VanderWeele \emph{et al.} 2020}). We
present this solution in Table~\ref{tbl-04} Row 2 Col 3.

\subsubsection{3. Confounding by Collider Stratification (Conditioning
on a Common
Effect)}\label{confounding-by-collider-stratification-conditioning-on-a-common-effect}

Table~\ref{tbl-04} Row 1 presents a problem of collider bias.
Conditioning on a common effect, or collider stratification, occurs when
a variable thought to be a confounder, denoted by \(L\), is in fact
influenced by both the exposure, denoted by \(A\), and the outcome,
denoted by \(Y\).

Suppose that the decision to live closer to green spaces (exposure
\(A\)) and states of subjective happiness (outcome \(Y\)) are
independent such that \(A \coprod Y(a)\). Furthermore, assume physical
health \(L\) is an effect of both access to green space and states of
subjective happiness. That is, assume that \(L\) is an effect of \(A\)
and \(Y\). If we were to condition on \(L\) in this setting, we would
introduce \emph{collider stratification bias}. When we control for the
common effect \(L\) (physical health), we may inadvertently introduce
confounding. This happens because knowing something about \(L\) gives us
information about both \(A\) and \(Y\). If someone were high on physical
health but low an access to greenspace, this would imply that they are
higher in happiness. Likewise, if someone were low in physical health
but high in access to green space, this would imply lower happiness. As
a result of our conditioning strategy, it would appear that access to
green space and happiness are negatively associated. However, if we were
to avoid conditioning on the common outcome, we would find that the
treatment and outcome are not associated.

How can we avoid collider bias, the temporal sequence of measurement
affords a powerful strategy:

Ensure all common causes of \(A\) and \(Y\) -- call them \(L\) -- are
measured before the treatment \(A\) occurs. Ensure further that \(Y\)
occurs after \(A\) occurs. If the confounder \(L\) is not measured,
ensure that conditioning on its downstream proxy, \(L'\) does not induce
collider or mediator biases.

By adhering to this sequence, we can mitigate the risk of collider
stratification bias and better understand the causal relationships
between exposure, outcome, and their common effects.

\subsubsection{4. Confounding by Conditioning on a Descendant of a
Confounder}\label{confounding-by-conditioning-on-a-descendant-of-a-confounder}

Table~\ref{tbl-04} Row 4 presents a problem of collider bias by decent.
Recall the rules of d-separation also apply to conditioning on
descendants of a confounder. Thus, we may unwittingly evoke confounding
by proxy when conditioning on a measured descendant of an unmeasured
collider.

For example, if doctor visits were encoded in our data, and doctor
visits were an effect of poor health, conditioning on doctor visits
would function similarly to conditioning on poor health in the previous
example, introducing collider confounding.

\subsubsection{5. M-bias: Conditioning on Pre-Exposure
Collider}\label{m-bias-conditioning-on-pre-exposure-collider}

There are only five elementary structures of causality. Every
confounding scenario can be developed from these five elementary
structures. We next consider how we may combine these elementary causal
relationships in causal diagrams to create effective strategies for
confounding control.

Table~\ref{tbl-04} Row 5 presents a form of pre-exposure
over-conditioning confounding known as ``M-bias''. This bias combines
the collider structure and the fork structure, revealing what might not
otherwise be obvious: it is possible to induce confounding even if we
ensure that all variables have been measured \textbf{before} the
treatment. The collider structure is evident in the path \(U_Y \to L_0\)
and \(U_A \to L_0\). The collider rule shows that conditioning on
\(L_0\) opens a path between \(U_Y\) and \(U_A\). What is the result? We
find that \(U_Y\) is associated with the outcome \(Y\) and \(U_A\) is
associated with treatment \(A\). This is a fork (common cause)
structure. The association between treatment and outcome opened by
conditioning on \(L\) arises from an open back-door path that occurs
from the collider structure. We thus have confounding. How might such
confounding play out in a real-world setting?

In the context of green spaces, consider the scenario where an
individual's level of physical activity \(L\) is influenced by an
unmeasured factor related to their propensity to live near green spaces
\(A\) -- say childhood upbringing. Suppose further that another
unmeasured factor -- say a genetic factor -- increases both physical
activity \(L\) and happiness \(Y\). Here, physical activity \(L\) does
not affect the decision to live near green spaces \(A\) or happiness
\(Y\) but is a descendent of unmeasured variables that do. If we were to
condition on physical activity \(L\) in this scenario, we would create
the bias just described -- ``M-bias.''

How shall we respond to this problem? The solution is straightforward.
If \(L\) is neither a common cause of \(A\) and \(Y\) nor the effect of
a shared common cause, then \(L\) should not be included in a causal
model. In terms of the conditional exchangeability principle, we find
\(A \coprod Y(a)\) yet \(A \cancel{\coprod} Y(a)| L\). So we should not
condition on \(L\): do not control for exercise
(\citeproc{ref-cole2010}{Cole \emph{et al.} 2010}).\footnote{Note that
  when we draw a chronologically ordered path from left to right, the M
  shape for which ``M-bias'' takes its name changes to an E shape. We
  shall avoid proliferating jargon and retain the term ``M bias.''}

\subsubsection{6. Conditioning on a Descendent May Sometimes Reduce
Confounding}\label{conditioning-on-a-descendent-may-sometimes-reduce-confounding}

In Table~\ref{tbl-04} Row 6, we encounter a causal diagram in which an
unmeasured confounder opens a back-door path that links the treatment
and outcome. Here, we consider how we may use the rules of d-separation
to obtain unexpected strategies for confounding control.

Returning to our green space example, suppose an unmeasured genetic
factor \(U\) affects one's desire to seek out isolation in green spaces
\(A\) and independently affects one's happiness \(Y\). Were such an
unmeasured confounder to exist we could not obtain an unbiased estimate
for the causal effect of green space access on happiness. We have, it
seems, intractable confounding.

However, imagine a variable \(L^\prime\), a trait expressed later in
life that arises from this genetic factor. If such a trait could be
measured, even though the trait \(L'\) is expressed after the treatment
and outcome have occurred, controlling for \(L'\) would enable
investigators to close the backdoor path between the treatment and the
outcome. This strategy works because a measured effect is a \emph{proxy}
for its cause \(U\), the unmeasured confounder. By conditioning on the
late-adulthood trait, \(L'\), we partially condition on its cause,
\(U\), the confounder of \(A \to Y\). Thus, not all effective
confounding control strategies need to rely on measuring pre-exposure
variables. Thus, the elementary causal structures reveal a possibility
for confounding control by condition on a post-outcome variable. This
strategy is not intuitive. Although a common cause must occur before a
treatment (and outcome), its proxy need not! If we have a measure for
the latter but not the former, we should condition on the post-treatment
proxy of a pre-treatment common cause.

\subsubsection{7. Confounding Control with Three Waves of Data is
Powerful and Reveals Possibilities for Estimating an ``Incident
Exposure''
Effect}\label{confounding-control-with-three-waves-of-data-is-powerful-and-reveals-possibilities-for-estimating-an-incident-exposure-effect}

Table~\ref{tbl-04} row 7 presents another setting in which there is
unmeasured confounding. In response to this problem, we use the rules of
d-separation to develop a data collection and modelling strategy that
may greatly reduce the influence of unmeasured confounding.
Table~\ref{tbl-04} row 7 col 3, by collecting data for both the
treatment and the outcome at baseline and controlling for baseline
values of the treatment and outcome, any unmeasured association between
the treatment \(A_1\) and the outcome \(Y_2\) would need to be
\emph{independent} of their baseline measurements. As such, including
the baseline treatment and outcome, along with other measured covariates
that might be measured descendants of unmeasured confounders, is a
strategy that exerts considerable confounding control
(\citeproc{ref-vanderweele2020}{VanderWeele \emph{et al.} 2020}).

Furthermore, this causal graph makes evident a second benefit of this
strategy. Returning to our example, a model that controls for baseline
exposure would require that people initiate a change from the \(A_0\)
observed baseline level. Thus, by controlling for the baseline value of
the treatment, we may learn about the causal effect of shifting one's
access to green space status. This effect is called the ``incident
exposure effect.'' The incident exposure effect better emulates a
``target trial'' or the organisation of observational data into a
hypothetical experiment in which there is a ``time-zero'' initiation of
treatment in the data; see HernÃ¡n \emph{et al.}
(\citeproc{ref-hernuxe1n2016}{2016}); Danaei \emph{et al.}
(\citeproc{ref-danaei2012}{2012}); VanderWeele \emph{et al.}
(\citeproc{ref-vanderweele2020}{2020}); Bulbulia
(\citeproc{ref-bulbulia2022}{2023}). Without controlling for the
baseline treatment, we could only estimate a ``prevalent exposure
effect.'' If the initial exposure caused people some people to be
miserable, we would not be able to track this outcome. The prevalent
exposure effect would mask it, distorting causal inferences for the
quantity of interest, namely, what would happen, on average, if people
were to shift to having greater greenspace access.

Finally, we obtain further control for unmeasured confounding by
controlling for both the baseline treatment and the baseline outcome.
For an unmeasured confounder to affect both the treatment and the
outcome (and unmeasured fork structure), it would need to do so
independently of the baseline measures of the treatment and exposure
(\citeproc{ref-vanderweele2020}{VanderWeele \emph{et al.} 2020}). Note
again that we generally require repeated measures on the same unit over
time intervals to obtain an incident exposure effect and exert more
robust control for unmeasured confounding using past states of the
treatment and outcome.

\subsection{Part 4. Practical Guide For Constructing Causal Diagrams and
Reporting Results When Causal Structure is Unclear}\label{section-part4}

\paragraph{1. Clarify the Research Question and Target
Population}\label{clarify-the-research-question-and-target-population}

Before drawing a causal diagram, we must state the problem it addresses
and the population to whom it applies. Causal identification strategies
may vary by question. We have seen that if
\(A\to B\to C; A\to B; A\to C\) the confounding control strategy for
evaluating \(L \to Y\) differs from that for \(A \to Y\). Again,
reporting coefficients other than the association between \(A \to Y\) is
typically ill-advised; see Westreich and Greenland
(\citeproc{ref-westreich2013}{2013}); McElreath
(\citeproc{ref-mcelreath2020}{2020}); Bulbulia
(\citeproc{ref-bulbulia2023}{2024b}).

\subsubsection{2 Evaluate Bias in the Absence of a Treatment
Effect}\label{evaluate-bias-in-the-absence-of-a-treatment-effect}

Before attributing any statistical association to causality, we must
eliminate non-causal sources of correlation. We do this by, first, by
drawing the treatment (\(A\)) and outcome (\(Y\)) on our causal diagram
with no arrow linking them. We do not draw an arrow between \(A\) and
\(Y\) because we are evaluating bias in the absence of a treatment
effect. Second, we identify all common causes of \(A\) and \(Y\), and
consider whether they are measured or unmeasured. Third, we use the
rules of d-separation to evaluate whether conditioning on measured
common causes of \(A\) and \(Y\) will block all backdoor paths that
create indirect, non-causal associations between \(A\) and \(Y\). That
is, we consider whether \(A\) and \(Y\) are d-separated by the measured
common causes of \(A\) and \(Y\). If they are, then we have successfully
blocked all backdoor paths and can proceed to evaluate bias in the
presence of a treatment effect. If they are not, then we have failed to
block all backdoor paths and our estimate of the causal effect of \(A\)
on \(Y\) will be biased.

\paragraph{3. Draw the Most Recent Common Causes of Exposure and
Outcome}\label{draw-the-most-recent-common-causes-of-exposure-and-outcome}

Include all common causes (confounders) of both the exposure and the
outcome in your diagram, whether measured or unmeasured. Where possible,
group functionally similar common causes into a single variable (e.g.,
\(L_0\) for demographic variables).

\paragraph{4. Include All Ancestors of Measured
Confounders}\label{include-all-ancestors-of-measured-confounders}

Add any ancestors (precursors) of measured confounders associated with
the treatment, the outcome, or both. Simplify the causal diagram by
grouping similar variables. For example, suppose we believe that both
age and income are common causes of both the treatment and the outcome.
We may represent this belief by grouping age and income into a single
variable, \(L_0 = \{age,~income\}\).

\paragraph{5. Explicitly State Assumptions About Relative
Timing}\label{explicitly-state-assumptions-about-relative-timing}

Annotate the temporal sequence of events using subscripts (e.g.,
\(L_0\), \(A_1\), \(Y_2\)).

Note that it is imperative that causal DAGs are acyclic. The graph:
\(A \to Y \to A \to Y\) is not acyclic. \textbf{However, the demand for
an acyclic graph does not imply that dynamic causal relationships are
precluded from causal analysis.} For example
\(L_0 \to Y_1 \to  A_1 \to L_2 \to A_3 \to Y_5\) is an acyclic graph.
Here, past states of the confounders, treatment, and outcome are
depicted as affecting their future states. By indexing the relative
timing of states we ensure that the causal diagram is acyclic.

Note this example further illustrates the potential difficulties when
investigating multiple treatments. Suppose we are interested in causal
contrasts for multiple treatments over time, for example the contrast
between \(A_1 = 0, A_3 = 0\) and \(A_1 = 1, A_3 = 1\), with the outcome
\(Y_5\) measured at the end of study. Notice that \(L_2\) is a common
cause of \(A_3\) and \(Y_5\). We must condition on \(L_2\) to block the
back-door path from \(A_3\) to \(Y_5\). However, were we to condition on
\(L_2\), we would induce mediator bias because \(L_2\) blocks the path
from \(A_1\) to \(Y_5\). We cannot use standard regression, multi-level
regression, or structural equation modelling to estimate the
time-varying treatment regime of interest. We instead require special
methods (refer to Hernan and Robins
(\citeproc{ref-hernan2024WHATIF}{2024}); Bulbulia
(\citeproc{ref-bulbulia2024swigstime}{2024c})).

\paragraph{6. Arrange Temporal Order
Visually}\label{arrange-temporal-order-visually}

Arrange your diagram to reflect the temporal progression of causality,
either left-to-right or top-to-bottom. This enhances comprehension of
causal relations. Establishing temporal ordering is vital for evaluating
identification problems, as discussed in
\hyperref[sec-part3]{\textbf{Part 3}}.

\paragraph{7. Box Variables Adjusted for
Confounding}\label{box-variables-adjusted-for-confounding}

Mark variables for adjustment (e.g., confounders) with boxes or another
easy to understand convention. Be clear about this and other
conventions.

\paragraph{8. Present Paths Structurally, Not
Parametrically}\label{present-paths-structurally-not-parametrically}

Focus on whether paths exist, not their functional form (linear,
non-linear, etc.). Parametric descriptions are not relevant for bias
evaluation in a causal diagram. For an explanation of causal interaction
and diagrams, see Bulbulia (\citeproc{ref-bulbulia2023}{2024b}).

\paragraph{9. Minimise Paths to Those Necessary for the Identification
Problem}\label{minimise-paths-to-those-necessary-for-the-identification-problem}

Reduce clutter by including only paths critical for the specific
question (e.g., back-door paths, mediators).

\paragraph{10. Consider Potential Unmeasured
Confounders}\label{consider-potential-unmeasured-confounders}

Use domain expertise to identify potential unmeasured confounders and
represent them in your diagram. This proactive step helps anticipate and
address \emph{all} possible sources of confounding bias.

\paragraph{11. State Your Graphical
Conventions}\label{state-your-graphical-conventions}

Establish and explain the graphical conventions used in your diagram
(e.g., using red to highlight open back-door paths). Consistency in
symbol use enhances interpretability, while explicit descriptions
improve accessibility and understanding.

\paragraph{12. Prepare Sensitivity
Analyses.}\label{prepare-sensitivity-analyses.}

Because unmeasured confounding cannot be ruled out, investigators should
implement sensitivity analyses to determine how dependent conclusions
are on specific assumptions or parameters within your causal model. A
relatively simple sensitivity analysis is VanderWeele's E-value
(\citeproc{ref-vanderweele2017}{VanderWeele and Ding 2017})

\subsubsection{Specific Advice for Causal Analysis and Reporting in
Cross-Sectional
Designs}\label{specific-advice-for-causal-analysis-and-reporting-in-cross-sectional-designs}

\begin{table}

\caption{\label{tbl-cs}Cross-sectional designs typically require
multiple causal DAGS where the temporal order of variables cannot be
ensured.}

\centering{

\examplecrosssection

}

\end{table}%

\subsubsection{Recommendations for Conducting and Reporting Causal
Analyses with Cross-Sectional
Data}\label{recommendations-for-conducting-and-reporting-causal-analyses-with-cross-sectional-data}

When analysing and reporting analyses with cross-sectional data,
researchers face the challenge of making causal inferences without the
benefit of temporal information.

The following recommendations aim to guide researchers in navigating
these challenges effectively:

\paragraph{\texorpdfstring{1. \textbf{Draw multiple causal
diagrams}}{1. Draw multiple causal diagrams}}\label{draw-multiple-causal-diagrams}

As shown in Table~\ref{tbl-cs}, draw multiple causal diagrams to
represent different theoretical assumptions about the relationships and
timing of variables relevant to an identification problem. If some
causal pathways cannot be ruled out, clarify the implications of
assigning variables the roles for which consensus or which the time
ordering of the data do not resolve.

\paragraph{\texorpdfstring{2. \textbf{Perform and report analyses for
each
assumption}}{2. Perform and report analyses for each assumption}}\label{perform-and-report-analyses-for-each-assumption}

Conduct and transparently report separate analyses for each scenario
your causal diagrams depict. This practice ensures that your study is
theoretically grounded for each model. Presenting results from each
analytical approach and the underlying assumptions and statistical
methods promotes a balanced interpretation of findings.

\paragraph{\texorpdfstring{3. \textbf{Report divergent
findings}}{3. Report divergent findings}}\label{report-divergent-findings}

Approach conclusions with caution, especially when findings suggest
differing practical implications. Acknowledge the limitations of
cross-sectional data in establishing causality and the potential for
alternative explanations. Do not over-sell.

\paragraph{\texorpdfstring{4. \textbf{Identify avenues for future
research}}{4. Identify avenues for future research}}\label{identify-avenues-for-future-research}

Target future research that might clarify ambiguities. Consider the
design of longitudinal studies or experiments capable of clarifying
lingering uncertainties.

\paragraph{\texorpdfstring{5. \textbf{Supplement observational data with
simulated
data}}{5. Supplement observational data with simulated data}}\label{supplement-observational-data-with-simulated-data}

Leverage data simulation to understand the complexities of causal
inference. Simulating data based on various theoretical models allows
researchers to examine the effect of different assumptions on their
findings. This method tests analytical strategies under controlled
conditions, assessing the robustness of conclusions against assumption
violations or unobserved confounders.

\subsubsection{Specific Advice for Causal Analysis and Reporting in
Longitudinal
Designs}\label{specific-advice-for-causal-analysis-and-reporting-in-longitudinal-designs}

Longitudinal designs offer a substantial advantage over cross-sectional
designs for causal inference because sequential measurements allow us to
capture causation and quantify its magnitude. We typically do not need
to assert timing as in cross-sectional data settings. Because we know
when variables have been measured, we can reduce ambiguity about the
directionality of causal relationships. For instance, tracking changes
in ``happiness'' following changes in access to green spaces over time
can more definitively suggest causation than cross-sectional snapshots.

Despite this advantage, longitudinal researchers nevertheless face
assumptions regarding the absence of unmeasured confounders or the
stability of measured confounders over time. These assumptions must be
explicitly stated. As with cross-sectional designs, wherever assumptions
differ, researchers should draw different causal diagrams that reflect
these assumptions and subsequently conduct and report separate analyses.
Supplementary materials D and E provide examples of how to conduct and
report analyses for multiple causal diagrams. The following summarises
our advice.

\paragraph{1. Draw multiple causal
diagrams}\label{draw-multiple-causal-diagrams-1}

\begin{itemize}
\item
  \textbf{Identification problem diagram}: begin as usual by stating a
  causal question and population of interest. Then constructing a causal
  diagram that outlines your assumptions about the relationships among
  variables relevant for ensuring conditional exchangeability.
\item
  \textbf{Solution diagram}: next, create a separate causal diagram that
  proposes solutions to the identified problems. Having distinct
  diagrams for the problem and its proposed solutions clarifies your
  study's analytic strategy and theoretical underpinning.
\end{itemize}

Table~\ref{tbl-lg} provides an example of a table with multiple causal
diagrams clarifying potential sources of confounding threats and reports
strategies for addressing them.

\begin{table}

\caption{\label{tbl-lg}Use causal DAGs to report both the causal
identification problem and its solution.}

\centering{

\examplelongitudinal

}

\end{table}%

\paragraph{2. Attempt longitudinal designs with at least three waves of
data}\label{attempt-longitudinal-designs-with-at-least-three-waves-of-data}

Incorporating repeated measures data from at least three time intervals
considerably enhances your ability to infer causal relationships. For
example, by adjusting for physical activity measured before the
treatment, we can ensure that physical activity does not result from a
new initiation to green spaces, which we establish by measuring green
space access at baseline. Establishing chronological order allows us to
avoid confounding problems 1-4 in Table~\ref{tbl-04}.

\paragraph{3. Where causality is unclear, report results for multiple
causal
graphs}\label{where-causality-is-unclear-report-results-for-multiple-causal-graphs}

Given that the true causal structure may be complex and partially
unknown, analysing and reporting results under each plausible causal
diagram is prudent.

\paragraph{4. Address missing data at baseline and study
attrition}\label{address-missing-data-at-baseline-and-study-attrition}

Longitudinal studies often need help with missing data and attrition,
which can introduce bias and affect the validity of causal inferences.
Implement and report strategies for handling missing data, such as
multiple imputation or sensitivity analyses that assess the bias arising
from missing responses at the study's conclusion. (For more about
addressing missing data, see:
(\citeproc{ref-bulbulia2024PRACTICAL}{Bulbulia 2024a})).

By following these recommendations, you will more effectively navigate
the inherent limitations of observational longitudinal data, improving
the quality of your causal inferences.

\subsubsection{On the Differences Between Methods for Causal Inference
And The Statistical Modelling
Tradition}\label{on-the-differences-between-methods-for-causal-inference-and-the-statistical-modelling-tradition}

In traditional statistical methods such as regression analysis and
structural equation modelling (SEM), the focus often lies in estimating
associations among variables, sometimes treating all variables
symmetrically within the model. However, causal inference shifts the
focus to estimating pre-specified causal contrasts, defined explicitly
in terms of interventions and outcomes measured on a target population.
This approach takes us beyond measuring associations in the data to
understanding the effects of hypothetical or actual interventions.

Central to this shift is the recognition of nuisance
variables---variables that are not of direct interest but must be
accounted for to obtain valid causal effect estimates. In causal
inference, nuisance parameters often include confounders that need to be
adjusted to block non-causal associations between a treatment of
interest and the outcomes of interest. However, the coefficients
associated with these nuisance variables generally lack a causal
interpretation. For example, consider a causal pathway where
\(L \to A \to Y\), and we are interested in estimating the effect of
\(A\) on \(Y\) while controlling for \(L\). Although controlling for
\(L\) is necessary to obtain an unbiased estimate of the causal effect
of \(A\) on \(Y\), the coefficient of \(L\) in the model does not have a
straightforward causal interpretation and reporting it may be
misleading, especially if \(A\) mediates the effect of \(L\) on \(Y\).

By concentrating on pre-specified causal contrasts and appropriately
handling nuisance parameters, causal inference methods enable clarity
for pre-specified causal contrasts of interest. This approach departs
from traditional statistical methods by emphasising the estimation of
specific causal effects rather than modelling the entire system of
associations. It acknowledges that not all parameters in a model can
simultaneously be of equal interest; some parameters primarily serve to
adjust for confounding or other biases and should not be interpreted
causally.

Understanding and properly addressing nuisance parameters are critical
steps in a causal inference workflow. The observation that nuisance
parameters typically lack causal interpretation suggests that the
standard practice of reporting all model coefficients may be unwise or
even misleading (see Westreich and Greenland
(\citeproc{ref-westreich2013}{2013}); McElreath
(\citeproc{ref-mcelreath2020}{2020}); Bulbulia
(\citeproc{ref-bulbulia2023}{2024b})). When multiple treatments are of
interest, it is essential to ensure that the identification assumptions
required for each causal effect are satisfied---a task that can be more
challenging than it appears (see VanderWeele
(\citeproc{ref-vanderweele2015}{2015}); HernÃ¡n \emph{et al.}
(\citeproc{ref-hernan2008aObservationalStudiesAnalysedLike}{2008});
Robins (\citeproc{ref-robins1986}{1986}); Robins and Greenland
(\citeproc{ref-robins1992}{1992}); Robins and Hernan
(\citeproc{ref-robins2008estimation}{2008}); Bulbulia
(\citeproc{ref-bulbulia2024swigstime}{2024c})).

Our questions are nearly always causal. We want to know what would
happen, on average, if we were to intervene in the world. In the
observational psychological sciences, obtaining such understanding
requires a paradigm shift from traditional statistical modelling. These
paradigm shifts have become established in economics, computer science,
and epidemiology, and they are making strong inroads in political
science. However, in observational psychological sciences, including
environmental psychology, methods for causal inference remain rare. We
hope this chapter inspires readers to begin developing robust causal
workflows capable of addressing their causal questions and those that
have long animated the field's scientific interests but which have yet
to be coherently addressed.

\subsection{Where to Go Next?}\label{where-to-go-next}

There are many good resources available for learning causal directed
acyclic graphs (\citeproc{ref-barrett2021}{Barrett 2021};
\citeproc{ref-cinelli2022}{Cinelli \emph{et al.} 2022};
\citeproc{ref-greenland1999}{Greenland \emph{et al.} 1999},
\citeproc{ref-greenland1999}{1999};
\citeproc{ref-hernan2024WHATIF}{Hernan and Robins 2024};
\citeproc{ref-major2023exploring}{Major-Smith 2023};
\citeproc{ref-mcelreath2020}{McElreath 2020};
\citeproc{ref-morgan2014}{Morgan and Winship 2014};
\citeproc{ref-pearl2009a}{Pearl 2009b}; \citeproc{ref-rohrer2018}{Rohrer
2018}; \citeproc{ref-suzuki2020}{Suzuki \emph{et al.} 2020}). For those
just getting started on causal diagrams, we recommend Miguel Hernan's
free course here:
\href{https://www.edx.org/learn/data-analysis/harvard-university-causal-diagrams-draw-your-assumptions-before-your-conclusions}{https://www.edx.org/learn/data-analysis/harvard-university-causal-diagrams-draw-your-assumptions-before-your-conclusions,
accessed 10 June 2024}. For those seeking a slightly more technical but
still accessible introduction to causal inference and causal DAGs, we
recommend Brady Neal's introduction to causal inference course and
textbook, both freely available here
\href{https://www.bradyneal.com/causal-inference-course}{https://www.bradyneal.com/causal-inference-course,
accessed 10 June 2024}. For those interested in causal inference in
experiments refer to HernÃ¡n and Robins
(\citeproc{ref-hernan2017per}{2017}), Montgomery \emph{et al.}
(\citeproc{ref-montgomery2018}{2018}); Bulbulia
(\citeproc{ref-bulbulia_2024_experiments}{2024e}). For those interested
in causal mediation analysis and time-varying treatments refer to Robins
(\citeproc{ref-robins1986}{1986}); Robins and Greenland
(\citeproc{ref-robins1992}{1992}); Robins and Hernan
(\citeproc{ref-robins2008estimation}{2008}); Laan and Robins
(\citeproc{ref-vanderlaanRobins2003CensoringLongitudinal}{2003}); DÄ±Ìaz
\emph{et al.} (\citeproc{ref-diaz2021nonparametric}{2021}); Williams and
DÃ­az (\citeproc{ref-williams2021}{2021}); Hoffman \emph{et al.}
(\citeproc{ref-hoffman2023}{2023});Bulbulia
(\citeproc{ref-bulbulia2024swigstime}{2024c}). Moreover, the fifth
segment of Miguel Hernan's free online course covers time-varying
treatments:
\href{https://www.edx.org/learn/data-analysis/harvard-university-causal-diagrams-draw-your-assumptions-before-your-conclusions}{https://www.edx.org/learn/data-analysis/harvard-university-causal-diagrams-draw-your-assumptions-before-your-conclusions,
accessed 10 June 2024}. On the pitfalls of traditional path modelling
refer to Rohrer \emph{et al.} (\citeproc{ref-rohrer2022PATH}{2022}). We
have set aside complications arising from measurement error. For more on
this topic see HernÃ¡n and Cole
(\citeproc{ref-hernan2009MEASUREMENT}{2009}); Hernan and Robins
(\citeproc{ref-hernan2024WHATIF}{2024}); Bulbulia
(\citeproc{ref-bulbulia2024wierd}{2024d}); VanderWeele
(\citeproc{ref-vanderweele2022}{2022}); VanderWeele and Vansteelandt
(\citeproc{ref-vanderweele2022a}{2022}).

\newpage{}

\subsection{Funding}\label{funding}

This work is supported by a grant from the Templeton Religion Trust
(TRT0418). JB received support from the Max Planck Institute for the
Science of Human History. The funders had no role in preparing the
manuscript or deciding to publish it.

\subsection{Contributions}\label{contributions}

DH proposed the chapter. JB developed the approach and wrote the first
draft. Both authors contributed substantially to the final work.

\newpage{}

\subsection{References}\label{references}

\phantomsection\label{refs}
\begin{CSLReferences}{1}{0}
\bibitem[\citeproctext]{ref-barrett2021}
Barrett, M (2021) \emph{Ggdag: Analyze and create elegant directed
acyclic graphs}. Retrieved from
\url{https://CRAN.R-project.org/package=ggdag}

\bibitem[\citeproctext]{ref-bulbulia2022}
Bulbulia, JA (2023) A workflow for causal inference in cross-cultural
psychology. \emph{Religion, Brain \& Behavior}, \textbf{13}(3),
291--306.
doi:\href{https://doi.org/10.1080/2153599X.2022.2070245}{10.1080/2153599X.2022.2070245}.

\bibitem[\citeproctext]{ref-bulbulia2024PRACTICAL}
Bulbulia, JA (2024a) A practical guide to causal inference in three-wave
panel studies. \emph{PsyArXiv Preprints}.
doi:\href{https://doi.org/10.31234/osf.io/uyg3d}{10.31234/osf.io/uyg3d}.

\bibitem[\citeproctext]{ref-bulbulia2023}
Bulbulia, JA (2024b) Methods in causal inference part 1: Causal diagrams
and confounding. \emph{Evolutionary Human Sciences}, \textbf{6}.
Retrieved from \url{https://osf.io/b23k7}

\bibitem[\citeproctext]{ref-bulbulia2024swigstime}
Bulbulia, JA (2024c) Methods in causal inference part 2: Interaction,
mediation, and time-varying treatments. \emph{Evolutionary Human
Sciences}, \textbf{6}. Retrieved from
\url{https://osf.io/preprints/psyarxiv/vr268}

\bibitem[\citeproctext]{ref-bulbulia2024wierd}
Bulbulia, JA (2024d) Methods in causal inference part 3: Measurement
error and external validity threats. \emph{Evolutionary Human Sciences},
\textbf{6}. Retrieved from \url{https://osf.io/preprints/psyarxiv/kj7rv}

\bibitem[\citeproctext]{ref-bulbulia_2024_experiments}
Bulbulia, JA (2024e) Methods in causal inference part 4: Confounding in
experiments. \emph{Evolutionary Human Sciences}, \textbf{6}. Retrieved
from \url{https://osf.io/preprints/psyarxiv/6rnj5}

\bibitem[\citeproctext]{ref-bulbulia2023a}
Bulbulia, JA, Afzali, MU, Yogeeswaran, K, and Sibley, CG (2023)
Long-term causal effects of far-right terrorism in {N}ew {Z}ealand.
\emph{PNAS Nexus}, \textbf{2}(8), pgad242.

\bibitem[\citeproctext]{ref-cinelli2022}
Cinelli, C, Forney, A, and Pearl, J (2022) A Crash Course in Good and
Bad Controls. \emph{Sociological Methods \&Research}, 00491241221099552.
doi:\href{https://doi.org/10.1177/00491241221099552}{10.1177/00491241221099552}.

\bibitem[\citeproctext]{ref-cole2010}
Cole, SR, Platt, RW, Schisterman, EF, \ldots{} Poole, C (2010)
Illustrating bias due to conditioning on a collider. \emph{International
Journal of Epidemiology}, \textbf{39}(2), 417--420.
doi:\href{https://doi.org/10.1093/ije/dyp334}{10.1093/ije/dyp334}.

\bibitem[\citeproctext]{ref-danaei2012}
Danaei, G, Tavakkoli, M, and HernÃ¡n, MA (2012) Bias in observational
studies of prevalent users: lessons for comparative effectiveness
research from a meta-analysis of statins. \emph{American Journal of
Epidemiology}, \textbf{175}(4), 250--262.
doi:\href{https://doi.org/10.1093/aje/kwr301}{10.1093/aje/kwr301}.

\bibitem[\citeproctext]{ref-diaz2021nonparametric}
DÄ±Ìaz, I, Hejazi, NS, Rudolph, KE, and Der Laan, MJ van (2021)
Nonparametric efficient causal mediation with intermediate confounders.
\emph{Biometrika}, \textbf{108}(3), 627--641.

\bibitem[\citeproctext]{ref-edwards2015}
Edwards, JK, Cole, SR, and Westreich, D (2015) All your data are always
missing: Incorporating bias due to measurement error into the potential
outcomes framework. \emph{International Journal of Epidemiology},
\textbf{44}(4), 1452--1459.

\bibitem[\citeproctext]{ref-greenland1999}
Greenland, S, Pearl, J, and Robins, JM (1999) Causal diagrams for
epidemiologic research. \emph{Epidemiology (Cambridge, Mass.)},
\textbf{10}(1), 37--48.

\bibitem[\citeproctext]{ref-hernan2023}
Hernan, MA, and Robins, JM (2023) \emph{Causal inference}, Taylor \&
Francis. Retrieved from
\url{https://books.google.co.nz/books?id=/_KnHIAAACAAJ}

\bibitem[\citeproctext]{ref-hernan2024WHATIF}
Hernan, MA, and Robins, JM (2024) \emph{Causal inference: What if?},
Taylor \& Francis. Retrieved from
\url{https://www.hsph.harvard.edu/miguel-hernan/causal-inference-book/}

\bibitem[\citeproctext]{ref-hernan2008aObservationalStudiesAnalysedLike}
HernÃ¡n, MA, Alonso, A, Logan, R, \ldots{} Robins, JM (2008)
Observational studies analyzed like randomized experiments: An
application to postmenopausal hormone therapy and coronary heart
disease. \emph{Epidemiology}, \textbf{19}(6), 766.
doi:\href{https://doi.org/10.1097/EDE.0b013e3181875e61}{10.1097/EDE.0b013e3181875e61}.

\bibitem[\citeproctext]{ref-hernan2009MEASUREMENT}
HernÃ¡n, MA, and Cole, SR (2009) Invited commentary: Causal diagrams and
measurement bias. \emph{American Journal of Epidemiology},
\textbf{170}(8), 959--962.
doi:\href{https://doi.org/10.1093/aje/kwp293}{10.1093/aje/kwp293}.

\bibitem[\citeproctext]{ref-hernan2017per}
HernÃ¡n, MA, and Robins, JM (2017) Per-protocol analyses of pragmatic
trials. \emph{N Engl J Med}, \textbf{377}(14), 1391--1398.

\bibitem[\citeproctext]{ref-hernuxe1n2016}
HernÃ¡n, MA, Sauer, BC, HernÃ¡ndez-DÃ­az, S, Platt, R, and Shrier, I (2016)
Specifying a target trial prevents immortal time bias and other
self-inflicted injuries in observational analyses. \emph{Journal of
Clinical Epidemiology}, \textbf{79}, 70--75.

\bibitem[\citeproctext]{ref-hoffman2023}
Hoffman, KL, Salazar-Barreto, D, Rudolph, KE, and DÃ­az, I (2023)
Introducing longitudinal modified treatment policies: A unified
framework for studying complex exposures.
doi:\href{https://doi.org/10.48550/arXiv.2304.09460}{10.48550/arXiv.2304.09460}.

\bibitem[\citeproctext]{ref-holland1986}
Holland, PW (1986) Statistics and causal inference. \emph{Journal of the
American Statistical Association}, \textbf{81}(396), 945--960.

\bibitem[\citeproctext]{ref-hume1902}
Hume, D (1902) \emph{Enquiries Concerning the Human Understanding: And
Concerning the Principles of Morals}, Clarendon Press.

\bibitem[\citeproctext]{ref-vanderlaanRobins2003CensoringLongitudinal}
Laan, MJ, and Robins, JM (2003) \emph{Unified methods for censored
longitudinal data and causality}, Springer.

\bibitem[\citeproctext]{ref-lewis1973}
Lewis, D (1973) Causation. \emph{The Journal of Philosophy},
\textbf{70}(17), 556--567.
doi:\href{https://doi.org/10.2307/2025310}{10.2307/2025310}.

\bibitem[\citeproctext]{ref-major2023exploring}
Major-Smith, D (2023) Exploring causality from observational data: An
example assessing whether religiosity promotes cooperation.
\emph{Evolutionary Human Sciences}, \textbf{5}, e22.

\bibitem[\citeproctext]{ref-mcelreath2020}
McElreath, R (2020) \emph{Statistical rethinking: A {B}ayesian course
with examples in {R} and {S}tan}, CRC press.

\bibitem[\citeproctext]{ref-montgomery2018}
Montgomery, JM, Nyhan, B, and Torres, M (2018) How conditioning on
posttreatment variables can ruin your experiment and what to do about
It. \emph{American Journal of Political Science}, \textbf{62}(3),
760--775.
doi:\href{https://doi.org/10.1111/ajps.12357}{10.1111/ajps.12357}.

\bibitem[\citeproctext]{ref-morgan2014}
Morgan, SL, and Winship, C (2014) \emph{Counterfactuals and causal
inference: Methods and principles for social research}, 2nd edn,
Cambridge: Cambridge University Press.
doi:\href{https://doi.org/10.1017/CBO9781107587991}{10.1017/CBO9781107587991}.

\bibitem[\citeproctext]{ref-ogburn2021}
Ogburn, EL, and Shpitser, I (2021) Causal modelling: The two cultures.
\emph{Observational Studies}, \textbf{7}(1), 179--183.
doi:\href{https://doi.org/10.1353/obs.2021.0006}{10.1353/obs.2021.0006}.

\bibitem[\citeproctext]{ref-pearl1995}
Pearl, J (1995) Causal diagrams for empirical research.
\emph{Biometrika}, \textbf{82}(4), 669--688.

\bibitem[\citeproctext]{ref-pearl2009}
Pearl, J (2009a) \emph{\href{https://doi.org/10.1214/09-SS057}{Causal
inference in statistics: An overview}}.

\bibitem[\citeproctext]{ref-pearl2009a}
Pearl, J (2009b) \emph{Causality}, Cambridge University Press.

\bibitem[\citeproctext]{ref-pearl1995a}
Pearl, J, and Robins, JM (1995) Probabilistic evaluation of sequential
plans from causal models with hidden variables. In \emph{UAI}, Vol. 95,
Citeseer, 444--453.

\bibitem[\citeproctext]{ref-robins1986}
Robins, J (1986) A new approach to causal inference in mortality studies
with a sustained exposure period---application to control of the healthy
worker survivor effect. \emph{Mathematical Modelling}, \textbf{7}(9-12),
1393--1512.

\bibitem[\citeproctext]{ref-robins2008estimation}
Robins, J, and Hernan, M (2008) Estimation of the causal effects of
time-varying exposures. \emph{Chapman \& Hall/CRC Handbooks of Modern
Statistical Methods}, 553--599.

\bibitem[\citeproctext]{ref-robins1992}
Robins, JM, and Greenland, S (1992) Identifiability and exchangeability
for direct and indirect effects. \emph{Epidemiology}, \textbf{3}(2),
143--155.

\bibitem[\citeproctext]{ref-rohrer2018}
Rohrer, JM (2018) Thinking clearly about correlations and causation:
Graphical causal models for observational data. \emph{Advances in
Methods and Practices in Psychological Science}, \textbf{1}(1), 27--42.

\bibitem[\citeproctext]{ref-rohrer2022PATH}
Rohrer, JM, HÃ¼nermund, P, Arslan, RC, and Elson, M (2022) That's a lot
to process! Pitfalls of popular path models. \emph{Advances in Methods
and Practices in Psychological Science}, \textbf{5}(2).
doi:\href{https://doi.org/10.1177/25152459221095827}{10.1177/25152459221095827}.

\bibitem[\citeproctext]{ref-rohrer2023withinbetween}
Rohrer, JM, and Murayama, K (2023) These are not the effects you are
looking for: Causality and the within-/between-persons distinction in
longitudinal data analysis. \emph{Advances in Methods and Practices in
Psychological Science}, \textbf{6}(1), 25152459221140842.

\bibitem[\citeproctext]{ref-rubin1976}
Rubin, DB (1976) Inference and missing data. \emph{Biometrika},
\textbf{63}(3), 581--592.
doi:\href{https://doi.org/10.1093/biomet/63.3.581}{10.1093/biomet/63.3.581}.

\bibitem[\citeproctext]{ref-neyman1923}
Splawa-Neyman, J (1990 (orig. 1923)) On the application of probability
theory to agricultural experiments. Essay on principles. Section 9.
(1923). \emph{Statistical Science}, \textbf{5}(4), 465--472.

\bibitem[\citeproctext]{ref-suzuki2020}
Suzuki, E, Shinozaki, T, and Yamamoto, E (2020) Causal Diagrams:
Pitfalls and Tips. \emph{Journal of Epidemiology}, \textbf{30}(4),
153--162.
doi:\href{https://doi.org/10.2188/jea.JE20190192}{10.2188/jea.JE20190192}.

\bibitem[\citeproctext]{ref-vanderweele2015}
VanderWeele, TJ (2015) \emph{Explanation in causal inference: Methods
for mediation and interaction}, Oxford University Press.

\bibitem[\citeproctext]{ref-vanderweele2022}
VanderWeele, TJ (2022) Constructed measures and causal inference:
Towards a new model of measurement for psychosocial constructs.
\emph{Epidemiology}, \textbf{33}(1), 141.
doi:\href{https://doi.org/10.1097/EDE.0000000000001434}{10.1097/EDE.0000000000001434}.

\bibitem[\citeproctext]{ref-vanderweele2017}
VanderWeele, TJ, and Ding, P (2017) Sensitivity analysis in
observational research: Introducing the {E}-value. \emph{Annals of
Internal Medicine}, \textbf{167}(4), 268--274.
doi:\href{https://doi.org/10.7326/M16-2607}{10.7326/M16-2607}.

\bibitem[\citeproctext]{ref-vanderweele2013}
VanderWeele, TJ, and Hernan, MA (2013) Causal inference under multiple
versions of treatment. \emph{Journal of Causal Inference},
\textbf{1}(1), 1--20.

\bibitem[\citeproctext]{ref-vanderweele2020}
VanderWeele, TJ, Mathur, MB, and Chen, Y (2020) Outcome-wide
longitudinal designs for causal inference: A new template for empirical
studies. \emph{Statistical Science}, \textbf{35}(3), 437--466.

\bibitem[\citeproctext]{ref-vanderweele2022a}
VanderWeele, TJ, and Vansteelandt, S (2022) A statistical test to reject
the structural interpretation of a latent factor model. \emph{Journal of
the Royal Statistical Society Series B: Statistical Methodology},
\textbf{84}(5), 2032--2054.

\bibitem[\citeproctext]{ref-vansteelandt2012}
Vansteelandt, S, Bekaert, M, and Lange, T (2012) Imputation strategies
for the estimation of natural direct and indirect effects.
\emph{Epidemiologic Methods}, \textbf{1}(1), 131--158.

\bibitem[\citeproctext]{ref-wager2018}
Wager, S, and Athey, S (2018) Estimation and inference of heterogeneous
treatment effects using random forests. \emph{Journal of the American
Statistical Association}, \textbf{113}(523), 1228--1242.
doi:\href{https://doi.org/10.1080/01621459.2017.1319839}{10.1080/01621459.2017.1319839}.

\bibitem[\citeproctext]{ref-westreich2012berkson}
Westreich, D (2012) Berkson's bias, selection bias, and missing data.
\emph{Epidemiology (Cambridge, Mass.)}, \textbf{23}(1), 159.

\bibitem[\citeproctext]{ref-westreich2010}
Westreich, D, and Cole, SR (2010) Invited commentary: positivity in
practice. \emph{American Journal of Epidemiology}, \textbf{171}(6).
doi:\href{https://doi.org/10.1093/aje/kwp436}{10.1093/aje/kwp436}.

\bibitem[\citeproctext]{ref-westreich2015}
Westreich, D, Edwards, JK, Cole, SR, Platt, RW, Mumford, SL, and
Schisterman, EF (2015) Imputation approaches for potential outcomes in
causal inference. \emph{International Journal of Epidemiology},
\textbf{44}(5), 1731--1737.

\bibitem[\citeproctext]{ref-westreich2013}
Westreich, D, and Greenland, S (2013) The table 2 fallacy: Presenting
and interpreting confounder and modifier coefficients. \emph{American
Journal of Epidemiology}, \textbf{177}(4), 292--298.

\bibitem[\citeproctext]{ref-williams2021}
Williams, NT, and DÃ­az, I (2021) \emph{{l}mtp: Non-parametric causal
effects of feasible interventions based on modified treatment policies}.
doi:\href{https://doi.org/10.5281/zenodo.3874931}{10.5281/zenodo.3874931}.

\end{CSLReferences}



\end{document}
