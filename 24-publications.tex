% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames,x11names}{xcolor}
%
\documentclass[
  singlecolumn]{article}

\usepackage{amsmath,amssymb}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage[]{libertinus}
\ifPDFTeX\else  
    % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\usepackage[top=30mm,left=20mm,heightrounded]{geometry}
\setlength{\emergencystretch}{3em} % prevent overfull lines
\setcounter{secnumdepth}{-\maxdimen} % remove section numbering
% Make \paragraph and \subparagraph free-standing
\ifx\paragraph\undefined\else
  \let\oldparagraph\paragraph
  \renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
  \let\oldsubparagraph\subparagraph
  \renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi


\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
% definitions for citeproc citations
\NewDocumentCommand\citeproctext{}{}
\NewDocumentCommand\citeproc{mm}{%
  \begingroup\def\citeproctext{#2}\cite{#1}\endgroup}
\makeatletter
 % allow citations to break across lines
 \let\@cite@ofmt\@firstofone
 % avoid brackets around text for \cite:
 \def\@biblabel#1{}
 \def\@cite#1#2{{#1\if@tempswa , #2\fi}}
\makeatother
\newlength{\cslhangindent}
\setlength{\cslhangindent}{1.5em}
\newlength{\csllabelwidth}
\setlength{\csllabelwidth}{3em}
\newenvironment{CSLReferences}[2] % #1 hanging-indent, #2 entry-spacing
 {\begin{list}{}{%
  \setlength{\itemindent}{0pt}
  \setlength{\leftmargin}{0pt}
  \setlength{\parsep}{0pt}
  % turn on hanging indent if param 1 is 1
  \ifodd #1
   \setlength{\leftmargin}{\cslhangindent}
   \setlength{\itemindent}{-1\cslhangindent}
  \fi
  % set entry spacing
  \setlength{\itemsep}{#2\baselineskip}}}
 {\end{list}}
\usepackage{calc}
\newcommand{\CSLBlock}[1]{\hfill\break\parbox[t]{\linewidth}{\strut\ignorespaces#1\strut}}
\newcommand{\CSLLeftMargin}[1]{\parbox[t]{\csllabelwidth}{\strut#1\strut}}
\newcommand{\CSLRightInline}[1]{\parbox[t]{\linewidth - \csllabelwidth}{\strut#1\strut}}
\newcommand{\CSLIndent}[1]{\hspace{\cslhangindent}#1}

\input{/Users/joseph/GIT/templates/latex/custom-commands.tex}
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\AtBeginDocument{%
\ifdefined\contentsname
  \renewcommand*\contentsname{Table of contents}
\else
  \newcommand\contentsname{Table of contents}
\fi
\ifdefined\listfigurename
  \renewcommand*\listfigurename{List of Figures}
\else
  \newcommand\listfigurename{List of Figures}
\fi
\ifdefined\listtablename
  \renewcommand*\listtablename{List of Tables}
\else
  \newcommand\listtablename{List of Tables}
\fi
\ifdefined\figurename
  \renewcommand*\figurename{Figure}
\else
  \newcommand\figurename{Figure}
\fi
\ifdefined\tablename
  \renewcommand*\tablename{Table}
\else
  \newcommand\tablename{Table}
\fi
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Listing}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}
\makeatother
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\@ifpackageloaded{subcaption}{}{\usepackage{subcaption}}
\makeatother
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\usepackage{bookmark}

\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same} % disable monospaced font for URLs
\hypersetup{
  pdftitle={Causal Diagrams: A Practical Guide},
  pdfauthor={Joseph A. Bulbulia},
  pdfkeywords={Directed Acyclic Graph, Causal
Inference, Confounding, Feedback, Interaction, Internal
validity, External validity, Mediation},
  colorlinks=true,
  linkcolor={blue},
  filecolor={Maroon},
  citecolor={Blue},
  urlcolor={Blue},
  pdfcreator={LaTeX via pandoc}}

\title{Causal Diagrams: A Practical Guide}
\author{Joseph A. Bulbulia}
\date{2024-01-30}

\begin{document}
\maketitle
\begin{abstract}
In causal inference, accurately quantifying a causal effect requires
contrasting hypothetical counterfactual states simulated from data. This
complex task relies on a framework of explicit assumptions and
systematic, multi-step workflows. Causal diagrams (directed acyclic
graphs, or DAGs) are powerful tools for evaluating assumptions and
designing research. However, when misused, causal diagrams encourage
false confidence. This guide offers practical advice for creating
sequentially ordered causal diagrams that are effective and safe.
Focussing on the time structure of causation reveals the benefits of
sequential order in the spatial organisation of causal diagrams, both
for data analysis and collection. After reviewing fundamentals, we
employ \emph{sequentially ordered causal diagrams} to elucidate often
misunderstood concepts of causal interaction (moderation), mediation,
and dynamic longitudinal feedback, as well as measurement-error bias and
target validity. Overall, sequentially ordered causal diagrams
underscore causal inference's mission-critical demand for accuracy in
the relative timing of confounders, exposures, and outcomes recorded in
one's data.
\end{abstract}

\subsection{Introduction}\label{introduction}

Here, I offer readers of \emph{Evolutionary Human Science} practical
guidance for creating causal diagrams.

\textbf{Part 1} introduces core concepts and theories in causal data
science, emphasising the fundamental assumptions and the demands they
impose on causal inferential workflows. We review the motivation in
causal data science for adopting an `estimands first' approach ---
pre-specifying counterfactual contrasts to address a well-defined causal
question within a clearly defined target population.

\textbf{Part 2} introduces essential terminology and explores elementary
use cases, focussing on the benefits of sequentially order in one's
graph for evaluating confounding biases.

\textbf{Part 3} uses sequentially ordered causal diagrams to clarify
concepts of interaction (moderation) and effect modification.

\textbf{Part 4} clarifies biases arising from measurement error bias.

\textbf{Part 5} clarifies restriction biases arising from (a) attrition/
censoring; (b) mismatch between a study sample population and a target
sample population. We also consider heterogeniety biases arising from
failures to sufficiently restrict one's sample.

\textbf{Part 6} uses causal diagrams to clarify causal estimation in
which there are multiple exposures, focussing on causal mediation, and
time-varying treatments.

There are many good resources available for learning causal diagrams
(\citeproc{ref-barrett2021}{Barrett 2021};
\citeproc{ref-cinelli2022}{Cinelli \emph{et al.} 2022};
\citeproc{ref-greenland1999}{Greenland \emph{et al.} 1999};
\citeproc{ref-hernan2023}{Hernan and Robins 2023};
\citeproc{ref-mcelreath2020}{McElreath 2020};
\citeproc{ref-pearl2009}{Pearl 2009}; \citeproc{ref-rohrer2018}{Rohrer
2018}; \citeproc{ref-suzuki2020}{Suzuki \emph{et al.} 2020}). This work
hopes to contribute to these resources, first by providing additional
conceptual orientation to the frameworks and workflows of causal data
science, outside of which the application of causal diagrams is risky;
second, by underscoring the benefits of sequential order in one's causal
diagrams to clarify demands for data collection and analysis; third by
using causal diagrams to clarify threats to target validity arising from
sample/target population mismatch, fourth, by using causal diagrams to
clarify questions of causal interaction, mediation, and longitudinal
feedback, about which there remains considerable confusions among many
human scientists.

\subsection{Part 1. Overview of Causal Data
Science}\label{part-1.-overview-of-causal-data-science}

The first step in answering a causal question is to ask it
(\citeproc{ref-hernuxe1n2016}{Hern√°n \emph{et al.} 2016}). Causal
diagrams come later, when we consider which forms of data might enable
us to address our pre-specified causal questions.

\paragraph{1.1.1 The fundamental problem of causal
inference}\label{the-fundamental-problem-of-causal-inference}

To ask a causal question, we must consider the concept of causality
itself. Consider an intervention, \(A\), and its effect, \(Y\). We say
that \(A\) causes \(Y\) if altering \(A\) would lead to a change in
\(Y\) (\citeproc{ref-hume1902}{Hume 1902};
\citeproc{ref-lewis1973}{Lewis 1973}). If altering \(A\) would not
change \(Y\), we say that \(A\) has no causal effect on \(Y\).

In causal inference, we aim to quantitatively contrast the potential
outcomes of \(Y\) in response to different levels of a well-defined
intervention. Commonly, we refer to such interventions as `exposures' or
`treatments;' we refer to the possible effects of interventions as
`potential outcomes.'

Consider a binary treatment variable \(A \in \{0,1\}\). For each unit
\(i\) in the set \(\{1, 2, \ldots, n\}\), when \(A_i\) is set to 0, the
potential outcome under this condition is denoted, \(Y_i(0)\).
Conversely, when \(A_i\) is set to 1, the potential outcome is denoted,
\(Y_i(1)\). We refer to the terms \(Y_i(1)\) and \(Y_i(0)\) as
`potential outcomes' because until realised, the effects of
interventions describe counterfactual states.

Suppose that each unit \(i\) receives either \(A_i = 1\) or \(A_i = 0\).
The corresponding outcomes are realised as \(Y_i|A_i = 1\) or
\(Y_i|A_i = 0\). For now, let us assume that each realised outcome under
that intervention is equivalent to one of the potential outcomes
required for a quantitative causal contrast, such that
\([(Y_i(a)|A_i = a)] = (Y_i|A_i = a)\). Thus when \(A_i = 1\),
\(Y_i(1)|A_i = 1\) is observed. However, if \(A_i = 1\), it follows that
\(Y_i(0)|A_i = 1\) is not observed:

\[
Y_i|A_i = 1 \implies Y_i(0)|A_i = 1~ \text{is counterfactual}
\]

Conversely, if \(A_i = 0\), we may assume the potential outcome
\(Y_i(0)|A_i = 0\)) is observed as \(Y_i|A_i = 0\). However, the
potential outcome \(Y_i(1)|A_i = 0\) is never realised and so not
observed:

\[
Y_i|A_i = 0 \implies Y_i(1)|A_i = 0~ \text{is counterfactual}
\]

We define \(\tau_i\) as the individual causal effect for unit \(i\) and
express the individual causal effect:

\[
\tau_i = Y_i(1) - Y_i(0)
\]

Notice that each unit can only be exposed to only one level of the
exposure \(A_i = a\) at a time. This implies that \(\tau_i\), is not
merely unobserved but inherently \emph{unobservable} (see discussion in
Appendix 3).

Although we cannot observe potential outcomes that do not occur, it is
tempting to ask questions about them, `What if Isaac Newton had not
witnessed the falling apple?' What if Leonardo da Vinci had never
pursued art?' or `What if Archduke Ferdinand had not been assassinated?'
There are abundant examples from literature. Robert Frost contemplates,
`Two roads diverged in a yellow wood, and sorry I could not travel both,
and be one traveller, long I stood\ldots{}' (see: Robert Frost, `The
Road Not Taken':
https://www.poetryfoundation.org/poems/44272/the-road-not-taken). We
have counterfactual questions for our personal experiences: `What if I
had had not interviewed for that job?' `What if I had stayed in that
relationship?' We may speculate, with reasons, but we cannot directly
observe the potential outcomes we would need to verify our speculations.
The physics of middle-sized dry goods prevents the joint realisation of
the facts required for quantitative comparisons. That individual causal
effects cannot be identified from observations is known as `\emph{the
fundamental problem of causal inference}'
(\citeproc{ref-holland1986}{Holland 1986};
\citeproc{ref-rubin1976}{Rubin 1976}).

\paragraph{1.1.2 Causal effects from randomised
experiments}\label{causal-effects-from-randomised-experiments}

It is not feasible to compute individual causal effects. However, under
certain assumptions, we can estimate \emph{average} treatment effects --
also called `marginal effects,' -- by comparing groups that have
received different levels of treatment. The average treatment effect,
\(ATE\), is defined as the difference between the expected outcomes
under treatment and contrast conditions. Suppose the treatment, \(A\),
is a binary variable \(A \in \{0,1\}\):

\[
\text{Average Treatment Effect}  = \mathbb{E}[Y(1)] - \mathbb{E}[Y(0)].
\] A challenge remains in computing these treatment-group averages when
individual causal effects are unobservable. Consider the problem framed
in terms of \emph{full data} that would be required to compute these
averages --- that is in terms of the complete counterfactual dataset
where the missing potential outcomes, inherent in observational data,
were somehow available:

\[
\text{Average Treatment Effect} = \left(\underbrace{\underbrace{\mathbb{E}[Y(1)|A = 1]}_{\text{observed for } A = 1} + \underbrace{\mathbb{E}[Y(1)|A = 0]}_{\text{unobserved for } A = 0}}_{\text{effect among treated, by the law of total expectation}}\right) - \left(\underbrace{\underbrace{\mathbb{E}[Y(0)|A = 0]}_{\text{observed for } A = 0} + \underbrace{\mathbb{E}[Y(0)|A = 1]}_{\text{unobserved for } A = 1}}_{\text{effect among untreated, by the law of total expectation}}\right).
\]

Consider that for each treatment condition, half the observations needed
to compute a treatment-group average are (inherently) unobserved.

Randomisation allows investigators to recover the treatment group
averages even though treatment groups contain inherently missing
observations. When investigators effectively randomise units into
treatment conditions, and there is full adherence, the distributions of
confounding factors that could explain differences in the potential
outcomes are balanced across the conditions. Randomisation (in a perfect
experiment) ensures that nothing can explain a difference in treatment
group average except the treatment. This implies (by the law of iterated
expectations):

\[
\widehat{\mathbb{E}}[Y(0) | A = 1] = \widehat{\mathbb{E}}[Y(0) | A = 0]
\]

and

\[
\widehat{\mathbb{E}}[Y(1) | A = 1] = \widehat{\mathbb{E}}[Y(1) | A = 0]
\]

We assume, (by causal consistency, see: \(\S 1.2.1\)):

\[\widehat{\mathbb{E}}[Y(1) | A = 1] = \widehat{\mathbb{E}}[Y| A = 1]\]

and

\[\widehat{\mathbb{E}}[Y(0) | A = 0] = \widehat{\mathbb{E}}[Y| A = 0]\]

It follows that the average treatment effect of the randomised
experiment can be computed (by the law of iterated expectations):

\[
\text{The Estimated Average Treatment Effect} = \widehat{\mathbb{E}}[Y | A = 1] - \widehat{\mathbb{E}}[Y | A = 0].
\]

Understanding how randomisation obtains the missing counterfactual
outcomes that we require to consistently estimate average treatment
effects clarifies the tasks of causal inference in non-experimental
settings (\citeproc{ref-hernuxe1n2008a}{Hern√°n \emph{et al.} 2008};
\citeproc{ref-hernuxe1n2022}{Hern√°n \emph{et al.} 2022};
\citeproc{ref-hernuxe1n2006}{Hern√°n and Robins 2006}).

\subsubsection{1.2 Fundamental Identification
Assumptions}\label{fundamental-identification-assumptions}

There are three fundamental identification assumptions that must be
satisfied to consistently estimate causal effects with data.

\paragraph{1.2.1 Assumption 1: Causal
Consistency}\label{assumption-1-causal-consistency}

We satisfy the causal consistency assumption if, for each unit \(i\) in
the set \(\{1, 2, \ldots, n\}\), the observed outcome corresponds to one
of the specific counterfactual outcomes to be compared such that:

\[
Y_i^{observed}|A_i = 
\begin{cases} 
Y_i(a^*) & \text{if } A_i = a^* \\
Y_i(a) & \text{if } A_i = a
\end{cases}
\]

The causal consistency assumption implies that the observed outcome at a
specific exposure level equates to the counterfactual outcome for that
individual at the observed exposure level. Although it seems
straightforward to equate an individual's observed outcome with their
counterfactual outcome, treatment conditions vary, and treatment
heterogeneity poses considerable challenges for satisfying this
assumption. See: \textbf{Appendix A}

\paragraph{1.2.2 Assumption 2: Conditional Exchangeability (no
unmeasured
confounding)}\label{assumption-2-conditional-exchangeability-no-unmeasured-confounding}

We satisfy the conditional exchangeability assumption if the treatment
groups are conditionally balanced in the variables that could affect the
potential outcomes. In experimental designs, random assignment
facilitates satisfaction of the conditional exchangeability assumption.
In observational studies more effort is required. We must control for
any covariate that could account for observed correlations between \(A\)
and \(Y\) in the absence of a causal effect of \(A\) on \(Y\).

Let \(\coprod\) again denote independence. Let \(L\) denote the set of
covariates necessary to ensure this conditional independence. We satisfy
conditional exchangeability when:

\[
Y(a) \coprod A | L \quad \text{or equivalently} \quad A \coprod Y(a) | L
\]

Assuming conditional exchangeability is satisfied and the other
assumptions required for consistent causal inference also hold, we may
compute the average treatment effect (ATE) on the difference scale:

\[
ATE = \mathbb{E}[Y(1) | L] - \mathbb{E}[Y(0) | L]
\]

In the disciplines of cultural evolution, where experimental control is
impractical, causal inferences hinge on the plausibility of satisfying
this `no unmeasured confounding' assumption (see: \textbf{Appendix 1})

Importantly, causal diagrams primarily function to identify sources of
bias that may influence the association between an exposure and outcome.
They highlight those aspects of the assumed causal order relevant to the
assessment of `no-unmeasured confounding.' Although causal diagrams can
also be useful for examining structural features of measurement-error
bias and threats to target validity from sample/population mismatch,
certain of these threats are not amenable to Markov factorisation (see
\(\S 2.3\)). For instance, in \(\S 2.8\), we consider the limitations of
causal diagrams in addressing uncorrelated measurement error bias, which
does not manifest on causal diagrams. In \(\S 3.1.6\), we employ causal
diagrams to clarify threats to external validity from sample restriction
(effect-modifier-restriction bias). In these instances, we must
introduce colouring conventions that repurpose causal diagrams for
somewhat different problems than those they were originally designed to
address, namely, problems of confounding bias.

Finally, it is important to underscore that without randomisation, we
cannot fully ensure the no-unmeasured confounding assumption that
enables us to recover the missing counterfactuals we require to
consistently estimate causal effects from data
(\citeproc{ref-greifer2023}{Greifer \emph{et al.} 2023};
\citeproc{ref-stuart2015}{Stuart \emph{et al.} 2015}). Because we must
nearly always assume unmeasured confounding, the workflows of causal
data science must ultimately rely on sensitivity analyses to clarify how
much unmeasured confounding would be required to compromise a study's
findings (\citeproc{ref-vanderweele2019}{VanderWeele 2019}).

\paragraph{1.2.3 Assumption 3:
Positivity}\label{assumption-3-positivity}

We satisfy the positivity assumption if there is a non-zero probability
of receiving each treatment level for every combination of covariates
that occurs in the population. Where \(A\) is the exposure and \(L\) is
a vector of covariates, we say positivity is achieved if:

\[
0 < Pr(A = a | L = l) < 1, \quad \text{for all } a, l \text{ with } Pr(L = l) > 0
\]

There are two types of positivity violation:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Random non-positivity} occurs when an exposure is
  theoretically possible, but specific exposure levels are not
  represented in the data. Notably, random non-positivity is the only
  identifiability assumption verifiable with data.
\item
  \textbf{Deterministic non-positivity} occurs when the exposure is
  implausible by nature. For instance, a hysterectomy in biological
  males would appear biologically implausible.
\end{enumerate}

Satisfying the positivity assumption can present considerable data
challenges (\citeproc{ref-westreich2010}{Westreich and Cole 2010}).
Suppose we had access to extensive panel data that has tracked 20,000
individuals randomly sampled from the target population over three
years. Suppose further that we wanted to estimate a one-year causal
effect of weekly religious service attendance on charitable donations.
We control for baseline attendance to recover an incident exposure
effect estimate (see: \(\S 2.3\) and \textbf{Appendix 2}). Assume that
the natural transition rate from no religious service attendance to
weekly service attendance is low, say one in a thousand annually. In
that case, the effective sample for the treatment condition dwindles to
20. This example clarifies the problem. For rare exposures, the data
required for valid causal contrasts may be sparse, even in large
datasets. Where the positivity assumption is violated, causal diagrams
will be of limited utility because observations in the data do not
support valid causal inferences. (\textbf{Appendix 1} develops a worked
example that illustrates the difficulty of satisfying this assumption in
a setting of a cultural evolutionary question.)

\subsubsection{1.3 Conceptual, Data, and Modelling
Assumptions}\label{conceptual-data-and-modelling-assumptions}

We have reviewed the three fundamental assumptions of causal inference.
However, we must also consider further conceptual, data, and modelling
assumptions that, in addition to the foundational assumptions we just
reviewed, must also be satisfied to obtain valid causal inferences. We
next consider a subset of these assumptions. \textbf{Appendix B}
describes these further assumptions

\subsection{Part 2. Applications of Sequentially Ordered Causal Diagrams
for Understanding Structural Sources of
Bias}\label{part-2.-applications-of-sequentially-ordered-causal-diagrams-for-understanding-structural-sources-of-bias}

Having outlined basic features of the causal inference framework, we are
now in a position to use causal diagrams to elucidate elementary
structural sources of bias (\citeproc{ref-greenland1999}{Greenland
\emph{et al.} 1999}; \citeproc{ref-pearl1995}{Pearl 1995},
\citeproc{ref-pearl2009}{2009}). We begin by defining our terminology.

\subsubsection{2.1 Variable naming
conventions}\label{variable-naming-conventions}

\begin{table}

\caption{\label{tbl-00}Test}

\centering{

\terminologydirectedgraph 

}

\end{table}%

\subsubsection{2.2 Graphical conventions}\label{graphical-conventions}

\paragraph{2.2.1 Graphical conventions for the analysis of
confounding}\label{graphical-conventions-for-the-analysis-of-confounding}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Arrow: \textbf{A causes Y}:
\end{enumerate}

\[A_1 \rightarrow Y_2\]

Denotes a causal relationship where A causes Y.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  \textbf{Red arrow: non-causal path linking A and Y involving L}:
\end{enumerate}

\[ A_1 \leftarrowred L_0 \rightarrowred Y_2 \]

Denotes \(L_0\) causes both \(Y_2\) and \(A_1\). Note that obtaining
timing from the data is critical to avoid causal incoherence such as:

\[ A_2 \rightarrowred Y_0 \]

\phantomsection\label{refs}
\begin{CSLReferences}{1}{0}
\bibitem[\citeproctext]{ref-barrett2021}
Barrett, M (2021) \emph{Ggdag: Analyze and create elegant directed
acyclic graphs}. Retrieved from
\url{https://CRAN.R-project.org/package=ggdag}

\bibitem[\citeproctext]{ref-cinelli2022}
Cinelli, C, Forney, A, and Pearl, J (2022) A Crash Course in Good and
Bad Controls. \emph{Sociological Methods \& Research},
00491241221099552.
doi:\href{https://doi.org/10.1177/00491241221099552}{10.1177/00491241221099552}.

\bibitem[\citeproctext]{ref-greenland1999}
Greenland, S, Pearl, J, and Robins, JM (1999) Causal diagrams for
epidemiologic research. \emph{Epidemiology (Cambridge, Mass.)},
\textbf{10}(1), 37--48.

\bibitem[\citeproctext]{ref-greifer2023}
Greifer, N, Worthington, S, Iacus, S, and King, G (2023) \emph{Clarify:
Simulation-based inference for regression models}. Retrieved from
\url{https://iqss.github.io/clarify/}

\bibitem[\citeproctext]{ref-hernan2023}
Hernan, MA, and Robins, JM (2023) \emph{Causal inference}, Taylor \&
Francis. Retrieved from
\url{https://books.google.co.nz/books?id=/_KnHIAAACAAJ}

\bibitem[\citeproctext]{ref-hernuxe1n2008a}
Hern√°n, MA, Alonso, A, Logan, R, \ldots{} Robins, JM (2008)
Observational studies analyzed like randomized experiments: An
application to postmenopausal hormone therapy and coronary heart
disease. \emph{Epidemiology}, \textbf{19}(6), 766.
doi:\href{https://doi.org/10.1097/EDE.0b013e3181875e61}{10.1097/EDE.0b013e3181875e61}.

\bibitem[\citeproctext]{ref-hernuxe1n2006}
Hern√°n, MA, and Robins, JM (2006) Estimating causal effects from
epidemiological data. \emph{Journal of Epidemiology \& Community
Health}, \textbf{60}(7), 578--586.
doi:\href{https://doi.org/10.1136/jech.2004.029496}{10.1136/jech.2004.029496}.

\bibitem[\citeproctext]{ref-hernuxe1n2016}
Hern√°n, MA, Sauer, BC, Hern√°ndez-D√≠az, S, Platt, R, and Shrier, I (2016)
Specifying a target trial prevents immortal time bias and other
self-inflicted injuries in observational analyses. \emph{Journal of
Clinical Epidemiology}, \textbf{79}, 7075.

\bibitem[\citeproctext]{ref-hernuxe1n2022}
Hern√°n, MA, Wang, W, and Leaf, DE (2022) Target trial emulation: A
framework for causal inference from observational data. \emph{JAMA},
\textbf{328}(24), 2446--2447.
doi:\href{https://doi.org/10.1001/jama.2022.21383}{10.1001/jama.2022.21383}.

\bibitem[\citeproctext]{ref-holland1986}
Holland, PW (1986) Statistics and causal inference. \emph{Journal of the
American Statistical Association}, \textbf{81}(396), 945960.

\bibitem[\citeproctext]{ref-hume1902}
Hume, D (1902) \emph{Enquiries Concerning the Human Understanding: And
Concerning the Principles of Morals}, Clarendon Press.

\bibitem[\citeproctext]{ref-lewis1973}
Lewis, D (1973) Causation. \emph{The Journal of Philosophy},
\textbf{70}(17), 556--567.
doi:\href{https://doi.org/10.2307/2025310}{10.2307/2025310}.

\bibitem[\citeproctext]{ref-mcelreath2020}
McElreath, R (2020) \emph{Statistical rethinking: A {B}ayesian course
with examples in r and stan}, CRC press.

\bibitem[\citeproctext]{ref-pearl1995}
Pearl, J (1995) Causal diagrams for empirical research.
\emph{Biometrika}, \textbf{82}(4), 669--688.

\bibitem[\citeproctext]{ref-pearl2009}
Pearl, J (2009) \emph{\href{https://doi.org/10.1214/09-SS057}{Causal
inference in statistics: An overview}}.

\bibitem[\citeproctext]{ref-rohrer2018}
Rohrer, JM (2018) Thinking clearly about correlations and causation:
Graphical causal models for observational data. \emph{Advances in
Methods and Practices in Psychological Science}, \textbf{1}(1), 2742.

\bibitem[\citeproctext]{ref-rubin1976}
Rubin, DB (1976) Inference and missing data. \emph{Biometrika},
\textbf{63}(3), 581--592.
doi:\href{https://doi.org/10.1093/biomet/63.3.581}{10.1093/biomet/63.3.581}.

\bibitem[\citeproctext]{ref-stuart2015}
Stuart, EA, Bradshaw, CP, and Leaf, PJ (2015) Assessing the
Generalizability of Randomized Trial Results to Target Populations.
\emph{Prevention Science}, \textbf{16}(3), 475--485.
doi:\href{https://doi.org/10.1007/s11121-014-0513-z}{10.1007/s11121-014-0513-z}.

\bibitem[\citeproctext]{ref-suzuki2020}
Suzuki, E, Shinozaki, T, and Yamamoto, E (2020) Causal Diagrams:
Pitfalls and Tips. \emph{Journal of Epidemiology}, \textbf{30}(4),
153--162.
doi:\href{https://doi.org/10.2188/jea.JE20190192}{10.2188/jea.JE20190192}.

\bibitem[\citeproctext]{ref-vanderweele2019}
VanderWeele, TJ (2019) Principles of confounder selection.
\emph{European Journal of Epidemiology}, \textbf{34}(3), 211219.

\bibitem[\citeproctext]{ref-westreich2010}
Westreich, D, and Cole, SR (2010) Invited commentary: positivity in
practice. \emph{American Journal of Epidemiology}, \textbf{171}(6).
doi:\href{https://doi.org/10.1093/aje/kwp436}{10.1093/aje/kwp436}.

\end{CSLReferences}



\end{document}
