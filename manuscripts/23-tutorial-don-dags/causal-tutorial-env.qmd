---
title: "Causal Inference in Environmental Psychology"
abstract: |
  This chapter offers a practical guide for environmental psychology researchers to investigate causal questions, focusing on how Directed Acyclic Graphs (DAGs) can clarify relationships between greenspace access and happiness. We introduce core concepts of causal inference and the assumptions needed to extract causal insights from observational data. Using simulated data, we demonstrate how to construct DAGs that illuminate potential confounding in the greenspace-happiness link. When the confounding structure is complex, we recommend creating multiple DAGs to illustrate the uncertainty and report results under different scenarios. We offer guidelines for how to do this.
authors: 
  - name: Joseph A. Bulbulia
    orcid: 0000-0002-5861-2056
    email: joseph.bulbulia@vuw.ac.nz
    affiliation: 
      name: Victoria University of Wellington, New Zealand, School of Psychology, Centre for Applied Cross-Cultural Research
      department: Psychology/Centre for Applied Cross-Cultural Research
      city: Wellington
      country: New Zealand
      url: www.wgtn.ac.nz/cacr
  - name: Donald W Hine
    orcid: 0000-0002-3905-7026
    email: donald.hine@canterbury.ac.nz
    affiliation: 
      name: University of Canterbury, School of Psychology, Speech and Hearing
      city: Wellington
      country: New Zealand
      url: www.wgtn.ac.nz/cacr
keywords:
  - DAGS
  - Causal Inference
  - Confounding
  - Environmental
  - Psychology
  - Panel
format:
  pdf:
    sanitize: true
    keep-tex: true
    link-citations: true
    colorlinks: true
    documentclass: article
    classoption: [singlecolumn]
    lof: false
    lot: false
    geometry:
      - top=30mm
      - left=20mm
      - heightrounded
    header-includes:
      - \input{/Users/joseph/GIT/templates/latex/custom-commands.tex}
date: last-modified
execute:
  echo: false
  warning: false
  include: true
  eval: true
fontfamily: libertinus
bibliography: /Users/joseph/GIT/templates/bib/references.bib
csl: ./camb-a.csl
---

```{r}
#| label: load-libraries
#| echo: false
#| include: false
#| eval: false
#  fig-pos: 'htb'
#   html:
#    html-math-method: katex

# Include in YAML for Latex
# sanitize: true
# keep-tex: true
# include-in-header:
#       - text: |
#           \usepackage{cancel}


# for making graphs
library("tinytex")
library(extrafont)
loadfonts(device = "all")


# libraries for jb (when internet is not accessible)
# read libraries
source("/Users/joseph/GIT/templates/functions/libs2.R")

# read functions
source("/Users/joseph/GIT/templates/functions/funs.R")

# read data/ set to path in your computer
pull_path <-
  fs::path_expand(
    "/Users/joseph/v-project\ Dropbox/data/current/nzavs_13_arrow"
  )

# for saving models. # set path fo your computer
push_mods <-
  fs::path_expand(
    "/Users/joseph/v-project\ Dropbox/data/nzvs_mods/00drafts/23-causal-dags"
  )


# keywords: potential outcomes, DAGs, causal inference, evolution, religion, measurement, tutorial
# 10.31234/osf.io/b23k7

```

## Introduction

Causal inference seeks to answer a fundamental question: by how much does an intervention or "treatment" directly change another variable (the "outcome"), if at all? It goes beyond identifying correlations to quantify the actual magnitude of causal effects. While the ability to understand cause and effect exists across the animal kingdom [@mancuso2018revolutionary], methods for reliably estimating the size of these effects are a relatively recent development.

Though often considered the "gold standard" for establishing causation, randomized controlled experiments have limitations. They can be expensive, ethically complex, and results may still be affected by bias [@hernan2017per; @montgomery2018]. Observational data, abundant and readily available, offer a valuable alternative for accelerating knowledge – but only if valid causal inferences can be drawn from them.

Unfortunately, many researchers still attempt to analyze observational data using standard covariance analysis that does not address the problem of confounding and can worsen bias [@westreich2013; @robins1986; @hernan2023]. This has led to a "causality crisis" in social sciences, where correlation is acknowledged as insufficient, yet firm causal statements remain elusive [@bulbulia2023a].

Recent advances in causal inference from biostatistics and computer science offer a solution [@vanderweele2015]. This approach has immense potential for environmental psychology, a field where randomized controlled experiments often are impractical or impossible. This chapter introduces those methods, with the goal of helping environmental psychologists gain the understanding and tools they need to confidently extract robust causal insights from their data.

### Overview
#### Part 1: Potential Outcomes and Experimental Foundations

We introduce the potential outcomes framework—the cornerstone of causal inference [@hernan2023]. Within the familiar context of randomized experiments, we examine **three core assumptions** vital for reliable causal conclusions. Building intuition using experiments offers two benefits: it sheds light on the assumptions behind causal analysis, guiding both observational and experimental design, and it makes clear that even "gold standard" experiments rely on assumptions for their causal interpretations [@westreich2012berkson; @hernan2017per; @westreich2015; @robins2008estimation].

#### Part 2: Causal Diagrams - Visually Understanding Confounding

Next, we dive into Directed Acyclic Graphs (DAGs), visual models representing causal relationships and assumptions [@pearl2009a]. Although a full examination of their capabilities goes beyond this chapter, we equip you with essential strategies for their construction and interpretation. You will see how DAGs are **built from four basic graphical structures** ready to apply to your environmental psychology questions.

#### Part 3: Using DAGs for Causal Identification

We use seven examples to demonstrate how the four foundational graphical structures contribute to confounding problems. DAGs will illuminate strategies researchers can employ to address these issues. Some tactics may align with intuition, but DAGs can reveal insightful options we may not initially consider. Although DAGs are supported by rigorous mathematical proof, DAGs are surprisingly accessible: they require no complex calculations and thus empowering a broad range of researchers.

#### Part 4: a Practical Guide to DAGs

We address the uncertainties around true causal structures when confounding relationships are inherently unclear or debated.  Here, we explain the value of drawing multiple DAGs and simulate data to illustrate why this approach is useful.  Finally, we provide recommendations for reporting in two settings:

  (i) Only cross-sectional data are available
  (ii) Repeated measures longitudinal data are available

## Part 1: An Overview of the Potential Outcomes Framework for Causal Inference

The potential outcomes framework for causal inference originated in the work of Jerzy Neyman for the purpose of evaluating the effectiveness of agricultural experiments [@neyman1923]. It was later extended by Harvard statistician Donald Rubin, who demonstrated the framework may also facilitate causal inferences in non-experimental settings [@rubin1976]. Jamie Robins further generalized this framework to assess confounding in complex scenarios involving multiple treatments and time-varying treatments [@robins1986]. 

A core concept within this framework is that of a "counterfactual contrast" or "estimand." To quantitatively assess the magnitude of causality requires contrasting how the world would have turned out under two or more states, corresponding to different levels of intervention or treatment.  Notably, before any intervention, these states remain counterfactual. After any intervention, for every treatment applied, at most, only one of the two states of the world to be contrasted is realized. The other state remains counterfactual. Philosophers who have puzzled over the nature of causation have long realized that causality is **never directly observed** [@hume1902]. In a sense, we may think of causal inference as a form of counterfactual data science [@edwards2015; @bulbulia2023a] 

#### The Fundamental Problem of Causal Inference: causation is not directly observed

To grasp the implications of counterfactual contrasts in causal inference, imagine yourself at a pivotal point in your life. Having just completed your undergraduate studies, you have been accepted into your dream Environmental Psychology program at the University of Canterbury and are set to relocate to Christchurch, New Zealand. Suddenly, you receive a fantastic job offer from Acme Nuclear Fuels, a leader in renewable energy.  Both paths diverge significantly —  lifestyle, income, social networks, relationships, perhaps even life purpose hang in the balance. Which choice aligns with your ideal future?

Formally, let $D$ denote the decision, where $D = 1$ means attending graduate school and $D = 0$ means joining the workforce.  Your two potential outcomes under each path are described as $Y_{\text{you}}(1)$ and $Y_{\text{you}}(0)$.  Importantly, we assume a definite outcome exists for each choice you *could* have made.  Conceptually, we need to measure the difference $Y_{\text{you}}(1) - Y_{\text{you}}(0)$ to quantify the *magnitude* of the effect of your choice. Yet, this difference remains fundamentally unobservable. Your life goes down one path, forever obscuring the alternative:

$$
(Y_{\text{you}}|D_{\text{you}} = 1) = Y_{\text{you}}(1) \quad \text{implies} \quad Y_{\text{you}}(0)|D_{\text{you}} = 1~ \text{is counterfactual}.
$$

The same counterfactual issue arises if you had selected $D = 0$:

$$
(Y_{\text{you}}|D_{\text{you}} = 0) = Y_{\text{you}}(0) \quad \text{implies} \quad Y_{\text{you}}(1)|D_{\text{you}} = 0~ \text{is counterfactual}.
$$

This simple model mirrors the sorts of consequential "fork-in-the-road" decisions we regularly encounter. "The fundamental problem of causal inference" [@rubin2005; @holland1986] highlights that directly comparing one individual's outcome under both potential paths is impossible. 

This problem persists. Do not despair! Under certain assumptions, careful data collection and analysis can illuminate *average treatment effects*. Consider how experiments uncover these key assumptions. 

### Understanding Relationships of Cause and Effect Through Intervention Outcomes

Let us transition to an example of relevance to environmental psychology, estimating the average causal effect of easy access to urban green spaces on psychological well-being, with a focus on subjective happiness, hereafter referred to as "happiness." We assume this outcome is measurable and represent it with the letter $Y$. 

For simplicity, we classify the intervention "ample access to green space" as a binary variable. Define $A = 1$ as "having ample access to green space" and $A = 0$ as "lacking ample access to green space." We assume these conditions are mutually exclusive. This simplification does not limit the generality of our conclusions; the points we make about experiments apply to continuous treatments as well. It is crucial in causal inference to specify the population for whom we seek to evaluate causal effects, or the "target population." In this case, our target population is residents of New Zealand in the 2020s.

A preliminary causal question -- defined as a causal contrast or “estimand” might therefore be:

"In New Zealand, does proximity to abundant green spaces increase self-perceived happiness compared to environments lacking such spaces?"

For the sake of argument, suppose that it would be unethical to experimentally randomise individuals into different green-space access conditions, but we choose to overlook this ethical consideration. Assume we could assign people randomly to high and low green space access without objection or harm.

The first point to note in the context of causal inference, as alluded to earlier, is that even well-designed experiments confront the challenge of missing values in the potential outcomes. Once an individual is assigned to one treatment condition, we cannot observe that individual's outcome for the condition not assigned. The fundamental problem of causal inference remains constant: for each individual, we can only observe one of the potential outcomes at any given time. Breaking down the Average Treatment Effect (ATE) into observed and unobserved outcomes yields the following equation:

$$
\text{Average Treatment Effect} = \left(\underbrace{\underbrace{\mathbb{E}[Y(1)|A = 1]}_{\text{observed}} + \underbrace{\mathbb{E}[Y(1)|A = 0]}_{\text{unobserved}}}_{\text{treated}}\right) - \left(\underbrace{\underbrace{\mathbb{E}[Y(0)|A = 0]}_{\text{observed}} + \underbrace{\mathbb{E}[Y(0)|A = 1]}_{\text{unobserved}}}_{\text{untreated}}\right).
$$

In this expression, $\mathbb{E}[Y(1)|A = 1]$ represents the average outcome when the treatment is given, which is observable. However, $\mathbb{E}[Y(1)|A = 0]$ represents the average outcome if the treatment had been given to those who were actually untreated, which remains unobservable. Similarly, the quantity $\mathbb{E}[Y(0)|A = 1]$ also remains unobservable.

It is hopefully evident from this brief application of the potential outcomes framework to experiments that the fundamental problem of causal inference is an ever-present concern even in experiments. For each participant, it is impossible to determine the outcome they would have experienced under an alternative treatment condition, just as you cannot quantitatively describe the life you would have led had you chosen the job at Acme Nuclear Fuels instead of attending the University of Canterbury.

### Causal Inference in Experiments

How do experiments manage to estimate average treatment effects despite the inherent challenges? The solution involves addressing the concept of "confounding." Consider the concept of "confounding by common cause." This occurs when one or more variables causally affect both the intervention under study (the "treatment" or "exposure") and the outcome of interest, leading to a non-causal association between the treatment and outcome. By "non-causal," we mean that were we to intervene in the treatment but not the confounder, the outcome would not change. The common cause creates a misleading or exaggerated relationship that may be mistakenly interpreted as causal. For instance, when assessing the impact of access to green space on happiness, it is possible that the association could be entirely explained by income. If so, then an observed association between access to green space and happiness would be entirely misleading. Were we to relocate low-income individuals to high-access green areas (hopefully not against their will), we might not affect subjective happiness at all. Thus, accurately identifying and adjusting for confounding by common cause is crucial for determining the true causal relationship between two variables, ensuring that the observed association is not merely a result of extraneous influences.

#### Balance of Confounders Removes Confounding

Random assignment in experiments is the key to eliminating bias from confounding factors. It creates, on average, an equal distribution of confounders (denoted by $L$) across treatment groups. While confounders still exist, this balance allows us to confidently attribute outcome differences between groups to the treatment *itself*.

We express this lack of confounding mathematically in two equivalent ways (where $\coprod$ means "independent"):

1. **Potential Outcomes Independent of Treatment (given L):**  $Y(a) \coprod A \mid L$ 
2. **Treatment Assignment Independent of Potential Outcomes (given L):**   $A \coprod Y(a) \mid L$

Understanding this notation is helpful when using causal diagrams,  but the diagrams themselves visually encode these concepts. The core idea is simple: **a balance of confounders between treatment groups is the essence of both experimental and observational causal inference strategies.**  Randomization achieves this, ensuring $A \coprod Y(a)$.

This leads to our first key principle of causal inference from experiments:

1. **Conditional Exchangeability:** potential outcomes and treatment assignment are statistically independent after considering all measured confounders.  This allows us to attribute group differences to the treatment. Note: randomization achieves *unconditional* exchangeability, simplifying analysis.

#### Control Ensures Consistent Treatments Across All Confounders

Two more principles let us draw conclusions about average treatment effects, and they stem from the *control* in an experiment:

2. **Causal Consistency:** observed outcomes match the potential outcomes under the assigned treatment.  In notation, for individual 'i':

$$
\begin{aligned}
Y_{i}(1) &= (Y_{i}|A_{i} = 1) \quad \text{(Potential outcome if treated)} \\
Y_{i}(0) &= (Y_{i}|A_{i} = 0) \quad \text{(Potential outcome if untreated)}
\end{aligned}
$$

If exchangeability holds, we can then derive the Average Treatment Effect (ATE) from observed data:

$$
\begin{aligned}
\text{ATE} &= \mathbb{E}[Y(1)] - \mathbb{E}[Y(0)] \\
&= \mathbb{E}(Y|A=1) - \mathbb{E}(Y|A=0)
\end{aligned}
$$

It is the *control* in a Randomized Controlled Trial (RCT) that typically makes causal consistency a safe assumption; treatments are standardized. We will see this is harder to ensure in observational data.

3. **Positivity:**  every individual has a non-zero chance of receiving any treatment level within each combination of confounder values. Postivity also arises natural from experimental *control* and rarely needs explicit mention. But it is vital to check postivity in observational studies.

### Why Satisfying the Fundamental Assumptions of Causal Inference in Observational Settings is Difficult

Unlike experiments, observational studies lack researcher control over treatment allocation. The aim, though, is to mimic an experimental setting as closely as possible. The core hurdle is achieving that balance in confounders that lets us compare groups fairly.

##### The Conditional Exchangeability Assumption in Observational Settings

Conditional exchangeability demands that groups be alike *except* for the treatment received. But in our green spaces example, real-world data won't look this tidy:

* **Socioeconomic status:**  individuals' financial means likely play a role in  housing locations, and thus influence proximity to quality green spaces.
* **Age demographics:**  different age groups have varied needs and preferences when it comes to green spaces. 
* **Mental health:**  existing mental health conditions could cause  someone to actively seek out or avoid green spaces.
* **Lifestyle choices:**  green space proximity might align with a broader preference for outdoor activities. Is well-being directly caused by the area, or  a healthier lifestyle in general? 
* **Personal values and social connections:** environmental values or connections within the community could explain both residence choice and the way these spaces are enjoyed.

These and other hidden factors bias our observational study, making it tricky to isolate the actual impact of green spaces.

##### The Causal Consistency Assumption and Treatment Heterogeneity

'Proximity to green spaces'  itself hides huge variability. This variation makes the 'treatment' a moving target for consistent measurement:

* **Diversity of green spaces:** biodiversity and aesthetics range broadly  – comparing manicured urban parks to untended, overgrown spaces is not a fair matchup.
* **Availability of amenities:** access to paths, seating, etc. drastically influences how often and in what ways the space can be used.
* **Size and type of green space:** urban garden or sprawling forest? Each offers distinct benefits. 

##### The Positivity Assumption in Observational Settings 

Positivity requires every individual to have a chance of any treatment level. But sometimes, practical limits on  housing in certain areas create barriers for specific demographics, hindering inference across potential scenarios.

**Key Point:** although we try to mirror experiments in observational studies, getting close enough for truly confident causal conclusions can be incredibly hard.  Sometimes, our data simply won't allow it. Understanding the experimental ideal helps environmental psychologists recognize and explain the limits of observational causal claims.

## Part 2. Causal Diagrams

This section introduces causal diagrams, starting with essential terminology. Understanding this terminology can be a bit tricky for newcomers, but it is necessary for applying causal diagrams effectively. After explaining the terms, we will examine practical examples. We will see that various forms of confounding arise from four key causal structures. Recognizing these structures is crucial for applying them in practical scenarios. Note: **Appendix A** provides a comprehensive glossary.

Causal diagrams use specific symbols to represent elements important in causal inference, as established in seminal works [@pearl1995; @pearl2009; @greenland1999].  The symbols and their meanings are listed in Table @tbl-01:

* **$A$** is the treatment or exposure variable – the intervention or condition whose effect on an outcome is under investigation. **This symbol represents the cause**.
* **$Y$** is the outcome variable – the effect or result that is being studied. **This symbol represents the effect**.
* **$L$** includes all measured confounders – variables that may affect both the treatment and the outcome.
* **$U$** includes unmeasured confounders – variables not included in the analysis that could influence both the treatment and the outcome, potentially leading to biased conclusions.
* **$M$** is a mediator variable – a factor through which the treatment affects the outcome. The focus here is on identifying the total effect of treatment $A$ on an outcome $Y$, but it is also important to understand how controlling for mediators can affect estimates of this total effect. 

::: {#tbl-01}

```{=latex}
\terminologylocalconventionssimple
```
Terminology that is used in this article for causal diagrams (adapted from [@bulbulia2023]). 
:::

### Elements of Causal Diagrams

Having established the meanings of our symbols, we now turn to the fundamental components of causal diagrams themselves.  @tbl-02 describes the basic elements.  Key features are:

1. **Nodes**:  represent variables or events within a causal system. They stand for distinct elements that can influence or be influenced within the system.

2. **Edges**: signify the relationships between the variables represented by nodes.  In causal diagrams, edges are directed that define pathways of causal influence. Importantly, causal diagrams are non-parametric; thus, the representation of a relationship does not change with its nature—be it linear or non-linear, an arrow is used in both cases.

3. **Influence relationships**: Aa variable is termed a "child" if it is directly influenced by another variable, which is then referred to as its "parent."  This describes the direction of causal influence within the diagram.

4. **Acyclic**: causal diagrams must not contain cycles; that is, they cannot have feedback loops where a variable can be both a cause and effect of itself, directly or indirectly. This requirement ensures clarity in the direction of causal influence.  In scenarios involving repeated measurements, nodes should be indexed by time to maintain acyclicity.

5. **Conditioning**:  determining whether to statistically control for certain variables is important for unbiased estimation of the relationship between the treatment and the outcome. This process of "conditioning" or "adjustment" is visually represented by enclosing the variable in a box within the diagram. 

::: {#tbl-02}

```{=latex}
\terminologygeneralbasic
```
Basic conventions for causal diagrams (adapted from [@bulbulia2023]). 
:::



#### The Rules of D-separation 

Pearl proved how rules of d-separation may allow us to evaluate relationships between nodes in a causal diagram [@pearl1995].

When we write $X_1 \cancel\coprod X_2$, it means the probability distributions of  $X_1$ and $X_2$ are intertwined. In simpler terms, what happens with $X_1$ gives us clues about what is happening with $X_2$, indicating a connection or dependence between them.

Conversely, $X_1 \coprod X_2$ means $X_1$ and $X_1$ means the probability distributions of $X_1$ and $X_2$ are independent. Their probability distributions do not mix; knowing about $X_1$ does not help guess anything about $X_2$. 

We call a path "blocked" or "d-separated" if a node along it prevents the transmission of influence.  Two variables are deemed d-separated, denoted as $X_1 \coprod X_2$, if every pathway connecting them is obstructed, indicating no influence or statistical association passes through. Conversely, if at least one path remains unblocked, allowing for the transmission of influence, the variables are considered d-connected, denoted as $X_1 \cancel\coprod X_2$ [@pearl1995].  

Next, we define three basic rules that link causal relationships to statistical associations. All confounding results from these rules.[^notes]

[^notes]: Recall that to ensure balance in confounders across treatments, we must ensure that $A\coprod Y(a)|L$ -- that the potential outcomes are independent within levels of the treatments to be compared, conditional on measured covariates $L$.  We do not need to ensure that the treatment has no effect such that $A\coprod Y |L$. 

##### Two variables with no arrows: absence of causatoin

$$\xorxA$$
If $X_0$ and $X_1$ have no arrows between them, we assert there is no causal effect. Without a causal link, they are independent, denoted mathematically as $X_0 \coprod X_1$. This means knowing one variable tells you nothing about the other.

#### Fundamental causal association: two variables with a causal arrow

$$\xtoxA$$

Adding a causal arrow from $X_0 \to X_1$ indicates $X_0$ causes a change in $X_1$, or that $X_1$ "listens to" $X_0$. Causality creates a *statistical* dependency, denoted mathematically as $X_0 \cancel\coprod X_1$. This means knowing $X_0$ gives information about $X_1$

#### Three Variables: Fork, Chain, and Collider Structures

There are only three fundamental ways in which we may add another causal relationship to the basic two-node structure: the fork structure, the chain structure, and the collider structure. Each structure has a corresponding rule for how the probability distributions of the three nodes become related or unrelated when we condition on them.


##### The Fork Structure

$$\fork$$

The fork structure ($X_0 \rightarrow X_1$, $X_0 \rightarrow X_2$) shows $X_0$ causes both $X_1$ and $X_2$. Given $X_0$, $X_1$ and $X_2$ are independent, written as $X_1 \coprod X_2 | X_0$. This means knowing $X_0$ removes any link between $X_1$ and $X_2$. Thus, if $X_1$ and $X_2$ are statistically related after conditioning on their common cause this statistical association reflects causation. Here, association is causation.

**Fork Rule**:  *if interested in the causal effect of $X_1 \to X_2$, condition on $\boxed{X_1}$**.


##### The Chain Structure

$$\chain$$


The chain structure ($X_0 \rightarrow X_1 \rightarrow X_2$) describes a setting in which $X_0$ affects $X_1$ and $X_1$ affects $X_2$.
In this structure, $X_0$ and $X_2$ are conditionally independent given $X_1$. That is, if $\boxed{X_1}$, then $X_0 \coprod X_2 | X_1$). Hence, conditioning on $X_1$ *blocks* the association of $X_0$ and $X_2$. This implies that if there is a causal association between $X_0$ and $X_2$ we might not detect it after conditioning on $\boxed{X_1}$.

Chain Rule: **if interested in the (total) causal effect of $X_0 \to X_2$, do not condition on $X_1$**.


##### The Collider Structure

$$\immorality$$

The collider structure describes a setting in which both $X_0\to X_2$ and $X_1 \to X_2$.  In a collider structure, $X_0$ and $X_1$ are independent. However, conditioning on $X_2$ -- the common descendant of $X_2$, $X_3$ -- introduces a stastisical association between $X_0$ and $X_1$, such that $X_0 \cancel\coprod X_1 | X_2$. That is conditioning on the common effect (or its descendant) creates a pathway through which $X_0$ and $X_1$ share information about each other. Again, this statistical association between $X_0$ and $X_1$ can arise in the absence of any causal association.

Collider Rule: **if interested in the causal effect of $X_0 \to X_1$, do not condition on $X_2$**.


##### Extensions: all confounding bias results from combinations of basic causal associations and the three elementary structures (forks, chains, colliders)


For example, consider the implications of conditioning on the descendant of a collider:

$$\immoralityChild$$


Because every descendant is statistically associated with its parent (basic causality), a descendant acts as a proxy for its parent. In this scenario, conditioning on $X_3$ (the descendant of $X_2$, which is a collider) inadvertently opens a path between $X_0$ and $X_1$ that would otherwise be blocked by the unconditioned collider $X_2$. This occurs because $X_3$ inherits all statistical associations of $X_2$, here, its statistical associations with $X_0$ and $X_1$. Consequently, when we condition on $X_3$, we indirectly condition on $X_2$, thereby introducing a statistical association between $X_0$ and $X_1$ through $X_3$. 

This principle underscores the nuanced nature of causal inference, demonstrating how conditioning on certain variables, here a descendant of colliders, can inadvertently create statistical dependencies such that association is *not* causation. The implication: do not condition on a descendent of a collider. 

For those new to the causal diagrams, the fundamental structures of causation presented in Pearl's rules of d-separation might seem abstract. Similarly, individuals not versed in probability theory might find the notation of conditional dependencies and independencies challenging. Yet, with the groundwork of rules and conventions laid, applying causal diagrams to real-world problems is need not be intimidating. In fact, based on our experience, many find constructing causal diagrams to be enjoyable conversation starters. Once we know how conditioning affects statistical associations within causal structures, we can work backwards through statistics to make causal inferences.  Put another way, causal diagrams reveal how and when association is causation. This is good news for psychological questions that are inaccessible to experiments. However, we must temper our enthusiasm. We must remember that **all structure relationships in a graph except the focal treatment/outcome relationship must be assumed.** This because most observational datasets are typically compatible with many causal diagrams. Statistically "significant" associations do not in themselves reveal anything about causation. Causal diagrams are powerful aids because they help investigators to understand what follows from their assumptions the data. However, assumptions about the structural relationships in the data cannot be avoided. Because every path except the $A\to Y$ path is assumed, causal diagrams should be created in collaboration with area experts. Where there is expert disagreement, multiple causal diagrams should be proposed to reflect the implications of disagreements for causal inference. 

### How to Create Causal Diagrams 

#### Causal diagrams must address a clearly stated identification problem

The *identification problem* is the task of determining if the effect of a treatment ($A$) on an outcome ($Y$) can be accurately inferred from statistical accociations in the data. Causal diagrams, which incorporate *structural assumptions*, serve as tools for assessing the potential to identify causal effects from the patterns observed in data [@hernán2004a]. These assumptions, fundamental to causal inference, are generally not empirically testable through data alone. The identification challenge encompasses two principal considerations:

1. **In the absence of a causality**, when all known confounders are controlled for, is there statistical independence between the treatment and outcome? This involves identifying all potential confounders, implementing strategies to adjust for these confounders, and then checking if there are any indirect connections, known as *backdoor paths*, between $A$ and $Y$. A *backdoor path* refers to a sequence of links in the causal diagram that could introduce spurious associations between $A$ and $Y$ if not properly adjusted for. (Below we work through seven examples). The goal here is to evaluate whether $A$ and $Y$ are d-separated, indicating that any observed association between $A$ and $Y$ does not stem from these indirect paths.

2. **With an existing causal link**, once all known confounders have been adjusted for, does a direct and unbiased relationship between the treatment and outcome remain observable? This verifies whether the statistical relationship between $A$ and $Y$, as analysed under the specified model, can be interpreted as reflecting a true causal effect, mindful of potential *over-conditioning biases*. *Over-conditioning bias* occurs when the adjustment process inadvertently introduces or magnifies associations that misrepresent the actual causal relationship.

The process breaks down into two tasks:

- **First task**: verify that $A$ and $Y$ are not linked after adjusting for known common causes. This step involves ensuring that no *backdoor paths* -- indirect paths that could falsely suggest a relationship between $A$ and $Y$ -- remain open after an adjustment strategy is applied.
  
- **Second task**: confirm that an assocation between $A$ and $Y$ remains unbiased after the conditioning strategy from the first task. This step is crucial for validating that the observed statistical association accurately reflects the causal relationship, free from distortions caused by the adjustment strategy.


#### Steps in creating causal diagrams that aaddress identification problems

1. **Include all common causes of the exposure and outcome**

There are two main types of common causes: measured and unmeasured. Functionally similar common causes should be grouped under a single variable, where possible. For instance, instead of listing every demographic variable, consider using a single label, such as $L_0$.

2. **Include all ancestors of measured confounders linked with the treatment, the outcome, or both**

Include in your causal diagram any measured variables that are ancestors of unmeasured confounders if this variable is associated with the exposure or outcome or both. This inclusion facilitates identifying strategies where conditioning on a proxy of an unmeasured cause reduces bias from the unmeasured common cause and helps prevent M-bias, a form of over-conditioning bias described below. Again, group functionally similar variables under a common label to simplify the diagram and highlight critical variables affecting the relationship between $A_1$ and $Y_0$.

3. **Explicitly state assumptions about the relative timing of events**

Denote the assumed timing of events with time subscripts (e.g., $L_0$, $A_1$, $Y_2$), where subscripts indicate their relative order in time. This clarifies acyclicity and highlights the temporal nature of causal relationships we assume to hold in our data.

4. **Arrange the temporal order of causality visually**

Organising the assumed temporal order of causality from left to right or top to bottom aids in understanding the causal assertions within the diagram [@bulbulia2023]. As we shall see in Part 3, ensuring appropriate temporal measurement of variables can effectively address common identification problems.

5. **Box variables that are conditioned on**

Generally, the exposure and outcome are not boxed unless measured with error, as they do not act as confounders.

6. **Use conventions to clarify sources of bias and clearly state these conventions**

The creation of a causal diagram lacks a universal approach; therefore we must be clear about our conventions. For instance, drawing open back door paths in red, as we do here, may help highlight and identify bias sources. However, not everyone has colour vision, and indeed not everyone has vision. Clarity demands verbally describing the contents of our causal diagrams.

7. **Represent paths structurally, not parametrically**

Avoid any attempt to graphically describe non-linear relationships between nodes. The purpose of a causal diagram is to evaluate confounding, regardless of the linearity of the paths. Attempting to denote non-linearities can clutter the diagram with assumptions that are not relevant to the task at hand, evaluating sources of bias.

8. **Minimise paths to those necessary for the identification problem**

Avoid unnecessary clutter in causal diagrams. Include only paths essential for addressing a stated identification problem, such as evaluating open back door paths or mediator bias.

9. **Clarify the research question evaluated by the causal diagram**

State the specific question your causal diagram addresses. For example, in causal mediation analysis, distinguishing between the indirect and direct effects of $A$ on $Y$ requires specific paths to be drawn and conditioned on. A strategy that would induce confounding for one question might not be relevant to for a different question.

Next, we shall consider how applying the the rules of forks, chains, and colliders, causal diagrams offer a powerful tool for researchers to obtain consistent causal estimates tailored to their specific questions.

## Part 3. Applying causal diagrams to causal inference: worked examples


::: {#tbl-04}
```{=latex}
\terminologyelconfoundersLONG
```
Worked examples: This table is adapted from [@bulbulia2023].

:::



### 1. The problem of confounding by a common cause

@tbl-04 1 describes the problem of confounding by common cause and its solution. We encountered this problem in Part 1. Such confounding arises when there is a variable or set of variables, denoted by $L$, that influence both the exposure, denoted by $A$, and the outcome, denoted by $Y.$ Because $L$ is a common cause of both $A$ and $Y$, $L$ may create a statistical association between $A$ and $Y$ that does not reflect a causal association.

For instance, in the context of green spaces, consider that people who choose to live closer to green spaces (exposure $A$) and their experience of improved happiness (outcome $Y$). A common cause might be socioeconomic status $L$. Individuals with higher socioeconomic status might have the financial capacity to afford housing near green spaces and simultaneously afford better healthcare and lifestyle choices, contributing to greater happiness. Thus, although the data may show a statistical association between living closer to green spaces $A$ and greater happiness $Y$, this association might not reflect a direct causal relationship owing to confounding by socioeconomic status $L$.

How might we obtain balance in this confounder for the treatments to be compared?  Addressing confounding by a common cause involves adjusting for the confounder in one's statistical model. This may be done through regression, or more complicated methods, such as inverse probability of treatment weighting, marginal structural models, and others see @hernán2023. Such adjustment effectively closes the backdoor path from the exposure to the outcome. Equivalently, conditioning on $L$ d-separates $A$ and $Y$.  

@tbl-04 Row 1, Column 3, emphasises that a confounder by common cause must precede both the exposure and the outcome. While it is often clear that a confounder precedes the exposure (e.g., a person's country of birth), in other cases, the timing might be uncertain. By positioning the confounder before the exposure in our causal diagrams, we assert its temporal precedence. However, when relying on cross-sectional data, such a timing assumption might be strong. In such cases, exploring causal scenarios where the confounder follows the treatment or outcome can be insightful. Causal diagrams are instrumental in examining possible timings and their implications for causal inference. 

Next, we examine the effects of conditioning on a variable that is an effect of the treatment.

### 2. Mediator Bias

Consider again the question of whether proximity to green spaces, $A$, affects happiness, $Y$. Suppose that physical activity is a mediator, $L$.

To fill out the example, imagine that living close to green spaces $A$ influences physical activity $L$, which subsequently affects happiness $Y$. Notice that if we were to condition on physical activity $L$, assuming it to be a confounder, we would then bias our estimates of the total effect of proximity to green spaces $A$ on happiness $Y$. Such a bias arises as a consequence of the chain rule. Conditioning on $L$ "d-separates" the total effect of $A$ on $Y$. This phenomenon is known as mediator bias. Notably, @montgomery2018 finds dozens of examples of mediator bias in *experiments* in which control is made for variables that occur after the treatment.  For example, the practice of obtaining demographic and other information from participants *after* a study is an invitation to mediator bias. If the treatment affects these variables, and the variables affect the outcome (as we assume by controlling for them), then researchers may induce mediator bias. 

To avoid mediator bias when estimating a total causal effect we should, of course, avoid conditioning on a mediator! The surest way to avoid this problem is to ensure that $L$ occurs before the treatment $A$ as well as before the outcome $Y$.  This solution is presented @tbl-04 Row 2 Col 3. 


### 3. Confounding by Collider Stratification (Conditioning on a Common Effect)

Conditioning on a common effect, also known as collider stratification, occurs when a variable, denoted by $L$, is influenced by both the exposure, denoted by $A$, and the outcome, denoted by $Y$.

Imagine, again the context of an access to green space question, that an individual's choice to live closer to green spaces (exposure $A$) and their happiness (outcome $Y$) both affect the individual's overall sense of physical health (common effect $L$). Initially, $A$ and $Y$ could be independent, that is $A \coprod Y(a)$, suggesting that the decision to live near green spaces is not directly a cause of happiness.

However, if we were to condition on physical health $L$ (the common effect of $A$ and $Y$), a backdoor path between $A$ and $Y$ would be opened. That is, by "controlling for" physical health we might *induce* a non-causal association between proximity to green spaces and happiness.

The reason that conditioning on physical health $L$ leads to confounding is that this variable provides information about both the proximity to green spaces $A$ and one's happiness $Y$. Once we learn something about one's physical health, for example, that it is poor, it becomes more probable that a person is happy if they have low access to green spaces (assuming the relationship between green space access and happiness is positive.) 

Causal diagrams point a way to respond to the problem of collider stratification bias: we should generally ensure that:

1.  All confounders $L$ that are common causes of the exposure $A$ and the outcome $Y$ are measured before $A$ has occurred, and

2.  $A$ is measured before $Y$ has occurred.

If such temporal order is preserved, $L$ cannot be an effect of $A$, and thus neither of $Y$.


### 4. Confounding by Conditioning on a Descendant of a Confounder  

The rules of d-separation also apply to conditioning on descendants of a confounder.  As shown in @tbl-04 Row 4, when conditioning on a measured descendant of an unmeasured collider we may unwittingly evoke confounding by proxy. For example, if doctor visits were encoded in our data and doctor visits were an effect of poor health, then conditioning on doctor visits would function in a similar way to conditioning on poor health, introducing collider confounding. 

There are only four elementary forms of confounding. Any confounding scenario we might imagine can be developed from these elementary forms. We next consider how we may combine these elementary causal relationships in causal diagrams to develop effective strategies for confounding control. 

### 5. M-bias: Conditioning on Pre-Exposure Collider

@tbl-04 Row 5 presents a form of pre-exposure over-conditioning confounding known as "M-bias".  This bias combines the collider structure and the fork structure revealing what might not otherwise be obvious: it is possible to induce confounding even if we ensure that all variables have been measured **before** the treatment. The collider structure is evident in the path $U_Y \to L_0$ and $U_A \to L_0$. We know from the collider rule that conditioning on $L_0$ opens a path between $U_Y$ and $U_A$. What is the result? We find that $U_Y$ is associated with the outcome $Y$ and $U_A$ is associated with treatment $A$. This is a fork (common cause) structure. The association between treatment and outcome that is opened by conditioning on $L$ arises from an open back-door path that occurs from the collider structure. We thus have confounding. How might such confounding play out in a real-world setting? 

In the context of green spaces, consider the scenario where an individual's level of physical activity $L$ is influenced by an unmeasured factor related to their propensity to live near green spaces $A$ -- say childhood upbringing. Suppose further that another unmeasured factor -- say a genetic factor -- increases both physical activity $L$ and happiness $Y$. Here, physical activity $L$ does not affect the decision to live near green spaces $A$ or happiness $Y$ but is a descendent of unmeasured variables that do. If we were to condition on physical activity $L$ in this scenario, we would create the bias just described --  "M-bias."  

How shall we respond to this problem? The solution is straightforward. If $L$ is neither a common cause of $A$ and $Y$ nor the effect of a shared common cause, then $L$ should not be included in a causal model. In terms of the conditional exchangeability principle, we find $A \coprod Y(a)$ yet $A \cancel{\coprod} Y(a)| L$. So we should not condition on $L$: do not control for exercise [@cole2010].[^3]

[^3]: Note that when we draw a chronologically ordered path from left to right the M shape for which "M-bias" takes its name changes to an E shape We shall avoid proliferating jargon and retain the term "M bias."

### 6. Conditioning on a Descendent May Sometimes Reduce Confounding

Consider how we may use the rules of d-separation to obtain unexpected strategies for confounding control. In @tbl-04 Row 6, we encounter a causal diagram in which an unmeasured confounder opens a back-door path that links the treatment and outcome. We have what appears to be intractable confounding.  Return to our green space example. Suppose an unmeasured genetic factor $U$ affects one's desire to seek out isolation in green spaces $A$ and also independently affects one's happiness $Y$.  Were such an unmeasured confounder to exist, we could not obtain an unbiased estimate for the causal effect of green space access on happiness. However, imagine a variable $L^\prime$ that is a trait that is expressed later in life, which arises from this genetic factor. If such a trait could be measured, even though the trait $L'$ is expressed after the treatment and outcome have occurred, controlling for $L'$ would enable investigators to close the backdoor path between the treatment and the outcome. The reason this strategy works is that a measured effect is a *proxy* for its cause $U$, the unmeasured confounder.  By conditioning on the late-adulthood trait, $L'$, we partially condition on its cause, $U$, the confounder of $A \to Y$. Thus, not all effective confounding control strategies need to rely on measuring pre-exposure variables. 

### 7. Confounding Control with Three Waves of Data is Powerful and Reveals Possibilities for Estimating an "Incident Exposure" Effect

@tbl-04 row 7 presents another setting in which there is unmeasured confounding. In response to this problem, we use the rules of d-separation to develop a strategy for data collection and modelling that may greatly reduce the influence of unmeasured confounding.  @tbl-04 row 7 col 3, by collecting data for both the treatment and the outcome at baseline and controlling for baseline values of the treatment and outcome, any unmeasured association between the treatment $A_1$ and the outcome $Y_2$ would need to be *independent* of their baseline measurements. As such, including the baseline treatment and outcome, along with other measured covariates that might be measured descendants of unmeasured confounders, is a strategy that exerts considerable confounding control [@vanderweele2020]. 

Furthermore, the graph makes evident a second benefit of this strategy. Consider that an ordinary regression would estimate what is called a "prevalence exposure effect." The prevalence exposure effect evaluates the association between the
exposure or treatment status at time $t1$ and the outcome observed at a later time $t2$. It is defined by the pathway $A_{1} \to Y_{2}$. It is expressed as:

$$
\text{Prevalence exposure effect:} \quad A_{1} \to Y_{2}
$$

Note that a prevalence exposure effect estimate does not consider the initial status of the exposure. As such, a prevalence exposure effect estimate describes the effect of current or ongoing exposures on outcomes. This is often *not* the effect of theoretical interest.

For example, imagine that living far from green spaces makes people so depressed that they never respond to surveys. When we observe the association between green space and happiness in the data, we are left with only those people who are so incurably happy that even living far from green space cannot bring them down. As such , it might appear in our data that $A_{1} \to Y_{2}$ is helpful when,
in fact, the treatment is initially harmful; see: @hernán2016; @danaei2012; @vanderweele2020; @bulbulia2022.

By contrast, adjusting for the baseline exposure and outcome enables us to recover an incident exposure effect. The incident exposure effect evaluates the causal association between the exposure or treatment status at time $t1$ and the outcome observed at a later time $t2$ conditional on the baseline exposure: $A_{0} \to A_{1} \to Y_{2}$. 

By including the baseline exposure, then, we consider the *transition* in treatment or exposure status from $A_0$ to $A_1$. The initiation of a treatment provides a clearer intervention from which to estimate a causal effect at $Y_2$, and we more closely emulate an experiment. It is expressed:

$$
\text{Incident exposure effect:} \quad \boxed{A_{0}} \to A_{1} \to Y_{2}
$$

Returning to our example, a model that controls for baseline exposure would require that people initiate a change from the level of $A_0$ observed baseline. Put differently, by controlling for the baseline value of the treatment, we may learn about the causal effect of shifting one's access to green space status. The incident exposure effect better emulates a "target trial" or the
the organisation of observational data into a hypothetical experiment in which there is a "time-zero" initiation of treatment in the data; see @hernán2016; @danaei2012; @vanderweele2020; @bulbulia2022.  

Finally, we obtain still further control for unmeasured confounding by including, in addition to  the baseline exposure $A_0$, the baseline outcome, $Y_0$, such that:

$$
\boxed{
\begin{aligned}
L_{0} \\
A_{0} \\
Y_{0}
\end{aligned}
}
\to A_{1} \to Y_{2}
$$


In this example, we discover that causal diagrams may novel insights both in data collection and data modelling. To obtain the incident exposure effect, we generally require that events in the data can be accurately classified into at least three relative time intervals and we must model the treatments and outcomes as separate elements in our statistical model. 


## Part 4. Putting Causal Diagrams to Use in Cross-Sectional and Longitudinal Designs

### Cross-sectional designs

In environmental psychology, researchers often grapple with whether causal inferences can be drawn from cross-sectional data, especially when longitudinal data are not available. The challenge is not unique to cross-sectional designs; even longitudinal studies require careful assumption-management. We next discuss how causal diagrams can guide inference in both data types, with examples relevant to environmental psychologists.

1. **Assumption mmnagement**: the crux of causal inference, regardless of data type, is the management of assumptions. Although cross-sectional analyses typically demand stronger assumptions owing to the snapshot nature of data, these assumptions, when transparently articulated, do not always bar causal analysis. For instance, if investigating the effect of green spaces on mental health, longitudinal data might allow discovery of effects owing to changes in access to green spaces, however cross-sectional data might still offer insights if we control for known confounders such as age, socioeconomic status, and urban vs. rural living environments.

2. **Stable confounders**: in cross-sectional studies, some confounders are inherently stable over time, such as ethnicity, year and place of birth, and biological gender. For environmental psychologists examining the relationship between access to natural environments and psychological well-being, these stable confounders can be adjusted for without concern for introducing bias from mediators or colliders. For example, conditioning on year of birth can help to isolate the effect of recent urban development on mental health, independent of generational differences in attitudes toward green spaces.

3. **Invariable confounders**: other confounders, while not immutable, are less likely to be influenced by the treatment. Variables such as sexual orientation, educational attainment, and often income level fall into this category. For instance, the effect of exposure to polluted environments on cognitive outcomes can be analysed by conditioning on education level, assuming that recent exposure to pollution is unlikely to retroactively change someone's educational history.

4. **Timing and reverse causation**: the sequence of treatment and outcome is crucial. In some cases, the temporal order is clear, reducing concerns about reverse causation. Mortality is a definitive outcome where the timing issue is unambiguous. If researching the effects of air quality on mortality, the causal direction (poor air quality leading to higher mortality rates) is straightforward.

5. **Multiple causal diagrams**: given the complexity of environmental influences on psychological outcomes, it's prudent to construct multiple causal diagrams to cover various hypothetical scenarios. For example, when studying the effect of community green space on stress reduction, one diagram might assume direct benefits of green space on stress, while another might include potential mediators like physical activity. By analysing and reporting findings based on multiple diagrams, researchers can explore the robustness of their conclusions across different theoretical frameworks.



@tbl-cs describes ambigious confounding control setting arising from cross-sectional data. Suppose again we are interested in the causal effect of access to greenspace denoted by $A$ on 'happiness', denoted by $Y$.   We are uncertain whether excercise, denoted by $L$, is a common cause of $A$ and $Y$ and thus a confounder, or whether excercise is a mediator along the path from $A$ to $Y$. We may use causal diagrams to investigate the consequences of such ambiguity. 

**Assumption 1: Exercise is a common cause of $A$ and $Y$**, this scenario is presented in @tbl-cs row 1. Here, our strategy for confounding control is to estimate the effect of $A$ on $Y$ conditioning on $L$. 


**Assumption 2: Exercise is a mediator of $A$ and $Y$**, this scenario is presented in @tbl-cs row 2. Here, our strategy for confounding control is to simply estimate the effect of $A$ on $Y$ without including $L$ (assuming there are no other common causes of the treatment and outcome). 



::: {#tbl-cs}

```{=latex}
\examplecrosssection
```
This table is adapted from [@bulbulia2023]
:::



To clarify how answer may differ we can simulate data and run separate regressions, reflecting the different conditioning strategies embedded in the different assumptions. The following simulation generates data from a process in which exercise is a mediator (Scenario 2). (See Appendix C)



```{r}
#| label: simulation_cross_sectional
#| tbl-cap: "Code for a simulation of a data generating process in which the effect of excercise (L) fully mediates the effect of greenspace (A) on happiness (Y)."
#| out-width: 80%
#| echo: false


# load libraries
library(gtsummary) # gtsummary: nice tables
library(kableExtra) #  tables in latex/markdown
library(clarify) # simulate ATE

# simulation seed
set.seed(123) #  reproducibility

# define the parameters 
n = 1000 # Number of observations
p = 0.5  # Probability of A = 1 (access to greenspace)
alpha = 0 # Intercept for L (excercise)
beta = 2  # Effect of A on L 
gamma = 1 # Intercept for Y 
delta = 1.5 # Effect of L on Y
sigma_L = 1 # Standard deviation of L
sigma_Y = 1.5 # Standard deviation of Y

# simulate the data: fully mediated effect 
A = rbinom(n, 1, p) # binary exposure variable
L = alpha + beta*A + rnorm(n, 0, sigma_L) # continuous mediator
Y = gamma + delta*L + rnorm(n, 0, sigma_Y) # continuous outcome

# make the data frame
data = data.frame(A = A, L = L, Y = Y)

# fit regression in which L is assume to be a mediator
fit_1 <- lm( Y ~ A + L, data = data)

# fit regression in which L is assume to be a mediator
fit_2 <- lm( Y ~ A, data = data)

# create gtsummary tables for each regression model
table1 <- tbl_regression(fit_1)
table2 <- tbl_regression(fit_2)

# merge the tables for comparison
table_comparison <- tbl_merge(
  list(table1, table2),
  tab_spanner = c("Model: Exercise assumed confounder", 
                  "Model: Exercise assumed to be a mediator")
)
# make latex table
markdown_table_0 <- as_kable_extra(table_comparison, 
                                   format = "latex", 
                                   booktabs = TRUE)
markdown_table_0
```


This table presents us the conditional treatment effect. Where an outcome is continuous and there are no interactions, the coeficient for the treatment ($A_1$) reflect the average treatment effect. With covariates and interactions, to estimate an average treatment effects requires additional steps.  We present code for obtaining marginal treatment effects in Appendix C 

```{r}
#| label: ate_simulation_cross_sectional
#| fig-cap: ""
#| out-width: 100%
#| echo: false

# use `clarify` package to obtain ATE
library(clarify)
# simulate fit 1 ATE
set.seed(123)
sim_coefs_fit_1 <- sim(fit_1)
sim_coefs_fit_2 <- sim(fit_2)

# marginal risk difference ATE, simulation-based: model 1 (L is a confounder)
sim_est_fit_1 <-
  sim_ame(
    sim_coefs_fit_1,
    var = "A",
    subset = A == 1,
    contrast = "RD",
    verbose = FALSE
  )

# marginal risk difference ATE, simulation-based: model 2 (L is a mediator)
sim_est_fit_2 <-
  sim_ame(
    sim_coefs_fit_2,
    var = "A",
    subset = A == 1,
    contrast = "RD",
    verbose = FALSE
  )

# obtain summaries
summary_sim_est_fit_1 <- summary(sim_est_fit_1, null = c(`RD` = 0))
summary_sim_est_fit_2 <- summary(sim_est_fit_2, null = c(`RD` = 0))

# get coefficients for reporting
# ate for fit 1, with 95% CI
ATE_fit_1 <- glue::glue(
  "ATE =
                        {round(summary_sim_est_fit_1[3, 1], 2)},
                        CI = [{round(summary_sim_est_fit_1[3, 2], 2)},
                        {round(summary_sim_est_fit_1[3, 3], 2)}]"
)
# ate for fit 2, with 95% CI
ATE_fit_2 <-
  glue::glue(
    "ATE = {round(summary_sim_est_fit_2[3, 1], 2)},
                        CI = [{round(summary_sim_est_fit_2[3, 2], 2)},
                        {round(summary_sim_est_fit_2[3, 3], 2)}]"
  )
```


On the assumptions outlined in , in which we *assert* that exercise is a confounder, the average treatment effect of access to green space on happiness is `r ATE_fit_1`.

However, on the assumptions outline in DAG , in which we *assert* that exercise is a mediator, the average treatment effect of access to green space on happiness is `r ATE_fit_2`.

These findings illustrate the role that assumptions about the relative timing of exercise as a confounder or as a mediator plays. 

### Recommendations for conducting and reporting causal analyses with cross-sectional data.

We offer the following recommendations for analysing and reporting analyses with cross-sectional data

1. **Draw multiple causal diagrams**: as the previous example illustrates, we may wish to draw a number of causal diagrams that represent different theoretical assumptions about the both the relationships and timing in the occurrance of variables relevant to an identification problem. This practice allows for a comprehensive exploration of potential causal pathways and the roles variables may play —- be they confounders, mediators, or colliders. For instance, when examining the effect of urban green spaces on mental health, consider diagrams that account for direct effects, as well as those that include mediators like physical activity or social interaction.

2. **Perform and report analyses for each assumption**: conduct separate analyses based on the different scenarios outlined by your causal diagrams. This approach ensures that the analytical strategy aligns with the theoretical underpinnings of each model. Transparently reporting the results from each analysis, including the assumptions and statistical methods employed, enhances balance in a study. Editors and reviewers will not be accustomed to this practice of conditioning inferences on different assumptions. However, for meaningful progress, it essential to persevere. The problem inherent to cross-sectional analyses must be addressed by broadining the scope of our imaginations beyond testing one or another favoured hypothesis. Rather, we should graph the structural assumptions encoded by different theories, and condition on specific combinations in the occurance of events.

3. **Interpret findings with attention to ambiguities**: Ccrefully interpret the results, paying close attention to any ambiguities or inconsistencies that emerge across the different analyses. Describe how different assumptions about structural relationships and timing can lead to different conclusions. For example, if the effect of green space access on mental health appears positive when treating exercise as a mediator but negative when treated as a confounder -- as in the present simulation -- discuss the theoretical and practical implications of these findings.

4. **Recommend caution when findings diverge**: wherever findings lead to different practical conclusions, embrace caution about drawing firm conclusions. 

5. **Suggest specific avenues for future research**: identify gaps in the current understanding that arise from ambiguities in your findings. Recommend specific, targeted data collection that might clarify the nature of the relationships among variables. For instance, longitudinal studies or experiments.

6. **Supplement observational data with simulated data**: data simulation is a powerful tool for understanding the complexities of causal inference in environmental psychology. By simulating data based on various theoretical models, researchers can explore how different assumptions about confounders, mediators, and the structure of causal relationships might influence their findings. Simulation studies allow for the testing of analytical strategies under controlled conditions, providing insights into the robustness of methods against violations of assumptions or the presence of unobserved confounders. Appendices C and D provides example code.

7. **Conduct sensitivity analyses to assess robustness**: sensitivity analyses are essential for evaluating the extent to which conclusions are dependent on specific assumptions or parameters within the causal model.  Data simulation can be a powerful tool for evaluating the sensitivity of results to assumptions.  In the next section, we consider a simple sensitivity analysis for assessing robustness to unmeasured confounding. 


### Longitudinal Designs

Causation occurs in time. Longitudinal designs offer a substantial advantage over cross-sectional designs for causal inference because sequential measurements allow us to capture causation, and quantify its magnitude. We typically do not need to assert timing as we do in cross-sectional data settings.  Despite this advantage, longitudinal studies neverhteless rely on assumptions. These must be stated, and carefully managed. 

1. **Temporal sequencing**: the core strength of longitudinal designs is their ability to capture temporal sequencing, reducing ambiguity about the directionality of causal relationships. For instance, tracking changes in "happiness" following changes in access to green spaces over time can more definitively suggest causation than cross-sectional snapshots.

2. **Managing assumptions**: although longitudinal data reduces the need to assert timing , researchers still face assumptions regarding the absence of unmeasured confounders and the stability of over time. These assumptions must be explicitly stated.  As with cross sectional designs, wherever assumptions differ, researchers should draw different causal diagrams that reflect these assumptions, and subsequently conduct and report separate analyses. 

3. **Conditioning on baselines**: In longitudinal designs, conditioning on baseline measures of both the treatment and outcome can significantly address problems of unmeasured confounding. As mentioned before, by conditioning on baseline levels of access to green spaces and baseline mental health, researchers can more accurately estimate the *incident effect* of changes in green space access on changes in mental health.




::: {#tbl-lg}

```{=latex}
\examplelongitudinal
```
This table is adapted from [@bulbulia2023]
:::


### Recommendations for conducting and reporting causal analyses with cross-sectional data.

We offer the following recommendations for analysing and reporting analyses with cross-sectional data


1. **Draw two causal diagrams: one that describes the identification problem and the other for its solutions**: not only is it beneficial to draw separate causal diagrams to represent distinct structural assumptions, it is often helpful to draw a distinct causal diagram -- or several -- for describing the  identification problem and a distinct causal diagram for describing its proposed solutions. As illustrated in @tbl-lg, we recommend constructing at least two causal diagrams: first, draft an initial causal diagram that depicts assumed relationships among variables, including potential confounders and mediators. Second, draft a subsequent causal diagram that depicts a strategy for addressing these relationships, for example by highlighting variables on which to condition to reveal the causal effect of interest.

2. *Use at least three-wave of data*: a three-wave longitudinal design offers a refined approach by incorporating data from three time points. This design enables the examination of temporal precedence and lagged effects, providing stronger evidence of causality. For instance, if increased access to green spaces (time 0) is followed by increased exercise (time 1), which then precedes improvements in happiness (time 2), the temporal pattern can clarify a causal pathway from green space access through exercise to mental health. @tbl-lg describe the core of a three-wave longitudinal problem together with a strategy for addressing it.

3. *Simulate data*

4. *Conduct sensitivity analyses such as the E-value* 

5. *Calculate average treatment effects for the entire population* 

6. *Effect-modification* 






## Summary 

This chapter has offered an introduction to the potential outcomes framework for causal inference and directed acylic graphs to environmental psychology. Part 1 introduced three crucial assumptions for estimating magnitudes average treatment effects from data:


1. **Conditional Exchangeability**: the allocation of treatment is randomised and independent of the potential outcomes, conditional on measured covariates.

2. **Causal Consistency**: the outcome observed under the treatment condition matches the outcome that would have been observed if the unit had received the treatment, and vice versa for the control condition.

3. **Positivity**: every unit has a non-zero probability of receiving the treatments under comparison.

Although (perfectly) randomised controlled experiments naturally satisfy these assumptions by design-- randomisation ensures exchangeability, control ensures consistency and positivity -- observational studies generally do not. To obtain consistent causal estimates from observational data we must evaluate whether and how these assumptions may be satisfied.

Part 2 discussed causal diagrams and their role in addressing the conditional exchangeability assumption, or the assumption of "no unmeasured confounders." We discovered four elementary structures from which all causal relationships are build. And within these structures we discovered elementary rules for evaluating the implications that conditioning on elements within these structures entail about statistical associations that are observable in data. As such, causal diagrams offer a simplified visual vocabulary for mapping complex causal relationships onto observations in data. However, the relationships encoded in graphs are assertions that are generally not themselves verifiable from data. The only causal relationships that are not asserted are those between treatments and outcomes. We use causal diagrams to evaluate structural sources of bias in the statistical associations of treatments and outcomes that causal diagrams that may arise from causal relationships in the world that we assume might statistically associate treatments with outcomes irrespective of their causal associations.

In Part 3, we applied causal diagrams to various confounding scenarios. We observed that a causal diagram need highlight only those aspects of a causal setting relevant to assessing structural sources of bias that link the treatment and outcome in the absence of causality. Throughout we omitted nodes and paths that were not strictly necessary for evaluating our stated identification problem. Again we demonstrated that causal diagrams are not merely related to context-dependent questions but also to the assumptions we make about the causal structure of the world -- structural assumptions. 

Although here we have only discussed seven specific applications of causal diagrams, their utility spans far beyond. The simple rules by which variables become associated and disassociated by conditioning on nodes within four elementary structures allow researchers to apply causal diagrams to quantitatively investigate causality across innumerably many questions. We discovered that such applications are not limited to the production of effective modelling strategies but may also inform strategies for data collection, such as repeated measures data collection.

We hope the material we present here will encourage environmental psychologists to learn more about causal inference and to integrate causal diagrams into their workflow. The tools for assessing causation from correlation have been developed. There is no longer any excuse to ignore them. 


{{< pagebreak >}}

## Funding

This work is supported by a grant from the Templeton Religion Trust (TRT0418).
JB received support from the Max Planck Institute for the Science of Human History.
The funders had no role in preparing the manuscript or the decision to publish it.

## Contributions

DH proposed the chapter. JB developed the approach and wrote the first draft. Both authors contributed substantially to the final work.@appendix_a


## References

::: {#refs}
:::



## Appendix A: Glossary {.appendix}


**Acyclic**: a causal diagram cannot contain feedback loops. More precisely, no variable can be an ancestor or descendant of itself. If variables are repeatedly measured here, it is especially important to index nodes by the relative timing of the nodes.

**Adjustment set**: a collection of variables we must either condition upon or deliberately avoid conditioning upon to obtain a consistent
causal estimate for the effect of interest [@pearl2009].

**Ancestor (parent)**: a node with a direct or indirect influence on others, positioned upstream in the causal chain.

**Arrow**: denotes a causal relationship linking nodes.

**Backdoor path**: a "backdoor path" between a treatment variable, $A$, and an outcome variable, $Y$, is a sequence of links in a causal diagram that starts with an arrow into $A$ and reaches $Y$ through common causes, introducing potential confounding bias such that stastical association does not reflect causality. To estimate the causal effect of $A$ on $Y$ without bias, these paths must be blocked by adjusting for confounders. The backdoor criterion guides the selection of variables for adjustment to ensure unbiased causal inference.

**Conditioning**: the process of explicitly accounting for a variable in our statistical analysis to address the identification problem. In
causal diagrams, we usually represent conditioning by drawing a box around a node of the conditioned variable, for example,
$\boxed{L_{0}}\to A_{1} \to L_{2}$. We do not box exposures and outcomes, because we assume they are included in a model by default.
Depending on the setting, we may condition by regression stratification, inverse-probability of treatment weighting, g-methods, doubly robust
machine learning algorithms, or other methods. We do not cover such methods in this tutorial, however see @hernan2023.

**Counterfactual**: a hypothetical outcome that would have occurred for the same individuals under a different treatment condition than the one they actually experienced.

**Direct effect**: the portion of the total effect of a treatment on an outcome that is not mediated by other variables within the causal pathway.

**Collider**: a variable in a causal diagram at which two incoming
paths meet head-to-head. For example if
$A \rightarrowred \boxed{L} \leftarrowred Y$, then $L$ is a collider. If
we do not condition on a collider (or its descendants), the path between
$A$ and $Y$ remains closed. Conditioning on a collider (or its
descendants) will induce an association between $A$ and $Y$.

**Confounder**: a member of an adjustment set. Notice a variable is a
'confounder' in relation to a specific adjustment set. 'Confounder' is a
relative concept [@lash2020].

**D-separation**: in a causal diagram, a path is 'blocked' or 'd-separated' if a node along it interrupts causation. Two variables are
d-separated if all paths connecting them are blocked, making them conditionally independent. Conversely, unblocked paths result in
'd-connected' variables, implying potential dependence [@pearl1995].

**Descendant (child)**: a node influenced, directly or indirectly, by upstream nodes (parents).


**Effect-modifier**: a variable is an effect-modifier, or 'effect-measure modifier' if its presence changes the magnitude or direction of the effect of an exposure or treatment on an outcome across the levels or values of this variable. In other words, the effect of the exposure is different at different levels of the effect-modifier. 

**External validity**: the extent to which causal inferences can be generalizsd to other populations, settings, or times, also called "Target Validity."

**Identification problem**: the challenge of estimating the causal effect of a variable using by adjusting for measured variables on units
in a study. Causal diagrams were developed to address the identification problem by application of the rules of d-separation to a causal diagram.

**Indirect effect (Mediated effect)**: The portion of the total effect that is transmitted through a mediator variable.

**Internal validity**: the degree to which the design and conduct of a study are likely to have prevented bias, ensuring that the causal relationship observed can be confidently attributed to the treatment and not to other factors.

**Instrumental variable**: an ancestor of the exposure but not of the outcome. An instrumental variable affects the outcome only through its effect on the exposure and not otherwise. Whereas conditioning on a variable causally associated with the outcome but not with the exposure will generally increase modelling precision, we should avoid conditioning on instrumental variables [@cinelli2022].  Second, when an instrumental variable is the descendant of an unmeasured confounder, we should generally condition the instrumental variable to provide a partial adjustment for a confounder.

**Mediator**: a variable that transmits the effect of the treatment variable on the outcome variable, part of the causal pathway between treatment and outcome.

**Modified Disjunctive Cause Criterion**: @vanderweele2019 recommends obtaining a maximally efficient adjustment which he calls a 'confounder set' A member of this set is any set of variables that can reduce or remove a structural sources of bias. The strategy is as follows:

a.  Control for any variable that causes the exposure, the outcome, or
    both.
b.  Control for any proxy for an unmeasured variable that is a shared
    cause of both the exposure and outcome.
c.  Define an instrumental variable as a variable associated with the
    exposure but does not influence the outcome independently, except
    through the exposure. Exclude any instrumental variable that is not
    a proxy for an unmeasured confounder from the confounder set
    [@vanderweele2019].

Note that the concept of a 'confounder set' is broader than that of an
'adjustment set.' Every adjustment set is a member of a confounder set.
Hence, the Modified Disjunctive Cause Criterion will eliminate bias when
the data permit. However, a confounder set includes variables that will
reduce bias in cases where confounding cannot be eliminated.

**Node**: characteristic or features of units in a population ('variable') represented on a causal diagram. In a causal diagram, nodes are drawn with reference to variables defomed for the target population.

**Randomisation**: The process of randomly assigning subjects to different treatments or control groups, aiming to eliminate selection bias in experimental studies.

**Reverse Causation**: $\atoyassert$, but in reality $\ytoa$

**Statistical model:** a mathematical representation of the
relationships between variables in which we quantify covariances and
their corresponding uncertainties in the data. Statistical models
typically correspond to multiple causal structures [@pearl2018;
@vanderweele2022b; @hernan2023]. That is, the causes of such covariances
cannot be identified without assumptions.

**Structural model:** defines assumptions about causal relationships.
Causal diagrams graphically encode these assumptions [@hernan2023],
leaving out the assumption about whether the exposure and outcome are
causally associated. Outside of randomised experiments, we cannot
compute causal effects in the absence of structural models. A structural
model is needed to interpret the statistical findings in causal terms.
Structural assumptions should be developed in consultation with experts.
The role of structural assumptions when interpreting statistical results
remains poorly understood across many human sciences and forms the
motivation for my work here.

**Time-varying confounding:** occurs when a confounder that changes over
time also acts as a mediator or collider in the causal pathway between
exposure and outcome. Controlling for such a confounder can introduce
bias. Not controlling for it can retain bias.



## Appendix B: Causal Consistency in observational settings {.appendix}

In observational research, there are typically multiple versions of treatment. The theory of causal inference under multiple versions of treatment proves we can consistently estimate causal effects where the different versions of treatment are conditionally independent of the outcomes [@vanderweele2009, @vanderweele2009; @vanderweele2013; @vanderweele2018] 

Let $\coprod$ denote independence.
Where there are $K$ different versions of treatment $A$ and no confounding for $K$'s effect on $Y$ given measured confounders $L$ such that

$$
Y(k) \coprod K | L
$$

Then it can be proved that causal consistency follows. According to the theory of causal inference under multiple versions of treatment, the measured variable $A$ functions as a "coarsened indicator" for estimating the causal effect of the multiple versions of treatment $K$ on $Y(k)$ [@vanderweele2009; @vanderweele2013; @vanderweele2018].  

In the context of green spaces, let $A$ represent the general action of moving closer to any green space and $K$ represent the different versions of this treatment. For instance, $K$ could denote moving closer to different types of green spaces such as parks, forests, community gardens, or green spaces with varying amenities and features.

Here, the conditional independence implies that, given measured confounders $L$ (e.g. socioeconomic status, age, personal values), the type of green space one moves closer to ($K$) is independent of the outcomes $Y(k)$ (e.g. mental well-being under the $K$ conditions). In other words, the version of green space one chooses to live near does not affect the $K$ potential outcomes, provided the confounders $L$ are properly controlled for in our statistical models.

Put simply, strategies for confounding control and for consistently estimating causal effects when there are multiple versions of treatment converge. However, the quantities we estimate under multiple versions of treatment might lack any clear interpretations.  For example, we cannot readily determine which of the many versions of treatment is most causal efficacious and which lack any causal effect, or are harmful.  


## Appendix C: Computing the Average Treatment Effect {.appendix}


```{r}
#| label: simulation_cross_sectionaltwo
#| tbl-cap: ""
#| out-width: 100%
#| echo: true
#| eval: false

# load libraries
library(gtsummary) # gtsummary: nice tables
library(kableExtra) #  tables in latex/markdown
library(clarify) # simulate ATE

# simulation seed
set.seed(123) #  reproducibility

# define the parameters 
n = 1000 # Number of observations
p = 0.5  # Probability of A = 1 (access to greenspace)
alpha = 0 # Intercept for L (excercise)
beta = 2  # Effect of A on L 
gamma = 1 # Intercept for Y 
delta = 1.5 # Effect of L on Y
sigma_L = 1 # Standard deviation of L
sigma_Y = 1.5 # Standard deviation of Y

# simulate the data: fully mediated effect 
A = rbinom(n, 1, p) # binary exposure variable
L = alpha + beta*A + rnorm(n, 0, sigma_L) # continuous mediator
Y = gamma + delta*L + rnorm(n, 0, sigma_Y) # continuous outcome

# make the data frame
data = data.frame(A = A, L = L, Y = Y)

# fit regression in which L is assume to be a mediator
fit_1 <- lm( Y ~ A + L, data = data)

# fit regression in which L is assume to be a mediator
fit_2 <- lm( Y ~ A, data = data)

# create gtsummary tables for each regression model
table1 <- tbl_regression(fit_1)
table2 <- tbl_regression(fit_2)

# merge the tables for comparison
table_comparison <- tbl_merge(
  list(table1, table2),
  tab_spanner = c("Model: Exercise assumed confounder", 
                  "Model: Exercise assumed to be a mediator")
)
# make latex table
markdown_table_0 <- as_kable_extra(table_comparison, 
                                   format = "latex", 
                                   booktabs = TRUE)
# print table
markdown_table_0
```




Next, we present the code for calculating an average treatment effect.  


```{r}
#| label: ate-sim-crosstwo
#| tbl-cap: "Code for calculating the average treatment effect as contrasts between simulated outcomes for the entire population."
#| echo: true
#| eval: false

# use `clarify` package to obtain ATE
library(clarify)
# simulate fit 1 ATE
set.seed(123)
sim_coefs_fit_1 <- sim(fit_1)
sim_coefs_fit_2 <- sim(fit_2)

# marginal risk difference ATE, simulation-based: model 1 (L is a confounder)
sim_est_fit_1 <-
  sim_ame(
    sim_coefs_fit_1,
    var = "A",
    subset = A == 1,
    contrast = "RD",
    verbose = FALSE
  )

# marginal risk difference ATE, simulation-based: model 2 (L is a mediator)
sim_est_fit_2 <-
  sim_ame(
    sim_coefs_fit_2,
    var = "A",
    subset = A == 1,
    contrast = "RD",
    verbose = FALSE
  )

# obtain summaries
summary_sim_est_fit_1 <- summary(sim_est_fit_1, null = c(`RD` = 0))
summary_sim_est_fit_2 <- summary(sim_est_fit_2, null = c(`RD` = 0))

# get coefficients for reporting
# ate for fit 1, with 95% CI
ATE_fit_1 <- glue::glue(
  "ATE =
                        {round(summary_sim_est_fit_1[3, 1], 2)},
                        CI = [{round(summary_sim_est_fit_1[3, 2], 2)},
                        {round(summary_sim_est_fit_1[3, 3], 2)}]"
)
# ate for fit 2, with 95% CI
ATE_fit_2 <-
  glue::glue(
    "ATE = {round(summary_sim_est_fit_2[3, 1], 2)},
                        CI = [{round(summary_sim_est_fit_2[3, 2], 2)},
                        {round(summary_sim_est_fit_2[3, 3], 2)}]"
  )
```



## Appendix D: Simulation of Different Confounding Control Strategies {.appendix}


Here are data illustrating how different confounding control strategies perform against the "ground truth" of simulated data. This simulation reveals that (1) standard control for confounding may not perform optimally when there is unmeasured confounding and (2) including the baseline treatment and outcome, if these variables are strongly correlated with the unmeasured confounder, may substantially reduce the effect of unmeasured confounding


In this simulation:

- The interaction between $A_1$ (treatment) and $L_0$ (baseline confounders) is specified by the coefficient $\theta_{A1L0} = 0.5$, indicating that the effect of $A_1$ on $Y_2$ varies depending on the value of $L_0$.
- The inclusion of this interaction allows us to examine how the baseline level of a confounder ($L_0$) modifies the effect of the treatment ($A_1$) on the outcome ($Y_2$).

```{r}
#| label: codelg
#| echo: true
#| eval: true

library(kableExtra)
library(gtsummary)
if(!require(grf)){install.packages("grf")} # causal forest
set.seed(123) # Ensure reproducibility

n <- 10000 # Number of observations

# Baseline covariates
U <- rnorm(n) # Unmeasured confounder
A_0 <- rbinom(n, 1, prob = plogis(U)) # Baseline exposure
Y_0 <- rnorm(n, mean = U, sd = 1) # Baseline outcome
L_0 <- rnorm(n, mean = U, sd = 1) # Baseline confounders

# Coefficients for treatment assignment
beta_A0 = 0.25
beta_Y0 = 0.3
beta_L0 = 0.2
beta_U = 0.1

# Simulate treatment assignment
A_1 <- rbinom(n, 1, prob = plogis(-0.5 + 
                                    beta_A0 * A_0 +
                                    beta_Y0 * Y_0 + 
                                    beta_L0 * L_0 + 
                                    beta_U * U))

# Coefficients for continuous outcome
delta_A1 = 0.3
delta_Y0 = 0.9
delta_A0 = 0.1
delta_L0 = 0.3
theta_A0Y0L0 = 0.5 # Interaction effect between A_1 and L_0
delta_U = 0.05

# Simulate continuous outcome including interaction
Y_2 <- rnorm(n,
             mean = 0 +
               delta_A1 * A_1 + 
               delta_Y0 * Y_0 + 
               delta_A0 * A_0 + 
               delta_L0 * L_0 + 
               theta_A0Y0L0 * Y_0 * 
               A_0 * L_0 + 
               delta_U * U,
             sd = .5)

# Data frame
data <- data.frame(Y_2, A_0, A_1, L_0, Y_0, U)

# models
# no control
fit_no_control <- lm(Y_2 ~ A_1, data = data)
#summary(fit_no_control)

# standard covariate control
fit_standard <- lm(Y_2 ~ A_1 + L_0, data = data)
#summary(fit_standard)

# interaction
fit_interaction  <- lm(Y_2 ~ A_1 + L_0 + A_0 + Y_0 + A_0:L_0:Y_0, data = data)
#summary(fit_interaction)


# create gtsummary tables for each regression model
tbl_fit_no_control<- tbl_regression(fit_no_control)  
tbl_fit_standard <- tbl_regression(fit_standard)
tbl_fit_interaction <-
  tbl_regression(fit_interaction)


# get only the treatment variable
tbl_list_modified <- lapply(list(
  tbl_fit_no_control,
  tbl_fit_standard,
  tbl_fit_interaction),
function(tbl) {
  tbl %>%
    modify_table_body(~ .x %>% dplyr::filter(variable == "A_1"))
})


# merge the tables
table_comparison <- tbl_merge(
  tbls = tbl_list_modified,
  tab_spanner = c(
    "No Control",
    "Standard",
    "Interaction")
) |>
  modify_table_styling(
    column = c(p.value_1, p.value_2, p.value_3),
    hide = TRUE
  )

#create latex table for publication
markdown_table <-
  as_kable_extra(table_comparison, format = "latex", booktabs = TRUE) |>
  kable_styling(latex_options = "scale_down")
# print it
markdown_table
```


Next we simulate average treatment effect

```{r}
#| label: ate-sim-long
#| tbl-cap: "Code for calculating the average treatment effect."
#| echo: true
#| eval: true

# use `clarify` package to obtain ATE
library(clarify)
# simulate fit 1 ATE
set.seed(123)
sim_coefs_fit_std <- sim(fit_standard)
sim_coefs_fit_int <- sim(fit_interaction)

# marginal risk difference ATE, simulation-based: model 1 (L is a confounder)
sim_est_fit_std <-
  sim_ame(
    sim_coefs_fit_std,
    var = "A_1",
    subset = A_1 == 1,
    contrast = "RD",
    verbose = FALSE
  )

# marginal risk difference ATE, simulation-based: model 2 (L is a mediator)
sim_est_fit_int <-
  sim_ame(
    sim_coefs_fit_int,
    var = "A_1",
    subset = A_1 == 1,
    contrast = "RD",
    verbose = FALSE
  )

# obtain summaries
summary_sim_est_fit_std <-
  summary(sim_est_fit_std, null = c(`RD` = 0))
summary_sim_est_fit_int <-
  summary(sim_est_fit_int, null = c(`RD` = 0))

# get coefficients for reporting
# ate for fit 1, with 95% CI
ATE_fit_std <- glue::glue(
  "ATE = {round(summary_sim_est_fit_std[3, 1], 2)}, 
  CI = [{round(summary_sim_est_fit_std[3, 2], 2)},
  {round(summary_sim_est_fit_std[3, 3], 2)}]"
)
# ate for fit 2, with 95% CI
ATE_fit_int <-
  glue::glue(
    "ATE = {round(summary_sim_est_fit_int[3, 1], 2)},
    CI = [{round(summary_sim_est_fit_int[3, 2], 2)},
    {round(summary_sim_est_fit_int[3, 3], 2)}]"
  )
# ATE_fit_std
# ATE_fit_int
```

Using the `clarify` package, we infer the ATE for the standard model is `r ATE_fit_std`.

Using the `clarify` package, we infer the ATE for the model that conditions on the baseline exposure and baseline outcome to be:  `r ATE_fit_int`, which is close to the values supplied to the data-generating mechanism. 

Because the baseline exposure and baseline outcome are often the most important variables to include when estimating an incident effect, we should endevour to obtain at least three waves of data such that these variables along with other baseline confounders are included in at time 0, the exposure is included at time 1, and the outcome is included at time 2. 

## Appendix E: Non-parametric Estimation of Average Treatment Effects Using Causal Forests

Semi-parametric and non-parametric estimators have many advantages, most especially the ability to fit non-restrictive statistical models. Here we include an example of such a model using the causal forest package. 

```{r}
#| label: causal_forest
#| echo: true

# load causal forest library 
library(grf) # estimate conditional and average treatment effects
library(glue) # reporting 

#  'data' is our data frame with columns 'A_1' for treatment, 'L_0' for a covariate, and 'Y_2' for the outcome
#  we also have the baseline exposure 'A_0' and 'Y_0'
#  ensure W (treatment) and Y (outcome) are vectors
W <- as.matrix(data$A_1)  # Treatment
Y <- as.matrix(data$Y_2)  # Outcome
X <- as.matrix(data[, c("L_0", "A_0", "Y_0")])

# fit causal forest model 
fit_causal_forest <- causal_forest(X, Y, W)

# estimate the average treatment effect (ATE)
ate <- average_treatment_effect(fit_causal_forest)

# make data frame for reporting using "glue' 
ate<- data.frame(ate)

# obtain ate for report
ATE_fit_causal_forest <-
  glue::glue(
    "ATE = {round(ate[1, 1], 2)}, se = {round(ate[2, 1], 2)}"
  )
```

Causal forest estimates the average treatment effect as `r ATE_fit_causal_forest`. This converges to to the true value supplied to the generating mechanism of 0.3

