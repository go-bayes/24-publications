---
title: "Causal Inference in Environmental Psychology"
abstract: |
  This chapter introduces causal inference within environmental psychology, emphasising the use of causal Directed Acyclic Graphs (causal DAGs) for identifying causation in observational data. It is structured into four sections: (1) a primer on principles of causal inference, (2) a tutorial on constructing causal diagrams, (3) practical examples of their application, and (4) guidelines for effective reporting. In settings where the confounding structure is unclear or contested, we recommend creating multiple causal diagrams, performing analyses corresponding to each, and reporting sensitivity analyses.
authors: 
  - name: Joseph A Bulbulia
    orcid: 0000-0002-5861-2056
    email: joseph.bulbulia@vuw.ac.nz
    affiliation: 
      name: Victoria University of Wellington, New Zealand, School of Psychology, Centre for Applied Cross-Cultural Research
      department: Psychology/Centre for Applied Cross-Cultural Research
      city: Wellington
      country: New Zealand
      url: www.wgtn.ac.nz/cacr
  - name: Donald W Hine
    orcid: 0000-0002-3905-7026
    email: donald.hine@canterbury.ac.nz
    affiliation: 
      name: University of Canterbury, School of Psychology, Speech and Hearing
      city: Canterbury
      country: New Zealand
      url: https://profiles.canterbury.ac.nz/Don-Hine
keywords:
  - DAGS
  - Causal Inference
  - Confounding
  - Environmental
  - Psychology
  - Panel
format:
  pdf:
    sanitize: true
    keep-tex: true
    link-citations: true
    colorlinks: true
    documentclass: article
    classoption: [singlecolumn]
    lof: false
    lot: false
    number-sections: false
    number-depth: 4
    highlight-style: github
    geometry:
      - top=30mm
      - left=20mm
      - heightrounded
    template-partials: 
      - /Users/joseph/GIT/templates/quarto/title.tex
    header-includes:
      - \input{/Users/joseph/GIT/latex/latex-for-quarto.tex}
date: last-modified
execute:
  echo: false
  warning: false
  include: true
  eval: true
fontfamily: libertinus
bibliography: /Users/joseph/GIT/templates/bib/references.bib
csl: ./camb-a.csl
---

```{r}
#| label: load-libraries
#| echo: false
#| include: false
#| eval: false
#  fig-pos: 'htb'
#   html:
#    html-math-method: katex

# Include in YAML for Latex
# sanitize: true
# keep-tex: true
# include-in-header:
#       - text: |
#           \usepackage{cancel}


# for making graphs
library("tinytex")
library(extrafont)
loadfonts(device = "all")


# libraries for jb (when internet is not accessible)
# read libraries
source("/Users/joseph/GIT/templates/functions/libs2.R")

# read functions
source("/Users/joseph/GIT/templates/functions/funs.R")

# read data/ set to path in your computer
pull_path <-
  fs::path_expand(
    "/Users/joseph/v-project\ Dropbox/data/current/nzavs_13_arrow"
  )

# for saving models. # set path fo your computer
push_mods <-
  fs::path_expand(
    "/Users/joseph/v-project\ Dropbox/data/nzvs_mods/00drafts/23-causal-dags"
  )


# keywords: potential outcomes, DAGs, causal inference, evolution, religion, measurement, tutorial
# 10.31234/osf.io/b23k7

```

## Introduction

Causal inference seeks to answer the question: How does intervening on a "treatment" variable affect an "outcome" variable? Although causal understanding is widespread across species [@mancuso2018revolutionary], methods for systematically quantitatively magnitudes of causal effect are relatively recent developments.

Although randomised controlled trials (experiments), when performed correctly, ensure no unmeasured confounding, experiments face limitations of cost, practicality, ethics, and generalisability. Experiments are also subject to confounding biases [@hernan2017per; @montgomery2018; @bulbulia_2024_experiments]. Observational data, which are more readily available, offer alternatives -- provided that the causal inferences we draw are valid. Regrettably, common practices of analysing observational data invalidate causal inferences [@westreich2013; @robins1986; @hernan2023; @bulbulia2023a]. Converging advances in causal inference from biostatistics, economics, and computer science allow us to leverage observational data to better ask, and answer, causal questions [@hernan2023]. This chapter provides and overview of these methods and their applications for environmental psychology.

[**Part 1**](#section-part1) introduces the potential outcomes framework and the [**three fundamental assumptions**](#sec-three-fundamental-assumptions) necessary for causal inferences [@hernan2023]. Here, we discover that causal inference hinges on our ability to use data to compute contrasts between *counterfactual* outcomes [@westreich2012berkson; @hernan2017per; @westreich2015; @robins2008estimation].

[**Part 2**](#section-part2) offers an introduction to causal diagrams or "Directed Acyclic Graphs (DAGs)" as tools for developing strategies for confounding control.

[**Part 3**](#section-part3) presents seven worked examples that put causal diagrams to practice.

[**Part 4**](#section-part4) offers reporting guidelines. Here we advocate for reporting multiple causal diagrams and analysis wherever causal assumptions are uncertain or the timing of variables in one's data is unclear.


## Part 1: An Overview of the Potential Outcomes Framework for Causal Inference {#section-part1}

The potential outcomes framework for causal inference originated in the work of Jerzy Neyman to evaluate the effectiveness of agricultural experiments [@neyman1923]. It was later extended by Harvard statistician Donald Rubin, who demonstrated the framework may also facilitate causal inferences in non-experimental settings [@rubin1976]. Jamie Robins further generalised this framework to assess confounding in complex scenarios involving multiple and time-varying treatments [@robins1986]. 

A fundamental principle in the potential outcomes framework is the concept of "counterfactual contrast" or "estimand." To quantify causal effects, one must compare the outcomes under different intervention or treatment scenarios. Notably, prior to any intervention, these scenarios are purely hypothetical. Post-intervention, only one scenario is actualised for each realised treatment, leaving the alternative as a non-observed counterfactual. For any individual unit to be treated, that only one of the two possible outcomes is realised underscores a critical property of causality: causality is **not directly observable** [@hume1902]. Causal inference, therefore, can only quantify causal effects by combining data with counterfactual simulation [@edwards2015; @bulbulia2023a]. The concept of a counterfactual data science --  may sound strange. However, anyone who has encountered a randomised experiment has encountered counterfactual data science.  Before building intuitions for causal inference from the familiar example of experiments, let's first build intuitions for the idea that causal quantities are never directly observed. 


#### The Fundamental Problem of Causal Inference: Causal Contrasts are Not Directly Observed

Imagine you are at a pivotal juncture in your life. You have just completed your undergraduate studies and have been accepted into your dream Environmental Psychology program at the University of Canterbury. You are set to relocate to Christchurch, New Zealand. However, while making preparations, you receive a job offer from Acme Nuclear Fuels, a leader in renewable energy. Should you embark on graduate study or take the job?  The course your life will take under each decision would appear to differ — your lifestyle, income, social networks, relationships, and perhaps even sense of life purpose hang in the balance. Which choice aligns with your ideal future?

Formally, let $D$ denote your decision, where $D = 1$ means attending graduate school and $D = 0$ means joining the workforce.  Let $Y$ denote your life outcome.  We use the symbol $|$ to denote conditionality such that the outcome. $Y(1)$ denotes your life outcome when $Y|D=1$; $Y(0)$ denotes your life outcome when $Y|D=0$. Your two potential outcomes under each path are described are given as $Y_{\text{you}}(1)$ and $Y_{\text{you}}(0)$. Conceptually, to quantify the *magnitude* of the effect of your decision on your life outcome, we must calculate the difference: 

$$Y_{\text{you}}(1) - Y_{\text{you}}(0)$$  

Yet, this difference cannot be calculated from data because when you choose one path, you obscure the other:

$$
(Y_{\text{you}}|D_{\text{you}} = 1) = Y_{\text{you}}(1) \quad \text{implies} \quad Y_{\text{you}}(0)|D_{\text{you}} = 1~ \text{is counterfactual}.
$$


This expression means: "The outcome we observe under option $D = 1$ can be measured. However, because option $D = 0$ is not realised, the outcome under option $D=0$ cannot be measured. Thus, the contrast between these two outcomes cannot be computed. At least one outcome remains purely counterfactual.

The same problem arises if you select $D = 0$. Then, the outcome under $D=1$ remains counterfactual. And so we cannot compute the contrast:

$$
(Y_{\text{you}}|D_{\text{you}} = 0) = Y_{\text{you}}(0) \quad \text{implies} \quad Y_{\text{you}}(1)|D_{\text{you}} = 0~ \text{is counterfactual}.
$$

Of course, you regularly make principled decisions about your life based on past experiences, instincts, and knowledge. Nevertheless, the *data* that you require to quantitatively compare life outcomes under one decision as opposed to the other is not available. Life, as it would have unfolded under the option you do not select, remains counterfactual -- it cannot be directly measured. A quantitative causal contrast here is not a matter of factual data science. This example, although contrived, perhaps resonates with similar crossroads you have encountered in your life. The dilemmas that you faced at these crossroads underscore what is known as "The fundamental problem of causal inference" [@rubin2005; @holland1986]: for any individual case, we cannot observe the potential outcomes that we require to quantify the magnitude of an individual causal contrast.

The fundamental problem of causal inference never goes away. However, by collecting, organising, and aggregating data under certain assumptions, we can obtain valid quantitative causal contrasts from data for *average treatment effects*. To clarify these assumptions, we next consider how experiments attach magnitudes to missing counterfactual outcomes to obtain average treatment effects.

### Causal inference in Experiments is a Missing Data Problem

Let us transition from the topic of life decisions to an example of relevance to environmental psychology, namely, estimating the average causal effect of easy access to urban green spaces on subjective happiness, hereafter referred to as “happiness.” We assume this outcome is measurable and represent it with $Y$. 

For simplicity, we classify the intervention "ample access to green space" as a binary variable. Define $A = 1$ as "having ample access to green space" and $A = 0$ as "lacking ample access to green space." We assume these conditions are mutually exclusive. This simplification does not limit the generality of our conclusions; the points we make about experiments also apply to continuous treatments. It is crucial in causal inference to specify the population for whom we seek to evaluate causal effects, or the "target population." In this case, our target population is residents of New Zealand in the 2020s.

A preliminary causal question -- defined as a causal contrast or “estimand” might, therefore be:

"In New Zealand, does proximity to abundant green spaces increase self-perceived happiness compared to environments lacking such spaces?"

Of course it would be unethical to experimentally randomise individuals into different green-space access conditions. However, for the purposes of illustration, assume experimentalists could assign people randomly to high and low green space access without objection or harm.

As alluded to earlier, the first point to note in the context of causal inference is that even well-designed experiments confront the challenge of missing values in the potential outcomes. Once an individual is assigned to one treatment condition, we cannot observe that individual's outcome for the condition not assigned. The fundamental problem of causal inference remains constant: for each individual, we can only observe one of the potential outcomes at any given time. Breaking down the Average Treatment Effect (ATE) into observed and unobserved outcomes yields the following equation:


$$
\text{Average Treatment Effect} = \left(\underbrace{\underbrace{\mathbb{E}[Y(1)|A = 1]}_{\text{observed}} + \underbrace{\mathbb{E}[Y(1)|A = 0]}_{\text{unobserved}}}_{\text{treated}}\right) - \left(\underbrace{\underbrace{\mathbb{E}[Y(0)|A = 0]}_{\text{observed}} + \underbrace{\mathbb{E}[Y(0)|A = 1]}_{\text{unobserved}}}_{\text{untreated}}\right).
$$

In this expression, $\mathbb{E}[Y(1)|A = 1]$ represents the average outcome when the treatment is given, which is observable. However, $\mathbb{E}[Y(1)|A = 0]$ represents the average outcome if the treatment had been given to those who were untreated, which remains unobservable. Similarly, the quantity $\mathbb{E}[Y(0)|A = 1]$ also remains unobservable.

It is hopefully evident from this brief application of the potential outcomes framework to experiments that the fundamental problem of causal inference is an ever-present concern even in experiments. For each participant, it is impossible to determine the outcome they would have experienced under an alternative treatment condition. You cannot quantitatively describe the life you would have led had you chosen the job at Acme Nuclear Fuels instead of attending the University of Canterbury. Nor, if you lived your life in a leafy suburb, could you determine how happy you would have been if your life had been devoid of green space? 


### In Experiments, Random Treatment Assignment Balances Confounders Across Treatments

How do experiments manage to estimate average treatment effects despite the inherent challenges? The solution involves addressing the concept of "confounding." Consider the concept of "confounding by common cause." This occurs when one or more variables causally affect both the intervention under study (the "treatment" or "exposure") and the outcome of interest, leading to a non-causal association between the treatment and outcome. By "non-causal," we mean that if we intervened in the treatment but not the confounder, the outcome would not change. The common cause creates a misleading or exaggerated relationship that may be mistakenly interpreted as causal. For instance, when assessing the effect of access to green space on happiness, it is possible that the association could be entirely explained by income. If so, then an observed association between access to green space and happiness would be entirely misleading. Were we to relocate low-income individuals to high-access green areas, we might not affect subjective happiness at all. Thus, accurately identifying and adjusting for confounding by common cause is crucial for determining the true causal relationship between two variables, ensuring that the observed association is not merely a result of extraneous influences.

We can express this principle of "no unmeasured confounding" mathematically in two complementary ways (where $A \coprod B$ signifies that $A$ is independent of $B$, and vice versa):

1. **Potential Outcomes Independent of Treatment (given L):** $Y(a) \coprod A \mid L$
2. **Treatment Assignment Independent of Potential Outcomes (given L):** $A \coprod Y(a) \mid L$

These formulations are crucial when working with causal diagrams, which visually encode the demands for identifying causal from statistical associations. The key idea is straightforward: ensuring a balance of confounders across treatment groups is fundamental to experimental and observational causal inference strategies. Randomisation facilitates this balance, achieving $A \coprod Y(a)$.

### The Three Fundamental Assumptions of Causal Inference {#sec-three-fundamental-assumptions}

Reviewing causal inference in experiments highlights three core assumptions essential for causal inference.

#### Fundamental Assumption 1: Conditional Exchangeability

We say that conditional exchangeability holds if the potential outcomes and treatment assignments are statistically independent, considering all measured confounders. It enables us to attribute observed group differences directly to the treatment. Randomisation provides *unconditional* exchangeability, simplifying the analytical process.


##### Challenge in Satisfying Conditional Exchangeability in Observational Settings

Achieving conditional exchangeability is challenging in observational studies. This condition requires the groups being compared to be similar in every aspect except for the treatment. Consider the example of the effect of living near green spaces on subjective happiness. In real-world data, individuals with access to green spaces may differ from those without access in several ways:

- **Socioeconomic status**: the economic capacity of individuals often determines their living environments, thereby affecting their access to quality green spaces.
- **Age demographics**: different age groups have unique preferences and necessities regarding green spaces, which could influence the observed outcomes.
- **Mental health**: pre-existing conditions might lead individuals to seek out or avoid green spaces, complicating the causal pathway.
- **Lifestyle choices**: the proximity to green spaces could correlate with a more active, outdoor lifestyle preference. Does the observed effect on well-being directly result from the green space, or does it indicate a healthier lifestyle?
- **Personal values and social connections**: environmental values and community ties may influence both the choice of residence and the utilisation of green spaces.

These and other unmeasured factors can lead to associations in the absence of causation, complicating the interpretation of causal relationships in observational studies.


#### Fundamental Assumption 2: Causal Consistency

We say that causal consistency holds if there is no heterogeneity in the treatments that would prevent us from assuming that the observed outcomes under treatments correspond to their potential outcomes. For an individual 'i', we must be able to assume:

$$
\begin{aligned}
Y_{i}(1) &= (Y_{i}|A_{i} = 1) \quad \text{(Potential outcome if treated)} \\
Y_{i}(0) &= (Y_{i}|A_{i} = 0) \quad \text{(Potential outcome if untreated)}
\end{aligned}
$$

If this assumption holds, as well as the assumptions of conditional exchangeability and positivity (reviewed below), we can calculate the Average Treatment Effect (ATE) from observed data as:

$$
\begin{aligned}
\text{ATE} &= \mathbb{E}[Y(1)] - \mathbb{E}[Y(0)] \\
&= \mathbb{E}(Y|A=1) - \mathbb{E}(Y|A=0)
\end{aligned}
$$

This contrast assumes that the potential outcome under treatment is observable when the treatment is administered, setting $Y_i(a)$ to $Y_i|A_i=a$. 

The standardisation of treatments in randomised controlled experiments generally ensures the validity of the causal consistency assumption, which is seldom disputed. However, in observational settings, we cannot typically control the treatments that people receive. This fact imposes considerable challenges for satisfying this assumption. (Discussed in [Appendix B](#appendix-b))

##### Challenges in Satisfying the Causal Consistency Assumption in Observational Settings

Again we consider our interest in quantifying the causal effect of living near green spaces. The definition of "proximity to green spaces" itself, varies significantly, leading to a diverse range of experiences classified under the same "treatment." Focussing on the variability of the green spaces themselves, this includes:

- **Diversity of green spaces**: Green spaces' ecological richness and visual appeal vary considerable. Equating well-maintained parks with neglected wild areas does not provide a like-for-like comparison.
- **Availability of amenities**: facilities such as walking paths and benches considerably affects the spaces' usability and enjoyment.
- **Size and type of green space**: the benefits derived from an urban garden versus a vast forest differ markedly, emphasising the need to consider the nature of the green space in the analysis.

#### Fundamental Assumption 3: Positivity

We must assume that there is a non-zero probability of receiving each treatment level within covariate-defined subgroups.

$$
P(A = a | L= l) > 0
$$

This assumption is also met by the *control* that experimentalists exert over randomised controlled experiments and is rarely stated explicitly. However, in observational settings, this condition must be verified to avoid extrapolating results beyond observed data.  

##### Challenges in Satisfying The Positivity Assumption in Observational Settings: The Relevant Treatments Do Not Exist in The Data

Positivity demands that each individual has the possibility of experiencing *every* level of the treatment to be compared. However, real-world constraints, such as housing availability in specific locales, may preclude some groups from accessing varied green spaces. Where the treatments of interest are absent or scarcely represented in our dataset, the resulting causal inferences will lack empirical support. Consequently, any coefficients derived will represent extrapolations from statistical models, challenging the validity of our causal inferences [@westreich2010; @hernan2023].



### Summary Part 1

In Part 1, we described the fundamental problem of causal inference, focusing on the critical distinction between correlation -- associations in the data, and causation -- the contrast between potential outcomes only one of which, at most, can be observed. We discussed how controlled experiments facilitate the estimation of average treatment effects (ATE) by systematically manipulating the variable of interest, allowing for the distribution of variables that might affect the outcome to be balanced across the treatment conditions. The discussion then shifted to observational data, emphasising the challenges inherent in extracting causal relationships from data without the benefit of controlled interventions. We underscored the necessity of three key assumptions —- conditional exchangeability, causal consistency, and positivity—for inferring average treatment effects from observational data. These assumptions ensure that the treatment groups are comparable, the treatment effect is consistent across the population, and every individual has a non-zero probability of receiving each treatment level, respectively. As we transition to discussing causal diagrams, it is essential to recognise that these graphical tools offer a systematic approach to identifying and controlling for confounding variables, thus aiding researchers in satisfying the first of the three fundamental assumptions required for causal inference. Causal diagrams, however, do not obviate the need for these assumptions; instead, they provide a framework for evaluating whether and how the first of the three assumptions -- conditional exchangeability -- may be satisfied in a given study.


## Part 2: Causal Diagrams - A Visual Approach to Understanding Confounding {#section-part2}

We now introduce causal directed acyclic graphs (causal DAGs), beginning with essential terminology. Grasping this vocabulary is crucial for effectively employing causal diagrams, though it may initially seem daunting. After laying out the terms, we will consider practical examples, uncovering the various forms of confounding embedded within five elementary causal structures. Identifying and understanding these structures is vital for their application in real-world analyses. Refer to [**Appendix A**](#appendix-a) for a detailed glossary.

### Elements of Causal Diagrams

Causal diagrams distil the essence of causal relationships within a system into visual representations. At their core, these diagrams consist of:

#### 1. **Nodes**

Nodes represent variables or events within a causal framework. Each node stands for a distinct element that either exerts influence or is subject to influence within the system. Nodes encapsulate the components of our causal inquiry. They denote: (1) treatment(s) (2) outcome(s) or (3) confounders. 

#### 2. **Arrows/Edges**

Arrows indicate the direction and presence of causal relationships between the variables denoted by nodes. Directed edges trace the assumed flow of causal influence. The originating variable is called a "parent," and the receiving variable is called a "child." These arrows define the causal architecture of the system, illustrating how we assume one variable causally affects another. Notably, the representation of causal relationships through arrows remains the same whether the assumed influence is linear or non-linear.

#### 3. **Conditioning**

In causal data science, deciding which variables to adjust for is crucial for estimating the true causal effect unconfounded by other factors. We denote a decision to "control for" or equivalently "condition on" or equivalently "adjust for" a variable by enclosing it in a box.

### The Rules of d-separation 

Judea Pearl demonstrated how the rules of d-separation allow us to analyse relationships within causal diagrams [@pearl1995]. These rules allow us to identify confounders and develop strategies for obtaining valid causal inferences from statistical associations in the data [@pearl1995]. The "d" in "d-separation" stands for directional. The algorithm that Pearl proved complete for identifying causal effects from data is called "the back door path criterion" [@pearl2009a].

**Basic Concepts**

#### Dependence

Denoted as $A \cancel\coprod B$, indicating that the probability distributions of $A$ and $B$ are interrelated. Knowledge about one variable provides insights into the other, suggesting a potential causal or associational link.

#### Independence

Denoted as $A \coprod B$, signifying that the probability distributions of $A$ and $B$ are independent. Information about one variable reveals nothing about the other, indicating no direct causal or associational connection.

#### Blocked Paths and d-Separation

A path is considered "blocked" when a node on this path prevents causal influence from propagating between variables. D-separation occurs when all paths between two variables are blocked ($A \coprod B$), indicating no direct statistical association between them. This condition is crucial for enabling unbiased causal inference.


#### Open Paths

If at least one path between variables remains unblocked, allowing for the transmission of association, even in the absence of causation ($A \cancel\coprod B$).


### The Five Elementary Graphical Structures of Causality and Five Rules for Confounding Control {#sec-five-elementary}


To uncover causal insights from statistical relationships, it is essential to understand five basic graphical structures. We next examine these structures, remembering that achieving balance in confounders across treatments requires ensuring statistical independence between potential outcomes and treatment ($A\coprod Y(a)|L$) within groups defined by measured covariates $L$. 

#### Absence of Causality: Two Variables with No Arrows

When any arrows do not connect $A$ and $B$, we assume do not share a causal relationship and are statistically independent. Graphically, we represent this relationship as:

$$\xorxALARGE$$


#### Causal Structure 1: Direct Causation Between Two Variables

A causal arrow ($A \to B$) signifies that changes in variable $A$ directly cause changes in variable $B$, creating a statistical dependence between them. This direct causal link is graphically depicted as:

$$\xtoxALARGE$$



#### Rule 1: Ensure That The Treatment Precedes The Outcome {#sec-four-rules}


 This rule follows from the nature of causality, which follows the arrow of time. An outcome occurs after the intervention that causes it.  Note that the assumption that a treatment precedes the outcome it causes cannot be ensured when the relative timing of events in one's data is unknown. 


##### Motivating Example

Suppose we find an association between conservation behaviours and happiness.  We might be tempted to infer that conservation behaviours cause happiness. However, the association might be entirely explained because happy people are more likely to engage in conservation behaviour. If we only have cross-sectional data, We cannot rule out the alternative explanation.


#### Causal Structure 2: The Fork Structure - Common Cause Scenario

The fork structure, indicated by $A \rightarrow B$ and $A \rightarrow C$, denotes the assumption that $A$ is a common cause that influences both $B$ and $C$. Graphically: 


$$\forkLARGE$$

Pearl proved that when we condition on the common cause $A$ (indicated by $\boxed{A}$), $B$ and $C$ become conditionally independent [@pearl2009a]. By adjusting for the common cause, any non-causal association between $B$ and $C$ is effectively blocked at node $A$.

##### Motivating Example

Suppose our observations reveal that areas with higher rates of bicycle commuting also present lower average levels of psychological distress. Does using bicycle commuting directly reduce average psychological distress? Not necessarily. A common environmental factor might influence both. Consider sunshine hours as the common cause:

   - Sunshine ($A$) encourages the use of bicycles ($B$).
   - Sunshine ($A$) contributes to lower psychological distress ($C$).

According to the rules of d-separation, if we were to account for the common cause (sunshine hours), isolating days with similar levels of sunshine, the apparent link between bicycle commuting and psychological distress levels would dissipate. Adjusting for the fork’s common cause would eliminate the spurious association we have assumed in this example.

#### Rule 2: The Fork Rule  {#sec-four-rules}

If interested in the causal effect of $B \to C$, condition on $\boxed{A}$.

#### Causal Structure 3. The Chain Structure: A Mediator

The chain structure ($A \rightarrow B \rightarrow C$) illustrates a setting in which $A$ causes $B$, and $B$ subsequently causes $C$. Conditioning on the intermediary variable $B$ (denoted: $\boxed{B}$) interrupts the causal pathway, rendering $A$ and $C$ conditionally independent. Graphically, we represent this relationship as:

$$\chainLARGE$$

##### Motivating Example

Suppose we wanted to assess the effect of green space renovation in urban areas $A$ on local community engagement $B$, which subsequently reduces neighbourhood crime rates, $C$. Assume the renovation of green spaces $A$ boosts community engagement $B$, which then leads to a decrease in crime rates $C$.

According to the rules of d-separation, controlling for the mediator, community engagement, in this case, might hide the broader effect of green space renovation. If the primary path through which green space renovation affects crime rates is via enhanced community engagement, then adjusting for community engagement could misleadingly suggest that green space renovation does not directly influence crime rates. The upshot: do not condition on a mediator when examining the effects of environmental changes on social outcomes.

##### Rule 3. The Chain Rule

If investigating the *total* causal effect of $A\to C$, *avoid* conditioning on the mediator $B$.  

**Important note:** Assessing causal mediation requires further assumptions, which we will not discuss here [@vanderweele2015; @bulbulia2024swigstime].


#### Causal Structure 4: The Collider Structure: A Common Effect

The collider ($A\to C$, $B \to C$) features two factors independently causing a common effect. Initially, $A$ and $B$ lack association. Conditioning on the collider $C$ (or its descendant) introduces a spurious statistical association between $A$ and $B$. Graphically:

$$\immoralityLARGE$$


##### Motivating Example

Suppose we were interested in whether access to green spaces ($A$) causes people to become happier ($B$)? Suppose we were to control for a variable $C$  -- say health -- which is a common effect of the treatment $A$ (access to green space) and the outcome $B$ (happiness).  Here, conditioning on $C$ would open a non-causal path between $A$ and $B$. That is, if $A$ causes $C$ and $B$ also causes $C$, controlling for $C$ can induce a spurious association between $A$ and $B$. Consider:

1. Among unhealthy individuals (low $C$), those with high access to green space ($A$) will appear less happy ($B$). When we stratify by $C$, a negative association between green space and happiness is observed, even if $A$ and $B$ are not causally asssociated. 

2. Among healthy individuals (high $C$), those with low access to green space ($A$) will be happier ($B$). Again, when we stratify by $C$ a negative association between green space and happiness is observed. 

In this scenario, the relationship between green space access and happiness, when health is controlled for, introduces a spurious negative association. This association is non-causal because it arises from controlling for a collider, health. Without controlling for health, assuming there are no other confounding paths between green space access and happiness, the misleading statistical association would not be present. 

This example illustrates the risk of confounding the analysis by conditioning on an outcome influenced by both variables of interest. 

#### **Rule 4: The Collider Rule:** when assessing the causal effect of $A\to B$, do not conditioning on a collider ($C$) or its descendants.  Doing so may introduce an association that appears causal but is not.


#### We build all complex causal relationships from the five elemental structures of causation

All forms of confounding bias stem from combinations of the five basic causal structures we have outlined (absence/presence of cause, forks, chains, and colliders). Understanding these elements in isolation and combination allows us to identify potential confounders based on our assumptions about the world as encoded in a causal diagram.  Here we consider an example that combines two structures: the collider structure ($A \rightarrowred \boxed{C} \leftarrowred
B$ and basic causality ($C\rightarrowNEW D$), This combination produce confounding by proxy: $A \rightarrowred \boxed{D} \leftarrowred B$.

Causation implies statistical association.  As such, causal inheritance implies *statistical dependence by inheritance*. The property of statistical inheritance makes descendants act as stand-ins or proxies for their parents.

##### Motivating Example 

Consider again the example of whether access to urban green spaces ($A$) affects wealth ($B$). Imagine they do not, but both independently contribute to well-being ($C$). Suppose that the only people who respond to our survey are those who are high in well-being. In effect, our survey is conditioning on one population stratum ($D$), which is a descendant of the collider -- well-being. Initially, green spaces and income independently affect well-being. However, when we specifically analyse data based on willingness to participate in the survey ($\boxed{D}$), we may inadvertently induce an association between green space access and socioeconomic status, inferring that those with access to green space tend to have a lower income.

$$\immoralityChildA$$

Colliders and their descendants set subtle "traps" that might induce spurious associations. 

However, as we shall see in the next section, conditioning on proxies of unmeasured confounders opens possibilities for confounding control beyond our measured variables. We can sometimes leverage proxies to reduce bias in our causal inferences.  


##### Rule 5: The Proxy Rule

Conditioning on a descendant is akin to conditioning on its parent. Put differently, a descendant is a *proxy$ for its parent. Avoid conditioning on descendants in settings where conditioning on the parent would induce misleading associations. 


#### Role of Assumptions

Causal diagrams bring structure to complex environmental psychology systems. They promote critical thinking about relationships, improving study design and the chances of isolating true causal effects. However, causal diagrams cannot avoid assumptions. Observational data alone cannot prove causation: many diagrams are typically consistent with the data.  The power of causal diagrams lies in helping investigators understand how their assumptions and the data interact. However, we should create causal diagrams in collaboration with subject area experts because every path except the $A\to Y$ path is assumed. When experts disagree, we should propose multiple causal diagrams to reflect the implications of disagreements for causal inference and report the outcomes of their corresponding confounding control strategies. 


### How to Create Causal Diagrams to Address Causal Identification Problems


The **identification problem** centres on whether we can derive the true causal effect of a treatment ($A$) on an outcome ($Y$) from observed data.  Addressing the identification problem has two core components:

#### Note, we must evaluate bias in the absence of a treatment effect

Before attributing any statistical association to causality, we must eliminate non-causal sources of correlation. We do this by:

* Identifying factors that influence both treatment ($A$) and outcome ($Y$).
* Developing adjustment strategies to control for confounders.
* Blocking backdoor paths that create indirect, non-causal links between $A$ and $Y$. By adjusting for confounders, we aim to achieve d-separation between $A$ and $Y$.

#### Note, we must also evaluate bias in the presence of a treatment effect

After addressing potential confounders, we must ensure any remaining association between $A$ and $Y$ reflects a true causal relationship. We address **over-conditioning bias** by:

* Avoiding mediator bias 
* Avoiding collider bias
* Verifying that any association between $A$ and $Y$ after in unbiased after all adjustments.

Pearl's backdoor path criterion requires that we identify and control for confounders while avoiding introducing new biases By over conditioning. With this task in mind, here is how investigators can construct causal diagrams that are effective at addressing identification problems. 

#### First, clarify the research question evaluated by the diagram, including the target population for whom results are meant to generalise

Before attempting to draw any causal diagram, state the problem your diagram addresses and the population to whom the problem applies.  Causal identification strategies may vary by question. For example, the confounding control strategy for evaluating the path $L\to Y$ will differ from that of assessing the path $A\to Y$.  For this reason, reporting coefficients other than the association between $A \to Y$ is typically ill-advised; see @westreich2013; @mcelreath2020; @bulbulia2023.

#### Second, draw the most recent common causes of the exposure and outcome

Incorporate all common causes (confounders) of both the exposure and the outcome into your diagram. This includes both measured and unmeasured variables. Where possible, aggregate functionally similar common causes into a single variable notation (e.g., $L_0$ for demographic variables).


#### Third, include all ancestors of measured confounders linked with the treatment, the outcome, or both

Include any ancestors (precursors) of measured confounders that are associated with either the treatment, the outcome, or both. This step is crucial for addressing hidden biases arising from unmeasured confounding. Simplify the diagram by grouping similar variables. 

#### Fourth, explicitly state assumptions about relative timing

Explicitly annotate the temporal sequence of events using subscripts (e.g., $L_0$, $A_1$, $Y_2$). It is imperative that causal diagrams are acyclic.

#### Fifth, arrange temporal order of causality visually

Arrange your diagram to reflect the temporal progression of causality, either left-to-right or top-to-bottom. This arrangement enhances the comprehensibility of causal relations and is vital for dissecting identification issues as discussed in [**Part 3**](#sec-part3), establishing temporal ordering is necessary for evaluating identification problems. 


#### Sixth, box variables are those variables that we adjust for to control confounding 

Mark variables for adjustment (e.g., confounders) with boxes.

#### Seventh, present paths structurally, not parametrically

Focus on whether paths exist, not their functional form (linear, non-linear, etc.). Parametric descriptions are not relevant for bias evaluation in a causal diagram. (For an explanation of causal interaction and diagrams, see: @bulbulia2023.)

#### Eighth, minimise paths to those necessary for the identification problem

Reduce clutter; only include paths critical for a specific question (e.g., backdoor paths, mediators).

#### Ninth, consider Potential Unmeasured Confounders

Leverage domain expertise to clarify potential unmeasured confounders and represent them in your diagram. This proactive step aids in anticipating and addressing *all* possible sources of confounding bias.

#### Tenth,  state your graphical conventions

Establish and explain the graphical conventions used in your diagram (e.g., using red to highlight open backdoor paths). Consistency in symbol use enhances interpretability, while explicit descriptions improve accessibility and understanding.

## Part 3. How to Use Causal Diagrams for Causal Identification Tasks- Worked Examples {#section-part3}

### Notation

Causal diagrams use specific symbols to represent elements essential in causal inference [@pearl1995; @pearl2009; @greenland1999].  However, as mentioned, no agreed-upon convention exists for creating causal diagrams. @tbl-01 lists the symbols and conventions we use in this chapter.

* **$A$** is the treatment or exposure variable – the intervention or condition whose effect on an outcome is under investigation. **This symbol represents the cause**.
* **$Y$** is the outcome variable – the effect or result that is being studied. **This symbol represents the effect**.
* **$L$** includes all measured confounders – variables that may affect both the treatment and the outcome.
* **$U$** includes unmeasured confounders – variables not included in the analysis that could influence both the treatment and the outcome, potentially leading to biased conclusions.
* **$M$** is a mediator variable – a factor through which the treatment affects the outcome. The focus here is on identifying the total effect of treatment $A$ on an outcome $Y$. Still, it is also essential to understand how controlling for mediators can affect estimates of this total effect. 


::: {#tbl-01}

```{=latex}
\terminologylocalconventionssimple
```
Terminology used in this article for causal diagrams. The graph is adapted from [@bulbulia2023]. 
:::


We next describe our graphical conventions and causal diagrams.  Again, because conventions may differ, it is always important to state them explicitly when reporting causal diagrams. @tbl-02 describes the basic conventions that we employ in this chapter. 

::: {#tbl-02}

```{=latex}
\terminologygeneralbasic
```
Basic conventions for causal diagrams (adapted from [@bulbulia2023]). 
:::



### Graphical Table


@tbl-04 provides seven worked examples that put causal diagrams to work.  Our example will focus on the question of whether access to green space affects happiness and approach this question by focusing on how different assumptions about (i) the structure of the world and (ii) the observational data that have been collected may affect strategies for confounding control and the confidence in our results.  Each example refers to a row in the table. 

::: {#tbl-04}
```{=latex}
\terminologyelconfoundersLONG
```
Worked examples: This table is adapted from [@bulbulia2023].

:::



### 1. The Problem of Confounding by a Common Cause

@tbl-04 Row 1 describes the confounding problem of a common cause. We encountered this problem in Part 1. Such confounding arises when there is a variable or set of variables, denoted by $L$, that influence both the exposure, denoted by $A$, and the outcome, denoted by $Y.$ Because $L$ is a common cause of both $A$ and $Y$, $L$ may create a statistical association between $A$ and $Y$ that does not reflect a causal association.

For instance, in the context of green spaces, consider people who live closer to green spaces (exposure $A$) and their experience of improved happiness (outcome $Y$). A common cause might be socioeconomic status $L$. Individuals with higher socioeconomic status might have the financial capacity to afford housing near green spaces and simultaneously afford better healthcare and lifestyle choices, contributing to greater happiness. Thus, although the data may show a statistical association between living closer to green spaces $A$ and greater happiness $Y$, this association might not reflect a direct causal relationship owing to confounding by socioeconomic status $L$.

How might we obtain balance in this confounder to compare the treatments?  Addressing confounding by a common cause involves adjusting for the confounder in one's statistical model. We may adjust through regression, or more complicated methods, such as the inverse probability of treatment weighting, marginal structural models, and others see @hernán2023. Such adjustment effectively closes the backdoor path from the exposure to the outcome. Equivalently, conditioning on $L$ d-separates $A$ and $Y$.  

@tbl-04 Row 1, Column 3, emphasises that a confounder by common cause must precede both the exposure and the outcome. While it is often clear that a confounder precedes the exposure (e.g., a person's country of birth), the timing might be uncertain in other cases. We assert its temporal precedence by positioning the confounder before the exposure in our causal diagrams. However, such a timing assumption might be strong when relying on cross-sectional data. Exploring causal scenarios where the confounder follows the treatment or outcome can be insightful in such cases. Causal diagrams are instrumental in examining possible timings and their implications for causal inference. 

Next, we examine the effects of conditioning on a variable that is an effect of the treatment.

### 2. Mediator Bias

 @tbl-04 Row 1 presents a problem of mediator bias. Consider again whether proximity to green spaces, $A$, affects happiness, $Y$. Suppose that physical activity is a mediator, $L$.

To fill out the example, imagine that living close to green spaces $A$ influences physical activity $L$, subsequently affecting happiness $Y$. If we were to condition on physical activity $L$, assuming it to be a confounder, we would then bias our estimates of the total effect of proximity to green spaces $A$ on happiness $Y$. Such a bias arises because of the chain rule. Conditioning on $L$ "d-separates" the total effect of $A$ on $Y$. This phenomenon is known as mediator bias. Notably, @montgomery2018 finds dozens of examples of mediator bias in *experiments* in which control is made for variables that occur after the treatment.  For example, obtaining demographic and other information from participants *after* a study is an invitation to mediator bias. If the treatment affects these variables, and the variables affect the outcome (as we assume by controlling for them), then researchers may induce mediator bias. 

To avoid mediator bias when estimating a total causal effect, we should never condition on a mediator! The surest way to prevent this problem is to ensure that $L$ occurs before the treatment $A$ and before the outcome $Y$.  We present this solution in @tbl-04 Row 2 Col 3. 


### 3. Confounding by Collider Stratification (Conditioning on a Common Effect)


 @tbl-04 Row 1 presents a problem of collider bias.  Conditioning on a common effect, or collider stratification, occurs when a variable, denoted by $L$, is influenced by both the exposure, denoted by $A$, and the outcome, denoted by $Y$.

Let us assume initial independence: the choice to live closer to green spaces (exposure $A$) and happiness (outcome $Y$) are independent: $A \coprod Y(a)$.

We furthermore assume physical health $L$ is an effect of green space access, and happiness increases physical health. Thus, $L$ is an effect of $A$ and $Y$. If we were to condition on $L$ in this setting, we would introduce *collider stratification bias*. When we control for the common effect $L$ (physical health), we may inadvertently introduce confounding. This happens because knowing something about $L$ gives us information about both $A$ and $Y$. If someone were high on physical health but low an access to greenspace, this would imply that they are higher in happiness. Likewise, if someone were low in physical health but high in access to green space, this would imply lower happiness. As a result of our conditioning strategy, it would appear that access to green space and happiness are negatively associated. However, if we were to avoid conditioning on the common outcome, we would find that the treatment and outcome are not associated.  

How can we avoid collider bias, the temporal sequence of measurement affords a powerful strategy:

Ensure all common causes of $A$ and $Y$  -- call them $L$ -- are measured before the treatment $A$ occurs. Ensure further that $Y$ occurs after $A$ occurs.  If the confounder $L$ is not measured, ensure that conditioning on its downstream proxy, $L'$ does not induce collider or mediator biases.

By adhering to this sequence, we can mitigate the risk of collider stratification bias and better understand the causal relationships between exposure, outcome, and their common effects.

### 4. Confounding by Conditioning on a Descendant of a Confounder  

 @tbl-04 Row 4 presents a problem of collider bias by decent. Recall the rules of d-separation also apply to conditioning on descendants of a confounder.  Thus, we may unwittingly evoke confounding by proxy when conditioning on a measured descendant of an unmeasured collider. 
 
For example, if doctor visits were encoded in our data, and doctor visits were an effect of poor health, conditioning on doctor visits would function similarly to conditioning on poor health in the previous example, introducing collider confounding. 


### 5. M-bias: Conditioning on Pre-Exposure Collider

There are only five elementary structures of causality. Every confounding scenario can be developed from these five elementary structures. We next consider how we may combine these elementary causal relationships in causal diagrams to create effective strategies for confounding control. 


@tbl-04 Row 5 presents a form of pre-exposure over-conditioning confounding known as "M-bias".  This bias combines the collider structure and the fork structure, revealing what might not otherwise be obvious: it is possible to induce confounding even if we ensure that all variables have been measured **before** the treatment. The collider structure is evident in the path $U_Y \to L_0$ and $U_A \to L_0$. The collider rule shows that conditioning on $L_0$ opens a path between $U_Y$ and $U_A$. What is the result? We find that $U_Y$ is associated with the outcome $Y$ and $U_A$ is associated with treatment $A$. This is a fork (common cause) structure. The association between treatment and outcome opened by conditioning on $L$ arises from an open back-door path that occurs from the collider structure. We thus have confounding. How might such confounding play out in a real-world setting? 

In the context of green spaces, consider the scenario where an individual's level of physical activity $L$ is influenced by an unmeasured factor related to their propensity to live near green spaces $A$ -- say childhood upbringing. Suppose further that another unmeasured factor -- say a genetic factor -- increases both physical activity $L$ and happiness $Y$. Here, physical activity $L$ does not affect the decision to live near green spaces $A$ or happiness $Y$ but is a descendent of unmeasured variables that do. If we were to condition on physical activity $L$ in this scenario, we would create the bias just described --  "M-bias."  

How shall we respond to this problem? The solution is straightforward. If $L$ is neither a common cause of $A$ and $Y$ nor the effect of a shared common cause, then $L$ should not be included in a causal model. In terms of the conditional exchangeability principle, we find $A \coprod Y(a)$ yet $A \cancel{\coprod} Y(a)| L$. So we should not condition on $L$: do not control for exercise [@cole2010].[^3]

[^3]: Note that when we draw a chronologically ordered path from left to right, the M shape for which "M-bias" takes its name changes to an E shape. We shall avoid proliferating jargon and retain the term "M bias."

### 6. Conditioning on a Descendent May Sometimes Reduce Confounding

 In @tbl-04 Row 6, we encounter a causal diagram in which an unmeasured confounder opens a back-door path that links the treatment and outcome.  Here, we consider how we may use the rules of d-separation to obtain unexpected strategies for confounding control. 
 
 Returning to our green space example, suppose an unmeasured genetic factor $U$ affects one's desire to seek out isolation in green spaces $A$ and independently affects one's happiness $Y$.  Were such an unmeasured confounder to exist we could not obtain an unbiased estimate for the causal effect of green space access on happiness. We have, it seems, intractable confounding.  
 
However, imagine a variable $L^\prime$, a trait expressed later in life that arises from this genetic factor. If such a trait could be measured, even though the trait $L'$ is expressed after the treatment and outcome have occurred, controlling for $L'$ would enable investigators to close the backdoor path between the treatment and the outcome. This strategy works because a measured effect is a *proxy* for its cause $U$, the unmeasured confounder.  By conditioning on the late-adulthood trait, $L'$, we partially condition on its cause, $U$, the confounder of $A \to Y$. Thus, not all effective confounding control strategies need to rely on measuring pre-exposure variables. Thus, the elementary causal structures reveal a possibility for confounding control by condition on a post-outcome variable.  This strategy is not intuitive. Although a common cause must occur before a treatment (and outcome), its proxy need not! If we have a measure for the latter but not the former, we should condition on the post-treatment proxy of a pre-treatment common cause.

### 7. Confounding Control with Three Waves of Data is Powerful and Reveals Possibilities for Estimating an "Incident Exposure" Effect

@tbl-04 row 7 presents another setting in which there is unmeasured confounding. In response to this problem, we use the rules of d-separation to develop a data collection and modelling strategy that may greatly reduce the influence of unmeasured confounding.  @tbl-04 row 7 col 3, by collecting data for both the treatment and the outcome at baseline and controlling for baseline values of the treatment and outcome, any unmeasured association between the treatment $A_1$ and the outcome $Y_2$ would need to be *independent* of their baseline measurements. As such, including the baseline treatment and outcome, along with other measured covariates that might be measured descendants of unmeasured confounders, is a strategy that exerts considerable confounding control [@vanderweele2020]. 

Furthermore, this causal graph makes evident a second benefit of this strategy.  Returning to our example, a model that controls for baseline exposure would require that people initiate a change from the $A_0$ observed baseline level. Thus, by controlling for the baseline value of the treatment, we may learn about the causal effect of shifting one's access to green space status. This effect is called the "incident exposure effect." The incident exposure effect better emulates a "target trial" or the organisation of observational data into a hypothetical experiment in which there is a "time-zero" initiation of treatment in the data; see @hernán2016; @danaei2012; @vanderweele2020; @bulbulia2022.  Without controlling for the baseline treatment, we could only estimate a "prevalent exposure effect." If the initial exposure caused people some people to be miserable, we would not be able to track this outcome. The prevalent exposure effect would mask it, distorting causal inferences for the quantity of interest, namely, what would happen, on average, if people were to shift to having greater greenspace access. 

Finally, we obtain further control for unmeasured confounding by controlling for both the baseline treatment and the baseline outcome. For an unmeasured confounder to affect both the treatment and the outcome (and unmeasured fork structure), it would need to do so independently of the baseline measures of the treatment and exposure [@vanderweele2020]. 

Thus, we generally require repeated measures on the same unit over time intervals to obtain an incident exposure effect and exert more robust control for unmeasured confounding using past states of the treatment and outcome.  We must then model the treatments and outcomes as separate elements in our statistical model. 


## Part 4.  Practical Guide For Constructing Causal Diagrams and Reporting Results When Causal Structure is Unclear {#section-part4}

### Cross-sectional designs

In environmental psychology, researchers often grapple with whether causal inferences can be drawn from cross-sectional data, especially when longitudinal data are unavailable. The challenge is common to cross-sectional designs.  However, it is important to appreciate that even longitudinal studies require careful assumption management. We next discuss how causal diagrams can guide inference in both data types, with examples relevant to environmental psychologists.

#### 1. Graphically encode causal assumptions

Causal inference turns on assumptions. Although cross-sectional analyses typically demand much stronger assumptions owing to the snapshot nature of data, these assumptions, when transparently articulated, do not permanently bar causal analysis. By stating different assumptions and modelling the data following these assumptions, we might find that certain causal conclusions are robust to these differences. Where the implications of different assumptions disagree, we can better determine the forms of data collection that would be required to settle such differences.  Below we consider an example where assumptions point to different conclusions, revealing the benefits of collecting time-series data to assess whether a variable is a confounder or a mediator. 


#### 2. Consider time-invariant confounders at baseline

In cross-sectional studies, some confounders are inherently stable over time, such as ethnicity, year and place of birth, and biological gender. For environmental psychologists examining the relationship between access to natural environments and psychological well-being, these stable confounders can be adjusted for without concern for introducing bias from mediators or colliders. For example, conditioning on one’s year of birth can help isolate recent urban development’s effect on mental health, independent of generational differences in attitudes toward green spaces.

#### 3. Consider stable confounders at baseline

While not immutable, other confounders are less likely to be influenced by the treatment. Variables such as sexual orientation, educational attainment, and often income level fall into this category. For instance, the effect of exposure to polluted environments on cognitive outcomes can be analysed by conditioning on education level, assuming that recent exposure to pollution is unlikely to change someone’s educational history retroactively.

#### 4. Consider time varying confounding

The sequence of treatment and outcome is crucial. Sometimes, the temporal order is clear, reducing concerns about reverse causation. Mortality is a definitive outcome where the timing issue is unambiguous. If researching the effects of air quality on mortality, the causal direction (poor air quality leading to higher mortality rates) is straightforward. However, consider the relationship between socio-economic status and health outcomes; the direction of causality is complex because socioeconomic factors can influence health (through access to resources), and poor health can affect socio-economic status (through reduced earning capacity).

#### 5. Create your causal diagrams

Given the complexity of environmental influences on psychological outcomes, it’s prudent to construct multiple causal diagrams to cover various hypothetical scenarios. For example, when studying the effect of community green space on stress reduction, one diagram might assume the direct benefits of green space on stress. At the same time, another might include potential mediators such as physical activity. By analysing and reporting findings based on multiple diagrams, researchers can examine the robustness of their conclusions across different theoretical frameworks and sets of assumptions.

@tbl-cs describes ambiguous confounding control arising from cross-sectional data. Suppose again we are interested in the causal effect of access to greenspace, denoted by $A$ on "happiness," denoted by $Y$.   We are uncertain whether exercise, denoted by $L$, is a common cause of $A$ and $Y$ and thus a confounder or whether exercise is a mediator along the path from $A$ to $Y$. That is: (1) those who exercise might seek access to green space, and (2) exercise might increase happiness. Alternatively, the availability of green space might encourage physical activity, which could subsequently affect happiness. Causal diagrams can disentangle these relationships by explicitly representing potential paths, thereby guiding appropriate strategies for confounding control selection. We recommend using multiple causal diagrams to investigate the consequences of different plausible structural assumptions. 

**Assumption 1: Exercise is a common cause of $A$ and $Y$**, this scenario is presented in @tbl-cs row 1. Here, our strategy for confounding control is to estimate the effect of $A$ on $Y$ conditioning on $L$. 

**Assumption 2: Exercise is a mediator of $A$ and $Y$**, this scenario is presented in @tbl-cs row 2. Here, our strategy for confounding control is simply estimating the effect of $A$ on $Y$ without including $L$ (assuming there are no other common causes of the treatment and outcome). 

::: {#tbl-cs}

```{=latex}
\examplecrosssection
```
This table is adapted from [@bulbulia2023]
:::


We can simulate data and run separate regressions to clarify how answers may differ, reflecting the different conditioning strategies embedded in the different assumptions. The following simulation generates data from a process in which exercise is a mediator (Scenario 2). (See Appendix C)



```{r}
#| label: simulation_cross_sectional-appendix
#| tbl-cap: "Code for a simulation of a data generating process in which the effect of exercise (L) fully mediates the effect of greenspace (A) on happiness (Y)."
#| out-width: 80%
#| echo: false
# load libraries
library(gtsummary) # gtsummary: nice tables
library(kableExtra) #  tables in latex/markdown
library(clarify) # simulate ATE

# simulation seed
set.seed(123) #  reproducibility

# define the parameters 
n = 1000 # Number of observations
p = 0.5  # Probability of A = 1 (access to greenspace)
alpha = 0 # Intercept for L (exercise)
beta = 2  # Effect of A on L 
gamma = 1 # Intercept for Y 
delta = 1.5 # Effect of L on Y
sigma_L = 1 # Standard deviation of L
sigma_Y = 1.5 # Standard deviation of Y

# simulate the data: fully mediated effect 
A = rbinom(n, 1, p) # binary exposure variable
L = alpha + beta*A + rnorm(n, 0, sigma_L) # continuous mediator
Y = gamma + delta*L + rnorm(n, 0, sigma_Y) # continuous outcome

# make the data frame
data = data.frame(A = A, L = L, Y = Y)

# fit regression in which L is assumed to be a mediator
fit_1 <- lm( Y ~ A + L, data = data)

# fit regression in which L is assumed to be a mediator
fit_2 <- lm( Y ~ A, data = data)

# create gtsummary tables for each regression model
table1 <- tbl_regression(fit_1)
table2 <- tbl_regression(fit_2)

# merge the tables for comparison
table_comparison <- tbl_merge(
  list(table1, table2),
  tab_spanner = c("Model: Exercise assumed confounder", 
                  "Model: Exercise assumed to be a mediator")
)
# make latex table
markdown_table_0 <- as_kable_extra(table_comparison, 
                                   format = "latex", 
                                   booktabs = TRUE)
# print                                   
markdown_table_0
```


This table presents the conditional treatment effect estimates.  We present code for obtaining marginal treatment effects in [Appendix C](#appendix-c) 

```{r}
#| label: ate_simulation_cross_sectional
#| fig-cap: ""
#| out-width: 100%
#| echo: false

# use `clarify` package to obtain ATE
library(clarify)
# simulate fit 1 ATE
set.seed(123)
sim_coefs_fit_1 <- sim(fit_1)
sim_coefs_fit_2 <- sim(fit_2)

# marginal risk difference ATE, simulation-based: model 1 (L is a confounder)
sim_est_fit_1 <-
  sim_ame(
    sim_coefs_fit_1,
    var = "A",
    subset = A == 1,
    contrast = "RD",
    verbose = FALSE
  )

# marginal risk difference ATE, simulation-based: model 2 (L is a mediator)
sim_est_fit_2 <-
  sim_ame(
    sim_coefs_fit_2,
    var = "A",
    subset = A == 1,
    contrast = "RD",
    verbose = FALSE
  )
# obtain summaries
summary_sim_est_fit_1 <- summary(sim_est_fit_1, null = c(`RD` = 0))
summary_sim_est_fit_2 <- summary(sim_est_fit_2, null = c(`RD` = 0))

# get coefficients for reporting
# ate for fit 1, with 95% CI
ATE_fit_1 <- glue::glue(
  "ATE =
                        {round(summary_sim_est_fit_1[3, 1], 2)},
                        CI = [{round(summary_sim_est_fit_1[3, 2], 2)},
                        {round(summary_sim_est_fit_1[3, 3], 2)}]"
)
# ate for fit 2, with 95% CI
ATE_fit_2 <-
  glue::glue(
    "ATE = {round(summary_sim_est_fit_2[3, 1], 2)},
                        CI = [{round(summary_sim_est_fit_2[3, 2], 2)},
                        {round(summary_sim_est_fit_2[3, 3], 2)}]"
  )
```



On the assumptions outlined in @tbl-cs row 1, in which we *assert* that exercise is a confounder, the average treatment effect of access to green space on happiness is `r ATE_fit_2`.

On the assumptions outlined in @tbl-cs row 2, in which we *assert* that exercise is a mediator, the average treatment effect of access to green space on happiness is `r ATE_fit_1`. 

Note that although the mediator $L$ is "highly statistically significant", including it in the model is a mistake. We obtain a negative effect estimate for the causal effect of green space access on happiness.

With only cross-sectional data, we must infer the results are inconclusive. Such understanding, although not the definitive answer we sought, is progress. The result tells us we should not be overly confident with our analysis (whatever p-values we recover!), and it clarifies that longitudinal data are needed. 

These findings illustrate the role that assumptions about the relative timing of exercise as a confounder or as a mediator play. 

### Recommendations for Conducting and Reporting Causal Analyses with Cross-Sectional Data

When analysing and reporting analyses with cross-sectional data, researchers face the challenge of making causal inferences without the benefit of temporal information. 

The following recommendations aim to guide researchers in navigating these challenges effectively:

**Warning**: before proceeding with cross-sectional analysis, examine whether panel data are available. Longitudinal data can provide crucial temporal information that aids in establishing causality, offering a more robust framework for causal inference. If longitudinal data are unavailable, the recommendations above become even more critical for using cross-sectional data best.

#### 1. **Draw multiple causal diagrams**

Draw multiple causal diagrams to represent different theoretical assumptions about the relationships and timing of variables relevant to an identification problem. If some causal pathways cannot be ruled out, clarify the implications of assigning variables the roles for which consensus or which the time ordering of the data do not resolve. For example, in studying the effect of urban green spaces on mental health from cross-sectional data, consider causal DAGs that assess effects in each direction.

#### 2. **Perform and report analyses for each assumption**

Conduct and transparently report separate analyses for each scenario your causal diagrams depict. This practice ensures that your study is theoretically grounded for each model. Presenting results from each analytical approach and the underlying assumptions and statistical methods promotes a balanced interpretation of findings. 

#### 3. **Interpret findings with attention to ambiguities**

Interpret results carefully, highlighting any ambiguities or inconsistencies across analyses. Discuss how varying assumptions about structural relationships and the timing of events can lead to divergent conclusions. 

#### 4. **Report divergent findings**

Approach conclusions with caution, especially when findings suggest differing practical implications. Acknowledge the limitations of cross-sectional data in establishing causality and the potential for alternative explanations. Do not over-sell.

#### 5. **Identify avenues for future research**

Target future research that might clarify ambiguities. Consider the design of longitudinal studies or experiments capable of clarifying lingering uncertainties.

#### 6. **Supplement observational data with simulated data** 

Leverage data simulation to understand the complexities of causal inference. Simulating data based on various theoretical models allows researchers to examine the effect of different assumptions on their findings. This method tests analytical strategies under controlled conditions, assessing the robustness of conclusions against assumption violations or unobserved confounders.

#### 7. **Conduct sensitivity analyses to assess robustness**

implement sensitivity analyses to determine how dependent conclusions are on specific assumptions or parameters within your causal model. A relatively simple sensitivity analysis is VanderWeele's E-value [@vanderweele2017]

Cross-sectional data are limiting; however, by appropriately bounding uncertainties in your causal inferences, you may use them to advance understanding. May your clarity and caution serve as an example for others.


### Longitudinal Designs

Causation occurs in time. Longitudinal designs offer a substantial advantage over cross-sectional designs for causal inference because sequential measurements allow us to capture causation and quantify its magnitude. We typically do not need to assert timing as in cross-sectional data settings. Because we know when variables have been measured, we can reduce ambiguity about the directionality of causal relationships. For instance, tracking changes in "happiness" following changes in access to green spaces over time can more definitively suggest causation than cross-sectional snapshots.


Despite this advantage, longitudinal researchers still face assumptions regarding the absence of unmeasured confounders or the stability of measured confounders over time. These assumptions must be explicitly stated.  As with cross-sectional designs, wherever assumptions differ, researchers should draw different causal diagrams that reflect these assumptions and subsequently conduct and report separate analyses. 


In this section, we simulate a dataset to demonstrate the benefits of incorporating both baseline exposure and baseline outcomes into analysing the effect of access to open green spaces on happiness. This approach allows us to control for initial levels of exposure and outcomes, offering a clearer understanding of the causal relationship. [Appendix D](#appendix-d-simulation-of-different-confounding-control-strategies) provides the code. [Appendix E](#appendix-e-non-parametric-estimation-of-average-treatment-effects-using-causal-forests) provides an example of a non-parametric estimator for the causal effect.  As mentioned before, by conditioning on baseline levels of access to green spaces and baseline mental health, researchers can more accurately estimate the *incident effect* of changes in green space access on changes in mental health. @tbl-lg offers an example of how we may use multiple causal diagrams to clarify the problem and our confounding control strategy. 


::: {#tbl-lg}

```{=latex}
\examplelongitudinal
```
This table is adapted from [@bulbulia2023]
:::


Our analysis assessed the average treatment effect (ATE) of access to green spaces on happiness across three distinct models: uncontrolled, standard controlled, and interaction controlled. These models were constructed using a hypothetical cohort of 10,000 individuals, incorporating baseline exposure to green spaces ($A_0$), baseline happiness ($Y_0$), baseline confounders ($L_0$), and an unmeasured confounder ($U$). The detailed simulation process and model construction are given in [Appendix D](#appendix-simulate-longitudinal-ate).



```{r}
#| label: codelg
#| echo: false
#| eval: true
# load libaries 
library(kableExtra)
if(!require(kableExtra)){install.packages("kableExtra")} # causal forest
if(!require(gtsummary)){install.packages("gtsummary")} # causal forest
if(!require(grf)){install.packages("grf")} # causal forest

# r_texmf()eproducibility
set.seed(123) 

# set number of observations
n <- 10000 

# baseline covariates
U <- rnorm(n) # Unmeasured confounder
A_0 <- rbinom(n, 1, prob = plogis(U)) # Baseline exposure
Y_0 <- rnorm(n, mean = U, sd = 1) # Baseline outcome
L_0 <- rnorm(n, mean = U, sd = 1) # Baseline confounders

# coefficients for treatment assignment
beta_A0 = 0.25
beta_Y0 = 0.3
beta_L0 = 0.2
beta_U = 0.1

# simulate treatment assignment
A_1 <- rbinom(n, 1, prob = plogis(-0.5 + 
                                    beta_A0 * A_0 +
                                    beta_Y0 * Y_0 + 
                                    beta_L0 * L_0 + 
                                    beta_U * U))

# coefficients for continuous outcome
delta_A1 = 0.3
delta_Y0 = 0.9
delta_A0 = 0.1
delta_L0 = 0.3
theta_A0Y0L0 = 0.5 # Interaction effect between A_1 and L_0
delta_U = 0.05

# simulate continuous outcome, including interaction
Y_2 <- rnorm(n,
             mean = 0 +
               delta_A1 * A_1 + 
               delta_Y0 * Y_0 + 
               delta_A0 * A_0 + 
               delta_L0 * L_0 + 
               theta_A0Y0L0 * Y_0 * 
               A_0 * L_0 + 
               delta_U * U,
             sd = .5)

# assemble data frame
data <- data.frame(Y_2, A_0, A_1, L_0, Y_0, U)

# model: no control
fit_no_control <- lm(Y_2 ~ A_1, data = data)

# model: standard covariate control
fit_standard <- lm(Y_2 ~ A_1 + L_0, data = data)

# model: interaction
fit_interaction  <- lm(Y_2 ~ A_1 + L_0 + A_0 + Y_0 + A_0:L_0:Y_0, data = data)

# create gtsummary tables for each regression model
tbl_fit_no_control<- tbl_regression(fit_no_control)  
tbl_fit_standard <- tbl_regression(fit_standard)
tbl_fit_interaction <- tbl_regression(fit_interaction)

# get only the treatment variable
tbl_list_modified <- lapply(list(
  tbl_fit_no_control,
  tbl_fit_standard,
  tbl_fit_interaction),
function(tbl) {
  tbl %>%
    modify_table_body(~ .x %>% dplyr::filter(variable == "A_1"))
})

# merge tables
table_comparison <- tbl_merge(
  tbls = tbl_list_modified,
  tab_spanner = c(
    "No Control",
    "Standard",
    "Interaction")
) |>
  modify_table_styling(
    column = c(p.value_1, p.value_2, p.value_3),
    hide = TRUE
  )

#create latex table for publication
markdown_table <-
  as_kable_extra(table_comparison, format = "latex", booktabs = TRUE) |>
  kable_styling(latex_options = "scale_down")
  
# print it
#markdown_table
```
```{r}
#| label: ate-sim-long
#| tbl-cap: "Code for calculating the average treatment effect."
#| echo: false
#| eval: true

# use `clarify` package to obtain ATE
if(!require(clarify)){install.packages("clarify")} # clarify package

# simulate fit 1 ATE
set.seed(123)
sim_coefs_fit_no_control<- sim(fit_no_control)  
sim_coefs_fit_std <- sim(fit_standard)
sim_coefs_fit_int <- sim(fit_interaction)

# marginal risk difference ATE, no controls
sim_est_fit_no_control <-
  sim_ame(
    sim_coefs_fit_no_control,
    var = "A_1",
    subset = A_1 == 1,
    contrast = "RD",
    verbose = FALSE
  )
# marginal risk difference ATE, simulation-based: model 1 (L is a confounder)
sim_est_fit_std <-
  sim_ame(
    sim_coefs_fit_std,
    var = "A_1",
    subset = A_1 == 1,
    contrast = "RD",
    verbose = FALSE
  )
# marginal risk difference ATE, simulation-based: model 2 (L is a mediator)
sim_est_fit_int <-
  sim_ame(
    sim_coefs_fit_int,
    var = "A_1",
    subset = A_1 == 1,
    contrast = "RD",
    verbose = FALSE
  )
# obtain summaries
summary_sim_coefs_fit_no_control <-
  summary(sim_est_fit_no_control, null = c(`RD` = 0))
summary_sim_est_fit_std <-
  summary(sim_est_fit_std, null = c(`RD` = 0))
summary_sim_est_fit_int <-
  summary(sim_est_fit_int, null = c(`RD` = 0))

# get coefficients for reporting
# ate for fit 1, with 95% CI
ATE_fit_no_control  <- glue::glue(
  "ATE = {round(summary_sim_coefs_fit_no_control[3, 1], 2)}, 
  CI = [{round(summary_sim_coefs_fit_no_control[3, 2], 2)},
  {round(summary_sim_coefs_fit_no_control[3, 3], 2)}]"
)
# ate for fit 2, with 95% CI
ATE_fit_std <- glue::glue(
  "ATE = {round(summary_sim_est_fit_std[3, 1], 2)}, 
  CI = [{round(summary_sim_est_fit_std[3, 2], 2)},
  {round(summary_sim_est_fit_std[3, 3], 2)}]"
)
# ate for fit 3, with 95% CI
ATE_fit_int <-
  glue::glue(
    "ATE = {round(summary_sim_est_fit_int[3, 1], 2)},
    CI = [{round(summary_sim_est_fit_int[3, 2], 2)},
    {round(summary_sim_est_fit_int[3, 3], 2)}]"
  )
# coefs
# ATE_fit_no_control
# ATE_fit_std
# ATE_fit_int
```

The ATE estimates from these models provide critical insights into the effects of green space exposure on individual happiness while accounting for various confounding factors. The model without control variables estimated `r ATE_fit_no_control`, significantly overestimating the treatment effect. Incorporating standard covariate control reduced this estimate to `r ATE_fit_std`, aligning more closely with the expected effect but still overestimating. Most notably, the model that included interactions among baseline exposure, outcome, and confounders yielded `r ATE_fit_int`, approximating the true effect of 0.3. This finding underscores the importance of including baseline values of the exposure and outcome wherever these data are available. 

### Recommendations for Conducting and Reporting Causal Analyses with Longitudinal Data

Longitudinal data offer strong advantages for causal inference by enabling researchers to establish the relative timing of confounders, treatments, and outcomes. The temporal sequence of events is crucial for establishing causality because causality occurs in time. The following recommendations aim to guide researchers in leveraging longitudinal data effectively to conduct and report causal analyses:

#### 1. Draw multiple causal diagrams
   - **Identification problem diagram**: begin by constructing a causal diagram that outlines your initial assumptions about the relationships among variables, identifying potential confounders and mediators. This diagram should illustrate the complexity of the identification problem.
   - **Solution diagram**: next, create a separate causal diagram that proposes solutions to the identified problems. Having distinct diagrams for the problem and its proposed solutions clarifies your study's analytic strategy and theoretical underpinning.

@tbl-lg provides an example of a table with multiple causal diagrams clarifying potential sources of confounding threats and reports strategies for addressing them. 

#### 2. Attempt longitudinal designs with at least three waves of data

Incorporating repeated measures data from at least three time intervals considerably enhances your ability to infer causal relationships. For example, by adjusting for physical activity measured before the treatment, we can ensure that physical activity does not result from a new initiation to green spaces, which we establish by measuring green space access at baseline. Establishing chronological order allows us to avoid confounding problems 1-4 in @tbl-04. 

#### 3. Calculate Average Treatment Effects for a clearly specified target population

Estimating the average treatment effect (ATE) across the entire study population provides a comprehensive measure of the intervention's effects. This step is crucial for understanding the treatment's overall effect in a specific population, which must be described in advance of data analysis because causal effects are averages within certain populations or stratums of populations. The concept of a causal effect absent a population in not available to data science, because individual causal effects are not observed.

#### 4. Where causality is unclear, report results for multiple causal graphs

Given that the true causal structure may be complex and partially unknown, analysing and reporting results under each plausible causal diagram is prudent. 

#### 5. Conduct sensitivity analyses

Sensitivity analyses are essential for assessing the robustness of your findings to various assumptions within the causal model. These analyses can include simulations, as illustrated in Appendices C and D, to examine bias arising of unmeasured confounding, model misspecification, and alternative causal pathways on the study conclusions. Sensitivity analyses help to identify the conditions under which the findings hold, enhancing the credibility of the causal inferences. (For more about addressing missing data, see: [@bulbulia2024PRACTICAL].) 

#### 6. Address missing data at baseline and study attrition

Longitudinal studies often need help with missing data and attrition, which can introduce bias and affect the validity of causal inferences. Implement and report strategies for handling missing data, such as multiple imputation or sensitivity analyses that assess the bias arising from missing responses at the study's conclusion. (For more about addressing missing data, see: [@bulbulia2024PRACTICAL]). 


By following these recommendations, you will more effectively navigate the inherent limitations of observational longitudinal data, improving the quality of your causal inferences.


## Summary 

This chapter has introduced the potential outcomes framework for causal inference and using directed acyclic graphs (DAGs) in environmental psychology. 

In [**Part 1**](#section-part1) we discussed three critical assumptions necessary for estimating average treatment effects from data:

1. **Conditional Exchangeability**: This assumption posits that treatment allocation is randomised and independent of potential outcomes, conditional on measured covariates.
2. **Causal Consistency**: This assumption asserts that the outcome observed under the treatment condition corresponds to the outcome that would have been observed had the unit received the treatment, and similarly for the control condition.
3. **Positivity**: This assumption asserts that every unit has a non-zero probability of receiving any treatments under comparison.

Although randomised controlled experiments naturally satisfy these assumptions through design—randomisation ensures exchangeability, control guarantees consistency, and design secures positivity -— observational studies typically do not. To obtain consistent causal estimates from observational data, we must assess the extent to which these assumptions can be satisfied.

In [**Part 2**](#section-part2), we explained how causal diagrams work and described their utility in addressing the assumption of conditional exchangeability, or the "no unmeasured confounders" assumption. We identified [five fundamental structures](#sec-five-elementary) underlying all causal relationships. We discovered [Five elementary rules](#sec-four-rules) for evaluating the implications of conditioning on elements within these structures regarding observable statistical associations in data. Thus, causal diagrams provide a simplified visual language for translating complex causal relationships into data observations. However, the relationships in these diagrams represent assertions that are not directly verifiable from the data. The causal relationships between treatments and outcomes are the only relationships not based on assertion. Causal diagrams help us identify structural sources of bias in the statistical associations between treatments and outcomes that may arise from assumed causal relationships, potentially associating treatments with outcomes irrespective of causal links.

In [**Part 3**](#section-part3), we applied causal diagrams to seven common confounding scenarios, demonstrating that a causal diagram needs to highlight only those aspects of a causal setting relevant for assessing structural sources of bias linking treatment and outcome in a non-causal manner. We focused on omitting nodes and paths not directly necessary for our stated identification problem, emphasising that causal diagrams are tailored to context-dependent questions and our assumptions about the world's causal structure.

In [**Part 4**](#section-part4), we showed how investigators might create multiple causal diagrams when the structure of a causal problem is ambiguous and illustrated the benefits of this approach through data simulation. We provided guidelines for reporting in scenarios where only cross-sectional data are available or when researchers have access to repeated measures of longitudinal data.

Although this discussion has focused on seven specific applications of causal diagrams, their applicability extends much further. The straightforward rules governing how variables become associated or disassociated through conditioning on nodes within basic structures enable the use of causal diagrams for addressing complex problems of time-varying confounding, for population restriction biases, and for measurement error (refer to @bulbulia2024swigstime; @bulbulia2024wierd). 

We hope this chapter will inspire environmental psychologists to deepen their understanding of causal inference and incorporate causal diagrams into their research practices. The methodologies for distinguishing causation from correlation are well-established; powerful tools for causal inference are accessible. There is no longer any justification for reporting associations and speculating about causes. It is within your reach to quantify magnitudes of causality conditional on assumptions encoded in your causal graphs. 


{{< pagebreak >}}

## Funding

This work is supported by a grant from the Templeton Religion Trust (TRT0418). JB received support from the Max Planck Institute for the Science of Human History. The funders had no role in preparing the manuscript or deciding to publish it.

## Contributions

DH proposed the chapter. JB developed the approach and wrote the first draft. Both authors contributed substantially to the final work.



<!-- 

### The Five Elementary Rules For Evaluating Confounding

To review, causal diagrams allow researchers to visualise and systematically identify potential confounders and strategies for adjusting for them. There are five basic graphical structures:

#### 1. **Causality Absent**  $A$ does not cause $B$: absent any common causes, there is no statistical association between them.

$$\xorxA$$ 

#### 2. **Causality Present**  $A$ causes $B$: absent conditioning that blocks them, $A$ and $B$ will be statistically associated.

$$\xtoxA$$

#### 3. **The Fork Structure** $A$ causes $B$ and $A$ causes $C$: absent conditioning on $A$, $B$ and $C$ will be statistically associated. Conditional on $A$, $B$ and $C$ will be independent.

$$\forkTINY$$

#### 4. **The Chain Structure**  $A$ causes $B$ and $B$ causes $C$: absent conditioning on $B$, $A$ and $C$ will be statistically associated. Conditioning on $B$, $A$ and $C$ will be independent. 

$$\chainTINY$$ 

#### 5. **A Collider Structure**  $A$ causes $C$ and $B$ causes $C$: absent conditioning on $C$, $A$ and $B$ will be statistically independant. Conditioning on $C$, $A$ and $B$ will be statistically associated. 

$$\immoralityTINY$$

From these five elementary structures, we discovered four rules that allow us to use these structures to evaluate confounding and its control:

#### 1. **The Fork Rule** 

When a common cause influences treatment and outcome, condition on the common cause to avoid bias.

#### 2. **The Chain Rule**

  (i)  For total effect estimates, avoid conditioning on mediators within the causal path.
  (ii) For mediation analysis, ensure potential confounders do not introduce bias. )(Note: mediation analysis is complex [@vanderweele2015; @vansteelandt2012; @bulbulia2023].)

#### 3. **The Collider Rule**

Conditioning on a common effect opens a path between the two variables that cause it.  

#### 4. **The Proxy Rule**

Conditioning on a descendant is a proxy for conditioning on its parent.  -->

{{< pagebreak >}}

## References

::: {#refs}
:::


{{< pagebreak >}}

## Appendix A: Glossary {#appendix-a}


This appendix provides a glossary of common terminology in causal inference.

**Acyclic**: a causal diagram cannot contain feedback loops. More precisely, no variable can be an ancestor or descendant of itself. If variables are repeatedly measured here, it is vital to index nodes by the relative timing of the nodes.

**Adjustment set**: a collection of variables we must either condition upon or deliberately avoid conditioning upon to obtain a consistent
causal estimate for the effect of interest [@pearl2009].

**Ancestor (parent)**: a node with a direct or indirect influence on others, positioned upstream in the causal chain.

**Arrow**: denotes a causal relationship linking nodes.

**Backdoor path**: a "backdoor path" between a treatment variable, $A$, and an outcome variable, $Y$, is a sequence of links in a causal diagram that starts with an arrow into $A$ and reaches $Y$ through common causes, introducing potential confounding bias such that statistical association does not reflect causality. To estimate the causal effect of $A$ on $Y$ without bias, these paths must be blocked by adjusting for confounders. The backdoor criterion guides the selection of variables for adjustment to ensure unbiased causal inference.

**Conditioning**: explicitly accounting for a variable in our statistical analysis to address the identification problem. In causal diagrams, we usually represent conditioning by drawing a box around a node of the conditioned variable, for example, $\boxed{L_{0}}\to A_{1} \to L_{2}$. We do not box exposures and outcomes because we assume they are included in a model by default. Depending on the setting, we may condition by regression stratification, inverse probability of treatment weighting, g-methods, doubly robust
machine learning algorithms, or other methods. We do not cover such methods in this tutorial; however, see @hernan2023.

**Counterfactual**: a hypothetical outcome that would have occurred for the same individuals under a different treatment condition than the one they experienced.

**Direct effect**: the portion of the total effect of a treatment on an outcome that is not mediated by other variables within the causal pathway.

**Collider**: a variable in a causal diagram at which two incoming paths meet head-to-head. For example, if $A \rightarrowred \boxed{L} \leftarrowred Y$, then $L$ is a collider. If we do not condition on a collider (or its descendants), the path between $A$ and $Y$ remains closed. Conditioning on a collider (or its descendants) will induce an association between $A$ and $Y$.

**Confounder**: a member of an adjustment set. Notice a variable is a "confounder" in relation to a specific adjustment set. "Confounder" is a
relative concept [@lash2020].

**d-separation**: in a causal diagram, a path is "blocked" or "d-separated" if a node along it interrupts causation. Two variables are
d-separated if all paths connecting them are blocked, making them conditionally independent. Conversely, unblocked paths result in
"d-connected" variables, implying potential dependence [@pearl1995].

**Descendant (child)**: a node directly or indirectly influenced by upstream nodes (parents).

**Effect-modifier**: a variable is an effect-modifier, or "effect-measure modifie" if its presence changes the magnitude or direction of the effect of an exposure or treatment on an outcome across the levels or values of this variable. In other words, the effect of the exposure is different at different levels of the effect modifier. 

**External validity**: the extent to which causal inferences can be generalised to other populations, settings, or times, also called "Target Validity."

**Identification problem**: the challenge of estimating the causal effect of a variable by adjusting for measured variables on units
in a study. Causal diagrams were developed to address the identification problem by application of the rules of d-separation to a causal diagram.

**Indirect effect (mediated effect)**: The portion of the total effect transmitted through a mediator variable.

**Internal validity**: the degree to which the design and conduct have prevented bias, ensuring that the causal relationship observed can be confidently attributed to the treatment and not to other factors.

**Instrumental variable**: an ancestor of the exposure but not of the outcome. An instrumental variable affects the outcome only through its effect on the exposure and not otherwise. Whereas conditioning on a variable causally associated with the outcome rather than with the exposure will generally increase modelling precision, we should refrain from conditioning on instrumental variables [@cinelli2022].  Second, when an instrumental variable is the descendant of an unmeasured confounder, we should generally condition the instrumental variable to provide a partial adjustment for a confounder.

**Mediator**: a variable that transmits the effect of the treatment variable on the outcome variable, part of the causal pathway between treatment and outcome.

**Modified Disjunctive Cause Criterion**: @vanderweele2019 recommends obtaining a maximally efficient adjustment, which he calls a "confounder set." A member of this set is any set of variables that can reduce or remove structural sources of bias. The strategy is as follows:

a.  Control for any variable that causes the exposure, the outcome, or
    both.
b.  Control for any proxy for an unmeasured variable that is a shared
    cause of the exposure and outcome.
c.  Define an instrumental variable as a variable associated with the
    exposure but does not influence the outcome independently, except
    through the exposure. Exclude any instrumental variable that is not
    a proxy for an unmeasured confounder from the confounder set
    [@vanderweele2019].

Note that the concept of a "confounder set"  is broader than that of an
"adjustment set"  Every adjustment set is a member of a confounder set.
Hence, the Modified Disjunctive Cause Criterion will eliminate bias when
the data permit. However, a confounder set includes variables that 
reduce bias in cases where confounding cannot be eliminated.

**Node**: characteristic or features of units in a population (a variable) represented on a causal diagram. In a causal diagram, nodes are drawn with reference to variable distributions for the target population.

**Randomisation**: the process of randomly assigning subjects to different treatments or control groups to eliminate selection bias in experimental studies.

**Reverse causation**: $\atoyassert$, but in reality $\ytoa$

**Statistical model:** a mathematical representation of the relationships between variables in which we quantify covariances and
their corresponding uncertainties in the data. Statistical models typically correspond to multiple causal structures [@pearl2018;
@vanderweele2022b; @hernan2023]. That is, the causes of such covariances cannot be identified without assumptions.

**Structural model:** defines assumptions about causal relationships. Causal diagrams graphically encode these assumptions [@hernan2023],
leaving out the assumption about whether the exposure and outcome are causally associated. We can only
compute causal effects outside of randomised experiments with structural models. A structural model is needed to interpret the statistical findings in causal terms.
Structural assumptions should be developed in consultation with experts. The role of structural assumptions when interpreting statistical results needs to be better understood across many human sciences and forms the motivation for my work here.

**Time-varying confounding:** occurs when a confounder that changes over time acts as a mediator or collider in the causal pathway between
exposure and outcome. Controlling for such a confounder can introduce bias. Not controlling for it can retain bias.

{{< pagebreak >}}


## Appendix B: Causal Consistency in observational settings {#appendix-b}

In observational research, there are typically multiple versions of the treatment. The theory of causal inference under multiple versions of treatment proves we can consistently estimate causal effects where the different versions of treatment are conditionally independent of the outcomes [@vanderweele2009, @vanderweele2009; @vanderweele2013; @vanderweele2018] 

Let $\coprod$ denote independence.
Where there are $K$ different versions of treatment $A$ and no confounding for $K$'s effect on $Y$ given measured confounders $L$ such that

$$
Y(k) \coprod K | L
$$

Then it can be proved that causal consistency follows. According to the theory of causal inference under multiple versions of treatment, the measured variable $A$ functions as a "coarsened indicator" for estimating the causal effect of the multiple versions of treatment $K$ on $Y(k)$ [@vanderweele2009; @vanderweele2013; @vanderweele2018].  

In the context of green spaces, let $A$ represent the general action of moving closer to any green space and $K$ represent the different versions of this treatment. For instance, $K$ could denote moving closer to different green spaces such as parks, forests, community gardens, or green spaces with varying amenities and features.

Here, the conditional independence implies that, given measured confounders $L$ (e.g. socioeconomic status, age, personal values), the type of green space one moves closer to ($K$) is independent of the outcomes $Y(k)$ (e.g. mental well-being under the $K$ conditions). In other words, the version of green space one chooses to live near does not affect the $K$ potential outcomes, provided the confounders $L$ are appropriately controlled for in our statistical models.

Put simply, strategies for confounding control and consistently estimating causal effects when multiple treatment versions converge. However, the quantities we estimate under multiple treatment versions might need clearer interpretations.  For example, we cannot readily determine which of the many treatment versions is most causally efficacious and which lack any causal effect or are harmful.  

{{< pagebreak >}}

### Appendix C Simulation of Cross-Sectional Data to Compute the Average Treatment Effect When Conditioning on a Mediator {#appendix-c}

This appendix outlines a simulation designed to demonstrate the potential pitfalls of conditioning on a mediator in cross-sectional analyses. The simulation examines the scenario where the effect of access to green space ($A$) on happiness ($Y$) is fully mediated by exercise ($L$). This setup aims to illustrate how incorrect assumptions about the role of a variable (mediator vs. confounder) can lead to misleading estimates of the Average Treatment Effect (ATE).

#### Methodology

**Data Generation**: we simulate a dataset for 1,000 individuals, where access to green space ($A$) influences exercise ($L$), which in turn affects happiness ($Y$$). The simulation is based on predefined parameters that establish $L$ as a mediator between $A$ and $Y$.

**Parameter Definitions**:

   - The probability of access to green space ($A$) is set at 0.5.
   - The effect of $A$ on $L$ (exercise) is given by $\beta = 2$.
   - The effect of $L$ on $Y$ (happiness) is given by $\delta = 1.5$.
   - Standard deviations for $L$ and $Y$ are set at 1 and 1.5, respectively.


**Model 1** (Correct Assumption): fits a linear regression model assuming $L$ as a mediator, including both $A$ and $L$ as regressors on $Y$. This model aligns with the data-generating process, and, by the rules of d-separation, induces mediator bias for the $A\to Y$ path.
  
**Model 2** (Incorrect Assumption): fits a linear regression model including only $A$ as a regressor on $Y$, omitting the mediator $L$. This model assesses the direct effect of A on Y without accounting for mediation.

**Analysis**: We compares the estimated effects of $A$ on $Y$ under each model specification.



```{r}
#| label: simulation_cross_sectional
#| tbl-cap: "Code for a simulation of a data generating process in which the effect of exercise (L) fully mediates the effect of greenspace (A) on happiness (Y)."
#| out-width: 80%
#| echo: true
#| eval: false

# load libraries
!require(kableExtra)){install.packages("kableExtra")} # tables
if(!require(gtsummary)){install.packages("gtsummary")} # tables

# simulation seed
set.seed(123) #  reproducibility

# define the parameters 
n = 1000 # Number of observations
p = 0.5  # Probability of A = 1 (access to greenspace)
alpha = 0 # Intercept for L (exercise)
beta = 2  # Effect of A on L 
gamma = 1 # Intercept for Y 
delta = 1.5 # Effect of L on Y
sigma_L = 1 # Standard deviation of L
sigma_Y = 1.5 # Standard deviation of Y

# simulate the data: fully mediated effect by L
A = rbinom(n, 1, p) # binary exposure variable
L = alpha + beta*A + rnorm(n, 0, sigma_L) # mediator L affect by A
Y = gamma + delta*L + rnorm(n, 0, sigma_Y) # Y affected only by L,

# make the data frame
data = data.frame(A = A, L = L, Y = Y)

# fit regression in which we control for L, a mediator
# (cross-sectional data is consistent with this model)
fit_1 <- lm( Y ~ A + L, data = data)

# fit regression in which L is assumed to be a mediator, not a confounder.
# (cross-sectional data is also consistent with this model)
fit_2 <- lm( Y ~ A, data = data)

# create gtsummary tables for each regression model
table1 <- gtsummary::tbl_regression(fit_1)
table2 <- gtsummary::tbl_regression(fit_2)

# merge the tables for comparison
table_comparison <- gtsummary::tbl_merge(
  list(table1, table2),
  tab_spanner = c("Model: Exercise assumed confounder", 
                  "Model: Exercise assumed to be a mediator")
)
# make latex table (for publication)
markdown_table_0 <- as_kable_extra(table_comparison, 
                                   format = "latex", 
                                   booktabs = TRUE)
# print latex table (note, you might prefer "markdown" or another format)                                
markdown_table_0
```

The following code is designed to estimate the Average Treatment Effect (ATE) using the `clarify` package in R, which is referenced here as [@greifer2023]. The procedure involves two steps: simulating coefficient distributions for regression models and then calculating the ATE based on these simulations. This process is applied to two distinct models to demonstrate the effects of including versus excluding a mediator variable in the analysis.


### Steps to Estimate the ATE

1. **Load the `clarify` Package**: this package provides functions to simulate regression coefficients and compute average marginal effects (AME), robustly facilitating the estimation of ATE.

2. **Set seed**: `set.seed(123)` ensures that the results of the simulations are reproducible, allowing for consistent outcomes across different code runs.

3. **Simulate the data distribution**:
   
   `sim_coefs_fit_1` and `sim_coefs_fit_2` are generated using the `sim` function from the `clarify` package, applied to two fitted models (`fit_1` and `fit_2`). These functions simulate the distribution of coefficients based on the specified models, capturing the uncertainty around the estimated parameters.

4. **Calculate ATE**:
  
  For both models, the `sim_ame` function calculates the ATE as the marginal risk difference (RD) when the treatment variable (`A`) is present (`A == 1`). This function uses the simulated coefficients to estimate the treatment effect across the simulated distributions, providing a comprehensive view of the ATE under each model.
  
  To streamline the output, the function is set to verbose mode off (`verbose = FALSE`).

5. **Results**:
  
  Summaries of these estimates (`summary_sim_est_fit_1` and `summary_sim_est_fit_2`) are obtained, providing detailed statistics including the estimated ATE and its 95% confidence intervals (CI).

6. **Presentation: report ATE and CIs**:
   
   Using the `glue` package, the ATE and its 95% CIs for both models are formatted into a string for easy reporting. This step transforms the statistical output into a more interpretable form, highlighting the estimated treatment effect and its precision.


```{r}
#| label: ate-sim-crosstwo
#| tbl-cap: "Code for calculating the average treatment effect as contrasts between simulated outcomes for the entire population."
#| echo: true
#| eval: false

# use `clarify` package to obtain ATE
if(!require(clarify)){install.packages("clarify")} # clarify package
# simulate fit 1 ATE
set.seed(123)
sim_coefs_fit_1 <- sim(fit_1)
sim_coefs_fit_2 <- sim(fit_2)

# marginal risk difference ATE, simulation-based: model 1 (L is a confounder)
sim_est_fit_1 <-
  sim_ame(
    sim_coefs_fit_1,
    var = "A",
    subset = A == 1,
    contrast = "RD",
    verbose = FALSE
  )
# marginal risk difference ATE, simulation-based: model 2 (L is a mediator)
sim_est_fit_2 <-
  sim_ame(
    sim_coefs_fit_2,
    var = "A",
    subset = A == 1,
    contrast = "RD",
    verbose = FALSE
  )
# obtain summaries
summary_sim_est_fit_1 <- summary(sim_est_fit_1, null = c(`RD` = 0))
summary_sim_est_fit_2 <- summary(sim_est_fit_2, null = c(`RD` = 0))

# reporting 
# ate for fit 1, with 95% CI
ATE_fit_1 <- glue::glue(
  "ATE =
                        {round(summary_sim_est_fit_1[3, 1], 2)},
                        CI = [{round(summary_sim_est_fit_1[3, 2], 2)},
                        {round(summary_sim_est_fit_1[3, 3], 2)}]"
)
# ate for fit 2, with 95% CI
ATE_fit_2 <-
  glue::glue(
    "ATE = {round(summary_sim_est_fit_2[3, 1], 2)},
                        CI = [{round(summary_sim_est_fit_2[3, 2], 2)},
                        {round(summary_sim_est_fit_2[3, 3], 2)}]"
  )
```



### Upshot of the Simulation and Analysis

- **Model 1 (L as a Confounder)**: this analysis assumes that `L` is a confounder in the relationship between the treatment (`A`) and the outcome (`Y`), and thus, it includes `L` in the model. The ATE estimated here reflects the effect of `A` while controlling for `L`.

- **Model 2 (L as a Mediator)**: in contrast, this analysis considers `L` to be a mediator, and the model either includes `L` explicitly in its estimation process or excludes it to examine the direct effect of `A` on `Y`. The approach to mediation analysis here is crucial as it influences the interpretation of the ATE.

By comparing the ATEs from both models, researchers can understand the effect of mediation (or the lack thereof) on the estimated treatment effect. This comparison sheds light on how assumptions about variable roles (confounder vs. mediator) can significantly alter causal inferences drawn from cross-sectional data.

**Wherever it is uncertain whether a variable is a confounder or a mediator, we suggest creating two causal diagrams and reporting both analyses.**

{{< pagebreak >}}

## Appendix D: Simulation of Different Confounding Control Strategies {#appendix-d}

This appendix outlines the methodology and results of a data simulation designed to compare different strategies for controlling confounding in the context of environmental psychology research. Specifically, the simulation examines the effect of access to open green spaces (treatment, $A_1$) on happiness (outcome, $Y_2$) while addressing the challenge of unmeasured confounding. The simulation incorporates baseline measures of exposure and outcome ($A_0$, $Y_0$), baseline confounders ($L_0$), and an unmeasured confounder ($U$) to evaluate the effectiveness of different analytical approaches.

###  Methodology

1.**Load Libraries `kableExtra`, `gtsummary`, and `grf`.**

1. **Target**: we simulate data for 10,000 individuals, including baseline exposure to green spaces ($A_0$), baseline happiness ($Y_0$), baseline confounders ($L_0$), and an unmeasured confounder ($U$). The simulation uses a logistic model for treatment assignment and a linear model for the continuous outcome, incorporating interactions to assess how baseline characteristics modify the treatment effect.

2. **Set seed and simulate the data distribution**:
  
  Treatment assignment coefficients: $\beta_{A0} = 0.25$, $\beta_{Y0} = 0.3$, $\beta_{L0} = 0.2$, and $\beta_{U} = 0.1$.
  Outcome model coefficients: $\delta_{A1} = 0.3$, $\delta_{Y0} = 0.9$, $\delta_{A0} = 0.1$, $\delta_{L0} = 0.3$, with an interaction effect ($\theta_{A0Y0L0} = 0.5$) indicating the combined influence of baseline exposure, outcome, and confounders on the follow-up outcome.

3. **Model comparison**:
   - **No control model**: estimates the effect of $A_1$ on $Y_2$ without controlling for any confounders.
   - **Standard covariate control model**: controls for baseline confounders ($L_0$) alongside treatment ($A_1$).
   - **Baseline exposure and outcome model**: extends the standard model by including baseline treatment and outcome ($A_0$, $Y_0$) and their interaction with $L_0$.

4. **Results**: each model's effectiveness in estimating the true treatment effect is assessed by comparing regression outputs. The simulation evaluates how well each model addresses the bias introduced by unmeasured confounding and the role of baseline characteristics in modifying treatment effects.

5. **Presentation**: the results are synthesised in a comparative table, formatted using the `kableExtra` {@zhu2021KableExtra] and `gtsummary` packages [@gtsummary2021], highlighting the estimated treatment effects and their statistical significance across models.

Overall, we use the simulation to illustrate the importance of incorporating baseline characteristics and their interactions to mitigate the influence of unmeasured confounding. 

Here is the simulation/model code:

```{r}
#| label: fig-codelg-appendix
#| echo: true
#| eval: false
library(kableExtra)
if(!require(kableExtra)){install.packages("kableExtra")} # causal forest
if(!require(gtsummary)){install.packages("gtsummary")} # causal forest
if(!require(grf)){install.packages("grf")} # causal forest

# r_texmf()eproducibility
set.seed(123) 

# set number of observations
n <- 10000 

# baseline covariates
U <- rnorm(n) # Unmeasured confounder
A_0 <- rbinom(n, 1, prob = plogis(U)) # Baseline exposure
Y_0 <- rnorm(n, mean = U, sd = 1) # Baseline outcome
L_0 <- rnorm(n, mean = U, sd = 1) # Baseline confounders

# coefficients for treatment assignment
beta_A0 = 0.25
beta_Y0 = 0.3
beta_L0 = 0.2
beta_U = 0.1

# simulate treatment assignment
A_1 <- rbinom(n, 1, prob = plogis(-0.5 + 
                                    beta_A0 * A_0 +
                                    beta_Y0 * Y_0 + 
                                    beta_L0 * L_0 + 
                                    beta_U * U))
# coefficients for continuous outcome
delta_A1 = 0.3
delta_Y0 = 0.9
delta_A0 = 0.1
delta_L0 = 0.3
theta_A0Y0L0 = 0.5 # Interaction effect between A_1 and L_0
delta_U = 0.05
# simulate continuous outcome including interaction
Y_2 <- rnorm(n,
             mean = 0 +
               delta_A1 * A_1 + 
               delta_Y0 * Y_0 + 
               delta_A0 * A_0 + 
               delta_L0 * L_0 + 
               theta_A0Y0L0 * Y_0 * 
               A_0 * L_0 + 
               delta_U * U,
             sd = .5)
# assemble data frame
data <- data.frame(Y_2, A_0, A_1, L_0, Y_0, U)

# model: no control
fit_no_control <- lm(Y_2 ~ A_1, data = data)

# model: standard covariate control
fit_standard <- lm(Y_2 ~ A_1 + L_0, data = data)

# model: interaction with baseline confounders, and baseline outcome and exposure
fit_interaction  <- lm(Y_2 ~ A_1 * (L_0 + A_0 + Y_0), data = data)

# create gtsummary tables for each regression model
tbl_fit_no_control<- tbl_regression(fit_no_control)  
tbl_fit_standard <- tbl_regression(fit_standard)
tbl_fit_interaction <- tbl_regression(fit_interaction)

# get only the treatment variable
tbl_list_modified <- lapply(list(
  tbl_fit_no_control,
  tbl_fit_standard,
  tbl_fit_interaction),
function(tbl) {
  tbl %>%
    modify_table_body(~ .x %>% dplyr::filter(variable == "A_1"))
})
# merge tables
table_comparison <- tbl_merge(
  tbls = tbl_list_modified,
  tab_spanner = c(
    "No Control",
    "Standard",
    "Interaction")
) |>
  modify_table_styling(
    column = c(p.value_1, p.value_2, p.value_3),
    hide = TRUE
  )
# latex table for publication
markdown_table <-
  as_kable_extra(table_comparison, format = "latex", booktabs = TRUE) |>
  kable_styling(latex_options = "scale_down")
print(markdown_table)
```

Next, in the following code, we calculate the Average Treatment Effect (ATE) using simulation-based approaches for two distinct models: one with standard covariate control and another incorporating interaction. This approach leverages the `clarify` package in R, which facilitates the simulation and interpretation of estimated coefficients from linear models to derive ATEs under different modelling assumptions [@greifer2023].

First, we use the `sim` function from the `clarify` package to generate simulated coefficient distributions for the standard model (`fit_standard`) and the interaction model (`fit_interaction`). This step is crucial for capturing the uncertainty in our estimates arising from sampling variability.

Next, we employ each model's `sim_ame` function to compute the average marginal effects (AME), focusing on the treatment variable (`A_1`). The calculation is done under the assumption that all individuals are treated (i.e., `A_1 == 1`), and we specify the contrast type as "RD" (Risk Difference) to directly obtain the ATE (Average Treatment Effect). The `sim_ame` function simulates the treatment effect across the distribution of simulated coefficients, providing a robust estimate of the ATE and its variability.

The summaries of these simulations (`summary_sim_est_fit_std` and `summary_sim_est_fit_int`) are then extracted to provide concise estimates of the ATE along with 95% confidence intervals (CIs) for both the standard and interaction models. This step is essential for understanding the magnitude and precision of the treatment effects estimated by the models.

Finally, we use the `glue` package to format these estimates into a human-readable form, presenting the ATE and its corresponding 95% CIs for each model. This presentation facilitates clear communication of the estimated treatment effects, allowing for direct comparison between the models and highlighting the effect of including baseline characteristics and their interactions on estimating the ATE [@hester2022GLUE].

This simulation-based approach to estimating the ATE underscores the importance of considering model complexity and the roles of confounders and mediators in causal inference analyses. By comparing the ATE estimates from different models, we can assess the sensitivity of our causal conclusions to various assumptions and modelling strategies.


```{r}
#| label: lst-atesimppendix
#| lst-cap: "Code."
#| out-width: 100%
#| tbl-cap: "Code for calculating the average treatment effect."
#| echo: true
#| eval: false

#| label: ate-sim-long
#| tbl-cap: "Code for calculating the average treatment effect."
#| echo: false
#| eval: true

# use `clarify` package to obtain ATE
if(!require(clarify)){install.packages("clarify")} # clarify package

# simulate fit 1 ATE
set.seed(123)
sim_coefs_fit_no_control<- sim(fit_no_control)  
sim_coefs_fit_std <- sim(fit_standard)
sim_coefs_fit_int <- sim(fit_interaction)

# marginal risk difference ATE, no controls
sim_est_fit_no_control <-
  sim_ame(
    sim_coefs_fit_no_control,
    var = "A_1",
    subset = A_1 == 1,
    contrast = "RD",
    verbose = FALSE
  )
# marginal risk difference ATE, simulation-based: model 1 (L is a confounder)
sim_est_fit_std <-
  sim_ame(
    sim_coefs_fit_std,
    var = "A_1",
    subset = A_1 == 1,
    contrast = "RD",
    verbose = FALSE
  )
# marginal risk difference ATE, simulation-based: model 2 (L is a mediator)
sim_est_fit_int <-
  sim_ame(
    sim_coefs_fit_int,
    var = "A_1",
    subset = A_1 == 1,
    contrast = "RD",
    verbose = FALSE
  )
# obtain summaries
summary_sim_coefs_fit_no_control <-
  summary(sim_est_fit_no_control, null = c(`RD` = 0))
summary_sim_est_fit_std <-
  summary(sim_est_fit_std, null = c(`RD` = 0))
summary_sim_est_fit_int <-
  summary(sim_est_fit_int, null = c(`RD` = 0))

# get coefficients for reporting
# ate for fit 1, with 95% CI
ATE_fit_no_control  <- glue::glue(
  "ATE = {round(summary_sim_coefs_fit_no_control[3, 1], 2)}, 
  CI = [{round(summary_sim_coefs_fit_no_control[3, 2], 2)},
  {round(summary_sim_coefs_fit_no_control[3, 3], 2)}]"
)
# ate for fit 2, with 95% CI
ATE_fit_std <- glue::glue(
  "ATE = {round(summary_sim_est_fit_std[3, 1], 2)}, 
  CI = [{round(summary_sim_est_fit_std[3, 2], 2)},
  {round(summary_sim_est_fit_std[3, 3], 2)}]"
)
# ate for fit 3, with 95% CI
ATE_fit_int <-
  glue::glue(
    "ATE = {round(summary_sim_est_fit_int[3, 1], 2)},
    CI = [{round(summary_sim_est_fit_int[3, 2], 2)},
    {round(summary_sim_est_fit_int[3, 3], 2)}]"
  )
# coefs they used in the manuscript
```


Using the `clarify` package, we infer the ATE for the standard model is `r ATE_fit_std`.

Using the `clarify` package, we infer the ATE for the model that conditions on the baseline exposure and baseline outcome to be:  `r ATE_fit_int`, which is close to the values supplied to the data-generating mechanism. 

**Take-home message:** 

The baseline exposure and baseline outcome are often the most important variables to include for confounding control. The baseline exposure also allows us to estimate an incident-exposure effect. For this reason, we should endeavour to obtain at least three waves of data such that these variables and other baseline confounders are included at time 0, the exposure is included at time 1, and the outcome is included at time 2. 

{{< pagebreak >}}


## Appendix E: Non-parametric Estimation of Average Treatment Effects Using Causal Forests {#appendix-causal-forests}

This appendix provides a practical example of estimating average treatment effects (ATE) using a non-parametric approach, specifically applying causal forests. Unlike traditional regression models, causal forests allow for estimating treatment effects without imposing strict assumptions about the form of the relationship between treatment, covariates, and outcomes. This flexibility makes them particularly useful for analysing complex datasets where the treatment effect may vary across observations.

#### Causal Forest Model Implementation

1. **Libraries**: the implementation begins with loading the necessary R libraries: `grf` for estimating conditional and average treatment effects using causal forests and `glue` for formatting the results for reporting.

2. 1. **Data generation**: the code assumes the presence of a data frame `data` generated from the previous code snippet containing the variables:
   - `A_1`: Treatment indicator.
   - `L_0`: A covariate.
   - `Y_2`: Outcome of interest.
   - `A_0` and `Y_0`: Baseline exposure and outcome, respectively. 
   
   Treatment (`W`) and outcome (`Y`) vectors are extracted from `data` alongside a matrix `X` that includes covariates and baseline characteristics.

3. **Causal Forest model**: a causal forest model is fitted using the `causal_forest` function from the `grf` package [@grf2024]. This function takes the covariate matrix `X`, the outcome vector `Y`, and the treatment vector `W` as inputs, and it returns a model object that can be used for further analysis.

4. **Average Treatment Effect estimation**: the `average_treatment_effect` function computes the ATE from the fitted causal forest model. This step is crucial as it quantifies the overall effect of the treatment across the population, adjusting for covariates included in the model.

5. **Reporting**: The estimated ATE and its standard error (se) are extracted and formatted for reporting using the `glue` package [@hester2022GLUE]. This facilitates clear communication of the results, showing the estimated effect size and its uncertainty.

#### Key Takeaways

First, causal forests offer a robust way to estimate treatment effects without making parametric solid assumptions. This approach is particularly advantageous in settings where the treatment effect may vary with covariates or across different subpopulations. 

Second, the model estimates the ATE as the difference in expected outcomes between treated and untreated units, averaged across the population. This estimate reflects the overall effect of the treatment, accounting for the distribution of covariates in the sample. 

Third, we find that the estimated ATE by the causal forest model converges to the actual value used in the data-generating process (assumed to be 0.3). This demonstrates the effectiveness of causal forests in uncovering the true treatment effect from complex data.

This example underscores the utility of semi-parametric and non-parametric methods, such as causal forests, in causal inference analyses.

```{r}
#| label: causal_forest
#| echo: true

# load causal forest library 
library(grf) # estimate conditional and average treatment effects
library(glue) # reporting 

#  'data' is our data frame with columns 'A_1' for treatment, 'L_0' for a covariate, and 'Y_2' for the outcome
#  we also have the baseline exposure 'A_0' and 'Y_0'
#  ensure W (treatment) and Y (outcome) are vectors
W <- as.matrix(data$A_1)  # Treatment
Y <- as.matrix(data$Y_2)  # Outcome
X <- as.matrix(data[, c("L_0", "A_0", "Y_0")])

# fit causal forest model 
fit_causal_forest <- causal_forest(X, Y, W)

# estimate the average treatment effect (ATE)
ate <- average_treatment_effect(fit_causal_forest)

# make data frame for reporting using "glue' 
ate<- data.frame(ate)

# obtain ate for report
ATE_fit_causal_forest <-
  glue::glue(
    "ATE = {round(ate[1, 1], 2)}, se = {round(ate[2, 1], 2)}"
  )
```

Causal forest estimates the average treatment effect as `r ATE_fit_causal_forest`. This approach converges to the true value supplied to the generating mechanism of 0.3





