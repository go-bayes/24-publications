---
title: 'Methods in Causal Inference Part 4: Confounding in Experiments'
abstract: |
  Confounding bias arises when a treatment and outcome share a common cause. In randomised controlled experiments (trials), treatment assignment is random, ostensibly eliminating confounding bias. Here, we use causal directed acyclic graphs (causal DAGs) to unveil eight structural sources of bias that nevertheless persist in these trials. This analysis highlights the crucial role of causal inference methods in the design and analysis of experiments, ensuring the validity of conclusions drawn from experimental data.
  
  **Keywords:** *Causal Inference*; *Experiments*; *DAGs*; *Evolution*; *Per Protocol Effect*; *Intention to Treat Effect*; *RCT*. 
author: 
  - name: Joseph A. Bulbulia
    affiliation: Victoria University of Wellington, New Zealand
    orcid: 0000-0002-5861-2056
    email: joseph.bulbulia@vuw.ac.nz
    corresponding: no
editor_options: 
  chunk_output_type: console
format:
  pdf:
    sanitise: true
    keep-tex: true
    link-citations: true
    colorlinks: true
    documentclass: article
    classoption: [single column]
    lof: false
    lot: false
    geometry:
      - top=30mm
      - left=25mm
      - heightrounded
      - headsep=22pt
      - headheight=11pt
      - footskip=33pt
      - ignorehead
      - ignorefoot
    template-partials: 
      - /Users/joseph/GIT/templates/quarto/title.tex
    header-includes:
      - \input{/Users/joseph/GIT/latex/latex-for-quarto.tex}
date: last-modified
execute:
  echo: false
  warning: false
  include: true
  eval: true
fontfamily: libertinus
bibliography: /Users/joseph/GIT/templates/bib/references.bib
csl: /Users/joseph/GIT/templates/csl/apa-7.csl
pdf-engine: lualatex
---


```{r}
#| label: load-libraries
#| echo: false
#| include: false
#| eval: true

## WARNING SET THIS PATH TO YOUR DATA ON YOUR SECURE MACHINE. 
# pull_path <-
#   fs::path_expand(
#     #'/Users/joseph/v-project\ Dropbox/Joseph\ Bulbulia/00Bulbulia\ Pubs/DATA/nzavs_refactor/nzavs_data_23'
#     '/Users/joseph/Library/CloudStorage/Dropbox-v-project/Joseph\ Bulbulia/00Bulbulia\ Pubs/DATA/nzavs-current/r-data/nzavs_data_qs'
#   )
# 


push_mods <-  fs::path_expand(
  '/Users/joseph/Library/CloudStorage/Dropbox-v-project/data/nzvs_mods/24/church-prosocial-v7'
)


#tinytext::tlmgr_update()

# WARNING:  COMMENT THIS OUT. JB DOES THIS FOR WORKING WITHOUT WIFI
#source('/Users/joseph/GIT/templates/functions/libs2.R')
# # WARNING:  COMMENT THIS OUT. JB DOES THIS FOR WORKING WITHOUT WIFI
# source('/Users/joseph/GIT/templates/functions/funs.R')

#ALERT: UNCOMMENT THIS AND DOWNLOAD THE FUNCTIONS FROM JB's GITHUB

# source(
#   'https://raw.githubusercontent.com/go-bayes/templates/main/functions/experimental_funs.R'
# )
# 
# source(
#   'https://raw.githubusercontent.com/go-bayes/templates/main/functions/funs.R'
# )

# check path:is this correct?  check so you know you are not overwriting other directors
#push_mods

# for latex graphs
# for making graphs
library('tinytex')
library('extrafont')
library('tidyverse')
library('kableExtra')
# Define a function to check if a package is installed and install it if necessary
# install_if_needed <- function(pkg) {
#   if (!require(pkg, character.only = TRUE)) {
#     install.packages(pkg, dependencies = TRUE)
#     library(pkg, character.only = TRUE)
#   }
# }

# # List of required packages
# packages <- c("tinytex", "extrafont", "tidyverse", "kableExtra")

# # Install and load the required packages
# lapply(packages, install_if_needed)
#devtools::install_github('go-bayes/margot')
#library(margot)
loadfonts(device = 'all')
```

## Introduction

"Does not randomisation, by its very nature, eliminate all systematic causes of treatment assignment and outcome?"

Yes.

"Does this mean that confounding bias is ruled out?"

No.

Assume large sample sizes to minimise random differences in variable distribution. Assume that the experimental trials are double-blind, with consistent treatment conditions across all arms, applied by meticulous investigators. Assume no chance event, other than randomisation. Finally, assume that the target population is not restricted in the sample population, ensuring that the experiments, if internally valid, will generalise. Assume no measurement error in the measures.

Nevertheless, biases can arise. Here, I use eight examples to illustrate common threats to valid causal inferences arising in experiments. Whereas certain risks arise from common flaws in experimental designs, such as post-randomisation selection criteria and post-randomisation covariate adjustment, hazards in estimating the 'per-protocol effect' of treatments do not arise from design errors. These typically require the use of methods for causal inference in 'real world' observational studies. The eight examples demonstrate the utility of causal directed acyclic graphs (causal DAGs) for easing the cognitive demand in diagnosing confounding bias in experimental designs. Overall, understanding how confounding occurs is crucial for experimental design, data analysis, and inference, demonstrating the utility of causal inference methods for diagnosing and addressing vulnerabilities in randomised experimental designs.

We begin by defining our terms. Note that supplementary materials **S1** provides a glossary of general terms used in causal inference.

### Terminology

- **Confounding bias**: Treatment and outcome are associated independently of causality or are disassociated in the presence of causality relative to the causal question at hand.

- **Intention-to-Treat Effect (or 'intent-to-treat effect')**: The effect of treatment assignment, analysed based on initial treatment assignment, reflecting real-world effectiveness but possibly obscuring mechanisms.

- **Per-protocol effect**: The effect of adherence to a randomly assigned treatment if adherence were perfect [@hernan2017per]. We have no guarantee that the intention-to-treat effect will be the same as the per-protocol effect. A safe assumption is that:

$$
\widehat{ATE}_{\text{target}}^{\text{Per-Protocol}} \ne \widehat{ATE}_{\text{target}}^{\text{Intention-to-Treat}}
$$

When evaluating evidence for causality, investigators should specify whether they are estimating an intention-to-treat or per-protocol effect. They should do this in addition to stating a causal contrast, effect measure, and target population, [@hernÃ¡n2004; @tripepi2007] and to evaluating sources of measurement error bias [@bulbulia2024wierd].


### Meaning of Symbols

We use the following conventions in our directed acyclic graphs:

- **Node**: A node or vertex represents characteristics or features of units within a population on a causal diagram -- that is, a 'variable'. In causal directed acyclic graphs, we draw nodes with respect to the *target population*, which is the population for whom investigators seek causal inferences [@suzuki2020]. A time-indexed node $X_t$ denotes relative chronology; $X_{\phi t}$ indicates assumed timing, possibly erroneous. 

- **Edge without an Arrow** ($\association$): Path of association, causality not asserted.

- **Red Edge without an Arrow** ($\associationred$): Confounding path: ignores arrows to clarify statistical dependencies. 

- **Arrow** ($\rightarrowNEW$): Denotes a causal relationship from the node at the base of the arrow (a parent) to the node at the tip of the arrow (a child). We typically refrain from drawing an arrow from treatment to outcome to avoid asserting a causal path from $A$ to $Y$ because the function of a causal directed acyclic graph is to evaluate whether causality can be identified for this path.

- **Red Arrow** ($\rightarrowred$): Path of non-causal association between the treatment and outcome. The path is associational and may run against arrows.

- **Dashed Arrow** ($\rightarrowdotted$): Denotes a true association between the treatment and outcome that becomes partially obscured when conditioning on a mediator, assuming $A$ causes $Y$.

- **Dashed Red Arrow** ($\rightarrowdottedred$): Highlights over-conditioning bias from conditioning on a mediator.

- **Boxed Variable** $\boxed{X}$: Conditioning or adjustment for $X$. 

- **Red-Boxed Variable** $\boxedred{X}$: Highlights the source of confounding bias from adjustment.

- **Dashed Circle** $\circledotted{X}$: Indicates no adjustment is made for a variable (implied for unmeasured confounders).

- **$\mathcal{R} \rightarrow A$**: Randomisation into a treatment condition.


### Review of d-separation for Causal Identification on a Graph


::: {#tbl-fiveelementary}

```{=latex}
\terminologydirectedgraph
```
The five elementary structures of causality from which all causal directed acyclic graphs can be built.
:::

Pearl demonstrated that causal dependencies could be evaluated by linking observable probability distributions to directed acyclic graphs [@pearl1995; @pearl2009a]. This means that, based on assumptions about causal structure, investigators could investigate strategies for identifying causal effects from the joint distributions of observed data.  The graphical rules that Pearl developed and proved are known as the rules of d-separation [@pearl1995], and are presented in @tbl-fiveelementary. 

The rules of d-separation are as follows:

\begin{enumerate}[a)]
     \item {\bf Fork rule} ($B \leftarrowNEW \boxed{A} \rightarrowNEW C$): $B$ and $C$ are independent when conditioning on $A$: ($B \coprod C \mid A$).
     \item {\bf Chain rule} ($A \rightarrowNEW \boxed{B} \rightarrowNEW C$): Conditioning on $B$ blocks the path between $A$ and $C$: ($A \coprod C \mid B$).
     \item {\bf Collider rule} ($A \rightarrowNEW \boxed{C} \leftarrowNEW B$): $A$ and $B$ are independent until conditioning on $C$, which introduces dependence: ($A \cancel{\coprod} B \mid C$). 
\end{enumerate}

The rules of d-separation give rise to the backdoor criterion which provides an identification algorithm conditional on the structural assumptions encoded in a causal directed acyclic graph [@pearl1995]. 

**Backdoor Adjustment**: In a causal directed acyclic graph, we say that a set of variables $L$ satisfies the backdoor adjustment theorem relative to the treatment $A$ and the outcome $Y$ if $L$ blocks every path between $A$ and $Y$ that contains an arrow pointing into $A$ (a backdoor path). Formally, $L$ must satisfy two conditions:

1. **No Path Condition**: No element of $L$ is a descendant of $A$.
2. **Blocking Condition**: $L$ blocks all backdoor paths from $A$ to $Y$.

If $L$ satisfies these conditions, we say the causal effect of $A$ on $Y$ is identified conditional on $\boxed{L}$ [@pearl2009a].


<!-- 
Consider @tbl-terminologygeneral $\mathcal{G}_1$: If we assume that $A$ and $B$ are not causally related, and further that they do not share common causes, then $A$ and $B$ will not be statistically related.

Consider @tbl-terminologygeneral $\mathcal{G}_2$: If we assume that $A$ and $B$ are causally related, that they do not share common causes or that their common causes have been accounted for, then $A$ and $B$ will be statistically related.

Consider @tbl-terminologygeneral $\mathcal{G}_3$: If we assume that $A$ causes $B$ and that $A$ causes $C$, then the rules of d-separation imply that we may condition on or 'control for' $A$ to consistently estimate the effect of $B$ on $C$.

Consider @tbl-terminologygeneral $\mathcal{G}_4$: If we assume that $A$ causes $B$ and that $B$ causes $C$, then the rules of d-separation imply that if we condition on $B$, the true causal effect of $A$ on $C$ will be obscured such that $A$ will be independent of $C$ despite being causally associated with $C$.

Finally, consider @tbl-terminologygeneral $\mathcal{G}_5$: If we assume that $A$ causes $C$ and that $B$ causes $C$, then the rules of d-separation imply that if we condition on $C$, the variables $A$ and $B$ will be associated, despite having no causal effect on each other.

If we assume that the variables encoded in the graph correspond to 'Structural Causal Models' then all causal relationships can be defined by the elementary structures presented in @tbl-terminologygeneral.

Now that we have clarified how causal directed graphs work, we may use them to clarify the first concept we consider confounding bias in randomised experiments. -->


## Eight Examples of Confounding Bias in Experiments

::: {#tbl-terminologyelconfoundersexperiments}
```{=latex}
\terminologyelconfoundersexperiments
```
Eight confounding biases in Randomised Controlled Trials.
:::


We use causal directed acyclic graphs to describe eight types of confounding bias in randomised controlled trials ('experiments'). We use the symbol $\mathcal{G}$ to denote a causal directed acyclic graph in the table. The first digit in the graph subscript indexes the example. The second digit in the graph subscript indexes the problem or the response to the problem. Specifically, if the subscript '1' is used, it refers to the graph associated with the problem; if '2' is used, it refers to the graph associated with the response. 

### Example 1: Post-treatment Adjustment Blocks Treatment Effect

@tbl-terminologyelconfoundersexperiments $\mathcal{G}_{1.1}$ illustrates the threat of confounding bias by conditioning on a post-treatment mediator [@montgomery2018, @mcelreath2020]. Imagine investigators are interested in whether the framing of an authority as religious or secular -- 'source framing' -- affects subjective ratings of confidence in the authority -- 'source credibility.' There are two conditions. A claim is presented from an authority. The content of the claim does not vary by condition. Participants are asked to rate the claim on a credibility scale. Next, imagine that the investigators decide they should control for religiosity. Furthermore, imagine there is a true effect of source framing. Finally, assume that the source framing not only affects source credibility but also affects religiosity. Perhaps viewing a religious authority makes religious people more religious. In this scenario, measuring religiosity following the experimental intervention will partially block the effect of the treatment on the outcome. It might make it appear that the treatment does not work for religious people, when in reality it works because it amplifies religiosity. (Note that in this graph we assume that $L_1$ occurs before $Y_2$, however, investigators may have measured $L_1$ after $Y_2$. Our time index pertains to the occurrence of the event, not to its measurement. This statement applies to all examples that follow.)
@tbl-terminologyelconfoundersexperiments $\mathcal{G}_{1.2}$ clarifies a response: do not control post-treatment variables, here the intermediary effects of 'religiosity'. If effect-modification by religiosity is scientifically interesting, measure religiosity before randomisation. Randomisation did not prevent confounding.

### Example 2: Post-treatment Adjustment Induces Collider Stratification Bias

@tbl-terminologyelconfoundersexperiments $\mathcal{G}_{2.1}$ illustrates the threat of confounding bias by conditioning on a post-treatment collider [@cole2010]. Imagine the same experiment as in Example 1 and the same conditioning strategy, where religiosity is measured following the treatment. We assume the treatment affects religiosity. However, in this example, religiosity has no causal effect on the outcome, source credibility. Finally, imagine an unmeasured variable affects both the mediator, religiosity ($L_1$), and the outcome, source credibility ($Y_2$). This unmeasured confounder might be religious education in childhood. In this scenario, conditioning on the post-treatment variable religiosity will open a backdoor path between the treatment and outcome, leading to an association in the absence of causation. Randomisation did not prevent confounding.

@tbl-terminologyelconfoundersexperiments $\mathcal{G}_{2.2}$ clarifies a response: do not control post-treatment variables.

The point that investigators should not condition on post-treatment variables can be illustrated with a common flaw in experimental designs: exclusion based on 'attention checks'. Consider that if an experimental condition affects attention and an unmeasured variable is a common cause of attention and the outcome, then selection on attention will induce confounding bias in a randomised experiment. For example, imagine that people are more attentive to the scientific authority design because science is interesting -- whether or not one is religious, yet religion is not interesting whether or not one is religious. Suppose further that an unmeasured 'altruistic disposition' affects both attention and ratings of source credibility. By selecting on attention, investigators may unwittingly destroy randomisation. If attention is a scientifically interesting effect modifier, it should be measured before random assignment to treatment.


### Example 3: Demographic Measures at End of Study Induce Collider Stratification Bias

@tbl-terminologyelconfoundersexperiments $\mathcal{G}_{3.1}$ illustrates the threat of confounding bias from adjusting for post-treatment variables, here, one affected by the treatment and outcome absent any unmeasured confounder. In our example, imagine both the treatment, source framing, and the outcome, source credibility, affect religiosity measured at the end of the study. Investigators measure religiosity at the end of the study and include this measure as a covariate. However, doing so induces collider bias such that if both the treatment and outcome are positively associated with religiosity, the collider, they will be negatively associated with each other. Conditioning on the collider risks the illusion of a negative experimental effect without causality.

@tbl-terminologyelconfoundersexperiments $\mathcal{G}_{3.2}$ clarifies a response: again, do not control post-treatment variables. Here, 'religiosity' is measured after the end of the study. If the scientific interest is in effect modification or obtaining statistical precision, measure covariates before randomisation.

### Example 4: Demographic Measures at End of Study Condition on a Collider That Opens a Backdoor Path

@tbl-terminologyelconfoundersexperiments $\mathcal{G}_{4.1}$ illustrates the threat of confounding bias by adjusting for post-treatment variables, here affected only by the treatment and an unmeasured cause of the outcome. Suppose source credibility affects religiosity (religious people are reminded of their faith), but there is no experimental effect of framing on credibility. Imagine further that there is an unmeasured common cause of the covariate religiosity and the outcome source credibility. This unmeasured confounder might be religious education in childhood. In this scenario, conditioning on the post-treatment variable religiosity will open a backdoor path between the treatment and outcome, leading to an association without causation. Again, we find that randomisation did not prevent confounding.

@tbl-terminologyelconfoundersexperiments $\mathcal{G}_{4.2}$ clarifies a response. Again, unless investigators can rule out an effect of treatment, they should not condition on a post-treatment covariate. The covariates of interest should be measured before randomisation.

### Example 5: Treatment Affects Attrition Biasing Measure of Outcome

@tbl-terminologyelconfoundersexperiments $\mathcal{G}_{5}$ Suppose that the experimental condition affects measurement error in self-reported source credibility $U_{\Delta Y}$. For example, suppose that source framing has no effect on credibility. However, those in the scientific authority condition are more likely to express credibility for science due to self-presentation bias. Likewise, perceiving the investigators to be irreligious, participants in the religious source framing condition might report less credibility for religious authorities than they secretly harbour. Directed measurement error from the treatment to the measurement error of the outcomes creates an association without true causality, which we denote by removing any arrow between the treatment $A$ and the true outcome $Y$. Note that the bias in this setting is not one of cnfounding bias. There is no common cause of treatment and outcome. Rather, the threat is from measurement error bias [refer to @bulbulia2024wierd]

@tbl-terminologyelconfoundersexperiments $\mathcal{G}_{5}$ suggests there is no easy solution to directed measurement error bias in this setting. If the magnitude of the measurement error bias were known, investigators could apply adjustments [@lash2009applying]. Additional experiments might be devised that are less prone to directed measurement error bias. Investigators might compute sensitivity analyses to examine how much measurement error bias would be required to explain away a result (refer to @linden2020EVALUE for a relatively easy-to-implement sensitivity analysis). The point we make here is that randomisation does not prevent bias arising from directed measurement error. Investigators must be vigilant. 


### Example 6: Per Protocol Effect Lost in Sustained Treatments Where Treatment Adherence Is Affected by a Measured Confounder

Setting aside self-inflicted injuries of post-treatment conditioning and directed measurement error, **randomisation recovers unbiased causal effect estimates for randomisation into treatment.**. Under perfect adherence, such estimates correspond to the causal effects of the treatments themselves. However, adherence is seldom perfect. The following examples reveal challenges for recovering per-protocol effects in settings where there is imperfect adherence. @tbl-terminologyelconfoundersexperiments $\mathcal{G}_{6-8}$ are adapted from @hernan2017per.

@tbl-terminologyelconfoundersexperiments $\mathcal{G}_{6}$ illustrates the threat for identifying the per-protocol effect in sustained treatments where treatment adherence is affected by a measured confounder. Consider a sequential experiment that investigates the effects of sustained adherence to yoga on psychological distress, measured at the end of the study. Suppose that inflexible people are less likely to adhere to the protocols set out in the experiment and therefore do not. Suppose that flexibility is measured by indicator $L$. If we do not condition on $L$, there is an open path from $A_1 \associationred L_0 \associationred U \associationred Y_2$. Although investigators may recover the effect of randomisation into treatment, the per-protocol effect is confounded.

@tbl-terminologyelconfoundersexperiments $\mathcal{G}_{6}$ also clarifies a response. Conditioning on $L_0$ and $L_1$ will block the backdoor path, leading to an unbiased per-protocol effect estimate.

### Example 7: Per protocol effect lost in sustained treatments where past treatments affect measured confounder of future treatment adherence


@tbl-terminologyelconfoundersexperiments $\mathcal{G}_{7}$ illustrates the threat for identifying the per-protocol effect in sustained treatments where past treatments affect measured confounder of future treatment adherence.  Suppose that yoga affects flexibility. We should condition on pre-treatment measures of flexibility to identify the per-protocol effect. However, conditioning on the post-treatment measure of flexibility, $\boxed{L_1}$ induces collider stratification bias. This path runs from $A_1 \associationred L_1 \associationred U \associationred Y_3$. However, if we do not condition on $L_1$ there is an open backdoor path from $A_1 \associationred U \associationred Y_3$.  We cannot estimate a per-protocol effect by conditioning strategies.  


@tbl-terminologyelconfoundersexperiments $\mathcal{G}_{7}$ suggests no easy remedy for obtaining valid causal inference in this setting.  However, in a sequential treatment with fixed strategies, in which there is sequential exchangeability -- or no unmeasured confounding at each time point -- valid estimators for the sequential treatments may be constructed (refer to @hernan2024WHATIF; @diaz2021nonparametric; @hoffman2023). Although we may naively obtain an intention-to-treat effect estimate without special methods, inferring an effect of doing yoga on well-being -- the per-protocol effect, requires special methods. Such methods are not yet routinely used in the human sciences [@bulbulia2024swigstime]. 


### Example 8: Per Protocol Effect Lost in Sustained Treatments Because Both Measured and Unmeasured Confounders Affect Treatment Adherence

@tbl-terminologyelconfoundersexperiments $\mathcal{G}_{8}$ illustrates the threat for identifying the per-protocol effect in sustained treatments with measured and unmeasured confounders. Suppose flexibility affects adherence, yoga affects flexibility, and an unmeasured variable, such as prejudice toward Eastern spiritual practices, affects adherence. We have no measures for this variable. There is unmeasured confounding.

If there were no effect of yoga on well-being except through flexibility, and furthermore if flexibility were not affected by the unmeasured antipathy toward Eastern spiritual practices, and further, if the effect of flexibility on yoga at each time point were conditionally independent of all future counterfactual data, both for the treatments and the outcomes, then it might be possible to construct special estimators that identify the per-protocol effect of yoga on well-being in the presence of unmeasured confounding that affects adherence (refer to @hernan2017per). These special estimators are quite different from the ANOVAs, regressions models, and multi-level regression models routinely deployed in experimental studies. However, if we seek to understand the effect of yoga on well-being and not the effect of random assignment to yoga on well-being the routine estimators will not work: we require special estimators [@hernan2024WHATIF; @hoffman2023; @diaz2023lmtp].

## Summary

The examples considered here do not exhaust all threats to causal inference in experiments. For example, I have not covered biases arising from sample attrition also known as right censoring bias (refer to @bulbulia2024wierd). However, I hope the eight examples presented persuade experimental investigators of the following:

First, there is no need to adjust for baseline confounders in a non-sequential randomised experiment. Although an unadjusted difference of means should be reported, Lin has shown that if a study is sufficiently powered, regression adjustment where the full set of treatments are interacted with baseline covariates may improve (and will not diminish) asymptotic precision [@lin2012regressexperiments]. In some settings, investigators will want to evaluate effect modification with strata of covariates at baseline. However, in sufficiently large samples, randomisation ensures balance.

Second, confounding biases can occur in randomised experiments even when randomisation succeeds. To evaluate such bias, we must first state whether our causal estimand is the intention-to-treat effect or the per-protocol effect. Randomisation recovers an unbiased estimate of the intention-to-treat effectâthat is, the effect of treatment assignment. Randomisation will only recover the per-protocol effect, the effect of following treatment, when those assigned to treatment adhere to their assignments.

Third, causal directed acyclic graphs are useful for clarifying sources of bias for both the intention-to-treat effect and the per-protocol effect. For the intention-to-treat effect, biases arise in two main ways: when investigators impose selection criteria on participants after randomisation (e.g., assessing treatment effects only in those who have followed protocols) or when investigators estimate treatment effects using covariates collected after randomisation. Both post-treatment selection and post-treatment conditioning are self-inflicted sources of confounding bias. The remedy is to not allow your design to compromise randomisation. For the per-protocol effect, randomisation cannot guarantee unbiased estimates. Obtaining consistent estimates for the per-protocol effect requires the same assumptions and methods that are required when estimating causal effects in observational studies.

Fourth, in a sequential randomised experiment, standard methods such as regression adjustment, statistical structural equation models, and multi-level models will often fail to yield unbiased estimands [@richardson2013; @young2014identification; @bulbulia2024swigstime]. Special estimators such as 'g-methods' [@hernan2024WHATIF] or targeted learning [@vanderlaan2018] may be necessary to recover per-protocol effects in sequential designs. The requirements for estimating per-protocol effects in experiments cannot be stated in isolation from the details of each study [@robins1986; @hernan2024WHATIF].


From these observations, we offer the following practical advice:

1. Ensure covariate data are collected before randomisation into treatments.
2. If attention is a relevant covariate, measure it before randomisation. Do not use 'attention checks' to select participants after randomisation into treatments.
3. If adjustment is used in a single-point treatment with baseline covariates, interact every level of treatment with the baseline covariates, following @lin2012regressexperiments.
4. For sequential treatments, collect data for adherence (where possible).
5. For sequential treatments, at each measurement interval, ensure covariate data collection for any variable that might affect adherence or that might be proxies for such variables, particularly if these variables, or proxies for these variables, might affect outcomes at the end of the study.
6. Do not infer per-protocol effects from the portion of the sample that followed experimental protocols. Such selection can lead to differences between the study population at the start and end, compromising external validity.
7. Where possible, report both the per-protocol effect and the intention-to-treat effect.


Describing the special methods for estimating per-protocol effects with multiple sequential treatments is beyond the scope of this commentary [@hernan2024WHATIF]. My aim has been to demonstrate that satisfying the assumptions for valid causal inferences in experiments is often more challenging than many experimental human scientists currently realise [refer to @montgomery2018].

For example, I was part of an international team that administered questionnaires about religious identification following randomisation in a cross-cultural study investigating the source-credibility of religious and scientific authority [@hoogeveen2022einstein]. This approach might have attenuated effect modification by religiosity. Causal inference methods have considerable potential to enhance the design, analysis, and interpretation of experimental research, including my own work.

{{< pagebreak >}}

## Acknowledgements

I am grateful to Charles Efferson and Ruth Mace for constructive feedback. 

Errors are my own.


## Conflict of Interest

The author declares no conflicts of interest


## Financial Support

This work is supported by a grant from the Templeton Religion Trust (TRT0418) and RSNZ Marsden 3721245, 20-UOA-123; RSNZ 19-UOO-090. I also received support from the Max Planck Institute for the Science of Human History. The Funders had no role in preparing the manuscript or deciding to publish it.


## Research Transparency and Reproducibility

No data were used in this manuscript.



{{< pagebreak >}}

## References

::: {#refs}
:::



