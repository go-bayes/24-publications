---
title: 'Causal Directed Acyclic Graphs (DAGs): A Practical Guide'
abstract: |
  Causal inference requires contrasting counterfactual states of the world under pre-specified interventions. Obtaining counterfactual contrasts from data relies on explicit assumptions and careful, multi-step workflows. Causal diagrams are powerful tools for clarifying whether and how the counterfactual contrasts we seek can be identified from data. Here, I explain how to use causal directed acyclic graphs (causal DAGs) to determine whether and how causal effects can be identified from 'real-world' non-experimental observational data. I offer practical tips for reporting and suggest ways to avoid common pitfalls.

  **KEYWORDS**: *Causal Inference*; *Culture*; *DAGs*; *Evolution*; *Human Sciences*; *Longitudinal*
author: 
  - name: Joseph A. Bulbulia
    affiliation: Victoria University of Wellington, New Zealand
    orcid: 0000-0002-5861-2056
    email: joseph.bulbulia@vuw.ac.nz
    corresponding: no
editor_options: 
  chunk_output_type: console
format:
  pdf:
    sanitise: true
    keep-tex: true
    link-citations: true
    colorlinks: true
    documentclass: article
    classoption: [single column]
    lof: false
    lot: false
    geometry:
      - top=30mm
      - left=25mm
      - heightrounded
      - headsep=22pt
      - headheight=11pt
      - footskip=33pt
      - ignorehead
      - ignorefoot
    template-partials: 
      - /Users/joseph/GIT/templates/quarto/title.tex
    header-includes:
      - \input{/Users/joseph/GIT/latex/latex-for-quarto.tex}
date: last-modified
execute:
  echo: false
  warning: false
  include: true
  eval: true
fontfamily: libertinus
bibliography: /Users/joseph/GIT/templates/bib/references.bib
csl: /Users/joseph/GIT/templates/csl/camb-a.csl
---
```{r}
#| label: load-libraries
#| echo: false
#| include: false
#| eval: true

## WARNING SET THIS PATH TO YOUR DATA ON YOUR SECURE MACHINE. 
# pull_path <-
#   fs::path_expand(
#     #'/Users/joseph/v-project\ Dropbox/Joseph\ Bulbulia/00Bulbulia\ Pubs/DATA/nzavs_refactor/nzavs_data_23'
#     '/Users/joseph/Library/CloudStorage/Dropbox-v-project/Joseph\ Bulbulia/00Bulbulia\ Pubs/DATA/nzavs-current/r-data/nzavs_data_qs'
#   )
# 


push_mods <-  fs::path_expand(
  '/Users/joseph/Library/CloudStorage/Dropbox-v-project/data/nzvs_mods/24/church-prosocial-v7'
)


#tinytext::tlmgr_update()

# WARNING:  COMMENT THIS OUT. JB DOES THIS FOR WORKING WITHOUT WIFI
#source('/Users/joseph/GIT/templates/functions/libs2.R')
# # WARNING:  COMMENT THIS OUT. JB DOES THIS FOR WORKING WITHOUT WIFI
# source('/Users/joseph/GIT/templates/functions/funs.R')

#ALERT: UNCOMMENT THIS AND DOWNLOAD THE FUNCTIONS FROM JB's GITHUB

# source(
#   'https://raw.githubusercontent.com/go-bayes/templates/main/functions/experimental_funs.R'
# )
# 
# source(
#   'https://raw.githubusercontent.com/go-bayes/templates/main/functions/funs.R'
# )

# check path:is this correct?  check so you know you are not overwriting other directors
#push_mods

# for latex graphs
# for making graphs
library('tinytex')
library('extrafont')
library('tidyverse')
library('kableExtra')
#devtools::install_github('go-bayes/margot')
library(margot)
loadfonts(device = 'all')
```


## Introduction {#id-sec-introduction}

Human research begins with two fundamental questions:

1. What do I want to know?
2. For which population does this knowledge generalise?

In the human sciences, our questions are typically causal. We aim to understand the effects of interventions on certain variables. However, many researchers report non-causal associations, collecting data, applying complex regressions, and reporting coefficients. We often speak of covariates as 'predicting' outcomes. Yet, even when our models predict well, it remains unclear how these predictions relate to the scientific questions that sparked our interest. Our predictions lack meaning and fail to address our core scientific questions.

Some say that association cannot imply causation. However, our experimental traditions reveal that when interventions are controlled and randomised, the coefficients we recover from statistical models can permit causal interpretations.

Despite familiarity with experimental protocols, many researchers struggle to emulate randomisation and control with non-experimental or 'real-world' data. Though we use terms such as 'control' and employ sophisticated adjustment strategies, such as multilevel modelling and structural equation models, our practices are not systematic. We often overlook that what we take as control can undermine our ability to consistently estimate causal effects [@montgomery2018]. Although the term 'crisis' is overused, the state of causal inference across many human sciences, including experimental sciences, has much headroom for improvement. 'Room for headroom' applies to poor experimental designs that unintentionally weaken causal claims [@hernan2017per; @bulbulia_2024_experiments]. Fortunately, recent decades have seen considerable progress in causal data science, commonly called 'causal inference.' The progress has transformed those areas of health science, economics, and computer science that have adopted it. Causal inference provides methods for obtaining valid causal inferences from data through careful, systematic workflows.

Within the workflows of causal inference, causal diagrams—or causal graphs—are powerful tools for evaluating whether and how causal effects can be identified from data. My purpose here is to explain where these tools fit within causal inference workflows and to illustrate their practical applications. I focus on causal directed acyclic graphs (DAGs) because they are relatively easy to use and clear for most applications. However, DAGs can be misused. I will also explain common pitfalls and how to avoid them.

In [Part 1](#id-sec-1), I review the conceptual foundations of causal inference. The basis of all causal inference lies in counterfactual contrasts. Although there are different philosophical approaches to counterfactual reasoning, they are largely similar in practice. The overview here builds on the Neyman-Rubin potential outcomes framework, extended for longitudinal data by epidemiologist James Robins. Although this is not the framework within which causal directed acyclic graphs were developed, the potential outcomes framework is easier to interpret. (I discuss Pearl's non-parametric structural equation approach in [Appendix D](#id-app-d)).

In [Part 2](#id-sec-2), I describe how causal directed acyclic graphs help identify causal effects. I outline five elementary graphical structures that encode all causal relations, forming the building blocks of all causal directed acyclic graphs. I then examine five rules that clarify whether and how investigators can identify causal effects from data.

In [Part 3](#id-sec-3), I apply causal directed acyclic graphs to practical problems, showing how repeated measures data collection can solve seven common identification issues. Timing is critical but not sufficient alone. I also use causal diagrams to explain the limitations of repeated-measures data collection for identifying causal effects, tempering enthusiasm for easy solutions. Indeed, I will review how many statistical structural equation models and sophisticated multi-level models are not well-calibrated for identifying causal effects.

In [Part 4](#id-sec-4), I offer practical suggestions for creating and reporting causal directed acyclic graphs in scientific research. These graphs represent investigator assumptions about causal (or structural) relationships in nature. These relationships cannot typically be derived from data alone and must be developed with scientific specialists. Where there is ambiguity or debate, investigators should report multiple causal diagrams and conduct distinct analyses for each.


## Part 1: Causal Inference as Counterfactual Data Science {#id-sec-1}

The first step in answering a causal question is to ask it [@hernán2016].

1. What causal quantity do I want to consistently estimate?
2. For which population does this knowledge generalise?

Causal diagrams come after we have stated a causal question and clarified the population for whom we hope to obtain valid causal inferences, the 'target population'. We begin by considering what is required to state a causal question and define a target population precisely.

#### The Fundamental Problem of Causal Inference: Missing Counterfactual Observations

To ask a causal question, we must consider the concept of causality itself. Consider an intervention, $A$, and its effect, $Y$. We say that $A$ causes $Y$ if altering $A$ would lead to a change in $Y$ [@hume1902; @lewis1973]. If altering $A$ would not change $Y$, we say that $A$ has no causal effect on $Y$.

In causal inference, we aim to quantitatively contrast the potential outcomes in response to different levels of a well-defined intervention. Commonly, we refer to such interventions as 'exposures' or 'treatments;' we refer to the possible effects of interventions as 'potential outcomes.'

Consider a binary treatment variable $A \in \{0,1\}$. For each unit $i$ in the set $\{1, 2, \ldots, n\}$, when $A_i$ is set to 0, the potential outcome under this condition is denoted $Y_i(0)$. Conversely, when $A_i$ is set to 1, the potential outcome is denoted $Y_i(1)$. We refer to the terms $Y_i(1)$ and $Y_i(0)$ as 'potential outcomes' because, until realised, the effects of interventions describe counterfactual states.

Suppose that each unit $i$ receives either $A_i = 1$ or $A_i = 0$. The corresponding outcomes are realised as $Y_i|A_i = 1$ or $Y_i|A_i = 0$. For now, we assume that each realised outcome under that intervention is equivalent to one of the potential outcomes required for a quantitative causal contrast, such that $[(Y_i(a)|A_i = a)] = (Y_i|A_i = a)$. Thus, when $A_i = 1$, $Y_i(1)|A_i = 1$ is observed. However, when $A_i = 1$, it follows that $Y_i(0)|A_i = 1$ is not observed:

$$
Y_i|A_i = 1 \implies Y_i(0)|A_i = 1~ \text{is counterfactual}
$$

Conversely:

$$
Y_i|A_i = 0 \implies Y_i(1)|A_i = 0~ \text{is counterfactual}
$$

We define $\delta_i$ as the individual causal effect for unit $i$ and express the individual causal effect as:

$$
\delta_i = Y_i(1) - Y_i(0)
$$

An individual causal effect is a contrast between treatments one of which is excluded by the other. That individual causal effects cannot be identified from observations is known as '*the fundamental problem of causal inference*' [@rubin1976; @holland1986].


#### Identifying Causal Effects Using Randomised Experiments

Although it is not typically feasible to compute individual causal effects, under certain assumptions, it may be possible to estimate *average* treatment effects, also called 'marginal effects'. We define an average treatment effect (ATE) as the difference between the expected or average outcomes under treatment and contrast conditions for a pre-specified population.

Consider a binary treatment, $A \in \{0,1\}$:

$$
\text{Average Treatment Effect} = \mathbb{E}[Y(1)] - \mathbb{E}[Y(0)]
$$

This is our pre-specified estimand for our target population. A challenge remains in computing these treatment-group averages, given that individual causal effects are unobservable. We can frame the problem by referring to the *full data* required to compute this estimand — that is, in terms of the complete counterfactual dataset where the missing potential outcomes, inherent in observational data, were somehow available. The text highlighted in red denotes inherently missing responses over the joint distribution of the full counterfactual dataset. We find that for each treatment condition, half the observations over the joint distribution of the counterfactual data are inherently unobserved.

$$
\text{Average Treatment Effect} = \left(\underbrace{\underbrace{\mathbb{E}[Y(1)|A = 1]}_{\text{observed for } A = 1} + \underbrace{\textcolor{red}{\mathbb{E}[Y(1)|A = 0]}}_{\textcolor{red}{\text{unobserved for } A = 0}}}_{\text{effect among treated}}\right) - \left(\underbrace{\underbrace{\mathbb{E}[Y(0)|A = 0]}_{\text{observed for } A = 0} + \underbrace{\textcolor{red}{\mathbb{E}[Y(0)|A = 1]}}_{\textcolor{red}{\text{unobserved for } A = 1}}}_{\text{effect among untreated}}\right)
$$

Randomisation allows investigators to recover the treatment group averages despite the inherently missing observations within these groups. We do not require the joint distribution over the full data (i.e., the counterfactual data) to obtain these averages. When investigators randomise units into treatment conditions, ensuring full adherence and a sufficiently large sample to rule out chance differences in group composition, we can generally attribute differences in treatment group averages to the treatment itself. Randomisation implies:

$$
\mathbb{E}[Y(0) | A = 1] = \mathbb{E}[Y(0) | A = 0]
$$

and

$$
\mathbb{E}[Y(1) | A = 1] = \mathbb{E}[Y(1) | A = 0]
$$

If we assume:

$$ 
\mathbb{E}[Y(1) | A = 1] = \mathbb{E}[Y | A = 1]
$$

and

$$
\mathbb{E}[Y(0) | A = 0] = \mathbb{E}[Y | A = 0]
$$

it follows that the average treatment effect of a randomised experiment can be computed:

$$
\text{Average Treatment Effect} = \widehat{\mathbb{E}}[Y | A = 1] - \widehat{\mathbb{E}}[Y | A = 0]
$$

There are four critical aspects of how ideally randomised experiments enable the estimation of average treatment effects worth highlighting.

First, we must specify a population for whom they seek to generalise their results. We refer to this population as the *target population*. If the study population differs from the target population in the distribution of covariates that interact with the treatment, we will have no guarantees our results will generalise (for discussions of sample/target population mismatch, refer to: @imai2008misunderstandings; @westreich2019target; @westreich2017; @pearl2022; @bareinboim2013general; @stuart2018generalizability; @webster2021directed).

Second, because the units in the study sample at randomisation may differ from the units in the study after randomisation, we must be careful to avoid biases that arise from sample/population mismatch over time. If there is sample attrition or non-response, the treatment effect we obtain for the sample may differ from the treatment effect in the target population.

Third, a randomised experiment recovers the causal effect of random treatment assignment, not of the treatment itself, which may differ if some participants do not adhere to their treatment (even if they remain in the study). The effect of randomised assignment is called the 'intent-to-treat effect' or equivalently the 'intention-to-treat effect'. The effect of perfect adherence is called the 'per-protocol effect' [@hernan2017per; @lash2020]. To obtain the per-protocol effect for randomised experiments, methods for causal inference in observational settings must be applied [@bulbulia_2024_experiments].

Fourth, I have presented the average treatment effect on the difference scale, that is, as a difference in average potential outcomes for the target population under two distinct levels of treatment. However, depending on the scientific question at hand, investigators may wish to estimate causal effects on the risk-ratio scale, the rate-ratio scale, the hazard-ratio scale, or another scale. Where there are interactions such that treatment effects vary across different strata of the population, an estimate of the causal effect on the risk difference scale will differ in at least one stratum to be compared from the estimate on the risk ratio scale [@greenland2003quantifying; @vanderweele2012]. The sensitivity of treatment effects in the presence of interactions to the scale of contrast underscores the importance of pre-specifying a scale for the causal contrast investigators hope to obtain.

Fifth, investigators may unintentionally spoil randomisation by adjusting for indicators that might be affected by the treatment, outcome, or both, by excluding participants using attention checks, by collecting covariate data that might be affected by the experimental conditions, by failing to account for non-response and loss-to-follow-up, and by committing any number of other self-inflicted injuries. Unfortunately, such practices are widespread [@montgomery2018]. Notably, causal graphical methods are useful for describing causal identification in experiments (refer to @hernan2017per), a topic we consider elsewhere [@bulbulia_2024_experiments].

In observational studies, investigators might wish to describe the target population of interest as a restriction of the study sample population. For example, investigators might wish to estimate the average treatment effect only in the population that received the treatment. This treatment effect is sometimes called the average treatment effect in the treated (ATT) and may be expressed as:

$$
\text{Average Treatment Effect in the Treated} = \mathbb{E}[Y(1) - Y(0) \mid A = 1]
$$

Consider that if investigators are interested in the average treatment effect in the treated, counterfactual comparisons are deliberately restricted to the sample population that was treated. That is, the investigators will seek to obtain the average of the missing counterfactual outcomes for the treated population were they not treated, without necessarily obtaining the counterfactual outcomes for the untreated population were they treated. This difference in focus may imply different assumptions and analytic workflows. Appendix B describes an example for which the assumptions required to estimate the average treatment effect may be preferred. In what follows, we will use the term ATE as a placeholder to mean the average treatment effect, or equivalently the `marginal effect', for a target population on a pre-specified scale of causal contrast.

Setting aside the important detail that the 'average treatment effect' requires considerable care in its specification, it is worth pausing to marvel at how an ideally conducted randomised controlled experiment provides a means for identifying inherently unobservable counterfactuals. It does so by using a Sherlock-Holmes method of inference by elimination of confounders, which randomisation balances across treatments.

When experimenters observe a difference in average treatment effects, and all else goes right, they may infer that the distribution of potential outcomes differs by treatment because randomisation exhausts every other explanation except that of the treatment. They are entitled to this inference because randomisation balances the distribution of potential confounders across the treatment groups to be compared.

However, we lack guarantees for balance in the confounders outside of randomised experiments. For this reason, we should prefer developing sound randomised experiments for addressing every causal question that experiments can address. Unfortunately, randomised experiments cannot address many scientifically important questions. This bitter constraint is familiar to evolutionary human scientists. We typically confront 'What if?' questions that are rooted in the unidirectional nature of human history.
However, understanding how randomisation obtains the missing counterfactual outcomes that we require to consistently estimate average treatment effects clarifies the tasks of causal inference in non-experimental settings [@hernán2008a; @hernán2006; @hernán2022].

Next, we examine these identification assumptions in greater detail because using causal diagrams without understanding these assumptions is unsafe.

### Fundamental Assumptions Required for Causal Inference in the Potential Outcomes Framework

There are three fundamental identification assumptions that must be satisfied to consistently estimate causal effects with data. These assumptions are typically satisfied in randomised controlled trials but not in real-world studies where randomised treatment assignment is absent.

#### Assumption 1: Causal Consistency

We satisfy the causal consistency assumption when, for each unit $i$ in the set $\{1, 2, \ldots, n\}$, the observed outcome corresponds to one of the specific counterfactual outcomes to be compared such that:

$$
Y_i^{observed}|A_i = 
\begin{cases} 
Y_i(a^*) & \text{if } A_i = a^* \\
Y_i(a) & \text{if } A_i = a
\end{cases}
$$

The causal consistency assumption implies that the observed outcome at a specific treatment level equates to the counterfactual outcome for that individual at the observed treatment level. Although it seems straightforward, treatment conditions vary, and treatment heterogeneity poses considerable challenges to satisfying this assumption. Refer to [Appendix C](#id-app-c) for further discussion on how investigators may satisfy causal consistency in real-world settings.

#### Assumption 2: Positivity

We satisfy the positivity assumption if there is a non-zero probability of receiving each treatment level for every combination of covariates that occurs in the population. Where $A$ is the treatment and $L$ is a vector of covariates sufficient for ensuring no unmeasured confounding, we say positivity is achieved if:

$$
0 < Pr(A = a | L = l) < 1, \quad \text{for all } a, l \text{ with } Pr(L = l) > 0
$$

There are two types of positivity violation:

1. **Random non-positivity**: When a treatment is theoretically possible but specific treatment levels are not represented in the data, random non-positivity is the only identifiability assumption verifiable with data.
2. **Deterministic non-positivity**: When the treatment is implausible by nature, such as a hysterectomy in biological males.

Satisfying the positivity assumption can present considerable data challenges [@westreich2010; @bulbulia2023a]. For instance, if we wanted to estimate a one-year causal effect of weekly religious service attendance on charitable donations, controlling for baseline attendance, and the natural transition rate to weekly service attendance is low, the effective sample size for the treatment condition may be insufficient. Where the positivity assumption is violated, causal diagrams will be of limited utility because observations in the data do not support valid causal inferences. ([Appendix B](#id-app-b) presents a worked example illustrating this difficulty in a cultural evolutionary context.)

#### Assumption 3: Conditional Exchangeability (also 'No Unmeasured Confounding', 'Conditional Ignorability', 'd-separation')

We satisfy the conditional exchangeability assumption if the treatment groups are conditionally balanced in the variables that could affect the potential outcomes. In experimental designs, random assignment facilitates this assumption. In observational studies, more effort is required to control for any covariate that could account for observed correlations between $A$ and $Y$ without a causal effect of $A$ on $Y$.

Let $\coprod$ denote independence, and let $L$ denote the set of covariates necessary to ensure this conditional independence. Conditional exchangeability is satisfied when:

$$
Y(a) \coprod A | L \quad \text{or equivalently} \quad A \coprod Y(a) | L
$$

If we assume that positivity and consistency assumptions also, we may compute the average treatment effect (ATE) on the difference scale:

$$
\text{Average Treatment Effect} = \mathbb{E}[Y(1) | L] - \mathbb{E}[Y(0) | L]
$$

In randomised controlled experiments, exchangeability is unconditional. Although adjusting by interacting the treatment with pre-treatment variables may improve efficiency and diminish threats to randomisation from chance imbalances, it is confusing to think of such an adjustment as 'control.'

In real-world observational studies, where measured confounders are sufficient to ensure conditional exchangeability, we obtain estimates for the average treatment effect by conditioning on the densities of measured confounders by treatment group. Where $A = a$ and $A = a^*$ are the treatment levels we seek to contrast:

$$
\widehat{\text{ATE}} =  \sum_l \big( \mathbb{E}[Y(a^*) \mid L] - \mathbb{E}[Y(a) \mid L] \big) \times Pr(L)
$$

By causal consistency, we obtain:

$$
\widehat{\text{ATE}} =  \sum_l \big( \mathbb{E}[Y \mid A = a^*, L] - \mathbb{E}[Y \mid A = a, L] \big) \times Pr(L)
$$

For continuous covariates $L$, we have:

$$
\widehat{\text{ATE}} = \int \big( \mathbb{E}[Y \mid A = a^*, L] - \mathbb{E}[Y \mid A = a, L] \big) dP(L)
$$

The primary function of a causal directed acyclic graph is to identify sources of bias that may lead to an association between an exposure and outcome in the absence of causation. These graphs visually encode features of a causal order necessary to evaluate the assumptions of conditional exchangeability—also called 'no-unmeasured confounding', 'ignorability', and 'd-separation' (explained below). Although causal directed acyclic graphs can also be useful for addressing broader threats and opportunities for causal inferences, it is important to understand that causal directed acyclic graphs are designed to evaluate the assumptions of conditional exchangeability.

Finally, without randomisation, we cannot fully ensure the no-unmeasured confounding assumption necessary to recover the missing counterfactuals required to consistently estimate causal effects from data [@stuart2015; @greifer2023]. Because unmeasured confounding is almost always a concern, causal data science workflows typically include sensitivity analyses to determine how much unmeasured confounding would be required to compromise a study’s findings [@vanderweele2019].

### Summary of Part 1

Causal data science is distinct from ordinary data science. The initial step involves formulating a precise causal question that clearly identifies the exposure, outcome, and population of interest. We must then satisfy the three fundamental assumptions required for causal inference, implicit in the ideal of a randomised experiment:

- **Causal consistency**: Outcomes at a specific exposure level must align with their counterfactual counterparts.
- **Positivity**: Each exposure level must have a non-zero probability across all covariates.
- **Conditional exchangeability**: There should be no unmeasured confounding, meaning treatment assignment is 'ignorable' conditional on measured confounders.


{{< pagebreak >}}



## Part 2: How Causal Directed Acyclic Graphs Clarify the Conditional Exchangeability Assumption {#id-sec-2}

We introduce causal directed acyclic graphs by describing the meaning of our symbols.


### Variable Naming Conventions

::: {#tbl-terminology}
```{=latex}
\terminologylocalconventionssimple
```
Variable naming conventions
:::
- **$X$**: Denotes a random variable without reference to its role.

- **$A$**: Denotes the 'treatment' or 'exposure'—a random variable. This is the variable for which we seek to understand the effect of intervening on it. It is the 'cause.'

- **$A=a$**: Denotes a fixed 'treatment' or 'exposure.' The random variable $A$ is set to level $A=a$.

- **$Y$**: Denotes the outcome or response of an intervention. It is the 'effect.'

- **$Y(a)$**: Denotes the counterfactual or potential state of $Y$ in response to setting the level of the treatment to a specific level, $A=a$. The outcome $Y$ as it would be observed when, perhaps contrary to fact, treatment $A$ is set to level $A=a$. Different conventions exist for expressing a potential or counterfactual outcome, such as $Y^a$, $Y_a$.

- **$L$**: Denotes a measured confounder or set of confounders. This set, if conditioned upon, ensures that any differences between the potential outcomes under different levels of the treatment are the result of the treatment and not the result of a common cause of the treatment and the outcome. Mathematically, we write this independence:

$$
Y(a) \coprod A \mid L
$$

- **$U$**: Denotes an unmeasured confounder or confounders. $U$ is a variable or set of variables that may affect both the treatment and the outcome, leading to an association in the absence of causality, even after conditioning on measured covariates:

$$
Y(a) \cancel{\coprod} A \mid L \quad \text{[because of unmeasured } U]
$$

- **$F$**: Denotes a modifier of the treatment effect. $F$ alters the magnitude or direction of the effect of treatment $A$ on an outcome $Y$.

- **$M$**: Denotes a mediator, a variable that transmits the effect of treatment $A$ on an outcome $Y$.

- **$\bar{X}$**: Denotes a sequence of variables, for example, a sequence of treatments.

- **$\mathcal{R}$**: Denotes a randomisation to treatment condition.

- **$\mathcal{G}$**: Denotes a graph, here, a causal directed acyclic graph.

Note that investigators use a variety of different symbols. There is no unique right way to create a causal directed acyclic graph, except that meaning must be clear and the graph must be capable of identifying relationships of conditional and unconditional independence between the treatment and outcome. Although directed acyclic graphs are accessible tools, general graphical models such as 'Single World Intervention Graphs,' which allow for the explicit representation of counterfactual dependencies, may be preferable for investigators to estimate causal effects under multiple interventions [@richardson2013; @bulbulia2024swigstime].



###  Convention We Use to Draw Causal Directed Acyclic Graphs

The conventions we use to describe components of our causal graphs are given in @tbl-general.

::: {#tbl-general}
```{=latex}
\terminologygeneraldags
```
Nodes, Edges, Conditioning Conventions.
:::

- **Node**: a node or vertex represents characteristics or features of units within a population on a causal diagram -- that is a 'variable.' In causal directed acyclic graphs, we draw nodes with respect to the *target population*, which is the population for whom investigators seek causal inferences [@suzuki2020]. Time-indexed node:  $X_t$ denotes relative chronology; $X_{\customphi{t}}$ is our convention for indicating that timing is assumed, perhaps erroneously.  

- **Edge without an Arrow** ($\association$): path of association, causality not asserted.

- **Red Edge without an Arrow** ($\associationred$): confounding path: ignores arrows to clarify statistical dependencies. 

- **Arrow** ($\rightarrowNEW$): denotes causal relationship from the node at the base of the arrow (a parent) to the node at the tip of the arrow (a child). We typically refrain from drawing an arrow from treatment to outcome to avoid asserting a causal path from $A$ to $Y$ because the function of a causal directed acyclic graph is to evaluate whether causality can be identified for this path.

- **Red Arrow** ($\rightarrowred$): path of non-causal association between the treatment and outcome. Path is associational and may run against arrows.

- **Dashed Arrow** ($\rightarrowdotted$): denotes a true association between the treatment and outcome that becomes partially obscured when conditioning on a mediator, assuming $A$ causes $Y$.

- **Dashed Red Arrow** ($\rightarrowdottedred$): highlights over-conditioning bias from conditioning on a mediator.

-  **Open Blue Arrow** ($\rightarrowblue$): Highlights effect modification, occurring when the treatment effect levels vary within covariate levels. We do not assess the causal effect of the effect modifier on the outcome, recognising that intervening on the effect modifier may be incoherent. This is an off-label convention we use to clarify our interest in effect modification within strata of a covariate when there is a true treatment effect. However, it is possible to replace these open blue arrows with ordinary nodes and explain that the edges are drawn not for identification but for evaluating generalisations [see @bulbulia2024swigstime].

- **Boxed Variable** $\boxed{X}$: conditioning or adjustment for $X$. 

- **Red-Boxed Variable** $\boxedred{X}$: highlights the source of confounding bias from adjustment.

- **Dashed Circle** $\circledotted{X}$: no adjustment is made for a variable (implied for unmeasured confounders.)

- **$\mathbf{\mathcal{R}}$** randomisation, for example, randomisation into treatment: $\mathcal{R} \rightarrow A$.

- **Presenting Temporal Order**: Causal directed acyclic graphs must be — as truth in advertising implies— *acyclic.* Directed edges or arrows define ancestral relations. No descendant node can cause an ancestor node. Therefore causal diagrams are, by default, sequentially ordered.

Nevertheless, to make our causal graphs more readable, we adopt the following conventions:

1. The layout of a causal diagram is structured from left to right to reflect the assumed sequence of causality as it unfolds.
2. We often index our nodes using $X_t$ to indicate their relative timing and chronological order, where $t$ represents the time point or sequence in the timeline of events.
3. Where temporal order is uncertain or unknown, we use the notation $X_{\phi t}$ to propose a temporal order that is uncertain.

Typically, the timing of unmeasured confounders is unknown, except that they occur before the treatments of interest; hence, we place confounders to the left of the treatments and outcomes they are assumed to affect, but without any time indexing.

Again, temporal order is implied by the relationship of nodes and edges. However, explicitly representing the order in the layout of one's causal graph often makes it easier to evaluate, and the convention representing uncertainty is useful, particularly when the data do not ensure the relative timing of the occurrence of the variable in a causal graph.

More generally, investigators use various conventions to convey causal structures on graphs. Whichever convention we adopt must be clear.

Finally, note that all nodes and paths on causal graphs—including the absence of nodes and paths—are asserted. Constructing causal diagrams requires expert judgment of the scientific system under investigation. It is a great power given to those who construct causal graphs, and with great power comes great responsibility to be transparent. When investigators are unclear or there is debate about which graphical model fits reality, they should present multiple causal graphs. Where identification is possible, they should perform and report multiple analyses.

<!-- ### Mathematical Terminology

The bottom panel of @tbl-general illustrates the mathematical notation we use to describe statistical and causal dependencies in the distributions of variables within a population.

- **Independence $\coprod$**: denotes independence. Depending on the context, the meaning of 'independence' may be statistical, causal, or both. For example, $A \coprod Y(a)$ indicates that the treatment assignment $A$ is independent of the potential outcomes $Y(a)$.

Note that it is the distributions of **counterfactual outcomes** that must be independent of the treatments. The distributions of observed outcomes will depend on treatments wherever the treatments are causally efficacious.

- **Dependence $\cancel{\coprod}$**: denotes dependence. Again, depending on the context, the meaning of 'independence' may be statistical, causal, or both. For example, $A \cancel{\coprod} Y(a)$ suggests that the treatment assignment $A$ is related to the potential outcomes $Y(a)$, which could introduce bias into causal estimates.

- **Conditioning $|$**: denotes conditioning on a variable. We can express both conditional independence and conditional dependence:

- **Conditional Independence ($A \coprod Y(a) \mid L$):** indicates that once we account for a set of variables $L$, the treatment $A$ and the potential outcomes $Y(a)$ are independent.
- **Conditional Dependence ($A \cancel{\coprod} Y(a) \mid L$):** indicates that the treatment $A$ and the potential outcomes $Y(a)$ are not independent after conditioning on $L$, or perhaps because we have conditioned on $L$. -->



### How Causal Directed Acyclic Graphs Relate Observations to Counterfactuals

#### Ancestral Relations in Directed Acyclic Graphs

We define the relation of 'parent' and 'child' on a directed acyclic graph as follows:

1. Node $A$ is a **parent** of node $B$ if there is a directed edge from $A$ to $B$, denoted $A \rightarrow B$.
2. Node $B$ is a **child** of node $A$ if there is a directed edge from $A$ to $B$, denoted $A \rightarrow B$.

It follows that a parent and child are **adjacent nodes** connected by a directed edge.

We denote the set of all parents of a node $B$ as $\text{pa}(B)$.

In a directed acyclic graph, the directed edge $A \rightarrow B$ indicates a statistical dependency where $A$ may provide information about $B$. In a causal directed acyclic graph, the directed edge $A \rightarrow B$ is interpreted as a causal relationship, meaning $A$ is a direct cause of $B$.


We further define the relations of **ancestor** and **descendant** on a directed acyclic graph as follows:

1. Node $A$ is an **ancestor** of node $C$ if there exists a directed path from $A$ to $C$. Formally, $A$ is an ancestor of $C$ if there exists a sequence of adjacent nodes $(A, B_1, B_2, \ldots, B_k, C)$ such that $A \rightarrow B_1 \rightarrow B_2 \rightarrow \cdots \rightarrow B_k \rightarrow C$.
2. Node $C$ is a **descendant** of node $A$ if there exists a directed path from $A$ to $C$. Formally, $C$ is a descendant of $A$ if there exists a sequence of adjacent nodes $(A, B_1, B_2, \ldots, B_k, C)$ such that $A \rightarrow B_1 \rightarrow B_2 \rightarrow \cdots \rightarrow B_k \rightarrow C$.

It follows that a node can have multiple ancestors and descendants.

#### Markov Factorisation and the Local Markov Assumption

@pearl2009a p 52 asks us to imagine the following. Suppose we have a distribution $P$ defined on n discrete variables, $X_1, X_2, \dots, X_n$. By the chain rule, the joint distribution for variables $X_1, X_2, \dots, X_n$ on a graph can be decomposed into the product of $n$ conditional distributions such that we may obtain the following factorisation:

$$
\Pr(x_1, \dots, x_n) = \prod_{j=1}^n \Pr(x_j \mid x_1, \dots, x_{j-1})
$$

We translate nodes and edges on a graph into a set of conditional independences that a graph implies over statistical distributions.

According to **the local Markov assumption**, given its parents in a directed acyclic graph, a node is said to be independent of all its non-descendants. Under this assumption, we obtain what Pearl calls Bayesian network factorisation, such that:

$$
\Pr(x_j \mid x_1, \dots, x_{j-1}) = \Pr(x_j \mid \text{pa}_j)
$$

This factorisation greatly simplifies the calculation of the joint distributions encoded in the directed acyclic graph (causal or non-causal) by reducing complex factorisations of the conditional distributions in $\mathcal{P}$ to simpler conditional distributions in the set $\text{PA}_j$, represented in the structural elements of a directed acyclic graph [@lauritzen1990; @pearl1988; @pearl1995; @pearl2009a].

#### Minimality Assumption

The minimality assumption combines (a) the local Markov assumption with (b) the assumption that adjacent nodes on the graph are dependent. This is needed for causal directed acyclic graphs because the local Markov assumption permits that adjacent nodes may be independent [@neal2020introduction].

#### Causal Edges Assumption

The **causal edges assumption** states that every parent is a direct cause of their children. Given minimalism, the causal edges assumption allows us to read causal dependence in directed acyclic graphs. In Pearl's formalism, we use non-parametric structural equations to evaluate causal assumptions using statistical distributions (refer to Appendix E; @neal2020introduction).

#### Compatibility Assumption

The compatibility assumption ensures that the joint distribution of variables aligns with the conditional independencies implied by the causal graph. This assumption requires that the probabilistic model conforms to the graph's structural assumptions. Demonstrating compatibility directly from data is challenging, as it involves verifying that all conditional independencies specified by the causal directed acyclic graph (DAG) are present in the data. Therefore, we typically assume compatibility rather than empirically proving it [@pearl2009a].

#### Faithfulness

**Faithfulness** complements Markov factorisation in causal diagrams. A causal diagram is considered faithful to a given set of data if all the conditional independencies present in the data are accurately depicted in the graph. Conversely, the graph is faithful if every dependency implied by the graph's structure can be observed in the data [@hernan2024WHATIF]. This concept ensures that the graphical representation of relationships between variables aligns with empirical evidence [@pearl2009a].

The distinction between **weak faithfulness** and **strong faithfulness** addresses the nature of observed independencies:

- **Weak faithfulness** allows for the possibility that some observed independencies might occur by chance, such as through cancellation of effects among multiple causal paths.

- **Strong faithfulness** assumes that all observed statistical relationships directly reflect the underlying causal structure, with no difference left to chance. 

The faithfulness assumption, whether weak or strong, is not directly testable from observed data [@pearl2009a].

### The Rules of d-separation

**d-separation**: in a causal diagram, a path is 'blocked' or 'd-separated' if a node along it interrupts causation. Two variables are d-separated if all paths connecting them are blocked, making them conditionally independent. Conversely, unblocked paths result in 'd-connected' variables, implying potential dependence [@pearl1995; @pearl2009a]. (Note that 'd' stands for 'directional'.)

The rules of d-separation are as follows:

1. **Fork rule** ($B \leftarrowNEW \boxed{A} \rightarrowNEW C$): $B$ and $C$ are independent when conditioning on $A$ ($B \coprod C \mid A$).
2. **Chain rule** ($A \rightarrowNEW \boxed{B} \rightarrowNEW C$): Conditioning on $B$ blocks the path between $A$ and $C$ ($A \coprod C \mid B$).
3. **Collider rule** ($A \rightarrowNEW \boxed{C} \leftarrowNEW B$): $A$ and $B$ are independent until conditioning on $C$, which introduces dependence ($A \cancel{\coprod} B \mid C$). Judea Pearl proved these theorems in the 1990s [@pearl1995; @pearl2009a].

According to these rules:

1. An open path (no variables conditioned on) is blocked only if two arrows point to the same node: $A \rightarrowred C \leftarrowred B$. The node of common effect (here $C$) is called a *collider*.
2. Conditioning on a collider does not block a path, such that $A \rightarrowred \boxed{C} \leftarrowred B$ can lead to an association between $A$ with $B$ in the absence of causation.
3. Conditioning on a descendant of a collider does not block a path, such that if $C \rightarrowNEW \boxed{C'}$, then $A \rightarrowred \boxed{C'} \leftarrowred B$ is open.
4. If a path does not contain a collider, any variable conditioned along the path is blocked, such that $A \rightarrowNEW \boxed{B} \rightarrowNEW C$ blocks the path from $A$ to $C$ [@pearl2009a; @hernan2023, p. 78].


**Backdoor Adjustment**

Pearl's general identification algorithm is known as the 'back door adjustment theorem' [@pearl2009a]. 


In a causal directed acyclic graph (DAG), a set of variables $L$ satisfies the backdoor adjustment theorem relative to the treatment $A$ and the outcome $Y$ if $L$ blocks every path between $A$ and $Y$ that contains an arrow pointing into $A$ (a backdoor path). Formally, $L$ must satisfy two conditions:

1. **No Path Condition**: No element of $L$ is a descendant of $A$.
2. **Blocking Condition**: $L$ blocks all backdoor paths from $A$ to $Y$.

If $L$ satisfies these conditions, the causal effect of $A$ on $Y$ can be estimated by conditioning on $\boxed{L}$ [@pearl2009a].

Pearl also proves a 'front-door adjustment' criterion, which is rarely used, refer to [Appendix E](#id-app-e).


<!-- 
@pearl2009a p.173 defines the backdoor path criterion more generally as follows: a set of variables $Z$ satisfies the backdoor criterion relative to variables $X_i$ and $X_j$ in a causal directed acyclic graph $\mathcal{G}$ if: 

1. No node in $Z$ is a descendant of $X_i$.
2. $Z$ blocks every path between $X_i$ and $X_j$ that includes an arrow pointing into $X_i$.

Furthermore, we say that if $X$ and $Y$ are two disjoint subsets of nodes in $\mathcal{G}$, then $Z$ meets the backdoor criterion relative to $(X, Y)$ if it meets the criteria for any pair $(X_i, X_j)$ where $X_i \in X$ and $X_j \in Y$ (@pearl2009a, p 173).

The name 'backdoor' refers to condition (2), which requires that only paths with arrows pointing at $X_i$ be blocked; these paths can be viewed as entering $X_i$ through the back door.

The backdoor criterion uses the rules of d-separation to identify and block all paths in a causal diagram that could introduce bias between a treatment (or exposure), $A$, and an outcome, $C$, in the absence of causation [@pearl2009a, p173]. 

Pearl's backdoor path criterion gives rise to Pearl's backdoor path adjustment theorem: if a set of variables $Z$ satisfies the backdoor criterion relative to $(X, Y)$ the causal effect of $X$ on $Y$ is identifiable given the formula:

$$ p(y|\hat{x}) = \sum_z p(y|x,z) \times p(z))$$ 


Backdoor path adjustment should look familiar.  Recall the definition of conditional exchangeability. We define $A$ as the treatment (a random variable) and $L$ as the set of all measured confounders sufficient to ensure exchangeability, or equivalently, to ensure ignorable treatment assignment. Where causal consistency holds, we compute a causal contrast from data by condition on the distributions of confounders where the entire population is exposed to each treatment condition that we compare:

$$
\widehat{\text{ATE}} =  \sum_l \big( \mathbb{E}[Y \mid A = a^*, L] - \mathbb{E}[Y \mid A = a, L] \big) \times Pr(L)
$$ -->


### Comment on Pearl's Do-Calculus versus the Potential Outcomes Framework

Here, we have developed counterfactual contrasts using the potential outcomes framework. Pearl develops counterfactual contrasts using operations on structural functionals, referred to as 'do-calculus'. In Pearl's framework, we obtain counterfactual inference by assuming that the nodes in a causal directed acyclic graph correspond to a system of structural equation models, refer to [Appendix D](#id-app-d).

Mathematically, potential outcomes and counterfactual interventions are equivalent, such that:

$$
\Pr(Y(a) = y) \equiv \Pr(Y = y \mid do(A = a))
$$

where the left-hand side of the equivalence is the potential outcomes framework formalisation of a potential outcome recovered by causal consistency, and the right-hand side is given by Pearl's do-calculus, which formalises interventional distributions on nodes of a graph that correspond to structural causal models or, equivalently, to non-parametric structural equation models with independent errors.

In practice, whether one uses the do-calculus or the potential outcomes framework to interpret causal inferences is often irrelevant to identification results. However, there are theoretically interesting debates about edge cases. For example, Pearl's structural causal models permit the identification of contrasts that cannot be falsified under any experiment [@richardson2013]. Because advocates of non-parametric structural equation models treat causality as primitive, they are less concerned with the requirement for falsification [@pearl2009a; @diaz2021nonparametric; @Diaz2023; @rudolph2024mediation]

<!-- 
. On the other hand, some advocates of the potential outcomes framework require falsifiability [@robins2010alternative; @richardson2023potential]. Conversely, the potential outcomes framework can obtain identification while the do-calculus does not because the potential outcomes framework does not require the non-independence of all error structures irrespective of a specific intervention level [@richardson2013].
 -->

I have presented the potential outcomes framework because it is easier to interpret, more general, and—to my mind—clearer and more intellectually compelling (moreover, one does not need to be a verificationist to adopt it). However, for nearly every practical purpose, the do-calculus and 'po-calculus' (potential outcomes framework, refer to @shpitser2016causal) are both mathematically and practically equivalent. 


### Five Elementary Structures of Causality

::: {#tbl-fiveelementary}

```{=latex}
\terminologydirectedgraph
```
The five elementary structures of causality from which all causal directed acyclic graphs can be built.
:::



<!-- #### Causal Relations with Two Variables

1. **Causality Absent:** There is no causal effect between variables $A$ and $B$. They do not influence each other, denoted as $A \coprod B$, indicating they are statistically independent.
2. **Causality:** Variable $A$ causally affects variable $B$. This relationship suggests an association between them, denoted as $A \cancel{\coprod} B$, indicating they are statistically dependent.

#### Causal Relations with Three Variables

3. **Fork Relation:** Variable $A$ causally affects both $B$ and $C$. Variables $B$ and $C$ are conditionally independent given $A$, denoted as $B \coprod C \mid A$. This structure implies that knowing $A$ removes any association between $B$ and $C$ due to their common cause.
4. **Chain Relation:** A causal chain exists where $C$ is affected by $B$, which in turn is affected by $A$. Variables $A$ and $C$ are conditionally independent given $B$, denoted as $A \coprod C \mid B$. This indicates that $B$ mediates the effect of $A$ on $C$, and knowing $B$ breaks the association between $A$ and $C$.
5. **Collider Relation:** Variable $C$ is affected by both $A$ and $B$, which are independent. However, conditioning on $C$ induces an association between $A$ and $B$, denoted as $A \cancel{\coprod} B \mid C$. This structure is important because it suggests that $A$ and $B$, while initially independent, become associated when we account for their common effect $C$.

Understanding the basic relationships between two variables allows us to build upon these to create more complex relationships.  -->


@tbl-fiveelementary presents five elementary structures of causality from which all causal directed acyclic graphs are built. These elementary structures can be assembled in different combinations to clarify the causal relationships presented in a causal directed acyclic graph.


{{< pagebreak >}}

### Five Elementary Rules for Causal Identification

@tbl-terminologyconfounders describes five elementary rules for identifying conditional independence using directed acyclic causal diagrams.

::: {#tbl-terminologyconfounders}
```{=latex}
\terminologyelconfounders
```
Five elementary rules for causal identification.
:::

There are no shortcuts to reasoning about causality. Each causal question must be asked in the context of a specific scientific question, and each causal graph must be built under the best lights of domain expertise. However, the following five elementary rules for confounding control are implied by the theorems that underpin causal directed acyclic graphs. They may be a useful start for evaluating the prospects for causal identification across a broad range of settings.

1. **Ensure That Treatments Precede Outcomes**: This rule is a logical consequence of our assumption that causality follows the arrow of time and that a causal directed acyclic graph is faithful to this ordering. However, the assumption that treatments precede outcomes may be easily violated where investigators cannot ensure the relative timing of events from their data.

Note that this assumption does not raise concerns in settings where past outcomes may affect future treatments. Indeed, an effective strategy for confounding control in such settings is to condition on past outcomes, and where relevant, on past treatments as well. For example, if we wish to identify the causal effect of $A_1$ on $Y_2$, and repeated-measures time series data are available, it may be useful to condition such that $\boxed{A_{-1}} \to \boxed{Y_0} \to A_1 \rightarrow Y_2$. Critically, the relations of variables must be arranged sequentially without cycles.

To estimate a causal effect of $Y$ on $A$, we would focus on: $\boxed{Y_{-1}} \to \boxed{A_0} \to Y_1 \rightarrow A_2$. Departing from convention, here $Y$ denotes the treatment and $A$ denotes the outcome. Causal directed acyclic graphs must be acyclic. Yet most processes in nature include feedback loops. However, there is no contradiction as long as we represent these loops as sequential events.

2. **Condition on Common Causes or Their Proxies**: This rule applies to settings in which the treatment $A$ and the outcome $Y$ share common causes. By conditioning on these common causes, we block the open backdoor paths that could introduce bias into our causal estimates. Controlling for these common causes (or their proxies) helps to isolate the specific effect of $A$ on $Y$. Note that we do not draw a path from $A \to Y$ in this context because it represents an interventional distribution. In a causal directed acyclic graph, conditioning does not occur on interventional distributions. We do not box $A$ and $Y$.

3. **Do Not Condition on a Mediator when Estimating Total Effects**: This rule applies to settings in which the variable $L$ is a mediator of $A \to Y$. Recall Pearl's backdoor path criterion requires that we do not condition on a descendant of the treatment. Here, conditioning on $L$ violates the backdoor path criterion, risking bias for a total causal effect estimate. We must not condition on a mediator if we are interested in total effect estimates. Note we draw the path from $A \to Y$ to underscore that this specific overconditioning threat occurs in the presence of a true treatment effect. Over-conditioning bias can operate in the absence of a true treatment effect. This is important because conditioning on a mediator might create associations without causation. In many settings, ensuring accuracy in the relative timing of events in our data will prevent the self-inflicted injury of conditioning on a common effect of the treatment.

4. **Do Not Condition on a Collider**: This rule applies to settings in which $L$ is a common effect of $A$ and $Y$. Conditioning on a collider may invoke a spurious association. Again, the backdoor path criterion requires that we do not condition on a descendant of the treatment. We would not be tempted to condition on $L$ if we knew that it was an effect of $A$. In many settings, ensuring accuracy in the relative timing of events in our data will prevent the self-inflicted injury of conditioning on a common effect of the treatment and outcome.

5. **Proxy Rule: Conditioning on a Descendant Is Akin to Conditioning on Its Parent**: This rule applies to settings where $L'$ is an effect from another variable $L$. The graph considers when $L'$ is downstream of a collider. Here again, in many settings, ensuring accuracy in the relative timing of events in our data will prevent the self-inflicted injury of conditioning on a common effect of the treatment and outcome.

### Summary Part 2

We use causal directed acyclic graphs to represent and evaluate structural sources of bias. We do not use these causal graphs to represent the entirety of the causal system in which we are interested, but rather only those features necessary to evaluate conditional exchangeability, or equivalently to evaluate d-separation. Moreover, causal directed acyclic graphs should not be confused with the structural equation models employed in the statistical structural equation modelling traditions (see also @rohrer2022PATH). Although Pearl's formalism is built upon 'Non-Parametric Structural Equation Models,' the term 'Structural Equation Model' can be misleading. Causal graphs are structural models that represent assumptions about reality, they are not statistical models. We use structural causal models to evaluate identifiability. We create causal graphs before we embark on statistical modelling. They aim to clarify how to write statistical models by elucidating which variables we must include in our statistical models and, equally important, which variables we must exclude to avoid invalidating our causal inferences. All causal graphs are grounded in our assumptions about the structures of causation. Although it is sometimes possible to use causal diagrams for causal discovery, their purpose is to clarify d-separation. 

This distinction between structural and statistical models is fundamental because absent clearly defined causal contrasts and carefully evaluated assumptions about structural sources of bias, the statistical structural equation modelling tradition offers no guarantees that the coefficients investigators recover are interpretable. Misunderstanding this difference between structural and statistical models has led to considerable confusion across the human sciences [@vanderweele2015; @vanderweele2022; @vanderweele2022b; @bulbulia2022].


## Part 3. How Causal Directed Acyclic Graphs Clarify the Importance of Timing of Events Recorded in Data {#id-sec-3}

::: {#tbl-elementary-chronological-hyg}
```{=latex}
\terminologychronologicalhygeine
```
:::

As hinted at in the previous section, the five elementary rules of confounding control reveal the importance of ensuring accurate timing in the occurrence of the variables whose structural features a causal directed acyclic graph encodes. We begin by considering seven examples of confounding problems resolved when such accuracy is ensured.

The first seven case studies illustrate the focus that causal directed acyclic graphs bring to the fundamental imperative to ensure accurate timing in the chronology of events recorded in data. These illustrations refer to causal graphs in @tbl-elementary-chronological-hyg. The symbol $\mathcal{G}$ denotes a graph. We use the convention: $\mathcal{G}_{\{\text{row}\}\{.\}\{1 = \text{problem}; 2 = \text{solution}\}}$ to identify a causal directed acyclic graph in the table. The context will also make it clear to understand which graph we are discussing.

### Example 1: Reverse Causation

@tbl-elementary-chronological-hyg $\mathcal{G}_{3.1}$ illustrates bias from reverse causation. Suppose we are interested in the causal effect of marriage on well-being. If we observe that married people are happier than unmarried people, we might erroneously infer that marriage causes happiness, or happiness causes marriage (refer to @mcelreath2020).

@tbl-elementary-chronological-hyg $\mathcal{G}_{3.2}$ clarifies a response. Ensure that the treatment is observed before the outcome is observed. Note further that the treatment, in this case, is not clearly specified because 'marriage' is unclear. There are at least four causal contrasts we might consider when thinking of 'marriage', namely:

1. $Y(0, 0)$: The potential outcome when there is no marriage.
2. $Y(0, 1)$: The potential outcome when there is marriage.
3. $Y(1, 0)$: The potential outcome when there is divorce.
4. $Y(1, 1)$: The potential outcome from marriage prevalence.

Each of these outcomes allows for a specific contrast. There are $\binom{4}{2}$ unique contrasts. Which of the six unique contrasts do we wish to consider? It is important that we can order our data in time. However, even before thinking about data, we must recognise that the question 'What is the causal effect of marriage on happiness?' is ill-defined. This question does not uniquely indicate which of the six causal contrasts to consider. The first step is to state this question. (For a worked example refer to @bulbulia2024swigstime).

### Example 2: Confounding by Common Cause

@tbl-elementary-chronological-hyg $\mathcal{G}_{3.2}$ illustrates confounding by common cause. Suppose there is a common cause, $L$, of the treatment, $A$, and outcome, $Y$. In this setting, $L$ may create a statistical association between $A$ and $Y$, implying causation in its absence. Most human scientists will be familiar with the threat to inference in this setting: a 'third variable' leads to association without causation.

Consider an example where smoking, $L$, is a common cause of both yellow fingers, $A$, and cancer, $Y$. Here, $A$ and $Y$ may show an association without causation. If investigators were to scrub the hands of smokers, this would not affect cancer rates.

@tbl-elementary-chronological-hyg $\mathcal{G}_{3.2}$ clarifies a response. Condition on the common cause, smoking. Within strata of smokers and non-smokers, there will be no association between yellow fingers and cancer.

### Example 3: Mediator Bias

@tbl-elementary-chronological-hyg $\mathcal{G}_{3.1}$ illustrates mediator bias. Conditioning on the effect of a treatment in this graph partially blocks the flow of information from treatment to outcome, biasing the total effect estimate.

Suppose investigators are interested in whether cultural 'beliefs in big Gods' $A$ affect social complexity $Y$. Suppose that 'economic trade', $L$, is both a common cause of the treatment and outcome. To address confounding by a common cause, we must condition on economic trade. However, timing matters. If we condition on measurements that reflect economic trade after the emergence of beliefs in big Gods, we may bias our total effect estimate.

@tbl-elementary-chronological-hyg $\mathcal{G}_{3.2}$ clarifies a response. Ensure that measurements of economic trade are obtained for cultural histories before big Gods arise. Do not condition on post-treatment instances of economic trade.

### Example 4: Collider Bias

@tbl-elementary-chronological-hyg $\mathcal{G}_{4.1}$ illustrates collider bias. Imagine a randomised experiment investigating the effects of different settings on individuals' self-rated health. In this study, participants are assigned to either civic settings (e.g., community centres) or religious settings (e.g., places of worship). The treatment of interest, $A$, is the type of setting, and the outcome, $Y$, is self-rated health. Suppose there is no effect of setting on self-rated health. However, suppose both setting and rated health independently influence a third variable: cooperativeness. Specifically, imagine religious settings encourage cooperative behaviour, and at the same time, individuals with better self-rated health are more likely to engage cooperatively. Now suppose the investigators decide to condition on cooperativeness, which in reality is the common effect of $A$ and the outcome $Y$. Their rationale might be to study the effects of setting on health among those who are more cooperative or perhaps to 'control for' cooperation in the health effects of religious settings. By introducing such 'control', the investigators would inadvertently introduce collider bias, because the control variable is a common effect of the treatment and the outcome.

@tbl-elementary-chronological-hyg $\mathcal{G}_{4.2}$ clarifies a response. If the worry is that cooperativeness is a confounder, ensure that cooperativeness is measured before the initiation of exposure to religious settings.

### Example 5: Collider Proxy Bias

@tbl-elementary-chronological-hyg $\mathcal{G}_{5.1}$ illustrates bias from conditioning on the proxy of a collider. Consider again the scenario described in $\sec 3.4$, but instead of controlling for cooperativeness, investigators control for charitable donations, a proxy for cooperativeness. Here, because the control variable is a descendant of a collider, conditioning on the proxy is akin to conditioning on the collider.

@tbl-elementary-chronological-hyg $\mathcal{G}_{5.2}$ clarifies a response. Do not condition on charitable donations, an effect of treatment.

### Example 6: Post-Treatment Collider Stratification Bias

@tbl-elementary-chronological-hyg $\mathcal{G}_{6.1}$ illustrates post-treatment collider stratification bias. Consider again an experiment investigating the effect of religious service on self-rated health. Suppose we measure 'religiosity' after the experiment, along with other demographic data. Suppose further that religious setting affects religiosity, as does an unmeasured confounder, such as childhood deprivation. Suppose that childhood deprivation affects self-reported health. Although our experiment ensured randomisation of the treatment and thus ensured no unmeasured common causes of the treatment and outcome, conditioning on the post-treatment variable 'religiosity' opens a back-door path from the treatment to the outcome. This path is $A_0 \associationred L_1 \associationred U \associationred Y_2$. We introduced confounding into our randomised experiment.

@tbl-elementary-chronological-hyg $\mathcal{G}_{6.2}$ clarifies a response. Do not condition on a variable that the treatment may affect (refer to @cole2010 for a discussion of theoretical examples; refer to @montgomery2018 for evidence of the widespread prevalence of post-treatment adjustment in published political science experiments).

### Example 7: Conditioning on Past Treatments and Past Outcomes to Control for Unmeasured Confounders

@tbl-elementary-chronological-hyg $\mathcal{G}_{7.1}$ illustrates the threat of unmeasured confounding. In 'real world' studies, this threat is ubiquitous.

@tbl-elementary-chronological-hyg $\mathcal{G}_{7.2}$ clarifies a response. With at least three repeated measurements, investigators may greatly reduce unmeasured confounding by controlling for past measurements of the treatment as well as past measurements of the outcome. With such control, any unmeasured confounder must be orthogonal to its effects at baseline (refer to @vanderweele2020). Moreover, controlling for past treatments allows investigators to estimate an incident exposure effect over a prevalence exposure effect. The prevalence exposure effect describes the effect of current or ongoing exposures (treatments) on outcomes. This effect risks leading to erroneous conclusions. The incident exposure effect targets initiation into treatment, which is typically the effect we obtain from experiments. To obtain the incident exposure effect, we generally require that events in the data can be accurately classified into at least three relative time intervals (refer to @hernán2016; @danaei2012; @vanderweele2020; @bulbulia2022).

## Summary Part 3

The examples in Part 3 reveal that the ability to order treatments, outcomes, and their common causes on a timeline is necessary for obtaining valid inferences. When timing is ensured, we can use Pearl's backdoor path adjustment algorithm to evaluate identification, subject to the assumptions encoded in a causal directed acyclic graph.

{{< pagebreak >}}

## Part 4  How Causal Directed Acyclic Graphs Clarify The Insufficiency of the Timing of Events Recorded in Data {#id-sec-4} 

We next present a series of illustrations that clarify ordering variables in time is insufficient insurance against confounding biases. All graphs in [Part 4](#id-sec-4) refer to @tbl-chronology-notenough.

::: {#tbl-chronology-notenough}
```{=latex}
\terminologychronologicalhygeineNOTENOUGH
```
Common confounding scenarios in which chronology is not enough.
:::



### Example 1: M-bias

@tbl-chronology-notenough $\mathcal{G}_{1.1}$ illustrates the threat of over-conditioning on pre-treatment variables -- 'M-bias'. Suppose we want to estimate the effect of religious service attendance on charitable donations. We obtain time-series data and include a rich set of covariates, including baseline measures of religious service and charity. Suppose there is no treatment effect. Suppose further that we condition on loyalty measures, yet loyalty neither affects religious service attendance nor charitable giving. However, imagine that loyalty is affected by two unmeasured confounders. Furthermore, imagine that one's childhood upbringing (an unmeasured variable) affects both loyalty and inclinations to religious service but not charitable giving. $U_A$ denotes this unmeasured confounder. Furthermore, suppose wealth affects loyalty and charitable giving but not religious service. $U_Y$ denotes this unmeasured confounder. In this setting, because loyalty is a collider of the unmeasured confounders, conditioning on loyalty opens a path between treatment and outcome. This path is $A \associationred U_A \associationred U_Y \associationred Y$.

@tbl-chronology-notenough $\mathcal{G}_{1.2}$ clarifies a response. If we are confident that $\mathcal{G}_{1.1}$ describes the structural features of confounding, we should not condition on loyalty.

### Example 2: M-bias Where the Pre-treatment Collider is a Confounder

@tbl-chronology-notenough $\mathcal{G}_{2.1}$ illustrates the threat of incorrigible confounding. Imagine the scenario in $\mathcal{G}_{1.1}$ and $\mathcal{G}_{1.2}$ but with one change. Loyalty is indeed a common cause of religious service attendance (the treatment) and charitable giving (the outcome). If we do not condition on loyalty, we have unmeasured confounding. This is bad. If we condition on loyalty, as we have just considered, we also have unmeasured confounding. This is also bad.

@tbl-chronology-notenough $\mathcal{G}_{2.2}$ clarifies a response. Suppose that although we have not measured wealth, we have measured a surrogate of wealth, say neighbourhood deprivation. Conditioning on this surrogate is akin to conditioning on the unmeasured confounder; we should adjust for neighbourhood deprivation.

### Example 3: Opportunities for Post-treatment Conditioning for Confounder Control

@tbl-chronology-notenough $\mathcal{G}_{3.1}$ illustrates the threat of unmeasured confounding. Suppose we are interested in whether curiosity affects educational attainment. The effect might be unclear. Curiosity might increase attention but it might also increase distraction. Consider an unmeasured genetic factor $U$ that influences both curiosity and educational attainment, say anxiety. Suppose we do not have early childhood measures of anxiety in our dataset. We have unmeasured confounding. This is bad.

@tbl-chronology-notenough $\mathcal{G}_{3.2}$ clarifies a response. Suppose $U$ also affects melanin production in hair follicles. If grey hair is an effect of a cause of curiosity, and if grey hair cannot be an effect of educational attainment, we could diminish unmeasured confounding by adjusting for grey hair in adulthood. This example illustrates how conditioning on a variable that occurs after the treatment has occurred, or even after the outcome has been observed, may prove useful for confounding control. When considering adjustment strategies, it is sometimes useful to consider adjustment on post-treatment confounders.

### Example 4: Residual Confounding After Conditioning on Past Treatments and Past Outcomes

@tbl-chronology-notenough $\mathcal{G}_{4}$ illustrates the threat of confounding even after adjusting for baseline measures of the treatment and the outcome. Imagine that childhood deprivation, an unmeasured variable, affects both religious service attendance and charitable giving. Despite adjusting for religious status and charitable giving at baseline, childhood deprivation might influence changes in one or both variables over time. This can create a longitudinal association between religious service attendance and charitable giving without a causal relationship. Strictly speaking, the causal effect cannot be identified. We may estimate an effect and perform sensitivity analyses to check how much unmeasured confounding would be required to explain way an effect (refer to @linden2020EVALUE); we may also seek negative controls (refer to @hernan2024WHATIF).

### Example 5: Intermediary Confounding in Causal Mediation

@tbl-chronology-notenough $\mathcal{G}_5$ illustrates the threat of treatment confounding in causal mediation. Imagine that the treatment is randomised; there is no treatment-outcome confounding. Nor is there treatment-mediator confounding. $\mathcal{R} \to A$ ensures that backdoor paths from the treatment to the outcome are closed. We may obtain biased results despite randomisation because the mediator is not randomised. Suppose we are interested in whether the effects of COVID-19 lockdowns on psychological distress were mediated by levels of satisfaction with the government. Suppose that assignment to COVID-19 lockdowns was random, and that time series data taken before COVID-19 provides comparable population-level contrasts. Despite random assignment to treatment, assume that there are variables that may affect both satisfaction with the government and psychological distress. For example, job security or relationship satisfaction might plausibly function as common causes of the mediator (government satisfaction) and the outcome (psychological distress). To obtain valid inference for the mediator-outcome path, we must control for these common causes.

@tbl-chronology-notenough $\mathcal{G}_5$ reveals the difficulty in decomposing the total effect of COVID-19 on psychological distress into the direct effect of COVID-19 that is not mediated by satisfaction with the government and the indirect effect that is mediated. Let us assume that confounders of the mediator-outcome path are themselves potentially affected by the treatment. In this example, imagine that COVID-19 lockdowns affect relationship satisfaction because couples are trapped in "captivity." Imagine further that COVID-19 lockdowns affect job security, which is reasonable if one owns a street-facing business. If we adjust for these intermediary variables along the path between the treatment and outcome, we will partially block the treatment-mediator path. This means that we will not be able to obtain a natural indirect effect estimate that decomposes the effect of the treatment into that part that goes through the intermediary path $A \associationred V \associationred M \associationred Y$ and that part that goes through the mediated path independently of $V$, namely $A \associationred V \associationred M \associationred Y$. However, it may be possible to estimate controlled direct effects—that is, direct effects when the mediator is fixed to different levels  [@greenland1999; @vanderweele2015;  @shpitser2022multivariate], or to obtain approximations of the natural direct effect (refer to @Diaz2023; @stensrud2023conditional; @bulbulia2024swigstime].

### Example 6: Treatment Confounder Feedback in Sequential Treatments

@tbl-chronology-notenough $\mathcal{G}_6$ illustrates the threat of treatment confounder feedback in sequential treatment regimes. Suppose we are interested in whether beliefs in big Gods affect social complexity. Suppose that beliefs in big Gods affect economic trade and that economic trade may affect beliefs in big Gods and social complexity. Suppose the historical record is fragmented such that there are unmeasured variables that affect both trade and social complexity. Even if these unmeasured variables do not affect the treatment, conditioning on the $L$ (a confounder) and sequential treatment opens a backdoor path $A \associationred L \associationred U \associationred Y$. We have confounding.

@tbl-chronology-notenough $\mathcal{G}_6$ reveals the difficulty of sequentially estimating causal effects. To estimate an effect requires special estimators under the assumption of sequential randomisation for fixed treatments and the assumption of strong sequential randomisation for time-varying treatments—that is, for treatments whose levels depend on past treatments and confounders (refer to @robins1986; @hernan2004STRUCTURAL; @vanderlaan2011; @vanderlaan2018; @haneuse2013estimation; @young2014identification; @rotnitzky2017multiply; @richardson2013; @diaz2021nonparametric; @williams2021; @hoffman2023).

Importantly, we have six potential contrasts for the two sequential treatments: beliefs in big Gods at both time points vs. beliefs in big Gods at neither time point; beliefs in big Gods first, then lost vs. never believing in big Gods at both:

| Type     | Description                             | Counterfactual Outcome |
|----------|-----------------------------------------|------------------------|
| Regime   | Always believe in big Gods              | $Y(1,1)$               |
| Regime   | Never believe in big Gods               | $Y(0,0)$               |
| Regime   | Believe once first, then scepticism     | $Y(1,0)$               |
| Regime   | Start with scepticism, then believe     | $Y(0,1)$               |
| Contrast | Always believe vs. Never believe        | $E[Y(1,1) - Y(0,0)]$   |
| Contrast | Always believe vs. Treat once first     | $E[Y(1,1) - Y(1,0)]$   |
| Contrast | Always believe vs. Treat once second    | $E[Y(1,1) - Y(0,1)]$   |
| Contrast | Never believe vs. Treat once first      | $E[Y(0,0) - Y(1,0)]$   |
| Contrast | Never believe vs. Treat once second     | $E[Y(0,0) - Y(0,1)]$   |
| Contrast | Believe once first vs. Believe once second | $E[Y(1,0) - Y(0,1)]$ |

: Table outlines four fixed treatment regimens and six causal contrasts in time-series data where treatments vary over time. {#tbl-regimens}

We can compute six causal contrasts for these four fixed regimens, as shown in @tbl-regimens.

A limitation of directed acyclic causal diagrams is that we do not project factorisations of the counterfactual contrasts onto the graphs themselves. To evaluate counterfactual identification, using Single World Intervention Graphs (refer to @robins2010alternative; @richardson2013swigsprimer; @richardson2023potential) can be helpful. We consider intermediate confounding in more detail in @bulbulia2024swigstime.

### Example 7: Collider Stratification Bias in Sequential Treatments

@tbl-chronology-notenough $\mathcal{G}_7$ illustrates the threat of confounding bias in sequential treatments even without treatment-confounder feedback. Assume the setting is  $\mathcal{G}_6$ with two differences. First, assume that the treatment, beliefs in big Gods, does not affect trade networks. However, assume that an unmeasured confounder affects both the beliefs in big Gods and the confounder, trade networks. Such a confounder might be openness to outsiders, a feature of ancient cultures for which no clear measures are available. We need not imagine that treatment affects future states of confounders for time-varying confounding. It would be sufficient to induce bias for an unmeasured confounder to affect the treatment and the confounder, in the presence of another confounder that affects both the confounder and the outcome.

@tbl-chronology-notenough $\mathcal{G}_7$ reveals the challenges of sequentially estimating causal effects. Yet again, to estimate causal effects here requires special estimators, under the assumption of sequential randomisation for fixed treatments, and the assumption of strong sequential randomisation for time-varying treatments (refer to @robins1986; @hernan2004STRUCTURAL; @vanderlaan2011; @vanderlaan2018; @haneuse2013estimation; @young2014identification; @rotnitzky2017multiply; @richardson2013; @diaz2021nonparametric; @williams2021; @hoffman2023). We note again that a specific causal contrast must be stated, and we must ask, for which cultures do causal effects generalise.

Readers should be aware that merely applying currently popular tools of time-series data analysis—multi-level models and structural equation models—will not overcome the threats of confounding in sequential treatments. Applying models to data will not recover consistent causal effect estimates. Again, space constraints prevent us from discussing statistical estimands and estimation here (refer to @bulbulia2024PRACTICAL).

### Summary Part 4

Directed acyclic graphs reveal that ensuring the timing of events in one's data does not ensure identification. In some cases, certain mediated effects cannot be identified by any data, as we discussed in the context of mediation analysis with intermediate confounding. However, across the human sciences, we often apply statistical models to data and interpret the outputs as meaningful. Causal diagrams demonstrate that standard approaches, no matter how sophisticated, are often hazardous and can lead to misleading conclusions.

## Part 5. Creating Causal Diagrams: Pitfalls and Tips {#id-sec-5}

The primary interest of causal diagrams is to address **identification problems**. Pearl's backdoor adjustment theorem proves that if we adopt an adjustment set such that $A$ and $Y$ are d-separated, and furthermore do not condition on a variable along the path from $A$ to $Y$, association is causation.

Here is how investigators may construct safe and effective directed acyclic graphs.

1. **Clarify the causal question and target population**

  An identification strategy is relative to the question at hand. The adjustment criteria for estimating an effect of  A  on  Y  will generally differ from those for estimating an effect of  $Y$  on  $A$. Before attempting to draw any causal diagram, state the problem your diagram addresses and the population to whom it applies. Additionally, when adopting a specific identification strategy for a treatment or set of treatments, the coefficients we obtain for the other variables in the model will often be biased causal effect estimates for those variables.

   Moreover, the coefficients obtained from statistical models developed to estimate causal effects will typically not have a marginal interpretation [@cole2008; @vanderweele2009a; @chatton2020]. This structural implication has wide-ranging consequences for scientific reporting. For example, if regression coefficients are reported at all, they should come with clear warnings against interpreting them as having any causal meaning or interpretation (refer to @westreich2013; @mcelreath2020; @bulbulia2023). Powerful machine learning algorithms treat these parameters as a nuisance, and in many cases, coefficients cannot be obtained. Referees of human science journals need to be alerted to this fact and retrained accordingly.

2. **Consider whether the three fundamental assumptions for causal inference may be satisfied**

   Merely possessing data, even if the data are richly detailed time-series data, does not mean our causal questions will find answers. Along with identification, we must also consider the causal consistency and positivity assumptions, refer to [Part 1](#id-sec-1).

3. **Clarify the meanings of symbols and conventions**

   It is fair to say that the state of terminology in causal inference is a dog's breakfast. Meanings and conventions vary not only for terminology but also for causal graphical conventions. For example, whereas we have denoted unmeasured confounders using the variable $U$, those who follow Pearl will often draw a bi-directional arrow. Although investigators will have their preferences, there is generally little substantive interest in one's conventions, only that they are made clear, frequently repeated (as I have done for each graph table), and applied correctly.

4. **Include all common causes of the treatment and outcome**

   Once we have stated our causal question, we are ready to create a draft of our causal graph. This graph should incorporate the most recent common causes (parents) of both the treatment and the outcome, or, where measures are not available, measures for available proxies.

   Where possible, aggregate functionally similar common causes and label them with a single node. For example, all baseline confounders that are a common cause of the treatment and outcome might be labelled $L_0$.  Time-varying confounders might be labelled $L_1, L_2, \dots L_{\tau -1}$, where $Y_\tau$ is the outcome at the end of study.

   How do we determine whether a variable is a common cause of the treatment and the outcome? We might not always be in a position to know. Remember that a causal directed acyclic graph (DAG) asserts structural assumptions. Expertise in crafting causal diagrams does not guarantee expertise in encoding plausible structural assumptions. Therefore, creating and revising causal DAGs should involve topic specialists. Additionally, the decision-making processes should be thoroughly documented in published research, even if this documentation is placed in supplementary materials.

5. **Consider potential unmeasured confounders**

   We leverage domain expertise not only to identify measured sources of confounding but also—and perhaps most importantly—to identify potential unmeasured confounders. These should be included in our causal diagrams. Because we cannot guard against all unmeasured confounding, it is essential to perform sensitivity analyses and to consider developing multiple analytic strategies to provide multiple channels of evidence for the question at hand, such as instrumental variables, negative control treatments, negative control outcomes, and mendelian randomisation [refer to @angrist2009mostly;@smith2022combining]. 

6. **Ensure the causal directed acyclic graph is acyclic**

   Although not strictly necessary, it may be useful to annotate the temporal sequence of events using subscripts (e.g., $L_0$, $A_1$, $Y_2$), as we have done here. Moreover, spatially ordering your directed acyclic graph to reflect the progression of causality in time—either left-to-right or top-to-bottom—will often enhance the graph's comprehensibility. 

7. **Represent paths structurally, not parametrically**

   Whether a path is linear is unimportant for causal identification—and remember causal diagrams are tools for causal identification. Focus on whether paths exist, not their functional form (linear, non-linear, etc.).

   Consider a subway map of Paris. We do not include all the streets on this map, all noteworthy sites, or a detailed overview of the holdings by room in the Louvre. We use other maps for these purposes.  Remember, the primary function of a causal diagram is to ensure d-separation. If a causal diagram is to be useful, it must remove almost every detail about the reality it assumes.

8. **Minimise paths to those necessary for addressing an identification problem**

   Reduce clutter; only include paths critical for a specific question (e.g., backdoor paths, mediators). For example, in @tbl-chronology-notenough $\mathcal{G} 6$ and @tbl-chronology-notenough $\mathcal{G} 7$, I did not draw arrows from the first treatment to the second treatment. Although I assume that such arrows exist, drawing them was not, in these examples, relevant to evaluating the identification problem at hand.

9. **When Temporal Order is Unknown, Explicitly Represent This Uncertainty on Your Causal Diagram**

   In many settings, the relevant timing of events cannot be ascertained with confidence. To address this, we adopt the convention of indexing nodes with uncertain timing using $X_{\phi t}$ notation. Although there is no widely adopted convention for representing uncertainty in timing, our primary obligation is to be clear.

10. **Create, Report, and Deploy Multiple Graphs**

   Causal inference hinges on assumptions, and experts might disagree. When the structure of reality encoded in a causal graph is uncertain or debated, investigators should produce multiple causal diagrams that reflect these uncertainties and debates.

   By stating different assumptions and adopting multiple modelling strategies that align with these assumptions, we might find that our causal conclusions are robust despite differences in structural assumptions. Even when different structural assumptions lead to opposing causal inferences, this knowledge can guide future data collection to resolve these differences. The primary goal of causal inference, as with all science, is to truthfully advance empirical understanding. Assertions are poor substitutes for honesty. Rather than asserting a single causal directed graph, investigators should follow the implications of several.


11. **Use Automated Identification Algorithms with Care**

   Automated software can assist with identification tasks, such as factorising complex conditional independencies. However, automated software may not converge on identifying the optimal set of confounders in the presence of intractable confounding.

   Consider Tyler VanderWeele's **modified disjunctive cause criterion**. @vanderweele2019 recommends obtaining a maximally efficient adjustment, termed a 'confounder set.' A member of this set is any variable that can reduce or remove structural sources of bias. The strategy is as follows:
    
   a. Control for any variable that causes the treatment, the outcome, or both.

   b. Control for any proxy of an unmeasured variable that is a shared cause of both the treatment and outcome.

   c. Define an instrumental variable as a variable associated with the treatment but not influencing the outcome independently, except through the treatment. Exclude any instrumental variable that is not a proxy for an unmeasured confounder from the confounder set [@vanderweele2019].

   VanderWeele's modified disjunctive cause criterion is an excellent strategy for selecting an optimal confounder set. However, this set might not remove all structural sources of confounding bias in most observational settings. As such, an automated algorithm might reject it. This rejection could be unwise because, in non-randomised treatment assignments, we should often include relations of unmeasured confounding in our causal graphs. Rejecting causal inferences in observational settings entirely would be imprudent, as many examples closely approximate randomised control trials [@hernan2016; @hernan2006estimating; @hernan2008aObservationalStudiesAnalysedLike].

   For example, consider @tbl-chronology-notenough $\mathcal{G}_{2.1}$, where we encountered intractable confounding. What if there were no proxy for an unmeasured confounder? Should we condition on the measured confounder and induce M-bias, leave the backdoor path from the measured confounder open, or not attempt causal inferences at all? The answer depends on assumptions about the relative strength of confounding in the causal diagram. Rather than relying on a generic strategy, we require subject-specialist expertise, sensitivity analyses, and multiple causal identification strategies [@smith2022combining].


12. **Clarify Assumptions about Structural Bias from Measurement Error and Target Population Restriction (also known as 'Selection Bias')**

   Space constraints prevented us from examining how causal directed acyclic graphs can clarify structural biases from measurement error and restrictions of the target population in the sample population at the start and end of the study. We can (and should) examine structural features of bias in these settings. For an overview, refer to @bulbulia2024wierd.

## Conclusions {#id-sec-6} 

#### Limitations

**First, I have focused on the application of causal diagrams to confounding bias; however, there are other biases that threaten causal inference besides confounding biases.** Causal directed acyclic graphs can also be extended to evaluate measurement-error biases and some features of target population restriction bias (also called 'selection restriction bias'). Valid causal inferences require addressing all structural sources of bias. This work does not aim for complete coverage of how causal diagrams may be useful for off-label applications other than assessing d-separation, but it hopes to stimulate curiosity [@hernan2017SELECTIONWITHOUTCOLLIDER; @liu2023application; @hernan2024WHATIF; @hernan2009MEASUREMENT; @vanderweele2012MEASUREMENT; @bulbulia2024wierd].

**Second, I have not reviewed other graphical tools for identification, such as Single World Intervention Graphs**: Although causal directed acyclic graphs are powerful tools for addressing identification problems, they are not the only graphical tools researchers use to investigate causality. For example, @robins1986 developed the 'finest fully randomized causally interpreted structured tree graph (FFRCISTG),' which has been more recently revived and simplified in Single World Intervention Graphs (refer to @richardson2013swigsprimer). These graphs explicitly factorise counterfactual states, which can be helpful for identification in complex longitudinal settings. For some, representing counterfactual states on a graph is more satisfying, as it allows inspection of the conditional independence of expectations over $Y(a^*)$ and $Y(a)$ separately. Refer to @bulbulia2024swigstime for use cases.

**Third, I have not reviewed workflows downstream of causal identification**: This discussion does not cover statistical estimands, statistical estimation, and the interpretation and reporting of causal inferences, which come downstream of causal graphs in causal inference workflows. Rapid developments in machine learning offer applied researchers new tools for handling model misspecification (refer to @vanderlaan2018; @van2012targeted; @diaz2021nonparametric; @williams2021; @hoffman2023) and assessing treatment effect heterogeneity (refer to @athey2019; @athey2021; @wager2018; @vansteelandt2022a). Those interested in workflows within the human sciences might consider @vanderweele2020. The workflows in my research group can be found here: @bulbulia2024PRACTICAL. However, readers should know that estimation workflows are quickly evolving and improving.

Nevertheless, after precisely stating our causal question, the most difficult and important challenge is considering whether and how it might be identified in the data. The 'statistical models first' approach routinely applied in most human sciences is soon ending. This approach has been attractive because it is relatively easy to implement—the methods do not require extensive training—and because the application of statistical models to data appears rigorous. However, if the coefficients we recover from these methods have meaning, this is typically accidental. Without a causal framework, these coefficients are not just uninformative about what works and why; they lack any meaning [@ogburn2021].

There are many good resources available for learning causal directed acyclic graphs [@rohrer2018; @hernan2023; @cinelli2022; @barrett2021; @mcelreath2020; @greenland1999; @suzuki2020; @pearl2009a; @hernan2024WHATIF; @major2023exploring; @greenland1999; @morgan2014]. This work aims to add to these resources by:

1. Providing additional conceptual orientation to the frameworks and workflows of causal data science, highlighting the risks of applying causal graphs without this understanding.
2. Using causal diagrams to emphasise the importance of ensuring relative timing for the variables whose causal relationships are represented on the graph.
3. Employing causal diagrams to clarify the limitations of longitudinal data for certain questions in causal mediation and time-varying confounding under time-varying treatments, which remain topics of confusion in many human sciences (see @bulbulia2024swigstime for a detailed explanation).


### Neurath's Boat: On the Priority of Assumptions in Science

We might wonder, if not from the data, where do our assumptions about causality come from?  We have said that our assumptions must come from expert knowledge. Our reliance on expert knowledge might seem counterintuitive for building scientific knowledge. Shouldn't we use data to build scientific knowledge, not the other way around? Isn't scientific history a record of expert opinions being undone?

The Austrian philosopher Otto Neurath famously described scientific progress using the metaphor of a ship that must be rebuilt at sea:

> ... every statement about any happening is saturated with hypotheses of all sorts and these in the end are derived from our whole world-view. We are like sailors who on the open sea must reconstruct their ship but are never able to start afresh from the bottom. Where a beam is taken away a new one must at once be put there, and for this the rest of the ship is used as support. In this way, by using the old beams and driftwood, the ship can be shaped entirely anew, but only by gradual reconstruction. [@neurath1973, p.199]

Neurath emphasises the iterative process of accumulating scientific knowledge; new insights are formed from the foundation of existing knowledge [@godfrey2006strategy; @godfrey2009theory; @quine1981theories].

Causal diagrams are at home in Neurath's boat. We should resist the tradition of science that believes knowledge develops solely from the results of statistical tests applied to data. The data have never fully contained the answers we seek. When reconstructing knowledge, we have always relied on assumptions. Causal graphs enable us to make these assumptions explicit and to understand what we obtain based on them.


{{< pagebreak >}}

## Funding

This work is supported by a grant from the Templeton Religion Trust (TRT0418) and RSNZ Marsden 3721245, 20-UOA-123; RSNZ 19-UOO-090. I also received support from the Max Planck Institute for the Science of Human History. The Funders had no role in preparing the manuscript or deciding to publish it.

## Acknowledgements

I am grateful to Dr Inkuk Kim for checking previous versions of this manuscript and offering
feedback. 

I am also grateful to two anonymous reviewers and the editor, Charles Efferson, of Evolutionary Human Sciences, for their constructive feedback, which improved this manuscript. 

Any remaining errors are my own.


{{< pagebreak >}}



## Appendix A: Glossary {#id-app-a}  


::: {#tbl-experiments}
```{=latex}
\glossaryTerms
```
Glossary
:::

{{< pagebreak >}}


## Appendix B: Causal Inference in History: The Difficulty in Satisfying the Three Fundamental Assumptions {#id-app-b}

Consider the Protestant Reformation of the 16th century, which initiated religious change throughout much of Europe. Historians have argued that Protestantism caused social, cultural, and economic changes in those societies where it took hold (see: @weber1905; @weber1993; @swanson1967; @swanson1971; @basten2013, and for an overview, see: @becker2016).

Suppose we want to estimate the Protestant Reformation's 'Average Treatment Effect'. Let $A = a^*$ denote the adoption of Protestantism. We compare this effect with that of remaining Catholic, represented as $A = a$. We assume that both the concepts of 'adopting Protestantism' and 'economic development' are well-defined (e.g., GDP +1 century after a country has a Protestant majority contrasted with remaining Catholic). The causal effect for any individual country is $Y_i(a^*) - Y_i(a)$. Although we cannot identify this effect, if the basic assumptions of causal inference are met, we can estimate the average or marginal effect by conditioning on the confounding effects of $L$:

$$
ATE_{\textnormal{economic~development}} = \mathbb{E}[Y(\textnormal{Became~Protestant}|L) - Y(\textnormal{Remained~Catholic}|L)]
$$

When asking causal questions about the economic effect of adopting Protestantism versus remaining Catholic, several challenges arise regarding the three fundamental assumptions required for causal inference.

**Causal Consistency**: This requires that the outcome under each level of treatment to be compared is well-defined. In this context, defining what 'adopting Protestantism' and 'remaining Catholic' mean may present challenges. The practices and beliefs of each religion might vary significantly across countries and time periods, making it difficult to create a consistent, well-defined treatment. Furthermore, the outcome—economic development—may also be challenging to measure consistently across different countries and time periods.

There is undoubtedly considerable heterogeneity in the 'Protestant treatment.' In England, Protestantism was closely tied to the monarchy [@collinson2003]. In Germany, Martin Luther's teachings emphasized individual faith in scripture, which, it has been claimed, supported economic development by promoting literacy [@gawthrop1984]. In England, King Henry VIII abolished Catholicism [@collinson2003]. The Reformation, then, occurred differently in different places. The treatment needs to be better defined.

There is also ample scope for interference: 16th-century societies were interconnected through trade, diplomacy, and warfare. Thus, the religious decisions of one society were unlikely to have been independent from those of other societies.

**Exchangeability**: This requires that given the confounders, the potential outcomes are independent of the treatment assignment. It might be difficult to account for all possible confounders in this context. For example, historical, political, social, and geographical factors could influence both a country's religious affiliations and its economic development.

**Positivity**: This requires that there is a non-zero probability of every level of treatment for every stratum of confounders. If we consider various confounding factors such as geographical location, historical events, or political circumstances, some countries might only ever have the possibility of either remaining Catholic or becoming Protestant, but not both. For example, it is unclear under which conditions 16th-century Spain could have been randomly assigned to Protestantism [@nalle1987; @westreich2010].

Perhaps a more credible measure of effect in the region of our interests is the Average Treatment Effect in the Treated (ATT) expressed:

$$
ATT_{\textnormal{economic~development}} = \mathbb{E}[(Y(a^*) - Y(a))|A = a^*,L]
$$

Where $Y(a^*)$ represents the potential outcome if treated, and $Y(a)$ represents the potential outcome if not treated. The expectation is taken over the distribution of the treated units (i.e., those for whom $A = a^*$). $L$ is a set of covariates on which we condition to ensure that the potential outcomes $Y(a^*)$ and $Y(a)$ are independent of the treatment assignment $A$, given $L$. This accounts for any confounding factors that might bias the estimate of the treatment effect.

Here, the ATT defines the expected difference in economic success for cultures that became Protestant compared with the expected economic success if those cultures had not become Protestant, conditional on measured confounders $L$, among the exposed ($A = a^*$). To estimate this contrast, our models would need to match Protestant cultures with comparable Catholic cultures effectively. By estimating the ATT, we avoid the assumption of non-deterministic positivity for the untreated. However, whether matching is conceptually plausible remains debatable. Ostensibly, it would seem that assigning a religion to a culture is not as easy as administering a pill [@watts2018].

{{< pagebreak >}}

### Appendix C: Causal Consistency Under Multiple Versions of Treatment {#id-app-c}

To better understand how the causal consistency assumption might fail, consider a question discussed in the evolutionary human science literature about whether a society's beliefs in big Gods affect its development of social complexity [@whitehouse2023; @slingerland2020coding; @beheim2021; @watts2015; @sheehan2022; @johnson2015; @norenzayan2016]. Historians and anthropologists report that such beliefs vary over time and across cultures in intensity, interpretations, institutional management, and rituals [@decoulanges1903; @wheatley1971; @bulbuliaj.2013; @geertz2013]. This variation in content and settings could influence social complexity. Moreover, the treatments realised in one society might affect those realised in other societies, resulting in *spill-over* effects in the exposures ('treatments') to be compared [@murray2021a; @shiba2023uncovering].

The theory of causal inference under multiple versions of treatment, developed by VanderWeele and Hernán, formally addresses this challenge of treatment-effect heterogeneity [@vanderweele2009; @vanderweele2013; @vanderweele2018]. The authors proved that if the treatment variations, $K$, are conditionally independent of the potential outcomes, $Y(k)$, given covariates $L$, then conditioning on $L$ allows us to consistently estimate causal effects over the heterogeneous treatments [@vanderweele2009].

Where $\coprod$ denotes independence, we may assume causal consistency where the interventions to be compared are independent of their potential outcomes, conditional on covariates, $L$:

$$
K \coprod Y(k) | L
$$

According to the theory of causal inference under multiple versions of treatment, we may think of $K$ as a 'coarsened indicator' for $A$. Although the theory of causal inference under multiple versions of treatment provides a formal solution to the problem of treatment-effect heterogeneity and treatment-effect dependencies (also known as SUTVA - the 'stable unit treatment value assumption', refer to @rubin1980randomization), computing and interpreting causal effect estimates under this theory can be challenging.

Consider the question of whether a reduction in Body Mass Index (BMI) affects health [@hernán2008]. Weight loss can occur through various methods, each with different health implications. Specific methods, such as regular exercise or a calorie-reduced diet, benefit health. However, weight loss might result from adverse conditions such as infectious diseases, cancers, depression, famine, or accidental amputations, which are generally not beneficial to health. Hence, even if causal effects of 'weight loss' could be consistently estimated when adjusting for covariates $L$, we might be uncertain about how to interpret the effect we are consistently estimating. This uncertainty highlights the need for precise and well-defined causal questions. For example, rather than stating the intervention vaguely as 'weight loss', we could state the intervention clearly and specifically as 'weight loss achieved through aerobic exercise over at least five years, compared with no weight loss.' This specificity in the definition of the treatment, along with comparable specificity in the statement of the outcomes, helps ensure that the causal estimates we obtain are not merely unbiased but also interpretable; for discussion, see @hernán2022; @murray2021a; @hernán2008.

Beyond uncertainties for the interpretation of heterogeneous treatment effect estimates, there is the additional consideration that we cannot fully verify from data whether the measured covariates $L$ suffice to render the multiple versions of treatment independent of the counterfactual outcomes. This problem is acute when there is *interference*, which occurs when treatment effects are relative to the density and distribution of treatment effects in a population. Scope for interference will often make it difficult to warrant the assumption that the potential outcomes are independent of the many versions of treatment that have been realised, dependently, on the administration of previous versions of treatments across the population [@bulbulia2023a; @ogburn2022; @vanderweele2013].

In short, although the theory of causal inference under multiple versions of treatment provides a formal solution for consistent causal effect estimation in observational settings, *treatment heterogeneity* remains a practical threat. Generally, we should assume that causal consistency is unrealistic unless proven innocent.

For now, we note that the causal consistency assumption provides a theoretical starting point for recovering the missing counterfactuals required for computing causal contrasts. It identifies half of these missing counterfactuals directly from observed data. The concept of conditional exchangeability, which we examine next, offers a means for recovering the remaining half.


{{< pagebreak >}}

## Appendix D Pearl's Do-Calculus and Structural Causal Models {#id-app-d}
<!-- 
Below is @díaz2021's formulation of @pearl2009a's mathematical representation of a structural causal model. I use @díaz2021's formulation because it allows us to present a structural causal model for dynamic treatment strategies @richardson2013, also known as longitudinal modified treatment policies @hoffman2023.


We begin by defining the sequence of variables in our model:

$$
S_i= (W, Y_0, L_1, A_1, L_2, A_2, ..., L_\tau, A_\tau, Y_{\tau}) \sim \mathbf{P}
$$
  
where $S_i$ is a sample from the distribution $\mathbf{P}$ and includes baseline covariates $W$, intermediate outcomes $L_t$, treatments $A_t$, and final outcomes $Y_{\tau}$ over time periods $t = 1, 2, \ldots, \tau$.

We define the final outcome:

$$
Y = A_{\tau + 1}
$$


We define the history of all variables up to treatment $A_t$ as:
   
$$
H_t = (\bar{A}_{t-1}, \bar{L}_t)
$$

Here, $\bar{A}_{t-1}$ represents the history of treatments up to time $t-1$, and $\bar{L}_t$ represents the history of intermediate outcomes up to time $t$.


We define the vector of exogenous variables (error terms). Note on Pearl's structural causal model account, but not the potential outcomes framework, the error terms must always be independent:

$$
U = (U_{L,t}, U_{A,t}, U_{Y}: t \in \{1 \dots \tau\})
$$

Where $U$ describes the set of exogenous variables affecting $L_t$, $A_t$, and $Y$.


We assume the following deterministic functions for the intermediate outcomes, treatments, and final outcome:
   
1. For intermediate outcomes:

$$
L_t = f_{L_t}(A_{t-1}, H_{t-1}, U_{L,t})
$$

2. For treatments:
   
$$
A_t = f_{A_t}(H_t, U_{A,t})
$$

3. For the final outcome:

$$
Y = f_{Y}(A_{\tau}, H_{\tau}, U_{Y})
$$


Longitudinal modified treatment policies (LMTPs) are defined as functions that assign treatments flexibly based on individual co-variate histories. Note that where there are multiple treatments, these histories will be partially counterfactual. 
   
We replace the deterministic function for treatments:

$$
A_t = f_{A_t}(H_t, U_{A,t})
$$

With the intervention function:

$$
A^\mathbf{g}_t
$$


On the structural causal model account, this intervention produces counterfactual histories given:

$$
L_t(\bar{A}^\mathbf{g}_{t-1}) = f_{L_t}(A^\mathbf{g}_{t-1}, H_{t-1}(\bar{A}^\mathbf{g}_{t-2}), U_{L,t})
$$


For treatments, the counterfactual variable $A_t(\bar{A}^\mathbf{g}_{t-1})$ is defined as the natural value of the treatment, i.e., the value of the treatment that would have been observed at time $t$ under the intervention history leading up to it at $t-1$, and then discontinued:

$$
A_t(\bar{A}^\mathbf{g}_{t-1}) = f_{A_t}(H_t(\bar{A}^\mathbf{g}_{t-1}), H_{t-1}(\bar{A}^\mathbf{g}_{t-2}), U_{L,t})
$$

When all variables are intervened on, the counterfactual final outcome is:

$$
Y(\bar{A}^\mathbf{g}) = f_Y(A^\mathbf{g}_\tau, H_\tau(\bar{A}^\mathbf{g}_{\tau-1}), U_{Y})
$$
 -->

In the potential outcomes framework, we represent interventions by setting variables to specific levels, e.g., setting the treatment to a specific value $A = \tilde{a}$. Counterfactual outcomes are conceived as the outcomes that would occur if, perhaps contrary to fact, the treatment were set to a specific level. We use the convention $Y_i(a)$ or equivalently $Y_i^{a}$ to denote the counterfactual or 'potential' outcome for individual $i$ when that individual's treatment is set to $A_i = a$. Because we assume individual treatments to be independent and identically distributed (i.i.d.), we drop the subscripts when describing the potential outcomes for multiple individuals under specific levels of treatment. Thus, we write $Y(a)$ or $Y^a$ as shorthand for the average potential outcome:

$$
Y(a) = \frac{1}{n} \sum_{i=1}^n Y_i(a)
$$

We say that the conditional exchangeability is satisfied if the potential outcomes are independent of the treatments assigned conditional on measured covariates:


$$
A \coprod Y(\tilde{a}) \mid L
$$

Causal directed acyclic graphs do not directly represent counterfactual outcomes. Instead, they evaluate whether causality can be identified from hypothetical interventions on the variables represented in a graph. Formally, causal directed acyclic graphs rely on Judea Pearl's do-calculus [@pearl2009a], which relies on the concept of an 'interventional distribution'. The idea is that any node in a graph can be intervened upon. Nodes and edges in a causal diagram correspond to non-parametric structural equations. Note that non-parametric structural equations are causal-structural models. They are fundamentally different from statistical structural equation models that are employed in may human sciences.

In a causal directed acyclic graph, non-parametric structural equations represent the underlying causal mechanisms without making specific parametric assumptions about the functional forms of relationships. For example, if $L$ denotes the common causes of treatment $A$ and outcome $Y$, then:

- The node $L$ corresponds to the non-parametric structural equation $L = f_L(U_L)$.
- The treatment is expressed $A = f_A(L, U_A)$.
- And the outcome is expressed $Y = f_Y(A, L, U_Y)$.

In Pearl's formalism, we assume that $U_L, U_A, U_Y$ are independent exogenous random variables. That is, there are no arrows linking $A$ to $Y$ except those from the parent node $L$.  Causal diagrams allow us to factorise the joint observations of $L$, $A$ and $Y$. Define $O$ as a distribution of independent identically distributed observations such that $O = (L, A, Y)$. The true distribution $P_O$ is factorised as:

$$
P_O = P_O(Y \mid A, L) P_O(A \mid L) P_O(L)
$$

Where:
- $P_O(L)$ is the marginal distribution of the covariates $L$.
- $P_O(A \mid L)$ is the conditional distribution of the treatment given the covariates.
- $P_O(Y \mid A, L)$ is the conditional expectation of the outcome given the treatment and covariates.

Pearl's do-calculus allows us to evaluate the consequences of intervening on variables represented in a causal DAG to interpret probabilistic dependencies and independencies in the conditional and marginal associations presented on a graph. By contrast, the potential outcomes framework considers potential outcomes to be fixed and real (even if assigned non-deterministically).

It is not inherently problematic that causal DAGs do not explicitly represent potential outcomes. The theory of counterfactual inference on which causal DAGs are based does not require this explicit representation. Judea Pearl's rules of d-separation and the backdoor adjustment criterion allow us to interpret conditional independencies based on hypothetical interventions on the nodes of a causal DAG [@pearl2009a; @neal2020introduction]. However, as we consider in [Part 4](#id-sec-4) and [Appendix F](#id-app-f), presenting counterfactual histories under specific interventions on Single World Intervention Graphs can help clarify structural relationships that are not well-represented on a causal DAG.

Again, mathematically the potential outcomes framework is mathematically equivalent to the structural causal model framework when it is assumed the the exogenous error structures are independent. However, the potential outcomes framework is more general and clarifies how identification is possible even when the error structures are not independent. See @robins1986, @richardson2013.


{{< pagebreak >}}


#### Appendix E: Front Door Path Criterion {#id-app-e}

To obtain an unbiased estimate for the causal effect of $A$ on $C$ using the front door criterion, we need to identify a set of variables $M$ that mediates the effect of $A$ on $C$. 

Pearl defines the front door criterion more generally as follows: a set of variables $B$ satisfies the front door criterion relative to variables $A$ and $C$ in a causal directed acyclic graph $\mathcal{G}$ if:

1. $B$ is affected by $A$.
2. $B$ affects $C$.
3. There are no backdoor paths from $A$ to $B$.
4. All backdoor paths from $B$ to $C$ are blocked by conditioning on $A$.

In other words, $B$ must be an intermediate variable that captures the entire causal effect of $A$ on $C$, with no confounding paths remaining between $A$ and $B$, and any confounding between $B$ and $C$ must be blocked by $A$.

The frontdoor criterion is less widely used than the backdoor criterion because it requires the identification of an appropriate mediator that fully captures the causal effect [@pearl2009a].  


{{< pagebreak >}}

## References

::: {#refs}
:::


<!-- **Internal Validity**: we say internal validity is compromised if the association between the treatment and outcome in a study does not consistently reflect causality in the sample population as defined at baseline. -->

<!-- **External validity**: we say external validity is compromised if the association between the treatment and outcome in a study does not consistently reflect causality in the target population as defined at baseline -->


<!-- The following failure modes threaten human science research: -->

<!--   - **Failures to Ask a Question**:  -->
<!--     - We have not stated the quantities we want to estimate. In causal inference, these are counterfactual quantities. -->
<!--     - We have not stated the population for whom knowledge is meant to generalise -->

<!--   - **Failures to Answer our Question**: -->
<!--     **Failures of Internal Validity**: the associations we obtain from applying models to data do not reflect causality. -->
<!--           **Poor Estimators**:  -->
<!--           **Poor Data** -->
<!--           **Poor Models** -->

<!--     **Failures of External Validity**: our findings do not generalise from the *sample population* to the *target population*.  We fail when our results do not generalise as we think. -->

<!--   -  **Other Failures of Interpretation**: we made a paper airplane, and imagined we could fly it to the moon.  -->




