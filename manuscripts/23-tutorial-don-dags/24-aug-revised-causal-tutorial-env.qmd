---
title: "An Invitation to Causal Inference in Environmental Psychology"
abstract: |
 This chapter introduces causal inference within environmental psychology, underscoring its fundamental differences from traditional statistical analysis. The content is organised into four main sections:  
 (1) A non-technical introduction that outlines how causal inference focuses on specific causal questions by defining a pre-specified counterfactual contrast between different treatment conditions, as experienced by an entire population. Importantly, for any individual in this population, at most only one potential outcome may be observed. Thus, causal effects can only be estimated using assumptions.  
 (2) A tutorial on causal Directed Acyclic Graphs (DAGs), providing tools to approach causal identification problems by making causal assumptions explicit on a graph. Here we discover a secondary utility: causal DAGs clarify how commonly employed statistical modelling traditions may introduce biases.   
 (3) Practical examples that apply causal DAGs to common scenarios in observational environmental psychology, demonstrating their utility.
 (4) Guidelines for building causal inference workflows within environmental psychology.
 Our main objective in this chapter to guide environmental psychologists the robust causal workflows that have empowered a generation of economitricians and epidimiologists to systematically address causal questions, but which remain scarce in the the psychological sciences.
authors: 
  - name: Joseph A Bulbulia
    orcid: 0000-0002-5861-2056
    email: joseph.bulbulia@vuw.ac.nz
    affiliation: 
      name: Victoria University of Wellington, New Zealand, School of Psychology, Centre for Applied Cross-Cultural Research
      department: Psychology/Centre for Applied Cross-Cultural Research
      city: Wellington
      country: New Zealand
      url: www.wgtn.ac.nz/cacr
  - name: Donald W Hine
    orcid: 0000-0002-3905-7026
    email: donald.hine@canterbury.ac.nz
    affiliation: 
      name: University of Canterbury, School of Psychology, Speech and Hearing
      city: Canterbury
      country: New Zealand
      url: https://profiles.canterbury.ac.nz/Don-Hine
keywords:
  - DAGS
  - Causal Inference
  - Confounding
  - Environmental
  - Longitudinal
  - Psychology
format:
  pdf:
    sanitize: true
    keep-tex: true
    link-citations: true
    colorlinks: true
    documentclass: article
    classoption: [singlecolumn]
    lof: false
    lot: false
    number-sections: false
    number-depth: 4
    highlight-style: github
    geometry:
      - top=30mm
      - left=20mm
      - heightrounded
    template-partials: 
      - /Users/joseph/GIT/templates/quarto/title.tex
    header-includes:
      - \input{/Users/joseph/GIT/latex/latex-for-quarto.tex}
date: last-modified
execute:
  echo: false
  warning: false
  include: true
  eval: true
fontfamily: libertinus
bibliography: /Users/joseph/GIT/templates/bib/references.bib
csl: ./camb-a.csl
---

```{r}
#| label: load-libraries
#| echo: false
#| include: false
#| eval: false
#  fig-pos: 'htb'
#   html:
#    html-math-method: katex

# Include in YAML for Latex
# sanitize: true
# keep-tex: true
# include-in-header:
#       - text: |
#           \usepackage{cancel}


# for making graphs
library("tinytex")
library(extrafont)
loadfonts(device = "all")


# libraries for jb (when internet is not accessible)
# read libraries
source("/Users/joseph/GIT/templates/functions/libs2.R")

# read functions
source("/Users/joseph/GIT/templates/functions/funs.R")

# read data/ set to path in your computer
pull_path <-
  fs::path_expand(
    "/Users/joseph/v-project\ Dropbox/data/current/nzavs_13_arrow"
  )

# for saving models. # set path fo your computer
push_mods <-
  fs::path_expand(
    "/Users/joseph/v-project\ Dropbox/data/nzvs_mods/00drafts/23-causal-dags"
  )


# keywords: potential outcomes, DAGs, causal inference, evolution, religion, measurement, tutorial
# 10.31234/osf.io/b23k7

```

## Introduction

## Introduction

Psychological scientists are taught that "correlation does not imply causation." By "correlation," we refer to statistical measures of association between variables. Most statistical techniques—from t-tests to structural equation models—aim to estimate associations from data. Although we know that measuring associations does not imply causation, in observational settings we often persist in reporting statistical associations as if they are meaningful, and perhaps even as tentative evidence for causation. The purpose of this chapter is to clarify why such reporting is confused and misleading, and to guide you toward better practices.

What do we mean by "causation"? Causation has been a topic of extensive interest and debate in philosophy [@lewis1973]. In this chapter, however, we narrow our focus to the estimation of causal effects within data science. In causal-effect estimation, or causal inference, investigators seek to quantify the difference that interventions would make to a population of interest on well-defined outcomes. This requires comparing at least two states of the world: one where the population experiences a treatment and another where they do not, or experience a different level of treatment. While the concept of causation in causal inference is more narrowly defined than causation itself, it draws intellectual inspiration from David Hume, who, in his *Enquiries Concerning Human Understanding* (1751), characterizes the cause-effect relationship as follows:

> "If the first object had not been, the second never would have existed" [@hume1902] (emphasis added).

Hume’s definition relies on *counterfactual thinking*—specifically, the comparison of two mutually exclusive states of the world: one where an event occurs and one where it does not. For Hume, assessing causation requires not just observing events as they happen but also considering how the world might have differed had those events not occurred. Such comparisons, where we consider scenarios in which treatments did not take place, are known as "counterfactual contrasts," and these contrasts are fundamental to causal-effect estimation.

While causation can be explored from various perspectives—such as uncovering mechanisms or describing sufficient causes—the central focus of contemporary causal data science, or causal inference, is on contrasting the effects of different interventions (referred to as "treatments" or "exposures") on an outcome (or set of outcomes) for a given population under varying levels of the treatment.

Although statistically evaluating associations from data is essential for estimating average treatment effects, causation cannot be derived from the study of associations alone. A careful and systematic workflow is required. By the end of this chapter, you will understand why, without such a workflow, common analytic techniques—such as linear regression, correlation, and structural equation modeling—lack causal interpretations and may mislead investigators.

[**Part 1**](#section-part1) introduces the counterfactual framework of causal inference, focusing on the [**three fundamental assumptions**](#sec-three-fundamental-assumptions) necessary for estimating average causal effects. We build intuition for these concepts by considering randomized controlled trials, where these assumptions are met through enforced randomization [@westreich2012berkson; @hernan2017per; @westreich2015; @robins2008estimation].

[**Part 2**](#section-part2) introduces causal Directed Acyclic Graphs (DAGs), powerful tools for visualizing and addressing the assumption of conditional exchangeability (also known as the "no unmeasured confounders" assumption). We discuss the *rules of d-separation*, which allow investigators to identify appropriate variables to adjust for confounding. While most psychological scientists are aware that regression adjustment is commonly used to control confounding, they may not be familiar with the formal criteria for selecting proper adjustment variables.

[**Part 3**](#section-part3) provides seven practical examples where causal diagrams address real-world causal questions. These examples clarify three key objectives: (1) constructing causal diagrams that accurately represent hypothesized relationships between variables; (2) identifying and evaluating the assumptions required to estimate causal effects from observational data, ensuring they align with the underlying causal structure; and (3) using DAGs to guide the estimation of causal effects, from identifying necessary adjustment sets to applying appropriate statistical methods for valid inference. These examples serve as practical guides for translating causal questions into analyzable causal identification models.

[**Part 4**](#section-part4) offers practical guidelines for environmental psychologists aiming to infer causal effects from observational data. Given that assumptions about causal relationships are often uncertain or subject to debate, we recommend reporting multiple causal diagrams and corresponding analysis strategies to capture different plausible pathways. This approach enhances transparency in how causal relationships are inferred.

We conclude by suggesting further readings and resources for those interested in learning more about causal inference.

## Part 1: An Overview of the Counterfactual Framework for Causal Inference {#section-part1}

## The Fundamental Problem of Causal Inference: Counterfactual Comparisons in Environmental Psychology

Imagine you are faced with a significant life decision: enrolling in a graduate program in environmental psychology in New Zealand or accepting a job offer from a leading renewable energy company. This choice will shape your future, influencing your lifestyle, income, and social network. Which option is best for you?

The challenge is that, once you choose one path, you cannot observe how your life would have unfolded on the other. If you go to graduate school, you will experience that outcome, but the outcome of taking the job remains unknown—and vice versa. This is the *fundamental problem of causal inference*: we can never observe both potential outcomes for the same individual, so the path not taken remains an unobservable "what if?"—a counterfactual that cannot be measured [@holland1986]. 

Counterfactual comparisons lie at the heart of causal inference, as they involve contrasting what actually happened with what would have happened under a different scenario. In environmental psychology, computing counterfactual contrasts is essential for understanding the effects of psychological and behavioural interventions on well-defined outcomes. However, the *full data* required to make such contrasts are inevitably partially missing [@edwards2015; @westreich2013].

### Causal Inference in Experiments: We Problem of Missing Counterfactuals

Consider a question relevant to environmental psychologists: What is the causal effect of access to green spaces on subjective happiness? Denote happiness by $Y$, where $Y_i$ represents the happiness of individual $i$.

Suppose "ample access to green space" is represented as a binary variable: $A = 1$ for "ample access" and $A = 0$ for "lack of ample access." These conditions are mutually exclusive. While we simplify the treatment to a binary variable, the concepts apply to more complex or continuous treatments. Estimating causal effects always requires a contrast between well-defined treatment conditions. Importantly, defining clear causal questions is essential but often neglected in psychological science outside experimental work.

Imagine our aim is to compare potential outcomes under different treatment conditions. Specifically, we contrast the happiness of individuals with access to green space ($A = 1$) against those without ($A = 0$). The target population for this contrast should be explicit—for example, all New Zealand residents in 2024.

A clear causal question, framed as a counterfactual contrast—also known as a "causal estimand"—might be:

> *"Among New Zealand residents, does access to abundant green space increase self-perceived happiness compared to environments without such spaces?"*

Now, imagine — hypothetically and ethically— that we could randomise individuals to high or low green space access. Even in this ideal experimental setup, causal inference faces a considerable challenge: missing data for potential outcomes. For each person, only one potential outcome is observed, depending on the treatment they receive. The outcome they would have experienced under the alternative treatment is unobserved—the counterfactual. Such missingness is the fundamental problem of causal inference, raised to the level of treatment groups. only observe $Y_i(1)$ for individuals with $A_i = 1$ and $Y_i(0)$ for individuals with $A_i = 0$. The other potential outcome for each individual remains unobserved.  However, although this missingness in the "full data" poses a challenge at the individual level, we can estimate the average treatment effect within the sampled population without needing to observe all individual-level treatment effects. 

Although individual causal effects are generally elusive, we can recover causal effect estimates by changing our causal question. For example, although randomised controlled experiments do not solve the fundamental problem of causal inference at the level of individuals, however randomised controlled experiments may obtain consistent causal effect estimates for average treatment effects at the level of populations. Randomised experiments solve the missing data problem at the heart of causal inference by satisifying three fundamental assumptions required for obtaining average treatment effects. These are the (1) Conditional Exchangeability Assumptions (2) The Causal Consistency Assumptions; (3) The Positivity Assumption. We call an "observational study" one in which the data have not been obtained by randomised treatment assignment and controlled administration of treatment. We then consider how when observational studies satisfy the three fundamental assumptions of causal inference investigators may obtain consistent causal effect estimates for average treatment effects.

#### Assumption 1: Conditional Exchangeability

First, we define the expected value of a treatment $A=a$ as the sum of individual counterfactual (or equivalently) "potential" outcomes for individuals within a specified population.

$$
\mathbb{E}[Y(a)] \equiv \frac{1}{n} \sum_{i=1}^n Y_i(a)
$$

Note that in causal inference, we assume these potential outcomes to be real, even if determined stochastically.

We next define the average treatement effect (ATE) as a contrast between the averages of the potential outcomes in each treatment condition.
$$
\text{ATE} = \mathbb{E}[Y(1)] - \mathbb{E}[Y(0)]
$$

Suppose that the individuals in a sample are representative of the population of interest - the target population. In this setting, randomisation ensures there is no common cause of the treatment and the potential outcomes that would be observed under treatment. That is, if treatment assignment is determined by chance, any difference in the average response witin a treatment groups is best explained by the treatment itself. 

Mathematically, we express the absence of a common cause of treatment assignment and the potential outcomes under treatment:

$$
Y(a) \coprod A
$$

This notation means that the potential outcomes $Y(a)$ are independent of ($\coprod$) the treatment assignment $A$. Importantly, the observed outcomes need not be independent of treatment. We do not require $Y|A$ Indeed, we should expect such differences except when there is no treatment effect. Randomisation achieves unconditional independence independence, allowing us to estimate causal effects.

Importantly, we can relax the requirement for unconditional independence and allow randomization to occur conditional on certain measurable features of the sampled population. For example, suppose we randomize treatment within different age cohorts such that older individuals have a greater probability of receiving the treatment than younger individuals. Assume that the treatment effect is constant across all ages. In this scenario, we would expect to see higher average treatment effects in older cohorts simply because a larger proportion of older individuals receive the treatment.

We can illustrate this with a simple numerical example.

Suppose a population of  200 individuals divided into two age cohorts: **Younger Cohort:** 100 individuals;**Older Cohort:** 100 individuals. Suppose in the **Younger Cohort** there is a 30% chance of receiving treatment, thus Treated: 30 individuals Untreated: 70 individuals. Suppose further in the **Older Cohort:** there is a 70% chance of receiving treatment. Treated: 70 individuals; Untreated: 30 individuals. Furhther suppose the treatment increases happiness by **10 points** on a standardised scale, and baseline happiness for all individuals is **50 points**.

Now, let's calculate the average happiness scores for each cohort.

1. **Younger Cohort:**

   - **Treated Individuals:**
     - Happiness Score: $50 \, (\text{baseline}) + 10 \, (\text{treatment effect}) = 60$
     - Total Happiness: $30 \, (\text{individuals}) \times 60 = 1,800$
   - **Untreated Individuals:**
     - Happiness Score: $50 \, (\text{baseline})$
     - Total Happiness: $70 \, (\text{individuals}) \times 50 = 3,500$
   - **Average Happiness:**
     - $\frac{1,800 + 3,500}{100} = \frac{5,300}{100} = 53$

2. **Older Cohort:**

   - **Treated Individuals:**
     - Happiness Score: $50 + 10 = 60$
     - Total Happiness: $70 \times 60 = 4,200$
   - **Untreated Individuals:**
     - Happiness Score: $50$
     - Total Happiness: $30 \times 50 = 1,500$
   - **Average Happiness:**
     - $\frac{4,200 + 1,500}{100} = \frac{5,700}{100} = 57$

Our finding is that: **Younger Cohort Average Happiness:** = 53, and **Older Cohort Average Happiness:** 57
Even though the treatment effect is constant (10 points) for both cohorts, the **older cohort shows a higher average happiness score**. This arises from the higher proportion of treated individuals in the older cohort (70%) compared to the younger cohort (30%). The increased average treatment effect in the older cohort arises from the conditional randomisation based on age, not from a difference in the treatment's efficacy.

This example demonstrates that when randomisation occurs conditional on a measurable feature such as age, and the treatment effect is constant, differences in treatment probabilities across groups can lead to variations in observed average treatment effects. However, if adjust for this difference in the probability of receiving treatment, or simply compare treatment effects with the different age strata, the illusion of a different treatment group disappears. 

 We next introduce the symbol $L$ to denoted measured variables that might be common causes of the treatment ($A$) and of the treament effect ($Y(a)$).  We write that the potential or counterfactual outcome $Y(a)$ is independent of $A$, conditional on $A$ as follows: 
$$
Y(a) \coprod A \mid L
$$

This assumption means that, within levels of $L$, the treatment assignment *is as good as random.* In experiments, randomisation ensures unconditional exchangeability ($Y(a) \coprod  A$). However to compute causal effects from data in which the treatments are not randomised, we must believe that within levels of measured covariates $L$, the treatment *is as good as random*. Practically this involves measuring all common causes of $A$ and $Y$.

##### Challenges in Observational Settings

In observational studies, achieving conditional exchangeability is challenging because treatment assignment is not controlled. Individuals with access to green spaces may differ from those without in various ways. We have noted that income might be a common cause of both access to greenspace and happiness. However there are other common causes such as:

- **Age**: different age groups may have varying access to green spaces and different happiness baselines.
- **Health status**: healthier individuals might choose to live near green spaces and also report higher happiness.
- **Past happiness**: happiness might cause people to seek green spaces, and happiness in one's past might be a cause of happiness in one's future.

Merely observing statistical associations between access to greenspace and happiness does not itself resolve the causal question of whether intervening on access across a population would affect happiness levels, and if so, in which direction and by how much. 

#### Assumption 2: Causal Consistency

Causal consistency requires that the potential outcome under the treatment actually received equals the observed outcome:

$$
Y_i = Y_i(a) \quad \text{if } A_i = a
$$

Note that in causal inference, we compare potential outcomes under at least two different treatments, say, $Y(a = 1)$ and $Y(a = 0)$ (in causal inference we use a lower case variable to note that the random variable $A$ is fixed to a certain level ($A = a$)). To compute causal contrasts the counterfactual or potential outcomes under treatment must be observable. The causal consistency assumption allows investigators to link counterfactual or potential outcomes to observed outcomes. It might seem obvious that if one receives a treatment, we can say that the observation of the outcome following treatment is no longer counterfactual -- it is actual. However, for the causal consistency assumption to hold across a population, we must assume that the treatments are well-defined and consistently administered. Thus the causal consistency assumption is a two-edged sword. On the one hand, we may use it -- if other assumptions are satisfied -- to compute causal contrasts. Simply put, causal consistency puts the factual into counterfactual. This is good. In *controlled* experiments we may take causal consistency for granted. The investigators administer consistent treatments. However, in observational settings no such control is ensured. For example, one reason it has been difficult to investigate the causal effects of weight loss from massive observational medical datasets is that there are many ways to lose weight -- eating less and healthfully and exercising causes people to lose weight. However, one may also lose weight from smoking, psychological distress, stomach stapling, amputation, cancer, and famine. The mechanisms of the latter forms of weight loss are unhealthy. Thus stating a causal contrast, for example, as the expected difference in all-cause mortality after five successive years of weight loss is an invitation for confusion. The treatments that lead to weight loss in medical data are not comparable across all cases. 

##### Challenges in Observational Settings

Similarly, in observational environmental psychology, the treatments of interest may not be standardised. For green space access, we might worry about variability in the green spaces we have measured: not all green spaces are equal—differences in size, quality, and amenities can affect outcomes. Moreover, there is likely considerable variability in exposure levels, the amount of time that individuals spend in green spaces can vary widely. These variations can violate causal consistency because the "treatment" isn't the same across individuals on which treatments are to be compared. Supplemental materials S-Appendix-B addresses the causal consistency assumption in detail. 

#### Assumption 3: Positivity

Positivity requires that every individual has a non-zero probability of receiving each level of the treatment, given their covariates $L$:

$$
P(A = a \mid L = l) > 0 \quad \text{for all } a, l
$$

This assumption ensures that we have data to compare treatment effects across all levels of $L$. For example, suppose we are interested in the causal effects of vasectomy on happiness.  It would not make sense to include biological females in this study because biological females cannot have vasectomies [@westreich2010; @hernan2023].

##### Challenges in Observational Settings

In practice, some individuals may have zero probability of receiving certain treatments. Consider:

- **Geographic constraints**: some regions may lack green spaces entirely.
- **Economic barriers**: low-income individuals may have no access to areas with ample green spaces.
- **Policy restrictions**: zoning laws may prevent certain individuals from accessing specific environments.

Violating the positivity assumption means we cannot estimate the causal effect for those individuals because we lack data on both treatment conditions for them.

### Summary

Causal modelling differs from traditional statistical modelling. Although statistical models aim to describe associations within observed data, they often do not distinguish between correlation and causation, leading to potentially misleading conclusions when inferring causal relationships. Causal inference requires statistical inference; however, statistics comes near the end of a workflow that begins by stating a causal quantity with reference to contrasts that would be obtained if we had the full data for the population of interest treated at different levels of the intervention [@ogburn2021]. We then seek to understand how these contrasts may be estimated from the data we have.

Common practices in statistical modelling, such as regression adjustment and structural equation modelling, frequently fall short in estimating causal effects because they do not account for the causal structure between variables. Including all available covariates in a regression model without understanding their causal roles can introduce bias. In the next section, we introduce causal directed acyclic graphs (causal DAGs), which are intuitive graphical tools that allow us to inspect graphs to evaluate the complex conditional dependencies that must be understood to evaluate the "no unmeasured confounders" assumption. Causal DAGs also demonstrate how common practices in statistical modelling such as over-adjustment in regression and mediation analysis in structural equation modelling may inadvertently introduce bias through inappropriate adjustment for variables. Such over-adjustment can distort causal pathways, potentially mask true relationships, or leading to associations that do not represent causal effects. In causal inference, such associations are aptly called "spurious.' By making the causal structure explicit and carefully considering which variables to adjust for, causal DAGs may help environmental psycholosts to better estimate causal effects in environmental psychology.

## Part 2: An Introduction to Causal Diagrams {#section-part2}

We now introduce causal directed acyclic graphs (causal DAGs), beginning with essential terminology. Refer to [**Appendix A**](#appendix-a) for a detailed glossary.

### Elements of Causal Diagrams

Causal diagrams distil causal relationships within a system into visual representations. At their core, these diagrams consist of:

#### 1. **Nodes**

Nodes represent variables or events in a causal framework. Each node stands for an element that either influences or is influenced within the system. Nodes encapsulate the components of our causal inquiry relevant for evaluating the conditional exchangeability assumption. The relevant nodes are treatments, outcomes, measured confounders and unmeasured confounders.

#### 2. **Arrows/Edges**

Arrows indicate the direction and presence of causal relationships between nodes. Directed edges trace the assumed flow of causal influence from a "parent" (originating variable) to a "child" (receiving variable). These arrows define the causal architecture, illustrating how we assume one variable causally affects another. Importantly, arrows represent causal relationships regardless of whether the influence is linear or non-linear. The reason is that the purpose of a causal DAG is to evaluate whether and how consistent causal effect estimates can be obtained from data. Please do not draw arrows into arrows! An arrow should start at a parent node and terminate at a child node. 

#### 3. **Conditioning**

We must decide which variables to adjust for to estimate causal effects without confounding -- that is, to satisfy the conditional exchangeability assumption of causal inference -- conditional on measured covariates, the treatments to be compared are "as good as random". We denote variables that we "control for," "condition on," or "adjust for" by enclosing them in a box.

### The Rules of d-separation

Judea Pearl introduced the concept of 'd-separation' to analyse relationships within causal diagrams [@pearl1995]. These rules help us identify confounders and develop strategies for valid causal inference from statistical associations. The "d" stands for "directional." Pearl's "back-door path criterion" is a complete algorithm for identifying causal effects from data [@pearl2009a]. We describe Pearl's algorithm and proof in detail elsewhere [@bulbulia2023]. In For the present purposes, it will be sufficient to summarise Pearl's work. 

**Basic Concepts**

#### Dependence

Denoted as $A \cancel\coprod B$, indicating that the probability distributions of $A$ and $B$ are interrelated. Knowledge about one variable provides insights into the other, suggesting a potential causal or associational link.

#### Independence

Denoted as $A \coprod B$, signifying that $A$ and $B$ are independent. Information about one variable reveals nothing about the other, indicating no direct causal or associational connection.

#### Blocked Paths and d-Separation

A path is "blocked" when a node on it prevents causal influence from passing between variables. D-separation occurs when all paths between two variables are blocked ($A \coprod B$), indicating no direct statistical association between them. This is crucial for enabling unbiased causal inference.

#### Open Paths

An "open" path exists if at least one route between variables remains unblocked, allowing the transmission of association—even without direct causation ($A \cancel\coprod B$).

### The Five Elementary Graphical Structures of Causality and Five Rules for Confounding Control {#sec-five-elementary}

To uncover causal insights from statistical relationships, we must understand five basic graphical structures. We next examine these structures, remembering that balancing confounders across treatments requires ensuring statistical independence between potential outcomes and treatment ($A \coprod Y(a) \mid L$) within groups defined by measured covariates $L$.  Because we can nearly always assume that there are unmeasured confounders $U$, it is always advisable to perform sensitivity analyses. 

#### Absence of Causality: Two Variables with No Arrows

When no arrows connect $A$ and $B$, we assume they do not share a causal relationship and are statistically independent. Graphically, we represent this relationship as:

$$
\xorxALARGE
$$

#### Causal Structure 1: Direct Causation Between Two Variables

A causal arrow ($A \to B$) signifies that changes in $A$ directly cause changes in $B$, creating statistical dependence between them. This direct causal link is graphically depicted as:

$$
\xtoxALARGE
$$

#### **Rule 1: Ensure That the Treatment Precedes the Outcome** {#sec-four-rules}

Causality follows the arrow of time; an outcome occurs after the intervention that causes it. If the timing of events in your data is unknown, you cannot ensure that the treatment precedes the outcome.

##### Motivating Example

Suppose we find an association between conservation behaviours and happiness. We might infer that conservation behaviours cause happiness. However, the association might be entirely because happy people are more likely to engage in conservation behaviours. With only cross-sectional data, we cannot rule out this alternative explanation.

#### Causal Structure 2: The Fork Structure—Common Cause Scenario

The fork structure, indicated by $A \rightarrow B$ and $A \rightarrow C$, denotes that $A$ is a common cause influencing both $B$ and $C$. Graphically:

$$
\forkLARGE
$$

Pearl proved that when we condition on the common cause $A$ (indicated by $\boxed{A}$), $B$ and $C$ become conditionally independent [@pearl2009a]. By adjusting for the common cause, any non-causal association between $B$ and $C$ is effectively blocked at node $A$.

##### Motivating Example

Suppose observations reveal that areas with higher rates of bicycle commuting also have lower levels of psychological distress. Does bicycle commuting directly reduce distress? Not necessarily. A common environmental factor might influence both. Consider sunshine hours as the common cause:

- Sunshine ($A$) encourages bicycle use ($B$).
- Sunshine ($A$) contributes to lower psychological distress ($C$).

By accounting for the common cause (sunshine hours) and comparing days with similar sunshine levels, the apparent link between bicycle commuting and distress disappears. By "spurious association" we mean that the two variables are not causally linked such that intervening on one would result in a change in the other corresponding to the magnitude of the statistical association in the target population. Adjusting for the fork’s common cause eliminates the spurious association.

#### **Rule 2: The Fork Rule**

If interested in the causal effect of $B \to C$, condition on $\boxed{A}$.

#### Causal Structure 3: The Chain Structure—A Mediator

The chain structure ($A \rightarrow B \rightarrow C$) illustrates a setting where $A$ causes $B$, and $B$ subsequently causes $C$. Conditioning on the intermediary variable $B$ (denoted $\boxed{B}$) interrupts the causal pathway, rendering $A$ and $C$ conditionally independent. Graphically:

$$
\chainLARGE
$$

##### Motivating Example

Suppose we assess the effect of green space renovation in urban areas ($A$) on local community engagement ($B$), which then reduces neighbourhood crime rates ($C$). Renovating green spaces boosts community engagement, leading to decreased crime rates.

Controlling for the mediator, community engagement, might hide the broader effect of green space renovation. If the primary path from renovation to crime reduction is via enhanced engagement, adjusting for engagement could misleadingly suggest that renovation doesn't influence crime rates. The takeaway: avoid conditioning on a mediator when examining environmental changes' effects on social outcomes.

##### **Rule 3: The Chain Rule**

If investigating the *total* causal effect of $A \to C$, *avoid* conditioning on the mediator $B$.

**Important note:** Assessing causal mediation requires further assumptions, which we won't discuss here [@vanderweele2015; @bulbulia2024swigstime].

#### Causal Structure 4: The Collider Structure—A Common Effect

The collider ($A \to C$, $B \to C$) features two factors independently causing a common effect. Initially, $A$ and $B$ lack association. Conditioning on the collider $C$ (or its descendant) introduces a spurious association between $A$ and $B$. Graphically:

$$
\immoralityLARGE
$$

##### Motivating Example

Are we interested in whether access to green spaces ($A$) causes people to become happier ($B$)? Suppose we control for health ($C$), a common effect of both $A$ and $B$. Conditioning on $C$ opens a non-causal path between $A$ and $B$:

1. Among unhealthy individuals (low $C$), those with high green space access ($A$) may appear less happy ($B$). When we stratify by $C$, a negative association emerges, even if $A$ and $B$ aren't causally linked.

2. Among healthy individuals (high $C$), those with low green space access ($A$) may be happier ($B$). Again, stratifying by $C$ shows a negative association.

Controlling for health introduces a spurious negative association between green space access and happiness. This arises from conditioning on a collider. Without controlling for health—and assuming no other confounding paths—the misleading association wouldn't be present.

This example illustrates the risk of confounding analysis by conditioning on an outcome influenced by both variables of interest.

#### **Rule 4: The Collider Rule**

When assessing the causal effect of $A \to B$, do not condition on a collider ($C$) or its descendants. Doing so may introduce an association that appears causal but is not.

#### Building Complex Causal Relationships from Basic Structures

All forms of confounding bias stem from combinations of the basic causal structures we've outlined (absence/presence of cause, forks, chains, and colliders). Understanding these elements allows us to identify potential confounders based on our assumptions encoded in a causal diagram. Consider an example combining two structures: the collider ($A \rightarrowred \boxed{C} \leftarrowred B$) and basic causality ($C \rightarrowNEW D$). This combination produces confounding by proxy: $A \rightarrowred \boxed{D} \leftarrowred B$.

Causation implies statistical association. Therefore, causal inheritance implies *statistical dependence by inheritance*. This property makes descendants act as stand-ins or proxies for their parents.

##### Motivating Example

Reconsider whether access to urban green spaces ($A$) affects wealth ($B$). Imagine they don't, but both independently contribute to well-being ($C$). Suppose only those high in well-being respond to our survey. By effectively conditioning on a stratum ($D$), a descendant of the collider well-being, we may inadvertently induce an association between green space access and socioeconomic status—inferring that those with access to green space tend to have lower income.

$$
\immoralityChildA
$$

Colliders and their descendants can set subtle "traps" that induce spurious associations.

However, as we'll see in the next section, conditioning on proxies of unmeasured confounders opens possibilities for confounding control beyond our measured variables. We can sometimes leverage proxies to reduce bias in our causal inferences.

##### **Rule 5: The Proxy Rule**

Conditioning on a descendant is akin to conditioning on its parent. A descendant is a *proxy* for its parent. Avoid conditioning on descendants when conditioning on the parent would induce misleading associations.

#### Role of assumptions

Causal diagrams bring structure to complex environmental psychology systems. They promote critical thinking about relationships, improving study design and the chances of isolating true causal effects. However, they cannot avoid assumptions. Observational data alone cannot prove causation; many diagrams can be consistent with the data. The power of causal diagrams lies in helping investigators understand how their assumptions interact with the data. We should create causal diagrams in collaboration with subject area experts because every path except the $A \to Y$ path is assumed. When experts disagree, we should propose multiple causal diagrams to reflect differing assumptions and report the outcomes of their corresponding confounding control strategies.

### How to Create Causal Diagrams to Address Identification Problems

The **causal identification problem**, or simply the **identification problem**  centres on whether we can derive the true causal effect of a treatment ($A$) on an outcome ($Y$) from observed data. Addressing this problem involves two core components:

#### Evaluating bias in the absence of a Treatment Effect

Before attributing any statistical association to causality, we must eliminate non-causal sources of correlation by:

- Identifying factors that influence both treatment ($A$) and outcome ($Y$).
- Developing adjustment strategies to control for measured confounders.
- Blocking backdoor paths that create indirect, non-causal links between $A$ and $Y$. By adjusting for confounders, we aim to achieve d-separation between $A$ and $Y$.

#### Evaluating bias in the presence of a treatment effect

After addressing potential confounders, we must ensure any remaining association between $A$ and $Y$ reflects a true causal relationship. We address **over-conditioning bias** by:

- Avoiding mediator bias
- Avoiding collider bias
- Verifying that the association between $A$ and $Y$ after adjustments is unbiased

Pearl's back-door path criterion requires us to identify and control for confounders while avoiding new biases from overconditioning. With this in mind, here's how investigators can construct effective causal diagrams:

#### 1. Clarify the Research Question and Target Population

Before drawing any causal diagram, state the problem it addresses and the population to whom it applies. Causal identification strategies may vary by question. For example, the confounding control strategy for evaluating $L \to Y$ differs from that for $A \to Y$. Reporting coefficients other than the association between $A \to Y$ is typically ill-advised; see @westreich2013; @mcelreath2020; @bulbulia2023.

#### 2. Draw the Most Recent Common Causes of Exposure and Outcome

Include all common causes (confounders) of both the exposure and the outcome in your diagram, whether measured or unmeasured. Where possible, group functionally similar common causes into a single variable (e.g., $L_0$ for demographic variables).

#### 3. Include All Ancestors of Measured Confounders

Add any ancestors (precursors) of measured confounders associated with the treatment, the outcome, or both. This step is crucial for addressing hidden biases from unmeasured confounding. Simplify the diagram by grouping similar variables.

#### 4. Explicitly State Assumptions About Relative Timing

Annotate the temporal sequence of events using subscripts (e.g., $L_0$, $A_1$, $Y_2$). It's imperative that causal diagrams are acyclic.

#### 5. Arrange Temporal Order Visually

Arrange your diagram to reflect the temporal progression of causality, either left-to-right or top-to-bottom. This enhances comprehension of causal relations. Establishing temporal ordering is vital for evaluating identification problems, as discussed in [**Part 3**](#sec-part3).

#### 6. Box Variables Adjusted for Confounding

Mark variables for adjustment (e.g., confounders) with boxes.

#### 7. Present Paths Structurally, Not Parametrically

Focus on whether paths exist, not their functional form (linear, non-linear, etc.). Parametric descriptions aren't relevant for bias evaluation in a causal diagram. For an explanation of causal interaction and diagrams, see @bulbulia2023.

#### 8. Minimise Paths to Those Necessary for the Identification Problem

Reduce clutter by including only paths critical for the specific question (e.g., back-door paths, mediators).

#### 9. Consider Potential Unmeasured Confounders

Use domain expertise to identify potential unmeasured confounders and represent them in your diagram. This proactive step helps anticipate and address *all* possible sources of confounding bias.

#### 10. State Your Graphical Conventions

Establish and explain the graphical conventions used in your diagram (e.g., using red to highlight open back-door paths). Consistency in symbol use enhances interpretability, while explicit descriptions improve accessibility and understanding.

## Part 3. How to Use Causal Diagrams for Causal Identification Tasks- Worked Examples {#section-part3}

### Notation

Causal diagrams use specific symbols to represent elements essential in causal inference [@pearl1995; @pearl2009; @greenland1999].  We use the following symbols:

* **$A$** is the treatment or exposure variable – the intervention or condition whose effect on an outcome is under investigation. **This symbol represents the cause**.
* **$Y$** is the outcome variable – the effect or result that is being studied. **Y(a) represents the effect on $Y$ when $A$ is set to a specific value, $a$**.
* **$L$** includes all measured confounders – variables that may affect both the treatment and the outcome.
* **$U$** includes unmeasured confounders – variables not included in the analysis that could influence both the treatment and the outcome, potentially leading to biased conclusions.
* **$M$** is a mediator variable – a factor through which the treatment affects the outcome. The focus here is on identifying the total effect of treatment $A$ on an outcome $Y$. Still, it is also essential to understand how controlling for mediators can affect estimates of this total effect. 


@tbl-04 provides seven worked examples that put causal diagrams to work.  Our example will focus on the question of whether access to green space affects happiness and approach this question by focusing on how different assumptions about (i) the structure of the world and (ii) the observational data that have been collected may affect strategies for confounding control and the confidence in our results.  Each example refers to a row in the table. 

::: {#tbl-04}
```{=latex}
\terminologyelconfoundersLONG
```
Worked examples: This table is adapted from [@bulbulia2023].

:::

### 1. The Problem of Confounding by a Common Cause

@tbl-04 Row 1 describes the confounding problem of a common cause. We encountered this problem in Part 1. Such confounding arises when there is a variable or set of variables, denoted by $L$, that influence both the exposure, denoted by $A$, and the outcome, denoted by $Y.$ Because $L$ is a common cause of both $A$ and $Y$, $L$ may create a statistical association between $A$ and $Y$ that does not reflect a causal association.

For instance, in the context of green spaces, consider people who live closer to green spaces (exposure $A$) and their experience of improved happiness (outcome $Y$). A common cause might be socioeconomic status $L$. Individuals with higher socioeconomic status might have the financial capacity to afford housing near green spaces and simultaneously afford better healthcare and lifestyle choices, contributing to greater happiness. Thus, although the data may show a statistical association between living closer to green spaces $A$ and greater happiness $Y$, this association might not reflect a direct causal relationship owing to confounding by socioeconomic status $L$.
Addressing confounding by a common cause involves adjusting for the confounder in one's statistical model. We may adjust through regression, or more complicated methods, such as the inverse probability of treatment weighting, marginal structural models, and others see @hernan2024WHATIF, Part 3.  Such adjustment effectively closes the backdoor path from the exposure to the outcome. Equivalently, conditioning on $L$ d-separates $A$ and $Y$.  

@tbl-04 Row 1, Column 3, emphasises that a confounder by common cause must precede both the exposure and the outcome. While it is often clear that a confounder precedes the exposure (e.g., a person's country of birth), the timing might be uncertain in other cases. We assert its temporal precedence by positioning the confounder before the exposure in our causal diagrams. However, such a timing assumption might be strong when relying on cross-sectional data. Exploring causal scenarios where the confounder follows the treatment or outcome can be insightful in such cases. Causal diagrams are instrumental in examining possible timings and their implications for causal inference. 

Next, we examine the effects of conditioning on a variable that is an effect of the treatment.

### 2. Mediator Bias

 @tbl-04 Row 1 presents a problem of mediator bias. Consider again whether proximity to green spaces, $A$, affects happiness, $Y$. Suppose that physical activity is a mediator, $L$.

To fill out the example, imagine that living close to green spaces $A$ influences physical activity $L$, subsequently affecting happiness $Y$. If we were to condition on physical activity $L$, assuming it to be a confounder, we would then bias our estimates of the total effect of proximity to green spaces $A$ on happiness $Y$. Such a bias arises because of the chain rule. Conditioning on $L$ "d-separates" the total effect of $A$ on $Y$. This phenomenon is known as mediator bias. Notably, @montgomery2018 finds dozens of examples of mediator bias in *experiments* in which control is made for variables that occur after the treatment.  For example, obtaining demographic and other information from participants *after* a study is an invitation to mediator bias. If the treatment affects these variables, and the variables affect the outcome (as we assume by controlling for them), then researchers may induce mediator bias. 

To avoid mediator bias when estimating a total causal effect, we should never condition on a mediator! The surest way to prevent this problem is to ensure that $L$ occurs before the treatment $A$ and before the outcome $Y$.  We present this solution in @tbl-04 Row 2 Col 3. 


### 3. Confounding by Collider Stratification (Conditioning on a Common Effect)


 @tbl-04 Row 1 presents a problem of collider bias.  Conditioning on a common effect, or collider stratification, occurs when a variable, denoted by $L$, is influenced by both the exposure, denoted by $A$, and the outcome, denoted by $Y$.

Let us assume initial independence: the choice to live closer to green spaces (exposure $A$) and happiness (outcome $Y$) are independent: $A \coprod Y(a)$.

We furthermore assume physical health $L$ is an effect of green space access, and happiness increases physical health. Thus, $L$ is an effect of $A$ and $Y$. If we were to condition on $L$ in this setting, we would introduce *collider stratification bias*. When we control for the common effect $L$ (physical health), we may inadvertently introduce confounding. This happens because knowing something about $L$ gives us information about both $A$ and $Y$. If someone were high on physical health but low an access to greenspace, this would imply that they are higher in happiness. Likewise, if someone were low in physical health but high in access to green space, this would imply lower happiness. As a result of our conditioning strategy, it would appear that access to green space and happiness are negatively associated. However, if we were to avoid conditioning on the common outcome, we would find that the treatment and outcome are not associated.  

How can we avoid collider bias, the temporal sequence of measurement affords a powerful strategy:

Ensure all common causes of $A$ and $Y$  -- call them $L$ -- are measured before the treatment $A$ occurs. Ensure further that $Y$ occurs after $A$ occurs.  If the confounder $L$ is not measured, ensure that conditioning on its downstream proxy, $L'$ does not induce collider or mediator biases.

By adhering to this sequence, we can mitigate the risk of collider stratification bias and better understand the causal relationships between exposure, outcome, and their common effects.

### 4. Confounding by Conditioning on a Descendant of a Confounder  

 @tbl-04 Row 4 presents a problem of collider bias by decent. Recall the rules of d-separation also apply to conditioning on descendants of a confounder.  Thus, we may unwittingly evoke confounding by proxy when conditioning on a measured descendant of an unmeasured collider. 
 
For example, if doctor visits were encoded in our data, and doctor visits were an effect of poor health, conditioning on doctor visits would function similarly to conditioning on poor health in the previous example, introducing collider confounding. 


### 5. M-bias: Conditioning on Pre-Exposure Collider

There are only five elementary structures of causality. Every confounding scenario can be developed from these five elementary structures. We next consider how we may combine these elementary causal relationships in causal diagrams to create effective strategies for confounding control. 


@tbl-04 Row 5 presents a form of pre-exposure over-conditioning confounding known as "M-bias".  This bias combines the collider structure and the fork structure, revealing what might not otherwise be obvious: it is possible to induce confounding even if we ensure that all variables have been measured **before** the treatment. The collider structure is evident in the path $U_Y \to L_0$ and $U_A \to L_0$. The collider rule shows that conditioning on $L_0$ opens a path between $U_Y$ and $U_A$. What is the result? We find that $U_Y$ is associated with the outcome $Y$ and $U_A$ is associated with treatment $A$. This is a fork (common cause) structure. The association between treatment and outcome opened by conditioning on $L$ arises from an open back-door path that occurs from the collider structure. We thus have confounding. How might such confounding play out in a real-world setting? 

In the context of green spaces, consider the scenario where an individual's level of physical activity $L$ is influenced by an unmeasured factor related to their propensity to live near green spaces $A$ -- say childhood upbringing. Suppose further that another unmeasured factor -- say a genetic factor -- increases both physical activity $L$ and happiness $Y$. Here, physical activity $L$ does not affect the decision to live near green spaces $A$ or happiness $Y$ but is a descendent of unmeasured variables that do. If we were to condition on physical activity $L$ in this scenario, we would create the bias just described --  "M-bias."  

How shall we respond to this problem? The solution is straightforward. If $L$ is neither a common cause of $A$ and $Y$ nor the effect of a shared common cause, then $L$ should not be included in a causal model. In terms of the conditional exchangeability principle, we find $A \coprod Y(a)$ yet $A \cancel{\coprod} Y(a)| L$. So we should not condition on $L$: do not control for exercise [@cole2010].[^3]

[^3]: Note that when we draw a chronologically ordered path from left to right, the M shape for which "M-bias" takes its name changes to an E shape. We shall avoid proliferating jargon and retain the term "M bias."

### 6. Conditioning on a Descendent May Sometimes Reduce Confounding

 In @tbl-04 Row 6, we encounter a causal diagram in which an unmeasured confounder opens a back-door path that links the treatment and outcome.  Here, we consider how we may use the rules of d-separation to obtain unexpected strategies for confounding control. 
 
 Returning to our green space example, suppose an unmeasured genetic factor $U$ affects one's desire to seek out isolation in green spaces $A$ and independently affects one's happiness $Y$.  Were such an unmeasured confounder to exist we could not obtain an unbiased estimate for the causal effect of green space access on happiness. We have, it seems, intractable confounding.  
 
However, imagine a variable $L^\prime$, a trait expressed later in life that arises from this genetic factor. If such a trait could be measured, even though the trait $L'$ is expressed after the treatment and outcome have occurred, controlling for $L'$ would enable investigators to close the backdoor path between the treatment and the outcome. This strategy works because a measured effect is a *proxy* for its cause $U$, the unmeasured confounder.  By conditioning on the late-adulthood trait, $L'$, we partially condition on its cause, $U$, the confounder of $A \to Y$. Thus, not all effective confounding control strategies need to rely on measuring pre-exposure variables. Thus, the elementary causal structures reveal a possibility for confounding control by condition on a post-outcome variable.  This strategy is not intuitive. Although a common cause must occur before a treatment (and outcome), its proxy need not! If we have a measure for the latter but not the former, we should condition on the post-treatment proxy of a pre-treatment common cause.

### 7. Confounding Control with Three Waves of Data is Powerful and Reveals Possibilities for Estimating an "Incident Exposure" Effect

@tbl-04 row 7 presents another setting in which there is unmeasured confounding. In response to this problem, we use the rules of d-separation to develop a data collection and modelling strategy that may greatly reduce the influence of unmeasured confounding.  @tbl-04 row 7 col 3, by collecting data for both the treatment and the outcome at baseline and controlling for baseline values of the treatment and outcome, any unmeasured association between the treatment $A_1$ and the outcome $Y_2$ would need to be *independent* of their baseline measurements. As such, including the baseline treatment and outcome, along with other measured covariates that might be measured descendants of unmeasured confounders, is a strategy that exerts considerable confounding control [@vanderweele2020]. 

Furthermore, this causal graph makes evident a second benefit of this strategy.  Returning to our example, a model that controls for baseline exposure would require that people initiate a change from the $A_0$ observed baseline level. Thus, by controlling for the baseline value of the treatment, we may learn about the causal effect of shifting one's access to green space status. This effect is called the "incident exposure effect." The incident exposure effect better emulates a "target trial" or the organisation of observational data into a hypothetical experiment in which there is a "time-zero" initiation of treatment in the data; see @hernán2016; @danaei2012; @vanderweele2020; @bulbulia2022.  Without controlling for the baseline treatment, we could only estimate a "prevalent exposure effect." If the initial exposure caused people some people to be miserable, we would not be able to track this outcome. The prevalent exposure effect would mask it, distorting causal inferences for the quantity of interest, namely, what would happen, on average, if people were to shift to having greater greenspace access. 

Finally, we obtain further control for unmeasured confounding by controlling for both the baseline treatment and the baseline outcome. For an unmeasured confounder to affect both the treatment and the outcome (and unmeasured fork structure), it would need to do so independently of the baseline measures of the treatment and exposure [@vanderweele2020]. 

Thus, we generally require repeated measures on the same unit over time intervals to obtain an incident exposure effect and exert more robust control for unmeasured confounding using past states of the treatment and outcome.  We must then model the treatments and outcomes as separate elements in our statistical model. 


## Part 4.  Practical Guide For Constructing Causal Diagrams and Reporting Results When Causal Structure is Unclear {#section-part4}

### Cross-sectional designs

In environmental psychology, researchers often grapple with whether causal inferences can be drawn from cross-sectional data, especially when longitudinal data are unavailable. The challenge is common to cross-sectional designs.  However, it is important to appreciate that even longitudinal studies require careful assumption management. We next discuss how causal diagrams can guide inference in both data types, with examples relevant to environmental psychologists.

#### 1. Graphically encode causal assumptions

Causal inference turns on assumptions. Although cross-sectional analyses typically demand much stronger assumptions owing to the snapshot nature of data, these assumptions, when transparently articulated, do not permanently bar causal analysis. By stating different assumptions and modelling the data following these assumptions, we might find that certain causal conclusions are robust to these differences. Where the implications of different assumptions disagree, we can better determine the forms of data collection that would be required to settle such differences.  Below we consider an example where assumptions point to different conclusions, revealing the benefits of collecting time-series data to assess whether a variable is a confounder or a mediator. 


#### 2. Consider time-invariant confounders at baseline

In cross-sectional studies, some confounders are inherently stable over time, such as ethnicity, year and place of birth, and biological gender. For environmental psychologists examining the relationship between access to natural environments and psychological well-being, these stable confounders can be adjusted for without concern for introducing bias from mediators or colliders. For example, conditioning on one’s year of birth can help isolate recent urban development’s effect on mental health, independent of generational differences in attitudes toward green spaces.

#### 3. Consider stable confounders at baseline

While not immutable, other confounders are less likely to be influenced by the treatment. Variables such as sexual orientation, educational attainment, and often income level fall into this category. For instance, the effect of exposure to polluted environments on cognitive outcomes can be analysed by conditioning on education level, assuming that recent exposure to pollution is unlikely to change someone’s educational history retroactively.

#### 4. Consider time varying confounding

The sequence of treatment and outcome is crucial. Sometimes, the temporal order is clear, reducing concerns about reverse causation. Mortality is a definitive outcome where the timing issue is unambiguous. If researching the effects of air quality on mortality, the causal direction (poor air quality leading to higher mortality rates) is straightforward. However, consider the relationship between socio-economic status and health outcomes; the direction of causality is complex because socioeconomic factors can influence health (through access to resources), and poor health can affect socio-economic status (through reduced earning capacity).

#### 5. Create your causal diagrams

Given the complexity of environmental influences on psychological outcomes, it’s prudent to construct multiple causal diagrams to cover various hypothetical scenarios. For example, when studying the effect of community green space on stress reduction, one diagram might assume the direct benefits of green space on stress. At the same time, another might include potential mediators such as physical activity. By analysing and reporting findings based on multiple diagrams, researchers can examine the robustness of their conclusions across different theoretical frameworks and sets of assumptions.

@tbl-cs describes ambiguous confounding control arising from cross-sectional data. Suppose again we are interested in the causal effect of access to greenspace, denoted by $A$ on "happiness," denoted by $Y$.   We are uncertain whether exercise, denoted by $L$, is a common cause of $A$ and $Y$ and thus a confounder or whether exercise is a mediator along the path from $A$ to $Y$. That is: (1) those who exercise might seek access to green space, and (2) exercise might increase happiness. Alternatively, the availability of green space might encourage physical activity, which could subsequently affect happiness. Causal diagrams can disentangle these relationships by explicitly representing potential paths, thereby guiding appropriate strategies for confounding control selection. We recommend using multiple causal diagrams to investigate the consequences of different plausible structural assumptions. 

**Assumption 1: Exercise is a common cause of $A$ and $Y$**, this scenario is presented in @tbl-cs row 1. Here, our strategy for confounding control is to estimate the effect of $A$ on $Y$ conditioning on $L$. 

**Assumption 2: Exercise is a mediator of $A$ and $Y$**, this scenario is presented in @tbl-cs row 2. Here, our strategy for confounding control is simply estimating the effect of $A$ on $Y$ without including $L$ (assuming there are no other common causes of the treatment and outcome). 

::: {#tbl-cs}

```{=latex}
\examplecrosssection
```
This table is adapted from [@bulbulia2023]
:::


We can simulate data and run separate regressions to clarify how answers may differ, reflecting the different conditioning strategies embedded in the different assumptions. The following simulation generates data from a process in which exercise is a mediator (Scenario 2). (See Appendix C)



```{r}
#| label: simulation_cross_sectional-appendix
#| tbl-cap: "Code for a simulation of a data generating process in which the effect of exercise (L) fully mediates the effect of greenspace (A) on happiness (Y)."
#| out-width: 80%
#| echo: false
# load libraries
library(gtsummary) # gtsummary: nice tables
library(kableExtra) #  tables in latex/markdown
library(clarify) # simulate ATE

# simulation seed
set.seed(123) #  reproducibility

# define the parameters 
n = 1000 # Number of observations
p = 0.5  # Probability of A = 1 (access to greenspace)
alpha = 0 # Intercept for L (exercise)
beta = 2  # Effect of A on L 
gamma = 1 # Intercept for Y 
delta = 1.5 # Effect of L on Y
sigma_L = 1 # Standard deviation of L
sigma_Y = 1.5 # Standard deviation of Y

# simulate the data: fully mediated effect 
A = rbinom(n, 1, p) # binary exposure variable
L = alpha + beta*A + rnorm(n, 0, sigma_L) # continuous mediator
Y = gamma + delta*L + rnorm(n, 0, sigma_Y) # continuous outcome

# make the data frame
data = data.frame(A = A, L = L, Y = Y)

# fit regression in which L is assumed to be a mediator
fit_1 <- lm( Y ~ A + L, data = data)

# fit regression in which L is assumed to be a mediator
fit_2 <- lm( Y ~ A, data = data)

# create gtsummary tables for each regression model
table1 <- tbl_regression(fit_1)
table2 <- tbl_regression(fit_2)

# merge the tables for comparison
table_comparison <- tbl_merge(
  list(table1, table2),
  tab_spanner = c("Model: Exercise assumed confounder", 
                  "Model: Exercise assumed to be a mediator")
)
# make latex table
markdown_table_0 <- as_kable_extra(table_comparison, 
                                   format = "latex", 
                                   booktabs = TRUE)
# print                                   
markdown_table_0
```


This table presents the conditional treatment effect estimates.  We present code for obtaining marginal treatment effects in [Appendix C](#appendix-c) 

```{r}
#| label: ate_simulation_cross_sectional
#| fig-cap: ""
#| out-width: 100%
#| echo: false

# use `clarify` package to obtain ATE
library(clarify)
# simulate fit 1 ATE
set.seed(123)
sim_coefs_fit_1 <- sim(fit_1)
sim_coefs_fit_2 <- sim(fit_2)

# marginal risk difference ATE, simulation-based: model 1 (L is a confounder)
sim_est_fit_1 <-
  sim_ame(
    sim_coefs_fit_1,
    var = "A",
    subset = A == 1,
    contrast = "RD",
    verbose = FALSE
  )

# marginal risk difference ATE, simulation-based: model 2 (L is a mediator)
sim_est_fit_2 <-
  sim_ame(
    sim_coefs_fit_2,
    var = "A",
    subset = A == 1,
    contrast = "RD",
    verbose = FALSE
  )
# obtain summaries
summary_sim_est_fit_1 <- summary(sim_est_fit_1, null = c(`RD` = 0))
summary_sim_est_fit_2 <- summary(sim_est_fit_2, null = c(`RD` = 0))

# get coefficients for reporting
# ate for fit 1, with 95% CI
ATE_fit_1 <- glue::glue(
  "ATE =
                        {round(summary_sim_est_fit_1[3, 1], 2)},
                        CI = [{round(summary_sim_est_fit_1[3, 2], 2)},
                        {round(summary_sim_est_fit_1[3, 3], 2)}]"
)
# ate for fit 2, with 95% CI
ATE_fit_2 <-
  glue::glue(
    "ATE = {round(summary_sim_est_fit_2[3, 1], 2)},
                        CI = [{round(summary_sim_est_fit_2[3, 2], 2)},
                        {round(summary_sim_est_fit_2[3, 3], 2)}]"
  )
```



On the assumptions outlined in @tbl-cs row 1, in which we *assert* that exercise is a confounder, the average treatment effect of access to green space on happiness is `r ATE_fit_2`.

On the assumptions outlined in @tbl-cs row 2, in which we *assert* that exercise is a mediator, the average treatment effect of access to green space on happiness is `r ATE_fit_1`. 

Note that although the mediator $L$ is "highly statistically significant", including it in the model is a mistake. We obtain a negative effect estimate for the causal effect of green space access on happiness.

With only cross-sectional data, we must infer the results are inconclusive. Such understanding, although not the definitive answer we sought, is progress. The result tells us we should not be overly confident with our analysis (whatever p-values we recover!), and it clarifies that longitudinal data are needed. 

These findings illustrate the role that assumptions about the relative timing of exercise as a confounder or as a mediator play. 

### Recommendations for Conducting and Reporting Causal Analyses with Cross-Sectional Data

When analysing and reporting analyses with cross-sectional data, researchers face the challenge of making causal inferences without the benefit of temporal information. 

The following recommendations aim to guide researchers in navigating these challenges effectively:

**Warning**: before proceeding with cross-sectional analysis, examine whether panel data are available. Longitudinal data can provide crucial temporal information that aids in establishing causality, offering a more robust framework for causal inference. If longitudinal data are unavailable, the recommendations above become even more critical for using cross-sectional data best.

#### 1. **Draw multiple causal diagrams**

Draw multiple causal diagrams to represent different theoretical assumptions about the relationships and timing of variables relevant to an identification problem. If some causal pathways cannot be ruled out, clarify the implications of assigning variables the roles for which consensus or which the time ordering of the data do not resolve. For example, in studying the effect of urban green spaces on mental health from cross-sectional data, consider causal DAGs that assess effects in each direction.

#### 2. **Perform and report analyses for each assumption**

Conduct and transparently report separate analyses for each scenario your causal diagrams depict. This practice ensures that your study is theoretically grounded for each model. Presenting results from each analytical approach and the underlying assumptions and statistical methods promotes a balanced interpretation of findings. 

#### 3. **Interpret findings with attention to ambiguities**

Interpret results carefully, highlighting any ambiguities or inconsistencies across analyses. Discuss how varying assumptions about structural relationships and the timing of events can lead to divergent conclusions. 

#### 4. **Report divergent findings**

Approach conclusions with caution, especially when findings suggest differing practical implications. Acknowledge the limitations of cross-sectional data in establishing causality and the potential for alternative explanations. Do not over-sell.

#### 5. **Identify avenues for future research**

Target future research that might clarify ambiguities. Consider the design of longitudinal studies or experiments capable of clarifying lingering uncertainties.

#### 6. **Supplement observational data with simulated data** 

Leverage data simulation to understand the complexities of causal inference. Simulating data based on various theoretical models allows researchers to examine the effect of different assumptions on their findings. This method tests analytical strategies under controlled conditions, assessing the robustness of conclusions against assumption violations or unobserved confounders.

#### 7. **Conduct sensitivity analyses to assess robustness**

implement sensitivity analyses to determine how dependent conclusions are on specific assumptions or parameters within your causal model. A relatively simple sensitivity analysis is VanderWeele's E-value [@vanderweele2017]

Cross-sectional data are limiting; however, by appropriately bounding uncertainties in your causal inferences, you may use them to advance understanding. May your clarity and caution serve as an example for others.


### Longitudinal Designs

Causation occurs in time. Longitudinal designs offer a substantial advantage over cross-sectional designs for causal inference because sequential measurements allow us to capture causation and quantify its magnitude. We typically do not need to assert timing as in cross-sectional data settings. Because we know when variables have been measured, we can reduce ambiguity about the directionality of causal relationships. For instance, tracking changes in "happiness" following changes in access to green spaces over time can more definitively suggest causation than cross-sectional snapshots.


Despite this advantage, longitudinal researchers still face assumptions regarding the absence of unmeasured confounders or the stability of measured confounders over time. These assumptions must be explicitly stated.  As with cross-sectional designs, wherever assumptions differ, researchers should draw different causal diagrams that reflect these assumptions and subsequently conduct and report separate analyses. 


In this section, we simulate a dataset to demonstrate the benefits of incorporating both baseline exposure and baseline outcomes into analysing the effect of access to open green spaces on happiness. This approach allows us to control for initial levels of exposure and outcomes, offering a clearer understanding of the causal relationship. [Appendix D](#appendix-d-simulation-of-different-confounding-control-strategies) provides the code. [Appendix E](#appendix-e-non-parametric-estimation-of-average-treatment-effects-using-causal-forests) provides an example of a non-parametric estimator for the causal effect.  As mentioned before, by conditioning on baseline levels of access to green spaces and baseline mental health, researchers can more accurately estimate the *incident effect* of changes in green space access on changes in mental health. @tbl-lg offers an example of how we may use multiple causal diagrams to clarify the problem and our confounding control strategy. 


::: {#tbl-lg}

```{=latex}
\examplelongitudinal
```
This table is adapted from [@bulbulia2023]
:::


Our analysis assessed the average treatment effect (ATE) of access to green spaces on happiness across three distinct models: uncontrolled, standard controlled, and interaction controlled. These models were constructed using a hypothetical cohort of 10,000 individuals, incorporating baseline exposure to green spaces ($A_0$), baseline happiness ($Y_0$), baseline confounders ($L_0$), and an unmeasured confounder ($U$). The detailed simulation process and model construction are given in [Appendix D](#appendix-simulate-longitudinal-ate).



```{r}
#| label: codelg
#| echo: false
#| eval: true
# load libaries 
library(kableExtra)
if(!require(kableExtra)){install.packages("kableExtra")} # causal forest
if(!require(gtsummary)){install.packages("gtsummary")} # causal forest
if(!require(grf)){install.packages("grf")} # causal forest

# r_texmf()eproducibility
set.seed(123) 

# set number of observations
n <- 10000 

# baseline covariates
U <- rnorm(n) # Unmeasured confounder
A_0 <- rbinom(n, 1, prob = plogis(U)) # Baseline exposure
Y_0 <- rnorm(n, mean = U, sd = 1) # Baseline outcome
L_0 <- rnorm(n, mean = U, sd = 1) # Baseline confounders

# coefficients for treatment assignment
beta_A0 = 0.25
beta_Y0 = 0.3
beta_L0 = 0.2
beta_U = 0.1

# simulate treatment assignment
A_1 <- rbinom(n, 1, prob = plogis(-0.5 + 
                                    beta_A0 * A_0 +
                                    beta_Y0 * Y_0 + 
                                    beta_L0 * L_0 + 
                                    beta_U * U))

# coefficients for continuous outcome
delta_A1 = 0.3
delta_Y0 = 0.9
delta_A0 = 0.1
delta_L0 = 0.3
theta_A0Y0L0 = 0.5 # Interaction effect between A_1 and L_0
delta_U = 0.05

# simulate continuous outcome, including interaction
Y_2 <- rnorm(n,
             mean = 0 +
               delta_A1 * A_1 + 
               delta_Y0 * Y_0 + 
               delta_A0 * A_0 + 
               delta_L0 * L_0 + 
               theta_A0Y0L0 * Y_0 * 
               A_0 * L_0 + 
               delta_U * U,
             sd = .5)

# assemble data frame
data <- data.frame(Y_2, A_0, A_1, L_0, Y_0, U)

# model: no control
fit_no_control <- lm(Y_2 ~ A_1, data = data)

# model: standard covariate control
fit_standard <- lm(Y_2 ~ A_1 + L_0, data = data)

# model: interaction
fit_interaction  <- lm(Y_2 ~ A_1 + L_0 + A_0 + Y_0 + A_0:L_0:Y_0, data = data)

# create gtsummary tables for each regression model
tbl_fit_no_control<- tbl_regression(fit_no_control)  
tbl_fit_standard <- tbl_regression(fit_standard)
tbl_fit_interaction <- tbl_regression(fit_interaction)

# get only the treatment variable
tbl_list_modified <- lapply(list(
  tbl_fit_no_control,
  tbl_fit_standard,
  tbl_fit_interaction),
function(tbl) {
  tbl %>%
    modify_table_body(~ .x %>% dplyr::filter(variable == "A_1"))
})

# merge tables
table_comparison <- tbl_merge(
  tbls = tbl_list_modified,
  tab_spanner = c(
    "No Control",
    "Standard",
    "Interaction")
) |>
  modify_table_styling(
    column = c(p.value_1, p.value_2, p.value_3),
    hide = TRUE
  )

#create latex table for publication
markdown_table <-
  as_kable_extra(table_comparison, format = "latex", booktabs = TRUE) |>
  kable_styling(latex_options = "scale_down")
  
# print it
#markdown_table
```
```{r}
#| label: ate-sim-long
#| tbl-cap: "Code for calculating the average treatment effect."
#| echo: false
#| eval: true

# use `clarify` package to obtain ATE
if(!require(clarify)){install.packages("clarify")} # clarify package

# simulate fit 1 ATE
set.seed(123)
sim_coefs_fit_no_control<- sim(fit_no_control)  
sim_coefs_fit_std <- sim(fit_standard)
sim_coefs_fit_int <- sim(fit_interaction)

# marginal risk difference ATE, no controls
sim_est_fit_no_control <-
  sim_ame(
    sim_coefs_fit_no_control,
    var = "A_1",
    subset = A_1 == 1,
    contrast = "RD",
    verbose = FALSE
  )
# marginal risk difference ATE, simulation-based: model 1 (L is a confounder)
sim_est_fit_std <-
  sim_ame(
    sim_coefs_fit_std,
    var = "A_1",
    subset = A_1 == 1,
    contrast = "RD",
    verbose = FALSE
  )
# marginal risk difference ATE, simulation-based: model 2 (L is a mediator)
sim_est_fit_int <-
  sim_ame(
    sim_coefs_fit_int,
    var = "A_1",
    subset = A_1 == 1,
    contrast = "RD",
    verbose = FALSE
  )
# obtain summaries
summary_sim_coefs_fit_no_control <-
  summary(sim_est_fit_no_control, null = c(`RD` = 0))
summary_sim_est_fit_std <-
  summary(sim_est_fit_std, null = c(`RD` = 0))
summary_sim_est_fit_int <-
  summary(sim_est_fit_int, null = c(`RD` = 0))

# get coefficients for reporting
# ate for fit 1, with 95% CI
ATE_fit_no_control  <- glue::glue(
  "ATE = {round(summary_sim_coefs_fit_no_control[3, 1], 2)}, 
  CI = [{round(summary_sim_coefs_fit_no_control[3, 2], 2)},
  {round(summary_sim_coefs_fit_no_control[3, 3], 2)}]"
)
# ate for fit 2, with 95% CI
ATE_fit_std <- glue::glue(
  "ATE = {round(summary_sim_est_fit_std[3, 1], 2)}, 
  CI = [{round(summary_sim_est_fit_std[3, 2], 2)},
  {round(summary_sim_est_fit_std[3, 3], 2)}]"
)
# ate for fit 3, with 95% CI
ATE_fit_int <-
  glue::glue(
    "ATE = {round(summary_sim_est_fit_int[3, 1], 2)},
    CI = [{round(summary_sim_est_fit_int[3, 2], 2)},
    {round(summary_sim_est_fit_int[3, 3], 2)}]"
  )
# coefs
# ATE_fit_no_control
# ATE_fit_std
# ATE_fit_int
```

The ATE estimates from these models provide critical insights into the effects of green space exposure on individual happiness while accounting for various confounding factors. The model without control variables estimated `r ATE_fit_no_control`, significantly overestimating the treatment effect. Incorporating standard covariate control reduced this estimate to `r ATE_fit_std`, aligning more closely with the expected effect but still overestimating. Most notably, the model that included interactions among baseline exposure, outcome, and confounders yielded `r ATE_fit_int`, approximating the true effect of 0.3. This finding underscores the importance of including baseline values of the exposure and outcome wherever these data are available. 

### Recommendations for Conducting and Reporting Causal Analyses with Longitudinal Data

Longitudinal data offer strong advantages for causal inference by enabling researchers to establish the relative timing of confounders, treatments, and outcomes. The temporal sequence of events is crucial for establishing causality because causality occurs in time. The following recommendations aim to guide researchers in leveraging longitudinal data effectively to conduct and report causal analyses:

#### 1. Draw multiple causal diagrams
   - **Identification problem diagram**: begin by constructing a causal diagram that outlines your initial assumptions about the relationships among variables, identifying potential confounders and mediators. This diagram should illustrate the complexity of the identification problem.
   - **Solution diagram**: next, create a separate causal diagram that proposes solutions to the identified problems. Having distinct diagrams for the problem and its proposed solutions clarifies your study's analytic strategy and theoretical underpinning.

@tbl-lg provides an example of a table with multiple causal diagrams clarifying potential sources of confounding threats and reports strategies for addressing them. 

#### 2. Attempt longitudinal designs with at least three waves of data

Incorporating repeated measures data from at least three time intervals considerably enhances your ability to infer causal relationships. For example, by adjusting for physical activity measured before the treatment, we can ensure that physical activity does not result from a new initiation to green spaces, which we establish by measuring green space access at baseline. Establishing chronological order allows us to avoid confounding problems 1-4 in @tbl-04. 

#### 3. Calculate Average Treatment Effects for a clearly specified target population

Estimating the average treatment effect (ATE) across the entire study population provides a comprehensive measure of the intervention's effects. This step is crucial for understanding the treatment's overall effect in a specific population, which must be described in advance of data analysis because causal effects are averages within certain populations or stratums of populations. The concept of a causal effect absent a population in not available to data science, because individual causal effects are not observed.

#### 4. Where causality is unclear, report results for multiple causal graphs

Given that the true causal structure may be complex and partially unknown, analysing and reporting results under each plausible causal diagram is prudent. 

#### 5. Conduct sensitivity analyses

Sensitivity analyses are essential for assessing the robustness of your findings to various assumptions within the causal model. These analyses can include simulations, as illustrated in Appendices C and D, to examine bias arising of unmeasured confounding, model misspecification, and alternative causal pathways on the study conclusions. Sensitivity analyses help to identify the conditions under which the findings hold, enhancing the credibility of the causal inferences. (For more about addressing missing data, see: [@bulbulia2024PRACTICAL].) 

#### 6. Address missing data at baseline and study attrition

Longitudinal studies often need help with missing data and attrition, which can introduce bias and affect the validity of causal inferences. Implement and report strategies for handling missing data, such as multiple imputation or sensitivity analyses that assess the bias arising from missing responses at the study's conclusion. (For more about addressing missing data, see: [@bulbulia2024PRACTICAL]). 


By following these recommendations, you will more effectively navigate the inherent limitations of observational longitudinal data, improving the quality of your causal inferences.

<!-- 
## Summary 

This chapter has introduced the potential outcomes framework for causal inference and using directed acyclic graphs (DAGs) in environmental psychology. 

In [**Part 1**](#section-part1) we discussed three critical assumptions necessary for estimating average treatment effects from data:

1. **Conditional Exchangeability**: This assumption posits that treatment allocation is randomised and independent of potential outcomes, conditional on measured covariates.
2. **Causal Consistency**: This assumption asserts that the outcome observed under the treatment condition corresponds to the outcome that would have been observed had the unit received the treatment, and similarly for the control condition.
3. **Positivity**: This assumption asserts that every unit has a non-zero probability of receiving any treatments under comparison.

Although randomised controlled experiments naturally satisfy these assumptions through design—randomisation ensures exchangeability, control guarantees consistency, and design secures positivity -— observational studies typically do not. To obtain consistent causal estimates from observational data, we must assess the extent to which these assumptions can be satisfied.

In [**Part 2**](#section-part2), we explained how causal diagrams work and described their utility in addressing the assumption of conditional exchangeability, or the "no unmeasured confounders" assumption. We identified [five fundamental structures](#sec-five-elementary) underlying all causal relationships. We discovered [Five elementary rules](#sec-four-rules) for evaluating the implications of conditioning on elements within these structures regarding observable statistical associations in data. Thus, causal diagrams provide a simplified visual language for translating complex causal relationships into data observations. However, the relationships in these diagrams represent assertions that are not directly verifiable from the data. The causal relationships between treatments and outcomes are the only relationships not based on assertion. Causal diagrams help us identify structural sources of bias in the statistical associations between treatments and outcomes that may arise from assumed causal relationships, potentially associating treatments with outcomes irrespective of causal links.

In [**Part 3**](#section-part3), we applied causal diagrams to seven common confounding scenarios, demonstrating that a causal diagram needs to highlight only those aspects of a causal setting relevant for assessing structural sources of bias linking treatment and outcome in a non-causal manner. We focused on omitting nodes and paths not directly necessary for our stated identification problem, emphasising that causal diagrams are tailored to context-dependent questions and our assumptions about the world's causal structure.

In [**Part 4**](#section-part4), we showed how investigators might create multiple causal diagrams when the structure of a causal problem is ambiguous and illustrated the benefits of this approach through data simulation. We provided guidelines for reporting in scenarios where only cross-sectional data are available or when researchers have access to repeated measures of longitudinal data.

Although this discussion has focused on seven specific applications of causal diagrams, their applicability extends much further. The straightforward rules governing how variables become associated or disassociated through conditioning on nodes within basic structures enable the use of causal diagrams for addressing complex problems of time-varying confounding, for population restriction biases, and for measurement error (refer to @bulbulia2024swigstime; @bulbulia2024wierd). 

We hope this chapter will inspire environmental psychologists to deepen their understanding of causal inference and incorporate causal diagrams into their research practices. The methodologies for distinguishing causation from correlation are well-established; powerful tools for causal inference are accessible. There is no longer any justification for reporting associations and speculating about causes. It is within your reach to quantify magnitudes of causality conditional on assumptions encoded in your causal graphs.  -->

## Where to go from here
### The Origins of Causal Inference

Causal inference began in the early 20$^{th}$ century with Jerzy Neyman's invention of the potential outcomes framework, initially developed for agricultural experiments. Neyman realized that understanding the causal effect of an experimental treatment required comparing potential outcomes under different treatment conditions, even though only one outcome could be observed for each unit [@neyman1923]. This framework laid the foundation for modern causal inference.

Donald Rubin extended this approach to observational settings, into what is now known as the Neyman-Rubin Causal Model [@rubin1976; @holland1986]. James Robins advanced causal inference by introducing the mathematical and conceptual framework for understanding causal effects in settings where there are two or more sequential treatments over time [@robins1986]. Robins and his colleagues also developed statistical tools such as marginal structural models and structural nested models to enable quantitative assessment of time-varying treatments and time-varying confounding [@hernan2024WHATIF]. The computer scientist Judea Pearl developed causal Directed Acyclic Graphs (DAGs), and proved that these graphical models may be applied to evaluate one of the three fundamental assumptions of causal inference, namely "no unmeasured confounding", also known as "conditional exchangeability", "selection on observables" and "ignorability", or in Pearl's formalism, "d-separation" or equivalently, "no open backdoor path"[@pearl1995a; @pearl2009]. The jargon that has evolved in causal inference is not restricted to the concept of no unmeasured confounding. In [**Part 2**](#section-part2) we encounter the concept of "causal identification," which is unrelated to the concept of "statistical identification."  

Although terminology differs across the subfields of causal inference, the mathematical basis of causal inference evolved nearly independently in the fields of biostatistics, economics, and computer science. This shared foundation, anchored in proofs, has enabled a remarkable agreement across disciplinary lines despite terminological differences. In every approach to causal inference, a causal effect is conceptualized as a contrast between two states of the world, only one of which may be observed on any individual -- a "counterfactual contrast" [@hernan2024WHATIF]. Our experience is that many just getting started with causal inference find the concept of a counterfactual contrast hard to understand. However, we may take comfort that we are already familiar with how to recover the "full data" we need to compute causal contrasts in the design of a randomized controlled experiment. 

A fundamental principle in the potential outcomes framework is the concept of "counterfactual contrast" or "estimand." To quantify causal effects, one must compare the outcomes under different intervention or treatment scenarios. Notably, prior to any intervention, these scenarios are purely hypothetical. Post-intervention, only one scenario is actualised for each realised treatment, leaving the alternative as a non-observed counterfactual. For any individual unit to be treated, that only one of the two possible outcomes is realised underscores a critical property of causality: causality is **not directly observable** [@hume1902]. Causal inference, therefore, can only quantify causal effects by combining data with counterfactual simulation [@edwards2015; @bulbulia2023a]. The concept of a counterfactual data science --  may sound strange. However, anyone who has encountered a randomised experiment has encountered counterfactual data science.  Before building intuitions for causal inference from the familiar example of experiments, let's first build intuitions for the idea that causal quantities are never directly observed. 



There are many good resources available for learning causal directed acyclic graphs [@rohrer2018; @hernan2024WHATIF; @cinelli2022; @barrett2021; @mcelreath2020; @greenland1999; @suzuki2020; @pearl2009a; @major2023exploring; @greenland1999; @morgan2014].  For those just getting started on causal diagrams, we recommend Miguel Hernan's free course here: [https://www.edx.org/learn/data-analysis/harvard-university-causal-diagrams-draw-your-assumptions-before-your-conclusions, accessed 10 June 2024](https://www.edx.org/learn/data-analysis/harvard-university-causal-diagrams-draw-your-assumptions-before-your-conclusions). For those seeking a slightly more technical but still accessible introduction to causal inference and causal DAGs, I recommend Brady Neal's introduction to causal inference course and textbook, both freely available here [https://www.bradyneal.com/causal-inference-course, accessed 10 June 2024](https://www.bradyneal.com/causal-inference-course).


{{< pagebreak >}}

## Funding

This work is supported by a grant from the Templeton Religion Trust (TRT0418). JB received support from the Max Planck Institute for the Science of Human History. The funders had no role in preparing the manuscript or deciding to publish it.

## Contributions

DH proposed the chapter. JB developed the approach and wrote the first draft. Both authors contributed substantially to the final work.



<!-- 

### The Five Elementary Rules For Evaluating Confounding

To review, causal diagrams allow researchers to visualise and systematically identify potential confounders and strategies for adjusting for them. There are five basic graphical structures:

#### 1. **Causality Absent**  $A$ does not cause $B$: absent any common causes, there is no statistical association between them.

$$\xorxA$$ 

#### 2. **Causality Present**  $A$ causes $B$: absent conditioning that blocks them, $A$ and $B$ will be statistically associated.

$$\xtoxA$$

#### 3. **The Fork Structure** $A$ causes $B$ and $A$ causes $C$: absent conditioning on $A$, $B$ and $C$ will be statistically associated. Conditional on $A$, $B$ and $C$ will be independent.

$$\forkTINY$$

#### 4. **The Chain Structure**  $A$ causes $B$ and $B$ causes $C$: absent conditioning on $B$, $A$ and $C$ will be statistically associated. Conditioning on $B$, $A$ and $C$ will be independent. 

$$\chainTINY$$ 

#### 5. **A Collider Structure**  $A$ causes $C$ and $B$ causes $C$: absent conditioning on $C$, $A$ and $B$ will be statistically independant. Conditioning on $C$, $A$ and $B$ will be statistically associated. 

$$\immoralityTINY$$

From these five elementary structures, we discovered four rules that allow us to use these structures to evaluate confounding and its control:

#### 1. **The Fork Rule** 

When a common cause influences treatment and outcome, condition on the common cause to avoid bias.

#### 2. **The Chain Rule**

  (i)  For total effect estimates, avoid conditioning on mediators within the causal path.
  (ii) For mediation analysis, ensure potential confounders do not introduce bias. )(Note: mediation analysis is complex [@vanderweele2015; @vansteelandt2012; @bulbulia2023].)

#### 3. **The Collider Rule**

Conditioning on a common effect opens a path between the two variables that cause it.  

#### 4. **The Proxy Rule**

Conditioning on a descendant is a proxy for conditioning on its parent.  -->

{{< pagebreak >}}

## References

::: {#refs}
:::

