% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames,x11names}{xcolor}
%
\documentclass[
  singlecolumn]{article}

\usepackage{amsmath,amssymb}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage[]{libertinus}
\ifPDFTeX\else  
    % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\usepackage[top=30mm,left=20mm,heightrounded]{geometry}
\setlength{\emergencystretch}{3em} % prevent overfull lines
\setcounter{secnumdepth}{-\maxdimen} % remove section numbering
% Make \paragraph and \subparagraph free-standing
\makeatletter
\ifx\paragraph\undefined\else
  \let\oldparagraph\paragraph
  \renewcommand{\paragraph}{
    \@ifstar
      \xxxParagraphStar
      \xxxParagraphNoStar
  }
  \newcommand{\xxxParagraphStar}[1]{\oldparagraph*{#1}\mbox{}}
  \newcommand{\xxxParagraphNoStar}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
  \let\oldsubparagraph\subparagraph
  \renewcommand{\subparagraph}{
    \@ifstar
      \xxxSubParagraphStar
      \xxxSubParagraphNoStar
  }
  \newcommand{\xxxSubParagraphStar}[1]{\oldsubparagraph*{#1}\mbox{}}
  \newcommand{\xxxSubParagraphNoStar}[1]{\oldsubparagraph{#1}\mbox{}}
\fi
\makeatother


\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
% definitions for citeproc citations
\NewDocumentCommand\citeproctext{}{}
\NewDocumentCommand\citeproc{mm}{%
  \begingroup\def\citeproctext{#2}\cite{#1}\endgroup}
\makeatletter
 % allow citations to break across lines
 \let\@cite@ofmt\@firstofone
 % avoid brackets around text for \cite:
 \def\@biblabel#1{}
 \def\@cite#1#2{{#1\if@tempswa , #2\fi}}
\makeatother
\newlength{\cslhangindent}
\setlength{\cslhangindent}{1.5em}
\newlength{\csllabelwidth}
\setlength{\csllabelwidth}{3em}
\newenvironment{CSLReferences}[2] % #1 hanging-indent, #2 entry-spacing
 {\begin{list}{}{%
  \setlength{\itemindent}{0pt}
  \setlength{\leftmargin}{0pt}
  \setlength{\parsep}{0pt}
  % turn on hanging indent if param 1 is 1
  \ifodd #1
   \setlength{\leftmargin}{\cslhangindent}
   \setlength{\itemindent}{-1\cslhangindent}
  \fi
  % set entry spacing
  \setlength{\itemsep}{#2\baselineskip}}}
 {\end{list}}
\usepackage{calc}
\newcommand{\CSLBlock}[1]{\hfill\break\parbox[t]{\linewidth}{\strut\ignorespaces#1\strut}}
\newcommand{\CSLLeftMargin}[1]{\parbox[t]{\csllabelwidth}{\strut#1\strut}}
\newcommand{\CSLRightInline}[1]{\parbox[t]{\linewidth - \csllabelwidth}{\strut#1\strut}}
\newcommand{\CSLIndent}[1]{\hspace{\cslhangindent}#1}

\usepackage{booktabs}
\usepackage{longtable}
\usepackage{array}
\usepackage{multirow}
\usepackage{wrapfig}
\usepackage{float}
\usepackage{colortbl}
\usepackage{pdflscape}
\usepackage{tabu}
\usepackage{threeparttable}
\usepackage{threeparttablex}
\usepackage[normalem]{ulem}
\usepackage{makecell}
\usepackage{xcolor}
\input{/Users/joseph/GIT/latex/latex-for-quarto.tex}
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\AtBeginDocument{%
\ifdefined\contentsname
  \renewcommand*\contentsname{Table of contents}
\else
  \newcommand\contentsname{Table of contents}
\fi
\ifdefined\listfigurename
  \renewcommand*\listfigurename{List of Figures}
\else
  \newcommand\listfigurename{List of Figures}
\fi
\ifdefined\listtablename
  \renewcommand*\listtablename{List of Tables}
\else
  \newcommand\listtablename{List of Tables}
\fi
\ifdefined\figurename
  \renewcommand*\figurename{Figure}
\else
  \newcommand\figurename{Figure}
\fi
\ifdefined\tablename
  \renewcommand*\tablename{Table}
\else
  \newcommand\tablename{Table}
\fi
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Listing}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}
\makeatother
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\@ifpackageloaded{subcaption}{}{\usepackage{subcaption}}
\makeatother
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\usepackage{bookmark}

\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same} % disable monospaced font for URLs
\hypersetup{
  pdftitle={An Invitation to Causal Inference in Environmental Psychology},
  pdfkeywords={DAGS, Causal
Inference, Confounding, Environmental, Longitudinal, Psychology},
  colorlinks=true,
  linkcolor={blue},
  filecolor={Maroon},
  citecolor={Blue},
  urlcolor={Blue},
  pdfcreator={LaTeX via pandoc}}

\title{An Invitation to Causal Inference in Environmental Psychology}

\usepackage{academicons}
\usepackage{xcolor}

  \author{Joseph A Bulbulia}
            \affil{%
             \small{     Victoria University of Wellington, New Zealand,
School of Psychology, Centre for Applied Cross-Cultural Research
          ORCID \textcolor[HTML]{A6CE39}{\aiOrcid} ~0000-0002-5861-2056 }
              }
      \usepackage{academicons}
\usepackage{xcolor}

  \author{Donald W Hine}
            \affil{%
             \small{     University of Canterbury, School of Psychology,
Speech and Hearing
          ORCID \textcolor[HTML]{A6CE39}{\aiOrcid} ~0000-0002-3905-7026 }
              }
      


\date{2024-10-06}
\begin{document}
\maketitle
\begin{abstract}
This chapter introduces causal inference within environmental
psychology, underscoring its fundamental differences from traditional
statistical analysis. The content is organised into four main sections:
1. \textbf{Non-technical introduction}: this section explains how causal
inference focuses on answering specific causal questions by defining a
pre-specified counterfactual contrast between treatment conditions, as
experienced by an entire population. Crucially, for each individual in
the population, only one potential outcome is observed. Therefore,
causal effects must be estimated using assumptions. 2. \textbf{Causal
Directed Acyclic Graphs (DAGs) tutorial}: we provide tools to approach
causal identification by explicitly stating causal assumptions on a
graph. Causal DAGs also have a secondary utility: they illustrate how
traditional statistical modelling approaches can introduce biases. 3.
\textbf{Practical examples}: we apply causal DAGs to common scenarios in
observational environmental psychology, demonstrating their practical
utility. 4. \textbf{Guidelines for building causal workflows}: we
provide guidelines on establishing causal inference workflows tailored
for environmental psychology. The main aim of this chapter is to guide
environmental psychologists in adopting the robust causal workflows that
have enabled econometricians and epidemiologists to systematically
address causal questions, but which remain scarce in psychological
science.
\end{abstract}

Psychological scientists are taught that ``correlation does not imply
causation.'' By ``correlation,'' we refer to statistical measures of
association between variables. Most statistical techniques---from
t-tests to structural equation models---estimate associations from data.
Although we know that measuring associations does not imply causation,
in observational settings we often persist in reporting statistical
associations as if they are meaningful and, perhaps even as tentative
evidence for causation. The purpose of this chapter is to clarify why
such reporting is confused and misleading, and to guide you toward
better practices.

What do we mean by ``causation''? Causation has been a topic of
extensive interest and debate in philosophy
(\citeproc{ref-lewis1973}{Lewis 1973}). Here, we narrow our focus. We
consider the assumptions under which it is possible to estimate causal
effects from data and how to estimate them. In causal-effect estimation,
or `causal inference,' investigators seek to quantify the average
differences across a specified population or sub-population (the
``target population'') that interventions would produce on well-defined
outcomes. This requires comparing at least two states of the world: one
where the population experiences a treatment and another where they do
not, or experience a different level of treatment. While the concept of
causation in causal inference is more narrowly defined than causation
itself, it draws intellectual inspiration from David Hume, who, in his
Enquiries Concerning Human Understanding (1751), characterises the
cause-effect relationship as follows:

\begin{quote}
``If the first object had not been, the second never would have
existed'' (\citeproc{ref-hume1902}{Hume 1902}) (emphasis added).
\end{quote}

This conceptualisation aligns closely with the counterfactual approach
in causal inference, which considers what would have happened to an
outcome in both the presence and the absence of a treatment.

Hume's definition relies on counterfactual thinking---specifically, the
comparison of two mutually exclusive states of the world: one where an
event occurs and one where it does not. For Hume, assessing causation
requires not just observing events as they happen but also considering
how the world might have differed had those events not occurred. Such
comparisons, where we consider scenarios in which treatments did not
take place, are known as ``counterfactual contrasts;'' such contrasts
are fundamental to causal-effect estimation. In modern causal inference,
these contrasts are formalised within the potential outcomes framework,
which estimates the average difference in outcomes between treated and
control conditions.

Importantly, although statistically evaluating associations from data is
essential for estimating average treatment effects, causation estimation
cannot be derived from the study of associations alone. A careful and
systematic workflow is required. By the end of this chapter, you will
understand why, without such a workflow, common analytic
techniques---such as linear regression, correlation, and structural
equation modelling---lack causal interpretations and may mislead
investigators.

\hyperref[section-part1]{\textbf{Part 1}} introduces the counterfactual
framework of causal inference, focusing on the three fundamental
assumptions necessary for estimating average causal effects. We build
intuition for these concepts by considering randomised controlled
trials, where these assumptions are met through enforced randomisation
(\citeproc{ref-hernan2017per}{HernÃ¡n and Robins 2017};
\citeproc{ref-robins2008estimation}{Robins and Hernan 2008};
\citeproc{ref-westreich2012berkson}{Westreich 2012};
\citeproc{ref-westreich2015}{Westreich \emph{et al.} 2015}).

\hyperref[section-part2]{\textbf{Part 2}} introduces causal Directed
Acyclic Graphs (DAGs), powerful tools for visualising and addressing the
assumption of conditional exchangeability (also known as the ``no
unmeasured confounders'' assumption). We discuss the rules of
d-separation, which allow investigators to identify appropriate
variables to adjust for confounding. While most psychological scientists
are aware that regression adjustment is commonly used to control
confounding, they may not be fully aware of the formal criteria required
to select appropriate adjustment variables. However, understanding the
formal criteria is essential because over-adjustment may introduce bias
or reduce statistical power, leading to misleading conclusions.

\hyperref[section-part3]{\textbf{Part 3}} provides seven practical
examples where causal diagrams address real-world causal questions.
These examples clarify three key objectives: (1) constructing causal
diagrams that accurately represent hypothesised relationships between
variables; (2) identifying and evaluating the assumptions required to
estimate causal effects from observational data, ensuring they align
with the underlying causal structure; and (3) using DAGs to guide the
estimation of causal effects, from identifying necessary adjustment sets
to applying appropriate statistical methods for valid inference. These
examples serve as practical guides for translating causal questions into
analysable causal identification models, clarifying the assumptions
needed to estimate causal relationships from data.

\hyperref[section-part4]{\textbf{Part 4}} ooffers practical guidelines
for environmental psychologists aiming to infer causal effects from
observational data. Given that assumptions about causal relationships
are often uncertain or subject to debate, we recommend reporting
multiple causal diagrams and corresponding analysis strategies to
capture different plausible pathways. This approach enhances
transparency in how causal relationships are inferred.

We conclude by suggesting further readings and resources for those
interested in learning more about causal inference.

\subsection{Part 1: An Overview of the Counterfactual Framework for
Causal Inference}\label{section-part1}

\subsection{The Fundamental Problem of Causal Inference: Counterfactual
Comparisons in Environmental
Psychology}\label{the-fundamental-problem-of-causal-inference-counterfactual-comparisons-in-environmental-psychology}

Imagine you are faced with a significant life decision: enrolling in a
graduate programme in environmental psychology in New Zealand or
accepting a job offer from a leading renewable energy company. This
choice will shape your future, influencing your lifestyle, income, and
social network. Which option is best for you?

The challenge is that, once you choose one path, you cannot observe how
your life would have unfolded on the other. If you go to graduate
school, you will experience that outcome, but the outcome of taking the
job remains unknown---and vice versa. This is \textbf{the fundamental
problem of causal inference}: we can never observe both potential
outcomes for the same individual, so the path not taken remains an
unobservable ``what if?'' --- a counterfactual that cannot be measured
(\citeproc{ref-holland1986}{Holland 1986}).

Again, counterfactual comparisons lie at the heart of causal inference,
as they involve contrasting what actually happened with what would have
happened under a different scenario. In environmental psychology,
computing counterfactual contrasts is essential for understanding the
effects of psychological and behavioural interventions on well-defined
outcomes. However, the full data required to make such contrasts are
inevitably partially missing (\citeproc{ref-edwards2015}{Edwards
\emph{et al.} 2015}; \citeproc{ref-westreich2013}{Westreich and
Greenland 2013}).

Causal Inference in E

\subsubsection{Causal Inference in Experiments: We Problem of Missing
Counterfactuals}\label{causal-inference-in-experiments-we-problem-of-missing-counterfactuals}

Consider a question relevant to environmental psychologists: What is the
causal effect of access to green spaces on subjective happiness? Denote
happiness by \(Y\), where \(Y_i\) represents the happiness of individual
\(i\).

Suppose ``ample access to green space'' is represented as a binary
variable: \(A = 1\) for ``ample access'' and \(A = 0\) for ``lack of
ample access.'' These conditions are mutually exclusive. While we
simplify the treatment to a binary variable, the concepts apply to more
complex or continuous treatments. Estimating causal effects always
requires a contrast between well-defined treatment conditions.
Importantly, defining clear causal questions is essential but often
neglected in psychological science outside experimental work.

Imagine our aim is to compare potential outcomes under different
treatment conditions. Specifically, we contrast the happiness of
individuals with access to green space (\(A = 1\)) against those without
(\(A = 0\)). The target population for this contrast should be
explicit---for example, all New Zealand residents in 2024.

A clear causal question, framed as a counterfactual contrast---also
known as a ``causal estimand''---might be:

\begin{quote}
\emph{``Among New Zealand residents, does access to abundant green space
increase self-perceived happiness compared to environments without such
spaces?''}
\end{quote}

Now, imagine---hypothetically and ethically---that we could randomise
individuals to high or low green space access. Even in this ideal
experimental setup, causal inference faces a considerable challenge:
missing data for potential outcomes. For each person, only one potential
outcome is observed, depending on the treatment they receive. The
outcome they would have experienced under the alternative treatment is
unobserved---the counterfactual. Such missingness is the fundamental
problem of causal inference, raised to the level of treatment groups. We
only observe \(Y_i(1)\) for individuals with \(A_i = 1\) and \(Y_i(0)\)
for individuals with \(A_i = 0\). The other potential outcome for each
individual remains unobserved. However, although this missingness in the
``full data'' poses a challenge at the individual level, we can estimate
the average treatment effect within the sampled population without
needing to observe all individual-level treatment effects.

Although individual causal effects are generally elusive, we can recover
causal effect estimates by changing our causal question. For example,
although randomised controlled experiments do not solve the fundamental
problem of causal inference at the level of individuals, they may obtain
consistent causal effect estimates for average treatment effects at the
level of populations. Randomised experiments solve the missing data
problem at the heart of causal inference by satisfying three fundamental
assumptions required for obtaining average treatment effects. These are
the (1) Conditional Exchangeability Assumption, (2) The Causal
Consistency Assumption, and (3) The Positivity Assumption. We call an
``observational study'' one in which the data have not been obtained by
randomised treatment assignment and controlled administration of
treatment. We then consider how, when observational studies satisfy the
three fundamental assumptions of causal inference, investigators may
obtain consistent causal effect estimates for average treatment effects.

\paragraph{Assumption 1: Conditional
Exchangeability}\label{assumption-1-conditional-exchangeability}

First, we define the expected value of a treatment \(A=a\) as the sum of
individual counterfactual (or equivalently) ``potential'' outcomes for
individuals within a specified population:

\[
\mathbb{E}[Y(a)] \equiv \frac{1}{n} \sum_{i=1}^n Y_i(a)
\]

Note that in causal inference, we assume these potential outcomes to be
real, even if determined stochastically.

We next define the average treatment effect (ATE) as a contrast between
the averages of the potential outcomes in each treatment condition:

\[
\text{ATE} = \mathbb{E}[Y(1)] - \mathbb{E}[Y(0)]
\]

Suppose that the individuals in a sample are representative of the
population of interest---the target population. In this setting,
randomisation ensures there is no common cause of the treatment and the
potential outcomes that would be observed under treatment. That is, if
treatment assignment is determined by chance, any difference in the
average response within treatment groups is best explained by the
treatment itself.

Mathematically, we express the absence of a common cause of treatment
assignment and the potential outcomes under treatment:

\[
Y(a) \coprod A
\]

This notation means that the potential outcomes \(Y(a)\) are independent
of (\(\coprod\)) the treatment assignment \(A\). Importantly, the
observed outcomes need not be independent of treatment. We do not
require \(Y|A\). Indeed, we should expect such differences except when
there is no treatment effect. Randomisation achieves unconditional
independence, allowing us to estimate causal effects.

Importantly, we can relax the requirement for unconditional independence
and allow randomisation to occur conditional on certain measurable
features of the sampled population. For example, suppose we randomise
treatment within different age cohorts such that older individuals have
a greater probability of receiving the treatment than younger
individuals. Assume that the treatment effect is constant across all
ages. In this scenario, we would expect to see higher average treatment
effects in older cohorts simply because a larger proportion of older
individuals receive the treatment.

When randomisation occurs conditional on a measurable feature such as
age, and the treatment effect is constant, differences in treatment
probabilities across groups can lead to variations in observed average
treatment effects. However, if we adjust for this difference in the
probability of receiving treatment, or simply compare treatment effects
within the different age strata, the bias from differential treatment
group assignment disappears.

We next introduce the symbol \(L\) to denote measured variables that
might be common causes of the treatment (\(A\)) and of the treatment
effect (\(Y(a)\)). We write that the potential or counterfactual outcome
\(Y(a)\) is independent of \(A\), conditional on \(L\) as follows:

This assumption means that, within levels of \(L\), the treatment
assignment is as good as random. In experiments, randomisation ensures
unconditional exchangeability (\(Y(a) \coprod  A\)). However, to compute
causal effects from data in which the treatments are not randomised, we
must believe that within levels of measured covariates \(L\), the
treatment is as good as random. Practically, this involves measuring all
common causes of \(A\) and \(Y\).

\subparagraph{Challenges in Observational
Settings}\label{challenges-in-observational-settings}

In observational studies, achieving conditional exchangeability is
challenging because treatment assignment is not controlled. Individuals
with access to green spaces may differ from those without in various
ways. We have noted that income might be a common cause of both access
to greenspace and happiness. However there are other common causes such
as:

\begin{itemize}
\tightlist
\item
  \textbf{Age}: different age groups may have varying access to green
  spaces and different happiness baselines.
\item
  \textbf{Health status}: healthier individuals might choose to live
  near green spaces and also report higher happiness.
\item
  \textbf{Past happiness}: happiness might cause people to seek green
  spaces, and happiness in one's past might be a cause of happiness in
  one's future.
\end{itemize}

Merely observing statistical associations between access to greenspace
and happiness does not itself resolve the causal question of whether
intervening on access across a population would affect happiness levels,
and if so, in which direction and by how much.

\paragraph{Assumption 2: Causal
Consistency}\label{assumption-2-causal-consistency}

Causal consistency requires that the potential outcome under the
treatment actually received equals the observed outcome:

\[
Y_i = Y_i(a) \quad \text{if } A_i = a
\]

Note that in causal inference, we compare potential outcomes under at
least two different treatments, say, \(Y(a = 1)\) and \(Y(a = 0)\) (in
causal inference we use a lower case variable to note that the random
variable \(A\) is fixed to a certain level (\(A = a\))). To compute
causal contrasts the counterfactual or potential outcomes under
treatment must be observable. The causal consistency assumption allows
investigators to link counterfactual or potential outcomes to observed
outcomes. It might seem obvious that if one receives a treatment, we can
say that the observation of the outcome following treatment is no longer
counterfactual -- it is actual. However, for the causal consistency
assumption to hold across a population, we must assume that the
treatments are well-defined and consistently administered. Thus the
causal consistency assumption is a two-edged sword. On the one hand, we
may use it -- if other assumptions are satisfied -- to compute causal
contrasts. Simply put, causal consistency puts the factual into
counterfactual. This is good. In \emph{controlled} experiments we may
take causal consistency for granted. The investigators administer
consistent treatments. However, in observational settings no such
control is ensured. For example, one reason it has been difficult to
investigate the causal effects of weight loss from massive observational
medical datasets is that there are many ways to lose weight -- eating
less and healthfully and exercising causes people to lose weight.
However, one may also lose weight from smoking, psychological distress,
stomach stapling, amputation, cancer, and famine. The mechanisms of the
latter forms of weight loss are unhealthy. Thus stating a causal
contrast, for example, as the expected difference in all-cause mortality
after five successive years of weight loss is an invitation for
confusion. The treatments that lead to weight loss in medical data are
not comparable across all cases.

\subparagraph{Challenges in Observational
Settings}\label{challenges-in-observational-settings-1}

Similarly, in observational environmental psychology, the treatments of
interest may not be standardised. For green space access, we might worry
about variability in the green spaces we have measured: not all green
spaces are equal---differences in size, quality, and amenities can
affect outcomes. Moreover, there is likely considerable variability in
exposure levels, the amount of time that individuals spend in green
spaces can vary widely. These variations can violate causal consistency
because the ``treatment'' isn't the same across individuals on which
treatments are to be compared. Supplemental materials S-Appendix-B
addresses the causal consistency assumption in detail.

\paragraph{Assumption 3: Positivity}\label{assumption-3-positivity}

Positivity requires that every individual has a non-zero probability of
receiving each level of the treatment, given their covariates \(L\):

\[
P(A = a \mid L = l) > 0 \quad \text{for all } a, l
\]

This assumption ensures that we have data to compare treatment effects
across all levels of \(L\). For example, suppose we are interested in
the causal effects of vasectomy on happiness. It would not make sense to
include biological females in this study because biological females
cannot have vasectomies (\citeproc{ref-hernan2023}{Hernan and Robins
2023}; \citeproc{ref-westreich2010}{Westreich and Cole 2010}).

\subparagraph{Challenges in Observational
Settings}\label{challenges-in-observational-settings-2}

In practice, some individuals may have zero probability of receiving
certain treatments. Consider:

\begin{itemize}
\tightlist
\item
  \textbf{Geographic constraints}: some regions may lack green spaces
  entirely.
\item
  \textbf{Economic barriers}: low-income individuals may have no access
  to areas with ample green spaces.
\item
  \textbf{Policy restrictions}: zoning laws may prevent certain
  individuals from accessing specific environments.
\end{itemize}

Violating the positivity assumption means we cannot estimate the causal
effect for those individuals because we lack data on both treatment
conditions for them.

\subsubsection{Summary}\label{summary}

Causal modelling differs from traditional statistical modelling.
Although statistical models aim to describe associations within observed
data, they often do not distinguish between correlation and causation,
leading to potentially misleading conclusions when inferring causal
relationships. Causal inference requires statistical inference; however,
statistics comes near the end of a workflow that begins by stating a
causal quantity with reference to contrasts that would be obtained if we
had the full data for the population of interest treated at different
levels of the intervention (\citeproc{ref-ogburn2021}{Ogburn and
Shpitser 2021}). We then seek to understand how these contrasts may be
estimated from the data we have.

Common practices in statistical modelling, such as regression adjustment
and structural equation modelling, frequently fall short in estimating
causal effects because they do not account for the causal structure
between variables. Including all available covariates in a regression
model without understanding their causal roles can introduce bias. In
the next section, we introduce causal directed acyclic graphs (causal
DAGs), which are intuitive graphical tools that allow us to inspect
graphs to evaluate the complex conditional dependencies that must be
understood to evaluate the ``no unmeasured confounders'' assumption.
Causal DAGs also demonstrate how common practices in statistical
modelling such as over-adjustment in regression and mediation analysis
in structural equation modelling may inadvertently introduce bias
through inappropriate adjustment for variables. Such over-adjustment can
distort causal pathways, potentially mask true relationships, or leading
to associations that do not represent causal effects. In causal
inference, such associations are aptly called ``spurious.' By making the
causal structure explicit and carefully considering which variables to
adjust for, causal DAGs may help environmental psycholosts to better
estimate causal effects in environmental psychology.

\subsection{Part 2: An Introduction to Causal
Diagrams}\label{section-part2}

We now introduce causal directed acyclic graphs (causal DAGs), beginning
with essential terminology. Refer to
\hyperref[appendix-a]{\textbf{Appendix A}} for a detailed glossary.

\subsubsection{Elements of Causal
Diagrams}\label{elements-of-causal-diagrams}

Causal diagrams distil causal relationships within a system into visual
representations. At their core, these diagrams consist of:

\paragraph{\texorpdfstring{1. \textbf{Nodes}}{1. Nodes}}\label{nodes}

Nodes represent variables or events in a causal framework. Each node
stands for an element that either influences or is influenced within the
system. Nodes encapsulate the components of our causal inquiry relevant
for evaluating the conditional exchangeability assumption. The relevant
nodes are treatments, outcomes, measured confounders and unmeasured
confounders.

\paragraph{\texorpdfstring{2.
\textbf{Arrows/Edges}}{2. Arrows/Edges}}\label{arrowsedges}

Arrows indicate the direction and presence of causal relationships
between nodes. Directed edges trace the assumed flow of causal influence
from a ``parent'' (originating variable) to a ``child'' (receiving
variable). These arrows define the causal architecture, illustrating how
we assume one variable causally affects another. Importantly, arrows
represent causal relationships regardless of whether the influence is
linear or non-linear. The reason is that the purpose of a causal DAG is
to evaluate whether and how consistent causal effect estimates can be
obtained from data. Please do not draw arrows into arrows! An arrow
should start at a parent node and terminate at a child node.

\paragraph{\texorpdfstring{3.
\textbf{Conditioning}}{3. Conditioning}}\label{conditioning}

We must decide which variables to adjust for to estimate causal effects
without confounding -- that is, to satisfy the conditional
exchangeability assumption of causal inference -- conditional on
measured covariates, the treatments to be compared are ``as good as
random''. We denote variables that we ``control for,'' ``condition on,''
or ``adjust for'' by enclosing them in a box.

\subsubsection{The Rules of
d-separation}\label{the-rules-of-d-separation}

Judea Pearl introduced the concept of `d-separation' to analyse
relationships within causal diagrams (\citeproc{ref-pearl1995}{Pearl
1995}). These rules help us identify confounders and develop strategies
for valid causal inference from statistical associations. The ``d''
stands for ``directional.'' Pearl's ``back-door path criterion'' is a
complete algorithm for identifying causal effects from data
(\citeproc{ref-pearl2009a}{Pearl 2009b}). We describe Pearl's algorithm
and proof in detail elsewhere (\citeproc{ref-bulbulia2023}{Bulbulia
2024b}). In For the present purposes, it will be sufficient to summarise
Pearl's work.

\textbf{Basic Concepts}

\paragraph{Dependence}\label{dependence}

Denoted as \(A \cancel\coprod B\), indicating that the probability
distributions of \(A\) and \(B\) are interrelated. Knowledge about one
variable provides insights into the other, suggesting a potential causal
or associational link.

\paragraph{Independence}\label{independence}

Denoted as \(A \coprod B\), signifying that \(A\) and \(B\) are
independent. Information about one variable reveals nothing about the
other, indicating no direct causal or associational connection.

\paragraph{Blocked Paths and
d-Separation}\label{blocked-paths-and-d-separation}

A path is ``blocked'' when a node on it prevents causal influence from
passing between variables. D-separation occurs when all paths between
two variables are blocked (\(A \coprod B\)), indicating no direct
statistical association between them. This is crucial for enabling
unbiased causal inference.

\paragraph{Open Paths}\label{open-paths}

An ``open'' path exists if at least one route between variables remains
unblocked, allowing the transmission of association---even without
direct causation (\(A \cancel\coprod B\)).

\subsubsection{The Five Elementary Graphical Structures of Causality and
Five Rules for Confounding Control}\label{sec-five-elementary}

To uncover causal insights from statistical relationships, we must
understand five basic graphical structures. We next examine these
structures, remembering that balancing confounders across treatments
requires ensuring statistical independence between potential outcomes
and treatment (\(A \coprod Y(a) \mid L\)) within groups defined by
measured covariates \(L\). Because we can nearly always assume that
there are unmeasured confounders \(U\), it is always advisable to
perform sensitivity analyses.

\paragraph{Absence of Causality: Two Variables with No
Arrows}\label{absence-of-causality-two-variables-with-no-arrows}

When no arrows connect \(A\) and \(B\), we assume they do not share a
causal relationship and are statistically independent. Graphically, we
represent this relationship as:

\[
\xorxALARGE
\]

\paragraph{Causal Structure 1: Direct Causation Between Two
Variables}\label{causal-structure-1-direct-causation-between-two-variables}

A causal arrow (\(A \to B\)) signifies that changes in \(A\) directly
cause changes in \(B\), creating statistical dependence between them.
This direct causal link is graphically depicted as:

\[
\xtoxALARGE
\]

\paragraph{\texorpdfstring{\textbf{Rule 1: Ensure That the Treatment
Precedes the
Outcome}}{Rule 1: Ensure That the Treatment Precedes the Outcome}}\label{sec-four-rules}

Causality follows the arrow of time; an outcome occurs after the
intervention that causes it. If the timing of events in your data is
unknown, you cannot ensure that the treatment precedes the outcome.

\subparagraph{Motivating Example}\label{motivating-example}

Suppose we find an association between conservation behaviours and
happiness. We might infer that conservation behaviours cause happiness.
However, the association might be entirely because happy people are more
likely to engage in conservation behaviours. With only cross-sectional
data, we cannot rule out this alternative explanation.

\paragraph{Causal Structure 2: The Fork Structure---Common Cause
Scenario}\label{causal-structure-2-the-fork-structurecommon-cause-scenario}

The fork structure, indicated by \(A \rightarrow B\) and
\(A \rightarrow C\), denotes that \(A\) is a common cause influencing
both \(B\) and \(C\). Graphically:

\[
\forkLARGE
\]

Pearl proved that when we condition on the common cause \(A\) (indicated
by \(\boxed{A}\)), \(B\) and \(C\) become conditionally independent
(\citeproc{ref-pearl2009a}{Pearl 2009b}). By adjusting for the common
cause, any non-causal association between \(B\) and \(C\) is effectively
blocked at node \(A\).

\subparagraph{Motivating Example}\label{motivating-example-1}

Suppose observations reveal that areas with higher rates of bicycle
commuting also have lower levels of psychological distress. Does bicycle
commuting directly reduce distress? Not necessarily. A common
environmental factor might influence both. Consider sunshine hours as
the common cause:

\begin{itemize}
\tightlist
\item
  Sunshine (\(A\)) encourages bicycle use (\(B\)).
\item
  Sunshine (\(A\)) contributes to lower psychological distress (\(C\)).
\end{itemize}

By accounting for the common cause (sunshine hours) and comparing days
with similar sunshine levels, the apparent link between bicycle
commuting and distress disappears. By ``spurious association'' we mean
that the two variables are not causally linked such that intervening on
one would result in a change in the other corresponding to the magnitude
of the statistical association in the target population. Adjusting for
the fork's common cause eliminates the spurious association.

\paragraph{\texorpdfstring{\textbf{Rule 2: The Fork
Rule}}{Rule 2: The Fork Rule}}\label{rule-2-the-fork-rule}

If interested in the causal effect of \(B \to C\), condition on
\(\boxed{A}\).

\paragraph{Causal Structure 3: The Chain Structure---A
Mediator}\label{causal-structure-3-the-chain-structurea-mediator}

The chain structure (\(A \rightarrow B \rightarrow C\)) illustrates a
setting where \(A\) causes \(B\), and \(B\) subsequently causes \(C\).
Conditioning on the intermediary variable \(B\) (denoted \(\boxed{B}\))
interrupts the causal pathway, rendering \(A\) and \(C\) conditionally
independent. Graphically:

\[
\chainLARGE
\]

\subparagraph{Motivating Example}\label{motivating-example-2}

Suppose we assess the effect of green space renovation in urban areas
(\(A\)) on local community engagement (\(B\)), which then reduces
neighbourhood crime rates (\(C\)). Renovating green spaces boosts
community engagement, leading to decreased crime rates.

Controlling for the mediator, community engagement, might hide the
broader effect of green space renovation. If the primary path from
renovation to crime reduction is via enhanced engagement, adjusting for
engagement could misleadingly suggest that renovation doesn't influence
crime rates. The takeaway: avoid conditioning on a mediator when
examining environmental changes' effects on social outcomes.

\subparagraph{\texorpdfstring{\textbf{Rule 3: The Chain
Rule}}{Rule 3: The Chain Rule}}\label{rule-3-the-chain-rule}

If investigating the \emph{total} causal effect of \(A \to C\),
\emph{avoid} conditioning on the mediator \(B\).

\textbf{Important note:} Assessing causal mediation requires further
assumptions, which we won't discuss here
(\citeproc{ref-bulbulia2024swigstime}{Bulbulia 2024c};
\citeproc{ref-vanderweele2015}{VanderWeele 2015}).

\paragraph{Causal Structure 4: The Collider Structure---A Common
Effect}\label{causal-structure-4-the-collider-structurea-common-effect}

The collider (\(A \to C\), \(B \to C\)) features two factors
independently causing a common effect. Initially, \(A\) and \(B\) lack
association. Conditioning on the collider \(C\) (or its descendant)
introduces a spurious association between \(A\) and \(B\). Graphically:

\[
\immoralityLARGE
\]

\subparagraph{Motivating Example}\label{motivating-example-3}

Are we interested in whether access to green spaces (\(A\)) causes
people to become happier (\(B\))? Suppose we control for health (\(C\)),
a common effect of both \(A\) and \(B\). Conditioning on \(C\) opens a
non-causal path between \(A\) and \(B\):

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Among unhealthy individuals (low \(C\)), those with high green space
  access (\(A\)) may appear less happy (\(B\)). When we stratify by
  \(C\), a negative association emerges, even if \(A\) and \(B\) aren't
  causally linked.
\item
  Among healthy individuals (high \(C\)), those with low green space
  access (\(A\)) may be happier (\(B\)). Again, stratifying by \(C\)
  shows a negative association.
\end{enumerate}

Controlling for health introduces a spurious negative association
between green space access and happiness. This arises from conditioning
on a collider. Without controlling for health---and assuming no other
confounding paths---the misleading association wouldn't be present.

This example illustrates the risk of confounding analysis by
conditioning on an outcome influenced by both variables of interest.

\paragraph{\texorpdfstring{\textbf{Rule 4: The Collider
Rule}}{Rule 4: The Collider Rule}}\label{rule-4-the-collider-rule}

When assessing the causal effect of \(A \to B\), do not condition on a
collider (\(C\)) or its descendants. Doing so may introduce an
association that appears causal but is not.

\paragraph{Building Complex Causal Relationships from Basic
Structures}\label{building-complex-causal-relationships-from-basic-structures}

All forms of confounding bias stem from combinations of the basic causal
structures we've outlined (absence/presence of cause, forks, chains, and
colliders). Understanding these elements allows us to identify potential
confounders based on our assumptions encoded in a causal diagram.
Consider an example combining two structures: the collider
(\(A \rightarrowred \boxed{C} \leftarrowred B\)) and basic causality
(\(C \rightarrowNEW D\)). This combination produces confounding by
proxy: \(A \rightarrowred \boxed{D} \leftarrowred B\).

Causation implies statistical association. Therefore, causal inheritance
implies \emph{statistical dependence by inheritance}. This property
makes descendants act as stand-ins or proxies for their parents.

\subparagraph{Motivating Example}\label{motivating-example-4}

Reconsider whether access to urban green spaces (\(A\)) affects wealth
(\(B\)). Imagine they don't, but both independently contribute to
well-being (\(C\)). Suppose only those high in well-being respond to our
survey. By effectively conditioning on a stratum (\(D\)), a descendant
of the collider well-being, we may inadvertently induce an association
between green space access and socioeconomic status---inferring that
those with access to green space tend to have lower income.

\[
\immoralityChildA
\]

Colliders and their descendants can set subtle ``traps'' that induce
spurious associations.

However, as we'll see in the next section, conditioning on proxies of
unmeasured confounders opens possibilities for confounding control
beyond our measured variables. We can sometimes leverage proxies to
reduce bias in our causal inferences.

\subparagraph{\texorpdfstring{\textbf{Rule 5: The Proxy
Rule}}{Rule 5: The Proxy Rule}}\label{rule-5-the-proxy-rule}

Conditioning on a descendant is akin to conditioning on its parent. A
descendant is a \emph{proxy} for its parent. Avoid conditioning on
descendants when conditioning on the parent would induce misleading
associations.

\paragraph{Role of assumptions}\label{role-of-assumptions}

Causal diagrams bring structure to complex environmental psychology
systems. They promote critical thinking about relationships, improving
study design and the chances of isolating true causal effects. However,
they cannot avoid assumptions. Observational data alone cannot prove
causation; many diagrams can be consistent with the data. The power of
causal diagrams lies in helping investigators understand how their
assumptions interact with the data. We should create causal diagrams in
collaboration with subject area experts because every path except the
\(A \to Y\) path is assumed. When experts disagree, we should propose
multiple causal diagrams to reflect differing assumptions and report the
outcomes of their corresponding confounding control strategies.

\subsubsection{How to Create Causal Diagrams to Address Identification
Problems}\label{how-to-create-causal-diagrams-to-address-identification-problems}

The \textbf{causal identification problem}, or simply the
\textbf{identification problem} centres on whether we can derive the
true causal effect of a treatment (\(A\)) on an outcome (\(Y\)) from
observed data. Addressing this problem involves two core components:

\paragraph{Evaluating bias in the absence of a Treatment
Effect}\label{evaluating-bias-in-the-absence-of-a-treatment-effect}

Before attributing any statistical association to causality, we must
eliminate non-causal sources of correlation by:

\begin{itemize}
\tightlist
\item
  Identifying factors that influence both treatment (\(A\)) and outcome
  (\(Y\)).
\item
  Developing adjustment strategies to control for measured confounders.
\item
  Blocking backdoor paths that create indirect, non-causal links between
  \(A\) and \(Y\). By adjusting for confounders, we aim to achieve
  d-separation between \(A\) and \(Y\).
\end{itemize}

\paragraph{Evaluating bias in the presence of a treatment
effect}\label{evaluating-bias-in-the-presence-of-a-treatment-effect}

After addressing potential confounders, we must ensure any remaining
association between \(A\) and \(Y\) reflects a true causal relationship.
We address \textbf{over-conditioning bias} by:

\begin{itemize}
\tightlist
\item
  Avoiding mediator bias
\item
  Avoiding collider bias
\item
  Verifying that the association between \(A\) and \(Y\) after
  adjustments is unbiased
\end{itemize}

Pearl's back-door path criterion requires us to identify and control for
confounders while avoiding new biases from overconditioning. With this
in mind, here's how investigators can construct effective causal
diagrams:

\paragraph{1. Clarify the Research Question and Target
Population}\label{clarify-the-research-question-and-target-population}

Before drawing any causal diagram, state the problem it addresses and
the population to whom it applies. Causal identification strategies may
vary by question. For example, the confounding control strategy for
evaluating \(L \to Y\) differs from that for \(A \to Y\). Reporting
coefficients other than the association between \(A \to Y\) is typically
ill-advised; see Westreich and Greenland
(\citeproc{ref-westreich2013}{2013}); McElreath
(\citeproc{ref-mcelreath2020}{2020}); Bulbulia
(\citeproc{ref-bulbulia2023}{2024b}).

\paragraph{2. Draw the Most Recent Common Causes of Exposure and
Outcome}\label{draw-the-most-recent-common-causes-of-exposure-and-outcome}

Include all common causes (confounders) of both the exposure and the
outcome in your diagram, whether measured or unmeasured. Where possible,
group functionally similar common causes into a single variable (e.g.,
\(L_0\) for demographic variables).

\paragraph{3. Include All Ancestors of Measured
Confounders}\label{include-all-ancestors-of-measured-confounders}

Add any ancestors (precursors) of measured confounders associated with
the treatment, the outcome, or both. This step is crucial for addressing
hidden biases from unmeasured confounding. Simplify the diagram by
grouping similar variables.

\paragraph{4. Explicitly State Assumptions About Relative
Timing}\label{explicitly-state-assumptions-about-relative-timing}

Annotate the temporal sequence of events using subscripts (e.g.,
\(L_0\), \(A_1\), \(Y_2\)). It's imperative that causal diagrams are
acyclic.

\paragraph{5. Arrange Temporal Order
Visually}\label{arrange-temporal-order-visually}

Arrange your diagram to reflect the temporal progression of causality,
either left-to-right or top-to-bottom. This enhances comprehension of
causal relations. Establishing temporal ordering is vital for evaluating
identification problems, as discussed in
\hyperref[sec-part3]{\textbf{Part 3}}.

\paragraph{6. Box Variables Adjusted for
Confounding}\label{box-variables-adjusted-for-confounding}

Mark variables for adjustment (e.g., confounders) with boxes.

\paragraph{7. Present Paths Structurally, Not
Parametrically}\label{present-paths-structurally-not-parametrically}

Focus on whether paths exist, not their functional form (linear,
non-linear, etc.). Parametric descriptions aren't relevant for bias
evaluation in a causal diagram. For an explanation of causal interaction
and diagrams, see Bulbulia (\citeproc{ref-bulbulia2023}{2024b}).

\paragraph{8. Minimise Paths to Those Necessary for the Identification
Problem}\label{minimise-paths-to-those-necessary-for-the-identification-problem}

Reduce clutter by including only paths critical for the specific
question (e.g., back-door paths, mediators).

\paragraph{9. Consider Potential Unmeasured
Confounders}\label{consider-potential-unmeasured-confounders}

Use domain expertise to identify potential unmeasured confounders and
represent them in your diagram. This proactive step helps anticipate and
address \emph{all} possible sources of confounding bias.

\paragraph{10. State Your Graphical
Conventions}\label{state-your-graphical-conventions}

Establish and explain the graphical conventions used in your diagram
(e.g., using red to highlight open back-door paths). Consistency in
symbol use enhances interpretability, while explicit descriptions
improve accessibility and understanding.

\subsection{Part 3. How to Use Causal Diagrams for Causal Identification
Tasks- Worked Examples}\label{section-part3}

\subsubsection{Notation}\label{notation}

Causal diagrams use specific symbols to represent elements essential in
causal inference (\citeproc{ref-greenland1999}{Greenland \emph{et al.}
1999}; \citeproc{ref-pearl1995}{Pearl 1995},
\citeproc{ref-pearl2009}{2009a}). We use the following symbols:

\begin{itemize}
\tightlist
\item
  \textbf{\(A\)} is the treatment or exposure variable -- the
  intervention or condition whose effect on an outcome is under
  investigation. \textbf{This symbol represents the cause}.
\item
  \textbf{\(Y\)} is the outcome variable -- the effect or result that is
  being studied. \textbf{Y(a) represents the effect on \(Y\) when \(A\)
  is set to a specific value, \(a\)}.
\item
  \textbf{\(L\)} includes all measured confounders -- variables that may
  affect both the treatment and the outcome.
\item
  \textbf{\(U\)} includes unmeasured confounders -- variables not
  included in the analysis that could influence both the treatment and
  the outcome, potentially leading to biased conclusions.
\item
  \textbf{\(M\)} is a mediator variable -- a factor through which the
  treatment affects the outcome. The focus here is on identifying the
  total effect of treatment \(A\) on an outcome \(Y\). Still, it is also
  essential to understand how controlling for mediators can affect
  estimates of this total effect.
\end{itemize}

Table~\ref{tbl-04} provides seven worked examples that put causal
diagrams to work. Our example will focus on the question of whether
access to green space affects happiness and approach this question by
focusing on how different assumptions about (i) the structure of the
world and (ii) the observational data that have been collected may
affect strategies for confounding control and the confidence in our
results. Each example refers to a row in the table.

\begin{table}

\caption{\label{tbl-04}Worked examples: This table is adapted from
(\citeproc{ref-bulbulia2023}{Bulbulia 2024b}).}

\centering{

\terminologyelconfoundersLONG

}

\end{table}%

\subsubsection{1. The Problem of Confounding by a Common
Cause}\label{the-problem-of-confounding-by-a-common-cause}

Table~\ref{tbl-04} Row 1 describes the confounding problem of a common
cause. We encountered this problem in Part 1. Such confounding arises
when there is a variable or set of variables, denoted by \(L\), that
influence both the exposure, denoted by \(A\), and the outcome, denoted
by \(Y.\) Because \(L\) is a common cause of both \(A\) and \(Y\), \(L\)
may create a statistical association between \(A\) and \(Y\) that does
not reflect a causal association.

For instance, in the context of green spaces, consider people who live
closer to green spaces (exposure \(A\)) and their experience of improved
happiness (outcome \(Y\)). A common cause might be socioeconomic status
\(L\). Individuals with higher socioeconomic status might have the
financial capacity to afford housing near green spaces and
simultaneously afford better healthcare and lifestyle choices,
contributing to greater happiness. Thus, although the data may show a
statistical association between living closer to green spaces \(A\) and
greater happiness \(Y\), this association might not reflect a direct
causal relationship owing to confounding by socioeconomic status \(L\).
Addressing confounding by a common cause involves adjusting for the
confounder in one's statistical model. We may adjust through regression,
or more complicated methods, such as the inverse probability of
treatment weighting, marginal structural models, and others see Hernan
and Robins (\citeproc{ref-hernan2024WHATIF}{2024}), Part 3. Such
adjustment effectively closes the backdoor path from the exposure to the
outcome. Equivalently, conditioning on \(L\) d-separates \(A\) and
\(Y\).

Table~\ref{tbl-04} Row 1, Column 3, emphasises that a confounder by
common cause must precede both the exposure and the outcome. While it is
often clear that a confounder precedes the exposure (e.g., a person's
country of birth), the timing might be uncertain in other cases. We
assert its temporal precedence by positioning the confounder before the
exposure in our causal diagrams. However, such a timing assumption might
be strong when relying on cross-sectional data. Exploring causal
scenarios where the confounder follows the treatment or outcome can be
insightful in such cases. Causal diagrams are instrumental in examining
possible timings and their implications for causal inference.

Next, we examine the effects of conditioning on a variable that is an
effect of the treatment.

\subsubsection{2. Mediator Bias}\label{mediator-bias}

Table~\ref{tbl-04} Row 1 presents a problem of mediator bias. Consider
again whether proximity to green spaces, \(A\), affects happiness,
\(Y\). Suppose that physical activity is a mediator, \(L\).

To fill out the example, imagine that living close to green spaces \(A\)
influences physical activity \(L\), subsequently affecting happiness
\(Y\). If we were to condition on physical activity \(L\), assuming it
to be a confounder, we would then bias our estimates of the total effect
of proximity to green spaces \(A\) on happiness \(Y\). Such a bias
arises because of the chain rule. Conditioning on \(L\) ``d-separates''
the total effect of \(A\) on \(Y\). This phenomenon is known as mediator
bias. Notably, Montgomery \emph{et al.}
(\citeproc{ref-montgomery2018}{2018}) finds dozens of examples of
mediator bias in \emph{experiments} in which control is made for
variables that occur after the treatment. For example, obtaining
demographic and other information from participants \emph{after} a study
is an invitation to mediator bias. If the treatment affects these
variables, and the variables affect the outcome (as we assume by
controlling for them), then researchers may induce mediator bias.

To avoid mediator bias when estimating a total causal effect, we should
never condition on a mediator! The surest way to prevent this problem is
to ensure that \(L\) occurs before the treatment \(A\) and before the
outcome \(Y\). We present this solution in Table~\ref{tbl-04} Row 2 Col
3.

\subsubsection{3. Confounding by Collider Stratification (Conditioning
on a Common
Effect)}\label{confounding-by-collider-stratification-conditioning-on-a-common-effect}

Table~\ref{tbl-04} Row 1 presents a problem of collider bias.
Conditioning on a common effect, or collider stratification, occurs when
a variable, denoted by \(L\), is influenced by both the exposure,
denoted by \(A\), and the outcome, denoted by \(Y\).

Let us assume initial independence: the choice to live closer to green
spaces (exposure \(A\)) and happiness (outcome \(Y\)) are independent:
\(A \coprod Y(a)\).

We furthermore assume physical health \(L\) is an effect of green space
access, and happiness increases physical health. Thus, \(L\) is an
effect of \(A\) and \(Y\). If we were to condition on \(L\) in this
setting, we would introduce \emph{collider stratification bias}. When we
control for the common effect \(L\) (physical health), we may
inadvertently introduce confounding. This happens because knowing
something about \(L\) gives us information about both \(A\) and \(Y\).
If someone were high on physical health but low an access to greenspace,
this would imply that they are higher in happiness. Likewise, if someone
were low in physical health but high in access to green space, this
would imply lower happiness. As a result of our conditioning strategy,
it would appear that access to green space and happiness are negatively
associated. However, if we were to avoid conditioning on the common
outcome, we would find that the treatment and outcome are not
associated.

How can we avoid collider bias, the temporal sequence of measurement
affords a powerful strategy:

Ensure all common causes of \(A\) and \(Y\) -- call them \(L\) -- are
measured before the treatment \(A\) occurs. Ensure further that \(Y\)
occurs after \(A\) occurs. If the confounder \(L\) is not measured,
ensure that conditioning on its downstream proxy, \(L'\) does not induce
collider or mediator biases.

By adhering to this sequence, we can mitigate the risk of collider
stratification bias and better understand the causal relationships
between exposure, outcome, and their common effects.

\subsubsection{4. Confounding by Conditioning on a Descendant of a
Confounder}\label{confounding-by-conditioning-on-a-descendant-of-a-confounder}

Table~\ref{tbl-04} Row 4 presents a problem of collider bias by decent.
Recall the rules of d-separation also apply to conditioning on
descendants of a confounder. Thus, we may unwittingly evoke confounding
by proxy when conditioning on a measured descendant of an unmeasured
collider.

For example, if doctor visits were encoded in our data, and doctor
visits were an effect of poor health, conditioning on doctor visits
would function similarly to conditioning on poor health in the previous
example, introducing collider confounding.

\subsubsection{5. M-bias: Conditioning on Pre-Exposure
Collider}\label{m-bias-conditioning-on-pre-exposure-collider}

There are only five elementary structures of causality. Every
confounding scenario can be developed from these five elementary
structures. We next consider how we may combine these elementary causal
relationships in causal diagrams to create effective strategies for
confounding control.

Table~\ref{tbl-04} Row 5 presents a form of pre-exposure
over-conditioning confounding known as ``M-bias''. This bias combines
the collider structure and the fork structure, revealing what might not
otherwise be obvious: it is possible to induce confounding even if we
ensure that all variables have been measured \textbf{before} the
treatment. The collider structure is evident in the path \(U_Y \to L_0\)
and \(U_A \to L_0\). The collider rule shows that conditioning on
\(L_0\) opens a path between \(U_Y\) and \(U_A\). What is the result? We
find that \(U_Y\) is associated with the outcome \(Y\) and \(U_A\) is
associated with treatment \(A\). This is a fork (common cause)
structure. The association between treatment and outcome opened by
conditioning on \(L\) arises from an open back-door path that occurs
from the collider structure. We thus have confounding. How might such
confounding play out in a real-world setting?

In the context of green spaces, consider the scenario where an
individual's level of physical activity \(L\) is influenced by an
unmeasured factor related to their propensity to live near green spaces
\(A\) -- say childhood upbringing. Suppose further that another
unmeasured factor -- say a genetic factor -- increases both physical
activity \(L\) and happiness \(Y\). Here, physical activity \(L\) does
not affect the decision to live near green spaces \(A\) or happiness
\(Y\) but is a descendent of unmeasured variables that do. If we were to
condition on physical activity \(L\) in this scenario, we would create
the bias just described -- ``M-bias.''

How shall we respond to this problem? The solution is straightforward.
If \(L\) is neither a common cause of \(A\) and \(Y\) nor the effect of
a shared common cause, then \(L\) should not be included in a causal
model. In terms of the conditional exchangeability principle, we find
\(A \coprod Y(a)\) yet \(A \cancel{\coprod} Y(a)| L\). So we should not
condition on \(L\): do not control for exercise
(\citeproc{ref-cole2010}{Cole \emph{et al.} 2010}).\footnote{Note that
  when we draw a chronologically ordered path from left to right, the M
  shape for which ``M-bias'' takes its name changes to an E shape. We
  shall avoid proliferating jargon and retain the term ``M bias.''}

\subsubsection{6. Conditioning on a Descendent May Sometimes Reduce
Confounding}\label{conditioning-on-a-descendent-may-sometimes-reduce-confounding}

In Table~\ref{tbl-04} Row 6, we encounter a causal diagram in which an
unmeasured confounder opens a back-door path that links the treatment
and outcome. Here, we consider how we may use the rules of d-separation
to obtain unexpected strategies for confounding control.

Returning to our green space example, suppose an unmeasured genetic
factor \(U\) affects one's desire to seek out isolation in green spaces
\(A\) and independently affects one's happiness \(Y\). Were such an
unmeasured confounder to exist we could not obtain an unbiased estimate
for the causal effect of green space access on happiness. We have, it
seems, intractable confounding.

However, imagine a variable \(L^\prime\), a trait expressed later in
life that arises from this genetic factor. If such a trait could be
measured, even though the trait \(L'\) is expressed after the treatment
and outcome have occurred, controlling for \(L'\) would enable
investigators to close the backdoor path between the treatment and the
outcome. This strategy works because a measured effect is a \emph{proxy}
for its cause \(U\), the unmeasured confounder. By conditioning on the
late-adulthood trait, \(L'\), we partially condition on its cause,
\(U\), the confounder of \(A \to Y\). Thus, not all effective
confounding control strategies need to rely on measuring pre-exposure
variables. Thus, the elementary causal structures reveal a possibility
for confounding control by condition on a post-outcome variable. This
strategy is not intuitive. Although a common cause must occur before a
treatment (and outcome), its proxy need not! If we have a measure for
the latter but not the former, we should condition on the post-treatment
proxy of a pre-treatment common cause.

\subsubsection{7. Confounding Control with Three Waves of Data is
Powerful and Reveals Possibilities for Estimating an ``Incident
Exposure''
Effect}\label{confounding-control-with-three-waves-of-data-is-powerful-and-reveals-possibilities-for-estimating-an-incident-exposure-effect}

Table~\ref{tbl-04} row 7 presents another setting in which there is
unmeasured confounding. In response to this problem, we use the rules of
d-separation to develop a data collection and modelling strategy that
may greatly reduce the influence of unmeasured confounding.
Table~\ref{tbl-04} row 7 col 3, by collecting data for both the
treatment and the outcome at baseline and controlling for baseline
values of the treatment and outcome, any unmeasured association between
the treatment \(A_1\) and the outcome \(Y_2\) would need to be
\emph{independent} of their baseline measurements. As such, including
the baseline treatment and outcome, along with other measured covariates
that might be measured descendants of unmeasured confounders, is a
strategy that exerts considerable confounding control
(\citeproc{ref-vanderweele2020}{VanderWeele \emph{et al.} 2020}).

Furthermore, this causal graph makes evident a second benefit of this
strategy. Returning to our example, a model that controls for baseline
exposure would require that people initiate a change from the \(A_0\)
observed baseline level. Thus, by controlling for the baseline value of
the treatment, we may learn about the causal effect of shifting one's
access to green space status. This effect is called the ``incident
exposure effect.'' The incident exposure effect better emulates a
``target trial'' or the organisation of observational data into a
hypothetical experiment in which there is a ``time-zero'' initiation of
treatment in the data; see HernÃ¡n \emph{et al.}
(\citeproc{ref-hernuxe1n2016}{2016}); Danaei \emph{et al.}
(\citeproc{ref-danaei2012}{2012}); VanderWeele \emph{et al.}
(\citeproc{ref-vanderweele2020}{2020}); Bulbulia
(\citeproc{ref-bulbulia2022}{2023}). Without controlling for the
baseline treatment, we could only estimate a ``prevalent exposure
effect.'' If the initial exposure caused people some people to be
miserable, we would not be able to track this outcome. The prevalent
exposure effect would mask it, distorting causal inferences for the
quantity of interest, namely, what would happen, on average, if people
were to shift to having greater greenspace access.

Finally, we obtain further control for unmeasured confounding by
controlling for both the baseline treatment and the baseline outcome.
For an unmeasured confounder to affect both the treatment and the
outcome (and unmeasured fork structure), it would need to do so
independently of the baseline measures of the treatment and exposure
(\citeproc{ref-vanderweele2020}{VanderWeele \emph{et al.} 2020}).

Thus, we generally require repeated measures on the same unit over time
intervals to obtain an incident exposure effect and exert more robust
control for unmeasured confounding using past states of the treatment
and outcome. We must then model the treatments and outcomes as separate
elements in our statistical model.

\subsection{Part 4. Practical Guide For Constructing Causal Diagrams and
Reporting Results When Causal Structure is Unclear}\label{section-part4}

\subsubsection{Cross-sectional designs}\label{cross-sectional-designs}

In environmental psychology, researchers often grapple with whether
causal inferences can be drawn from cross-sectional data, especially
when longitudinal data are unavailable. The challenge is common to
cross-sectional designs. However, it is important to appreciate that
even longitudinal studies require careful assumption management. We next
discuss how causal diagrams can guide inference in both data types, with
examples relevant to environmental psychologists.

\paragraph{1. Graphically encode causal
assumptions}\label{graphically-encode-causal-assumptions}

Causal inference turns on assumptions. Although cross-sectional analyses
typically demand much stronger assumptions owing to the snapshot nature
of data, these assumptions, when transparently articulated, do not
permanently bar causal analysis. By stating different assumptions and
modelling the data following these assumptions, we might find that
certain causal conclusions are robust to these differences. Where the
implications of different assumptions disagree, we can better determine
the forms of data collection that would be required to settle such
differences. Below we consider an example where assumptions point to
different conclusions, revealing the benefits of collecting time-series
data to assess whether a variable is a confounder or a mediator.

\paragraph{2. Consider time-invariant confounders at
baseline}\label{consider-time-invariant-confounders-at-baseline}

In cross-sectional studies, some confounders are inherently stable over
time, such as ethnicity, year and place of birth, and biological gender.
For environmental psychologists examining the relationship between
access to natural environments and psychological well-being, these
stable confounders can be adjusted for without concern for introducing
bias from mediators or colliders. For example, conditioning on one's
year of birth can help isolate recent urban development's effect on
mental health, independent of generational differences in attitudes
toward green spaces.

\paragraph{3. Consider stable confounders at
baseline}\label{consider-stable-confounders-at-baseline}

While not immutable, other confounders are less likely to be influenced
by the treatment. Variables such as sexual orientation, educational
attainment, and often income level fall into this category. For
instance, the effect of exposure to polluted environments on cognitive
outcomes can be analysed by conditioning on education level, assuming
that recent exposure to pollution is unlikely to change someone's
educational history retroactively.

\paragraph{4. Consider time varying
confounding}\label{consider-time-varying-confounding}

The sequence of treatment and outcome is crucial. Sometimes, the
temporal order is clear, reducing concerns about reverse causation.
Mortality is a definitive outcome where the timing issue is unambiguous.
If researching the effects of air quality on mortality, the causal
direction (poor air quality leading to higher mortality rates) is
straightforward. However, consider the relationship between
socio-economic status and health outcomes; the direction of causality is
complex because socioeconomic factors can influence health (through
access to resources), and poor health can affect socio-economic status
(through reduced earning capacity).

\paragraph{5. Create your causal
diagrams}\label{create-your-causal-diagrams}

Given the complexity of environmental influences on psychological
outcomes, it's prudent to construct multiple causal diagrams to cover
various hypothetical scenarios. For example, when studying the effect of
community green space on stress reduction, one diagram might assume the
direct benefits of green space on stress. At the same time, another
might include potential mediators such as physical activity. By
analysing and reporting findings based on multiple diagrams, researchers
can examine the robustness of their conclusions across different
theoretical frameworks and sets of assumptions.

Table~\ref{tbl-cs} describes ambiguous confounding control arising from
cross-sectional data. Suppose again we are interested in the causal
effect of access to greenspace, denoted by \(A\) on ``happiness,''
denoted by \(Y\). We are uncertain whether exercise, denoted by \(L\),
is a common cause of \(A\) and \(Y\) and thus a confounder or whether
exercise is a mediator along the path from \(A\) to \(Y\). That is: (1)
those who exercise might seek access to green space, and (2) exercise
might increase happiness. Alternatively, the availability of green space
might encourage physical activity, which could subsequently affect
happiness. Causal diagrams can disentangle these relationships by
explicitly representing potential paths, thereby guiding appropriate
strategies for confounding control selection. We recommend using
multiple causal diagrams to investigate the consequences of different
plausible structural assumptions.

\textbf{Assumption 1: Exercise is a common cause of \(A\) and \(Y\)},
this scenario is presented in Table~\ref{tbl-cs} row 1. Here, our
strategy for confounding control is to estimate the effect of \(A\) on
\(Y\) conditioning on \(L\).

\textbf{Assumption 2: Exercise is a mediator of \(A\) and \(Y\)}, this
scenario is presented in Table~\ref{tbl-cs} row 2. Here, our strategy
for confounding control is simply estimating the effect of \(A\) on
\(Y\) without including \(L\) (assuming there are no other common causes
of the treatment and outcome).

\begin{table}

\caption{\label{tbl-cs}This table is adapted from
(\citeproc{ref-bulbulia2023}{Bulbulia 2024b})}

\centering{

\examplecrosssection

}

\end{table}%

We can simulate data and run separate regressions to clarify how answers
may differ, reflecting the different conditioning strategies embedded in
the different assumptions. The following simulation generates data from
a process in which exercise is a mediator (Scenario 2). (See Appendix C)

\begin{table}
\caption{Code for a simulation of a data generating process in which the effect
of exercise (L) fully mediates the effect of greenspace (A) on happiness
(Y).}\tabularnewline

\centering
\begin{tabular}{lcccccc}
\toprule
\multicolumn{1}{c}{ } & \multicolumn{3}{c}{Model: Exercise assumed confounder} & \multicolumn{3}{c}{Model: Exercise assumed to be a mediator} \\
\cmidrule(l{3pt}r{3pt}){2-4} \cmidrule(l{3pt}r{3pt}){5-7}
\textbf{Characteristic} & \textbf{Beta} & \textbf{95\% CI} & \textbf{p-value} & \textbf{Beta} & \textbf{95\% CI} & \textbf{p-value}\\
\midrule
A & -0.27 & -0.53, -0.01 & 0.043 & 2.9 & 2.6, 3.2 & <0.001\\
L & 1.6 & 1.5, 1.7 & <0.001 &  &  & \\
\bottomrule
\multicolumn{7}{l}{\rule{0pt}{1em}\textsuperscript{1} CI = Confidence Interval}\\
\end{tabular}
\end{table}

This table presents the conditional treatment effect estimates. We
present code for obtaining marginal treatment effects in
\hyperref[appendix-c]{Appendix C}

On the assumptions outlined in Table~\ref{tbl-cs} row 1, in which we
\emph{assert} that exercise is a confounder, the average treatment
effect of access to green space on happiness is ATE = 2.92, CI =
{[}2.66, 3.21{]}.

On the assumptions outlined in Table~\ref{tbl-cs} row 2, in which we
\emph{assert} that exercise is a mediator, the average treatment effect
of access to green space on happiness is ATE = -0.27, CI = {[}-0.52,
-0.01{]}.

Note that although the mediator \(L\) is ``highly statistically
significant'', including it in the model is a mistake. We obtain a
negative effect estimate for the causal effect of green space access on
happiness.

With only cross-sectional data, we must infer the results are
inconclusive. Such understanding, although not the definitive answer we
sought, is progress. The result tells us we should not be overly
confident with our analysis (whatever p-values we recover!), and it
clarifies that longitudinal data are needed.

These findings illustrate the role that assumptions about the relative
timing of exercise as a confounder or as a mediator play.

\subsubsection{Recommendations for Conducting and Reporting Causal
Analyses with Cross-Sectional
Data}\label{recommendations-for-conducting-and-reporting-causal-analyses-with-cross-sectional-data}

When analysing and reporting analyses with cross-sectional data,
researchers face the challenge of making causal inferences without the
benefit of temporal information.

The following recommendations aim to guide researchers in navigating
these challenges effectively:

\textbf{Warning}: before proceeding with cross-sectional analysis,
examine whether panel data are available. Longitudinal data can provide
crucial temporal information that aids in establishing causality,
offering a more robust framework for causal inference. If longitudinal
data are unavailable, the recommendations above become even more
critical for using cross-sectional data best.

\paragraph{\texorpdfstring{1. \textbf{Draw multiple causal
diagrams}}{1. Draw multiple causal diagrams}}\label{draw-multiple-causal-diagrams}

Draw multiple causal diagrams to represent different theoretical
assumptions about the relationships and timing of variables relevant to
an identification problem. If some causal pathways cannot be ruled out,
clarify the implications of assigning variables the roles for which
consensus or which the time ordering of the data do not resolve. For
example, in studying the effect of urban green spaces on mental health
from cross-sectional data, consider causal DAGs that assess effects in
each direction.

\paragraph{\texorpdfstring{2. \textbf{Perform and report analyses for
each
assumption}}{2. Perform and report analyses for each assumption}}\label{perform-and-report-analyses-for-each-assumption}

Conduct and transparently report separate analyses for each scenario
your causal diagrams depict. This practice ensures that your study is
theoretically grounded for each model. Presenting results from each
analytical approach and the underlying assumptions and statistical
methods promotes a balanced interpretation of findings.

\paragraph{\texorpdfstring{3. \textbf{Interpret findings with attention
to
ambiguities}}{3. Interpret findings with attention to ambiguities}}\label{interpret-findings-with-attention-to-ambiguities}

Interpret results carefully, highlighting any ambiguities or
inconsistencies across analyses. Discuss how varying assumptions about
structural relationships and the timing of events can lead to divergent
conclusions.

\paragraph{\texorpdfstring{4. \textbf{Report divergent
findings}}{4. Report divergent findings}}\label{report-divergent-findings}

Approach conclusions with caution, especially when findings suggest
differing practical implications. Acknowledge the limitations of
cross-sectional data in establishing causality and the potential for
alternative explanations. Do not over-sell.

\paragraph{\texorpdfstring{5. \textbf{Identify avenues for future
research}}{5. Identify avenues for future research}}\label{identify-avenues-for-future-research}

Target future research that might clarify ambiguities. Consider the
design of longitudinal studies or experiments capable of clarifying
lingering uncertainties.

\paragraph{\texorpdfstring{6. \textbf{Supplement observational data with
simulated
data}}{6. Supplement observational data with simulated data}}\label{supplement-observational-data-with-simulated-data}

Leverage data simulation to understand the complexities of causal
inference. Simulating data based on various theoretical models allows
researchers to examine the effect of different assumptions on their
findings. This method tests analytical strategies under controlled
conditions, assessing the robustness of conclusions against assumption
violations or unobserved confounders.

\paragraph{\texorpdfstring{7. \textbf{Conduct sensitivity analyses to
assess
robustness}}{7. Conduct sensitivity analyses to assess robustness}}\label{conduct-sensitivity-analyses-to-assess-robustness}

implement sensitivity analyses to determine how dependent conclusions
are on specific assumptions or parameters within your causal model. A
relatively simple sensitivity analysis is VanderWeele's E-value
(\citeproc{ref-vanderweele2017}{VanderWeele and Ding 2017})

Cross-sectional data are limiting; however, by appropriately bounding
uncertainties in your causal inferences, you may use them to advance
understanding. May your clarity and caution serve as an example for
others.

\subsubsection{Longitudinal Designs}\label{longitudinal-designs}

Causation occurs in time. Longitudinal designs offer a substantial
advantage over cross-sectional designs for causal inference because
sequential measurements allow us to capture causation and quantify its
magnitude. We typically do not need to assert timing as in
cross-sectional data settings. Because we know when variables have been
measured, we can reduce ambiguity about the directionality of causal
relationships. For instance, tracking changes in ``happiness'' following
changes in access to green spaces over time can more definitively
suggest causation than cross-sectional snapshots.

Despite this advantage, longitudinal researchers still face assumptions
regarding the absence of unmeasured confounders or the stability of
measured confounders over time. These assumptions must be explicitly
stated. As with cross-sectional designs, wherever assumptions differ,
researchers should draw different causal diagrams that reflect these
assumptions and subsequently conduct and report separate analyses.

In this section, we simulate a dataset to demonstrate the benefits of
incorporating both baseline exposure and baseline outcomes into
analysing the effect of access to open green spaces on happiness. This
approach allows us to control for initial levels of exposure and
outcomes, offering a clearer understanding of the causal relationship.
\hyperref[appendix-d-simulation-of-different-confounding-control-strategies]{Appendix
D} provides the code.
\hyperref[appendix-e-non-parametric-estimation-of-average-treatment-effects-using-causal-forests]{Appendix
E} provides an example of a non-parametric estimator for the causal
effect. As mentioned before, by conditioning on baseline levels of
access to green spaces and baseline mental health, researchers can more
accurately estimate the \emph{incident effect} of changes in green space
access on changes in mental health. Table~\ref{tbl-lg} offers an example
of how we may use multiple causal diagrams to clarify the problem and
our confounding control strategy.

\begin{table}

\caption{\label{tbl-lg}This table is adapted from
(\citeproc{ref-bulbulia2023}{Bulbulia 2024b})}

\centering{

\examplelongitudinal

}

\end{table}%

Our analysis assessed the average treatment effect (ATE) of access to
green spaces on happiness across three distinct models: uncontrolled,
standard controlled, and interaction controlled. These models were
constructed using a hypothetical cohort of 10,000 individuals,
incorporating baseline exposure to green spaces (\(A_0\)), baseline
happiness (\(Y_0\)), baseline confounders (\(L_0\)), and an unmeasured
confounder (\(U\)). The detailed simulation process and model
construction are given in
\hyperref[appendix-simulate-longitudinal-ate]{Appendix D}.

The ATE estimates from these models provide critical insights into the
effects of green space exposure on individual happiness while accounting
for various confounding factors. The model without control variables
estimated ATE = 1.55, CI = {[}1.47, 1.63{]}, significantly
overestimating the treatment effect. Incorporating standard covariate
control reduced this estimate to ATE = 0.86, CI = {[}0.8, 0.92{]},
aligning more closely with the expected effect but still overestimating.
Most notably, the model that included interactions among baseline
exposure, outcome, and confounders yielded ATE = 0.29, CI = {[}0.27,
0.31{]}, approximating the true effect of 0.3. This finding underscores
the importance of including baseline values of the exposure and outcome
wherever these data are available.

\subsubsection{Recommendations for Conducting and Reporting Causal
Analyses with Longitudinal
Data}\label{recommendations-for-conducting-and-reporting-causal-analyses-with-longitudinal-data}

Longitudinal data offer strong advantages for causal inference by
enabling researchers to establish the relative timing of confounders,
treatments, and outcomes. The temporal sequence of events is crucial for
establishing causality because causality occurs in time. The following
recommendations aim to guide researchers in leveraging longitudinal data
effectively to conduct and report causal analyses:

\paragraph{1. Draw multiple causal
diagrams}\label{draw-multiple-causal-diagrams-1}

\begin{itemize}
\tightlist
\item
  \textbf{Identification problem diagram}: begin by constructing a
  causal diagram that outlines your initial assumptions about the
  relationships among variables, identifying potential confounders and
  mediators. This diagram should illustrate the complexity of the
  identification problem.
\item
  \textbf{Solution diagram}: next, create a separate causal diagram that
  proposes solutions to the identified problems. Having distinct
  diagrams for the problem and its proposed solutions clarifies your
  study's analytic strategy and theoretical underpinning.
\end{itemize}

Table~\ref{tbl-lg} provides an example of a table with multiple causal
diagrams clarifying potential sources of confounding threats and reports
strategies for addressing them.

\paragraph{2. Attempt longitudinal designs with at least three waves of
data}\label{attempt-longitudinal-designs-with-at-least-three-waves-of-data}

Incorporating repeated measures data from at least three time intervals
considerably enhances your ability to infer causal relationships. For
example, by adjusting for physical activity measured before the
treatment, we can ensure that physical activity does not result from a
new initiation to green spaces, which we establish by measuring green
space access at baseline. Establishing chronological order allows us to
avoid confounding problems 1-4 in Table~\ref{tbl-04}.

\paragraph{3. Calculate Average Treatment Effects for a clearly
specified target
population}\label{calculate-average-treatment-effects-for-a-clearly-specified-target-population}

Estimating the average treatment effect (ATE) across the entire study
population provides a comprehensive measure of the intervention's
effects. This step is crucial for understanding the treatment's overall
effect in a specific population, which must be described in advance of
data analysis because causal effects are averages within certain
populations or stratums of populations. The concept of a causal effect
absent a population in not available to data science, because individual
causal effects are not observed.

\paragraph{4. Where causality is unclear, report results for multiple
causal
graphs}\label{where-causality-is-unclear-report-results-for-multiple-causal-graphs}

Given that the true causal structure may be complex and partially
unknown, analysing and reporting results under each plausible causal
diagram is prudent.

\paragraph{5. Conduct sensitivity
analyses}\label{conduct-sensitivity-analyses}

Sensitivity analyses are essential for assessing the robustness of your
findings to various assumptions within the causal model. These analyses
can include simulations, as illustrated in Appendices C and D, to
examine bias arising of unmeasured confounding, model misspecification,
and alternative causal pathways on the study conclusions. Sensitivity
analyses help to identify the conditions under which the findings hold,
enhancing the credibility of the causal inferences. (For more about
addressing missing data, see:
(\citeproc{ref-bulbulia2024PRACTICAL}{Bulbulia 2024a}).)

\paragraph{6. Address missing data at baseline and study
attrition}\label{address-missing-data-at-baseline-and-study-attrition}

Longitudinal studies often need help with missing data and attrition,
which can introduce bias and affect the validity of causal inferences.
Implement and report strategies for handling missing data, such as
multiple imputation or sensitivity analyses that assess the bias arising
from missing responses at the study's conclusion. (For more about
addressing missing data, see:
(\citeproc{ref-bulbulia2024PRACTICAL}{Bulbulia 2024a})).

By following these recommendations, you will more effectively navigate
the inherent limitations of observational longitudinal data, improving
the quality of your causal inferences.

\subsection{Where to go from here}\label{where-to-go-from-here}

\subsubsection{The Origins of Causal
Inference}\label{the-origins-of-causal-inference}

Causal inference began in the early 20\(^{th}\) century with Jerzy
Neyman's invention of the potential outcomes framework, initially
developed for agricultural experiments. Neyman realized that
understanding the causal effect of an experimental treatment required
comparing potential outcomes under different treatment conditions, even
though only one outcome could be observed for each unit
(\citeproc{ref-neyman1923}{Splawa-Neyman 1990 (orig. 1923)}). This
framework laid the foundation for modern causal inference.

Donald Rubin extended this approach to observational settings, into what
is now known as the Neyman-Rubin Causal Model
(\citeproc{ref-holland1986}{Holland 1986};
\citeproc{ref-rubin1976}{Rubin 1976}). James Robins advanced causal
inference by introducing the mathematical and conceptual framework for
understanding causal effects in settings where there are two or more
sequential treatments over time (\citeproc{ref-robins1986}{Robins
1986}). Robins and his colleagues also developed statistical tools such
as marginal structural models and structural nested models to enable
quantitative assessment of time-varying treatments and time-varying
confounding (\citeproc{ref-hernan2024WHATIF}{Hernan and Robins 2024}).
The computer scientist Judea Pearl developed causal Directed Acyclic
Graphs (DAGs), and proved that these graphical models may be applied to
evaluate one of the three fundamental assumptions of causal inference,
namely ``no unmeasured confounding'', also known as ``conditional
exchangeability'', ``selection on observables'' and ``ignorability'', or
in Pearl's formalism, ``d-separation'' or equivalently, ``no open
backdoor path''(\citeproc{ref-pearl2009}{Pearl 2009a};
\citeproc{ref-pearl1995a}{Pearl and Robins 1995}). The jargon that has
evolved in causal inference is not restricted to the concept of no
unmeasured confounding. In \hyperref[section-part2]{\textbf{Part 2}} we
encounter the concept of ``causal identification,'' which is unrelated
to the concept of ``statistical identification.''

Although terminology differs across the subfields of causal inference,
the mathematical basis of causal inference evolved nearly independently
in the fields of biostatistics, economics, and computer science. This
shared foundation, anchored in proofs, has enabled a remarkable
agreement across disciplinary lines despite terminological differences.
In every approach to causal inference, a causal effect is conceptualized
as a contrast between two states of the world, only one of which may be
observed on any individual -- a ``counterfactual contrast''
(\citeproc{ref-hernan2024WHATIF}{Hernan and Robins 2024}). Our
experience is that many just getting started with causal inference find
the concept of a counterfactual contrast hard to understand. However, we
may take comfort that we are already familiar with how to recover the
``full data'' we need to compute causal contrasts in the design of a
randomized controlled experiment.

A fundamental principle in the potential outcomes framework is the
concept of ``counterfactual contrast'' or ``estimand.'' To quantify
causal effects, one must compare the outcomes under different
intervention or treatment scenarios. Notably, prior to any intervention,
these scenarios are purely hypothetical. Post-intervention, only one
scenario is actualised for each realised treatment, leaving the
alternative as a non-observed counterfactual. For any individual unit to
be treated, that only one of the two possible outcomes is realised
underscores a critical property of causality: causality is \textbf{not
directly observable} (\citeproc{ref-hume1902}{Hume 1902}). Causal
inference, therefore, can only quantify causal effects by combining data
with counterfactual simulation (\citeproc{ref-bulbulia2023a}{Bulbulia
\emph{et al.} 2023}; \citeproc{ref-edwards2015}{Edwards \emph{et al.}
2015}). The concept of a counterfactual data science -- may sound
strange. However, anyone who has encountered a randomised experiment has
encountered counterfactual data science. Before building intuitions for
causal inference from the familiar example of experiments, let's first
build intuitions for the idea that causal quantities are never directly
observed.

There are many good resources available for learning causal directed
acyclic graphs (\citeproc{ref-barrett2021}{Barrett 2021};
\citeproc{ref-cinelli2022}{Cinelli \emph{et al.} 2022};
\citeproc{ref-greenland1999}{Greenland \emph{et al.} 1999},
\citeproc{ref-greenland1999}{1999};
\citeproc{ref-hernan2024WHATIF}{Hernan and Robins 2024};
\citeproc{ref-major2023exploring}{Major-Smith 2023};
\citeproc{ref-mcelreath2020}{McElreath 2020};
\citeproc{ref-morgan2014}{Morgan and Winship 2014};
\citeproc{ref-pearl2009a}{Pearl 2009b}; \citeproc{ref-rohrer2018}{Rohrer
2018}; \citeproc{ref-suzuki2020}{Suzuki \emph{et al.} 2020}). For those
just getting started on causal diagrams, we recommend Miguel Hernan's
free course here:
\href{https://www.edx.org/learn/data-analysis/harvard-university-causal-diagrams-draw-your-assumptions-before-your-conclusions}{https://www.edx.org/learn/data-analysis/harvard-university-causal-diagrams-draw-your-assumptions-before-your-conclusions,
accessed 10 June 2024}. For those seeking a slightly more technical but
still accessible introduction to causal inference and causal DAGs, I
recommend Brady Neal's introduction to causal inference course and
textbook, both freely available here
\href{https://www.bradyneal.com/causal-inference-course}{https://www.bradyneal.com/causal-inference-course,
accessed 10 June 2024}.

\newpage{}

\subsection{Funding}\label{funding}

This work is supported by a grant from the Templeton Religion Trust
(TRT0418). JB received support from the Max Planck Institute for the
Science of Human History. The funders had no role in preparing the
manuscript or deciding to publish it.

\subsection{Contributions}\label{contributions}

DH proposed the chapter. JB developed the approach and wrote the first
draft. Both authors contributed substantially to the final work.

\newpage{}

\subsection{References}\label{references}

\phantomsection\label{refs}
\begin{CSLReferences}{1}{0}
\bibitem[\citeproctext]{ref-barrett2021}
Barrett, M (2021) \emph{Ggdag: Analyze and create elegant directed
acyclic graphs}. Retrieved from
\url{https://CRAN.R-project.org/package=ggdag}

\bibitem[\citeproctext]{ref-bulbulia2022}
Bulbulia, JA (2023) A workflow for causal inference in cross-cultural
psychology. \emph{Religion, Brain \& Behavior}, \textbf{13}(3),
291--306.
doi:\href{https://doi.org/10.1080/2153599X.2022.2070245}{10.1080/2153599X.2022.2070245}.

\bibitem[\citeproctext]{ref-bulbulia2024PRACTICAL}
Bulbulia, JA (2024a) A practical guide to causal inference in three-wave
panel studies. \emph{PsyArXiv Preprints}.
doi:\href{https://doi.org/10.31234/osf.io/uyg3d}{10.31234/osf.io/uyg3d}.

\bibitem[\citeproctext]{ref-bulbulia2023}
Bulbulia, JA (2024b) Methods in causal inference part 1: Causal diagrams
and confounding. \emph{Evolutionary Human Sciences}, \textbf{6}.
Retrieved from \url{https://osf.io/b23k7}

\bibitem[\citeproctext]{ref-bulbulia2024swigstime}
Bulbulia, JA (2024c) Methods in causal inference part 2: Interaction,
mediation, and time-varying treatments. \emph{Evolutionary Human
Sciences}, \textbf{6}. Retrieved from
\url{https://osf.io/preprints/psyarxiv/vr268}

\bibitem[\citeproctext]{ref-bulbulia2023a}
Bulbulia, JA, Afzali, MU, Yogeeswaran, K, and Sibley, CG (2023)
Long-term causal effects of far-right terrorism in {N}ew {Z}ealand.
\emph{PNAS Nexus}, \textbf{2}(8), pgad242.

\bibitem[\citeproctext]{ref-cinelli2022}
Cinelli, C, Forney, A, and Pearl, J (2022) A Crash Course in Good and
Bad Controls. \emph{Sociological Methods \&Research}, 00491241221099552.
doi:\href{https://doi.org/10.1177/00491241221099552}{10.1177/00491241221099552}.

\bibitem[\citeproctext]{ref-cole2010}
Cole, SR, Platt, RW, Schisterman, EF, \ldots{} Poole, C (2010)
Illustrating bias due to conditioning on a collider. \emph{International
Journal of Epidemiology}, \textbf{39}(2), 417--420.
doi:\href{https://doi.org/10.1093/ije/dyp334}{10.1093/ije/dyp334}.

\bibitem[\citeproctext]{ref-danaei2012}
Danaei, G, Tavakkoli, M, and HernÃ¡n, MA (2012) Bias in observational
studies of prevalent users: lessons for comparative effectiveness
research from a meta-analysis of statins. \emph{American Journal of
Epidemiology}, \textbf{175}(4), 250--262.
doi:\href{https://doi.org/10.1093/aje/kwr301}{10.1093/aje/kwr301}.

\bibitem[\citeproctext]{ref-edwards2015}
Edwards, JK, Cole, SR, and Westreich, D (2015) All your data are always
missing: Incorporating bias due to measurement error into the potential
outcomes framework. \emph{International Journal of Epidemiology},
\textbf{44}(4), 1452--1459.

\bibitem[\citeproctext]{ref-greenland1999}
Greenland, S, Pearl, J, and Robins, JM (1999) Causal diagrams for
epidemiologic research. \emph{Epidemiology (Cambridge, Mass.)},
\textbf{10}(1), 37--48.

\bibitem[\citeproctext]{ref-hernan2023}
Hernan, MA, and Robins, JM (2023) \emph{Causal inference}, Taylor \&
Francis. Retrieved from
\url{https://books.google.co.nz/books?id=/_KnHIAAACAAJ}

\bibitem[\citeproctext]{ref-hernan2024WHATIF}
Hernan, MA, and Robins, JM (2024) \emph{Causal inference: What if?},
Taylor \& Francis. Retrieved from
\url{https://www.hsph.harvard.edu/miguel-hernan/causal-inference-book/}

\bibitem[\citeproctext]{ref-hernan2017per}
HernÃ¡n, MA, and Robins, JM (2017) Per-protocol analyses of pragmatic
trials. \emph{N Engl J Med}, \textbf{377}(14), 1391--1398.

\bibitem[\citeproctext]{ref-hernuxe1n2016}
HernÃ¡n, MA, Sauer, BC, HernÃ¡ndez-DÃ­az, S, Platt, R, and Shrier, I (2016)
Specifying a target trial prevents immortal time bias and other
self-inflicted injuries in observational analyses. \emph{Journal of
Clinical Epidemiology}, \textbf{79}, 70--75.

\bibitem[\citeproctext]{ref-holland1986}
Holland, PW (1986) Statistics and causal inference. \emph{Journal of the
American Statistical Association}, \textbf{81}(396), 945--960.

\bibitem[\citeproctext]{ref-hume1902}
Hume, D (1902) \emph{Enquiries Concerning the Human Understanding: And
Concerning the Principles of Morals}, Clarendon Press.

\bibitem[\citeproctext]{ref-lewis1973}
Lewis, D (1973) Causation. \emph{The Journal of Philosophy},
\textbf{70}(17), 556--567.
doi:\href{https://doi.org/10.2307/2025310}{10.2307/2025310}.

\bibitem[\citeproctext]{ref-major2023exploring}
Major-Smith, D (2023) Exploring causality from observational data: An
example assessing whether religiosity promotes cooperation.
\emph{Evolutionary Human Sciences}, \textbf{5}, e22.

\bibitem[\citeproctext]{ref-mcelreath2020}
McElreath, R (2020) \emph{Statistical rethinking: A {B}ayesian course
with examples in {R} and {S}tan}, CRC press.

\bibitem[\citeproctext]{ref-montgomery2018}
Montgomery, JM, Nyhan, B, and Torres, M (2018) How conditioning on
posttreatment variables can ruin your experiment and what to do about
It. \emph{American Journal of Political Science}, \textbf{62}(3),
760--775.
doi:\href{https://doi.org/10.1111/ajps.12357}{10.1111/ajps.12357}.

\bibitem[\citeproctext]{ref-morgan2014}
Morgan, SL, and Winship, C (2014) \emph{Counterfactuals and causal
inference: Methods and principles for social research}, 2nd edn,
Cambridge: Cambridge University Press.
doi:\href{https://doi.org/10.1017/CBO9781107587991}{10.1017/CBO9781107587991}.

\bibitem[\citeproctext]{ref-ogburn2021}
Ogburn, EL, and Shpitser, I (2021) Causal modelling: The two cultures.
\emph{Observational Studies}, \textbf{7}(1), 179--183.
doi:\href{https://doi.org/10.1353/obs.2021.0006}{10.1353/obs.2021.0006}.

\bibitem[\citeproctext]{ref-pearl1995}
Pearl, J (1995) Causal diagrams for empirical research.
\emph{Biometrika}, \textbf{82}(4), 669--688.

\bibitem[\citeproctext]{ref-pearl2009}
Pearl, J (2009a) \emph{\href{https://doi.org/10.1214/09-SS057}{Causal
inference in statistics: An overview}}.

\bibitem[\citeproctext]{ref-pearl2009a}
Pearl, J (2009b) \emph{Causality}, Cambridge University Press.

\bibitem[\citeproctext]{ref-pearl1995a}
Pearl, J, and Robins, JM (1995) Probabilistic evaluation of sequential
plans from causal models with hidden variables. In \emph{UAI}, Vol. 95,
Citeseer, 444--453.

\bibitem[\citeproctext]{ref-robins1986}
Robins, J (1986) A new approach to causal inference in mortality studies
with a sustained exposure period---application to control of the healthy
worker survivor effect. \emph{Mathematical Modelling}, \textbf{7}(9-12),
1393--1512.

\bibitem[\citeproctext]{ref-robins2008estimation}
Robins, J, and Hernan, M (2008) Estimation of the causal effects of
time-varying exposures. \emph{Chapman \& Hall/CRC Handbooks of Modern
Statistical Methods}, 553--599.

\bibitem[\citeproctext]{ref-rohrer2018}
Rohrer, JM (2018) Thinking clearly about correlations and causation:
Graphical causal models for observational data. \emph{Advances in
Methods and Practices in Psychological Science}, \textbf{1}(1), 27--42.

\bibitem[\citeproctext]{ref-rubin1976}
Rubin, DB (1976) Inference and missing data. \emph{Biometrika},
\textbf{63}(3), 581--592.
doi:\href{https://doi.org/10.1093/biomet/63.3.581}{10.1093/biomet/63.3.581}.

\bibitem[\citeproctext]{ref-neyman1923}
Splawa-Neyman, J (1990 (orig. 1923)) On the application of probability
theory to agricultural experiments. Essay on principles. Section 9.
(1923). \emph{Statistical Science}, \textbf{5}(4), 465--472.

\bibitem[\citeproctext]{ref-suzuki2020}
Suzuki, E, Shinozaki, T, and Yamamoto, E (2020) Causal Diagrams:
Pitfalls and Tips. \emph{Journal of Epidemiology}, \textbf{30}(4),
153--162.
doi:\href{https://doi.org/10.2188/jea.JE20190192}{10.2188/jea.JE20190192}.

\bibitem[\citeproctext]{ref-vanderweele2015}
VanderWeele, TJ (2015) \emph{Explanation in causal inference: Methods
for mediation and interaction}, Oxford University Press.

\bibitem[\citeproctext]{ref-vanderweele2017}
VanderWeele, TJ, and Ding, P (2017) Sensitivity analysis in
observational research: Introducing the {E}-value. \emph{Annals of
Internal Medicine}, \textbf{167}(4), 268--274.
doi:\href{https://doi.org/10.7326/M16-2607}{10.7326/M16-2607}.

\bibitem[\citeproctext]{ref-vanderweele2020}
VanderWeele, TJ, Mathur, MB, and Chen, Y (2020) Outcome-wide
longitudinal designs for causal inference: A new template for empirical
studies. \emph{Statistical Science}, \textbf{35}(3), 437--466.

\bibitem[\citeproctext]{ref-westreich2012berkson}
Westreich, D (2012) Berkson's bias, selection bias, and missing data.
\emph{Epidemiology (Cambridge, Mass.)}, \textbf{23}(1), 159.

\bibitem[\citeproctext]{ref-westreich2010}
Westreich, D, and Cole, SR (2010) Invited commentary: positivity in
practice. \emph{American Journal of Epidemiology}, \textbf{171}(6).
doi:\href{https://doi.org/10.1093/aje/kwp436}{10.1093/aje/kwp436}.

\bibitem[\citeproctext]{ref-westreich2015}
Westreich, D, Edwards, JK, Cole, SR, Platt, RW, Mumford, SL, and
Schisterman, EF (2015) Imputation approaches for potential outcomes in
causal inference. \emph{International Journal of Epidemiology},
\textbf{44}(5), 1731--1737.

\bibitem[\citeproctext]{ref-westreich2013}
Westreich, D, and Greenland, S (2013) The table 2 fallacy: Presenting
and interpreting confounder and modifier coefficients. \emph{American
Journal of Epidemiology}, \textbf{177}(4), 292--298.

\end{CSLReferences}



\end{document}
