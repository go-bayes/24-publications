% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames,x11names}{xcolor}
%
\documentclass[
  singlecolumn]{article}

\usepackage{amsmath,amssymb}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage[]{libertinus}
\ifPDFTeX\else  
    % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\usepackage[top=30mm,left=20mm,heightrounded]{geometry}
\setlength{\emergencystretch}{3em} % prevent overfull lines
\setcounter{secnumdepth}{-\maxdimen} % remove section numbering
% Make \paragraph and \subparagraph free-standing
\ifx\paragraph\undefined\else
  \let\oldparagraph\paragraph
  \renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
  \let\oldsubparagraph\subparagraph
  \renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi


\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
% definitions for citeproc citations
\NewDocumentCommand\citeproctext{}{}
\NewDocumentCommand\citeproc{mm}{%
  \begingroup\def\citeproctext{#2}\cite{#1}\endgroup}
\makeatletter
 % allow citations to break across lines
 \let\@cite@ofmt\@firstofone
 % avoid brackets around text for \cite:
 \def\@biblabel#1{}
 \def\@cite#1#2{{#1\if@tempswa , #2\fi}}
\makeatother
\newlength{\cslhangindent}
\setlength{\cslhangindent}{1.5em}
\newlength{\csllabelwidth}
\setlength{\csllabelwidth}{3em}
\newenvironment{CSLReferences}[2] % #1 hanging-indent, #2 entry-spacing
 {\begin{list}{}{%
  \setlength{\itemindent}{0pt}
  \setlength{\leftmargin}{0pt}
  \setlength{\parsep}{0pt}
  % turn on hanging indent if param 1 is 1
  \ifodd #1
   \setlength{\leftmargin}{\cslhangindent}
   \setlength{\itemindent}{-1\cslhangindent}
  \fi
  % set entry spacing
  \setlength{\itemsep}{#2\baselineskip}}}
 {\end{list}}
\usepackage{calc}
\newcommand{\CSLBlock}[1]{\hfill\break\parbox[t]{\linewidth}{\strut\ignorespaces#1\strut}}
\newcommand{\CSLLeftMargin}[1]{\parbox[t]{\csllabelwidth}{\strut#1\strut}}
\newcommand{\CSLRightInline}[1]{\parbox[t]{\linewidth - \csllabelwidth}{\strut#1\strut}}
\newcommand{\CSLIndent}[1]{\hspace{\cslhangindent}#1}

\input{/Users/joseph/GIT/templates/latex/custom-commands.tex}
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\AtBeginDocument{%
\ifdefined\contentsname
  \renewcommand*\contentsname{Table of contents}
\else
  \newcommand\contentsname{Table of contents}
\fi
\ifdefined\listfigurename
  \renewcommand*\listfigurename{List of Figures}
\else
  \newcommand\listfigurename{List of Figures}
\fi
\ifdefined\listtablename
  \renewcommand*\listtablename{List of Tables}
\else
  \newcommand\listtablename{List of Tables}
\fi
\ifdefined\figurename
  \renewcommand*\figurename{Figure}
\else
  \newcommand\figurename{Figure}
\fi
\ifdefined\tablename
  \renewcommand*\tablename{Table}
\else
  \newcommand\tablename{Table}
\fi
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Listing}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}
\makeatother
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\@ifpackageloaded{subcaption}{}{\usepackage{subcaption}}
\makeatother
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same} % disable monospaced font for URLs
\hypersetup{
  pdftitle={Causal Inference in Environmental Psychology},
  pdfkeywords={DAGS, Causal
Inference, Confounding, Environmental, Psychology, Panel},
  colorlinks=true,
  linkcolor={blue},
  filecolor={Maroon},
  citecolor={Blue},
  urlcolor={Blue},
  pdfcreator={LaTeX via pandoc}}

\title{Causal Inference in Environmental Psychology}
\author{Donald W Hine \and Joseph A. Bulbulia}
\date{2024-02-02}

\begin{document}
\maketitle
\begin{abstract}
This chapter covers causal inference in environmental psychology,
focussing on how to understand and use causal diagrams, also known as
Directed Acyclic Graphs (DAGs). We start with the basics of causal
inference to help readers get a clear idea of the assumptions required
to estimate causal effects. Then, we explain how to create simple causal
diagrams, and apply the lessons scenarios that environmental
psychologists may face. The chapter is divided into three parts: the
first introduces causal inference principles, the second explains causal
diagrams, and the third discusses causal inference in experiments. It is
designed for researchers interested in making stronger causal claims
from both observational and experimental data.
\end{abstract}

\section{Introduction}\label{introduction}

Causal inference is the process of using empirical observations to
identify the causal relationships between variables or events. In this
simplist case, this involves assessing whether and intervention or
``treatments'' on one variable leads to subsequent changes in another
variable, the ``outcome.'' The goal of causal inference is to determine
whether cause-effect relationships are present, and to quantify their
magnitude (\citeproc{ref-cook2002experimental}{Cook \emph{et al.}
2002}).

It is often thought that causal inference is a complex cognitive process
unique to humans. Yet, it may be more accurately viewed as a phenomenon
that spans across the plant and animal kingdoms, varying in levels of
conscious awareness and complexity
(\citeproc{ref-mancuso2018revolutionary}{Mancuso 2018}). For instance,
plants and microorganisms have genetic predispositions to recognise and
react to environmental conditions critical for their reproduction and
survival. Birds and rodents demonstrate the ability to learn, albeit not
as complexly as larger mammals and primates. Over millennia, humans have
developed diverse frameworks for causal inference, ranging from the law
of karma in Hindu, Jain, and Buddhist traditions, to Aristotle's four
causes, early empirical and experimental approaches by Francis Bacon and
John Locke, Newtonian mechanics, and more recent theories based on
counterfactual reasoning.

Although the capacity to infer cause and effect is common to many
lineages and has evolved to high levels of sophistication with many
human cultures, methods for quantitatively estimating causal effects
from data are much less common, and have emerged only relatively
recently with the development of randomised controlled experiments. More
recent and specialised still are capacities for quantifying the
magnitudes of causal effects from what is sometimes called
``real-world'' data or ``observational data'' -- that is, data that have
not been collect through random assignment to controlled treatments.
This chapter aims to familiarise environmental psychologists with
powerful methods for analysing observational data to draw causal
inferences.

We believe there is considerable value in understanding these more
recently developed methods. Introductory undergraduate psychology
courses teach that correlations between variables do not necessarily
indicate causality. The question arises: Can valid causal inferences be
drawn from observational data, and if so, how? Although randomised
experiments provide causal insights, they can be costly and ethically
challenging. Moreover, even randomised controlled experiments do not
ensure valid causal inferences. Observational data, being more
plentiful, offers potentially valuable resources for accelerating
knowledge in environmental psychology, provided causal insights can be
gleaned from them. However, the manner in which many researchers draw
inferences from such data has yet to be developed. On the one side, we
assess correlations, and state that correlations are not causation. This
is unsatisfactory because we typically want to understand causation. On
the other hand, we continue to draw hesitant causal conclusions using
hedging language. However, lacking a appropriate methods, we often have
no entitlement or guarantee that such speculation is misleading
{[}Bulbulia (\citeproc{ref-bulbulia2022}{2022}); bulbulia@2023a{]}.

There are reasons to hope we may do better. Significant advances in
estimating causation from observational data over the past two decades
offer hope (\citeproc{ref-vanderweele2015}{VanderWeele 2015}). While
most progress has been outside psychology, psychological scientists are
increasingly interested (\citeproc{ref-mcelreath2020}{McElreath 2020};
\citeproc{ref-rohrer2018}{Rohrer 2018}). Incorporating methods of causal
inference into environmental psychology research offers significant
potential. This chapter aims to lay a foundational understanding,
aspiring to motivate environmental psychologists towards crafting more
robust and insightful causal inferences.

\subsection{Overview}\label{overview}

\textbf{Part 1} introduces the ``potential outcomes'' framework for
causal inference (\citeproc{ref-hernan2023}{Hernan and Robins 2023}).
This section describes the three core assumptions underpinning causal
inference, grounding these concepts within the context of randomized
experiments, which are likely familiar to many readers. By deriving
intuitions about causal inference from an understanding of experimental
designs, we can demystify the assumptions central to causal analysis and
provide clear guidelines for data collection.

\textbf{Part 2} introduces Directed Acyclic Graphs (DAGs) -- or causal
diagrams -- as powerful tools for examining causal assumptions. Although
an exhaustive review of causal diagrams' capabilities is beyond this
overview, we present fundamental strategies for constructing and
interpreting these diagrams, which should prove valuable for addressing
many questions within environmental psychology.

\textbf{Part 3} examines how experimental designs may benefit from
causal methods developed for observational settings. Although
discussions about the use of experiments often revolve around concerns
of internal and external validity, we use causal diagrams to explore how
experimental designs might be susceptible to internal validity threats,
independent of external validity considerations. We show how the
methodologies for causal inference in observational studies are equally
relevant for experimenters and argue that causal inference techniques
should be an integral component of experimental methodology education.

\subsection{Part 1: Introducing the Potential Outcomes Framework of
Causal
Inference}\label{part-1-introducing-the-potential-outcomes-framework-of-causal-inference}

The potential outcomes framework originated in the work of Jerzy Neyman
for evaluating agricultural experiments
(\citeproc{ref-neyman1923}{Neyman 1923}). It was later extended by
Harvard statistician Donald Rubin to facilitate causal inference in
non-experimental settings (\citeproc{ref-rubin1976}{Rubin 1976}). Jamie
Robins further generalised this framework to assessing confounding in
complex scenarios involving multiple treatments and time-varying factors
(\citeproc{ref-robins1986}{Robins 1986}). A core concept within this
framework is the concept of `counterfactual outcomes.'

To grasp the significance of counterfactual outcomes in causal analysis,
consider yourself at the cross-roads of a monumental life decision.
Imagine you are soon graduating university. You are accepted into your
ideal graduate program at the University of Canterbury; you make
preparations for a relocation to Christchurch, New Zealand.
Concurrently, you receive a compelling job offer from Acme Nuclear
Fuels, a pioneer in renewable energy solutions. Each option would set
you on a distinct path, affecting your daily life, income, social
circles, romantic relationships, and perhaps your life's purpose. Which
life will be better?

Formally, let \(A\) denote the choice to attend graduate school
(\(A = 1\)) or to embark on a career in industry (\(A = 0\)). The
potential outcomes, \(Y_{\text{you}}(1)\) and \(Y_{\text{you}}(0)\),
symbolize the hypothetical scenarios resulting from each decision. To
evaluate the causal effect of your choice, we examine the difference
\(Y_{\text{you}}(1) - Y_{\text{you}}(0)\). However, this differential is
fundamentally unobservable, as once a decision is made, the alternative
scenario remains unknowable.

\[
(Y_{\text{you}}|A_{\text{you}} = 1) = Y_{\text{you}}(1) \quad \text{implies} \quad Y_{\text{you}}(0)|A_{\text{you}} = 1~ \text{is counterfactual}.
\]

Similarly, the counterfactual applies in reverse when the choice is
\(A = 0\).

Consider the implications. Although you can, of course, make principled
decisions about which life to choose that are based on past experiences,
the data you require to compare life outcomes under one decision as
opposed to the are not available. Life as it would have unfolding under
the option you do not select remains forever counterfactual. This
example underscores `the fundamental problem of causal inference' as
articulated by Rubin and Holland (\citeproc{ref-holland1986}{Holland
1986}; \citeproc{ref-rubin2005}{Rubin 2005}): the impossibility of
observing both potential outcomes for an individual simultaneously.

\subsubsection{Understanding Relationships of Cause and Effect Through
Intervention
Outcomes}\label{understanding-relationships-of-cause-and-effect-through-intervention-outcomes}

Let's shift our example to a question in environmental psychology.
Suppose we we want to examine the causal effect of easy access to urban
green spaces on psychological well-being
(\citeproc{ref-nguyen2021green}{Nguyen \emph{et al.} 2021};
\citeproc{ref-reyes2021linking}{Reyes-Riveros \emph{et al.} 2021}). For
simplicity, suppose our interest is ``subjective happiness,'' hereafter
referred to as ``happiness'', and assume this quantity is measurable. We
will represent this outcome with the symbol \(Y\)

For simplicity, let's assume that intervention ``ample access to green
space'' is a binary variable. Define \(A = 1\) as ``having ample access
to green space'' and \(A = 0\) as ``lacking ample access to green
space.'''' Assume these conditions are mutually exclusive. The
observations we make here apply to continuous treatments so this
simplification does not come at the loss of generality. In causal
inference is it important to specify the population for whom we seek to
evaluate causal effects. We call this object the ``target population.''
Here, we will take the target population to be residents of New
Zealander in the 2020s. \textbf{Note that in causal inference, before
analysing any data we must state a clearly defined causal question with
respect to the target population.}

Our preliminary causal question might be:

`In New Zealand, does proximity to abundant green spaces increase
self-perceived happiness compared to environments lacking such spaces?'

Next, suppose we agree it would be unethical to experimentally randomise
individuals into different green-space access conditions. Let's ignore
this issue. Assume we had the means to assign people randomly to high
and low greenspace-access, and that no one objected.

The first point to notice is that in the context of causal inference,
even well-designed experiments face the challenge of missing values in
the potential outcomes. Once an individual is assigned to one treatment
condition, we cannot observe that individual's outcome for the treatment
condition that was not assigned. This issue is inherent to the nature of
causal inference where, for each individual, we can only observe one of
the potential outcomes. Breaking down the ATE into observed and
unobserved outcomes gives us:

\[
\text{Average Treatment Effect} = \left(\underbrace{\underbrace{\mathbb{E}[Y(1)|A = 1]}_{\text{observed for } A = 1} + \underbrace{\mathbb{E}[Y(1)|A = 0]}_{\text{unobserved for } A = 0}}_{\text{effect among treated}}\right) - \left(\underbrace{\underbrace{\mathbb{E}[Y(0)|A = 0]}_{\text{observed for } A = 0} + \underbrace{\mathbb{E}[Y(0)|A = 1]}_{\text{unobserved for } A = 1}}_{\text{effect among untreated}}\right).
\]

In this expression, \(\mathbb{E}[Y(1)|A = 1]\) represents the average
outcome when the treatment is given, which is observable. However,
\(\mathbb{E}[Y(1)|A = 0]\) represents the average outcome if the
treatment had been given to those who were actually untreated. This
quantity remains unobservable. Similarly, the quantity
\(\mathbb{E}[Y(0)|A = 1]\) remains unobservable.

Clearly, the fundamental problem of causal inference lurks in the
background of experiments. For each individual we cannot know how the
intervention would have turned out had they received the alternative
treatment to what they in fact received, any more than you can
quantitatively determine how life would have turned out had you decided
to take the job at Acme Nuclear Fuels after deciding in fact to attend
the University of Canterbury.

Indeed, the fundamental challenge of causal inference is also present in
experimental research. For each participant, it's impossible to
ascertain the outcome they would have experienced under an alternative
treatment condition, just as you cannot quantitatively predict the life
you would have led had you chosen the job at Acme Nuclear Fuels instead
of attending the University of Canterbury.

\subsubsection{Causal inference in
Experiments}\label{causal-inference-in-experiments}

So, how do experiments manage to estimate average treatment effects
despite this challenge? The key lies in addressing the concept of
`confounding.' Causal inference is complicated by various forms of
confounding, which we will delve into more thoroughly in Part 2. For
now, consider a specific type of confounding that is crucial for
understanding how experiments yield unbiased average treatment effects:
``confounding by common cause.''

Confounding by common cause occurs when an external variable, known as a
confounder, influences both the treatment (or intervention) and the
outcome of interest, potentially leading to a spurious association
between them. This confounder creates a false or exaggerated
relationship that may be mistakenly interpreted as causal. For example,
when assessing the role of access to green space in happiness, perhaps
the association is explained entirely by income. Notice the association
between green space and happiness would not merely be biased, it would
be entirely misleading. Were we to shift low-income people to
high-access green regions we might do nothing to affect subjective
happiness. Therefore, accurately identifying and adjusting for
confounding by common cause is critical for determining the true causal
relationship between two variables, ensuring that the observed
association is not merely a result of extraneous influences.

\paragraph{Balance of confounders removes
confounding}\label{balance-of-confounders-removes-confounding}

Notice that in experimental designs, random assignment of treatment
eliminates any systematic relationship between treatment conditions and
the distribution of variables that can affect the outcomes under
treatment, creating balance in confounders across treatment groups.
Confounders, of course, do not disappear. However the balance that
randomisation (ideally) brings ensures that confounding disappears. This
is because the distribution of confounders is, on average, identical
across the treatment groups. This equilibrium permits the assumption
that the any systematic difference between the average outcomes in these
groups arises from treatment itself.

Let us denote the set of all possible confounders by the letter \(L\).
In causal inference, the absence of confounding can be articulated in
one of two equivalent ways:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  The potential outcomes, given the treatment and confounders, are
  conditionally independent: \(Y(a) \coprod A \mid L\).
\item
  The treatment assignment, given the potential outcomes and
  confounders, is conditionally independent: \(A \coprod Y(a) \mid L\).
\end{enumerate}

Note this mathematical formalism might appear excessive, but it will
assist later when we assess causal inference strategies for
observational data that are aimed at emulating experimental
randomisation.

At the core of these strategies is an endeavour to ensure that we obtain
a balance across treatment conditions in the factors that might
influence the outcome, and balance implies independences of the
treatment and the potential outcomes, conditional on covariates \(L\).
Because which randomisation (if successful) ensures \(L\) is balanced on
the treatments, we may drop the \(L\) and state that randomisation
ensures \(A \coprod Y(a)\).

Randomisation in experiments, ideally, achieves balance in variables
associated with treatment and outcome, eliminating confounding and
enabling specific treatment effect inferences.

Thus, the first key principle for causal inference embodied in
randomised experiments is:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Conditional exchangeability}: we may assume statistical
  independence between potential outcomes and treatment assignment,
  given confounders, ensuring observed group differences are
  attributable to the treatment, not pre-existing differences.
\end{enumerate}

\paragraph{Control ensures consistent treatments are administered across
all
confounders}\label{control-ensures-consistent-treatments-are-administered-across-all-confounders}

There are two further principles embodied in experiments that allow
researchers to infer average treatment effects without observing
individual treatment effects. These are:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  \textbf{Causal consistency}: we may assume that the observed outcomes
  in experimental conditions correspond to the potential outcomes under
  the received treatment. That is for every individual we may assume
  that:\\
  For any individual \(i\), the observed outcome \(Y_i\) given their
  treatment status \(A_i = 1\) is equal to their potential outcome under
  treatment, denoted as \(Y_i(1)\). Similarly, when the treatment status
  for the same individual \(i\) is \(A_i = 0\), the observed outcome
  \(Y_i\) is equal to the potential outcome under no treatment, denoted
  as \(Y_i(0)\). This encapsulates the concept of causal consistency,
  which asserts that the observed outcome for an individual under the
  treatment condition they actually received aligns with their
  corresponding potential outcome. This can be formally expressed as:
\end{enumerate}

\[
\begin{aligned}
Y_{i}(1) &= (Y_{i}|A_{i} = 1) \quad \text{(Potential outcome observed if treated)} \\
Y_{i}(0) &= (Y_{i}|A_{i} = 0) \quad \text{(Potential outcome observed if untreated)}
\end{aligned}
\]

When conditional exchangeability is satisfied, we can extend our
analysis from comparisons of treatment effects observed in the data to
comparisons of the average treatment effects as if the entire population
had been treated or had been untreated. This is mathematically
represented as:

\[
\begin{aligned}
\text{Average Treatment Effect (ATE)} &= \mathbb{E}[Y(1)] - \mathbb{E}[Y(0)] \\
&= \mathbb{E}(Y|A=1) - \mathbb{E}(Y|A=0)
\end{aligned}
\]

Here, \(\mathbb{E}[Y(1)]\) and \(\mathbb{E}[Y(0)]\) denote the expected
outcomes if the entire population were treated or untreated,
respectively. \(\mathbb{E}(Y|A=1)\) and \(\mathbb{E}(Y|A=0)\) denote the
observed average outcomes among those who were treated (\(A=1\)) and
untreated (\(A=0\)), under the condition of exchangeability. By the
causal consistency and conditional exchangeability assumptions, then, we
may estimate the ATE by comparing the average outcomes between treated
and untreated groups within the \emph{observed data}.

Note that causal consistency assumption is generally satisfied by the
\emph{control} that experimentalists exert over randomised controlled
experiments. In an experiment, treatment groups get equivalent
treatments. However, as we shall see that in real-world data, the
assumption of consistent treatments is much harder to satisfy.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  \textbf{Positivity}: we may assume that there is a non-zero
  probability of receiving each treatment level within covariate-defined
  subgroups.
\end{enumerate}

\[ P(A = a | L= l) > 0 \]

This assumption is also satisfied by the \emph{control} that
experimentalists exert over randomised controlled experiments. However,
in observational settings, this condition must be verified to avoid
extrapolating results beyond observed data.

\paragraph{The three fundamental principles of causal
inference}\label{the-three-fundamental-principles-of-causal-inference}

Thus we have seen that three principles -- Exchangeability, Consistency,
and Positivity -- address the core challenge of causal inference: making
valid inferences about unobservable potential outcomes. In Section 3, we
shall understand that even randomised controlled experiments can produce
biased causal effect estimates, and that methods for causal inference in
observational settings offer powerful tools for clarifying sources of
bias. For now, it is these three principles that define the ideal of a
randomised controlled experiment, bringing focus to the demands required
for causal inference in observational `real-world' settings. We next
turn to this topic.

\subsubsection{Why the fundamental assumptions of causal inference in
observational settings are
hard}\label{why-the-fundamental-assumptions-of-causal-inference-in-observational-settings-are-hard}

In observational research, where researchers do not control the
allocation of treatments, the objective is to emulate a controlled
experimental environment as closely as possible. However, this emulation
introduces unique challenges, particularly in defining and consistently
applying treatments across the study population.

\subparagraph{The conditional exchangeability assumption in
observational
settings}\label{the-conditional-exchangeability-assumption-in-observational-settings}

Achieving conditional exchangeability, where the groups being compared
are similar in all aspects except for the treatment, poses a significant
challenge in observational studies. Return to our example about the
effect of living near green spaces on subjective happiness. When
observing real-world data, it is evident that individuals who live with
access to green spaces might systematically differ from those who do
not, in several respects:

\begin{itemize}
\item
  \textbf{Socioeconomic status}: there might be a correlation between an
  individual's economic background and their proximity to green spaces.
  More affluent individuals could afford housing in areas with better
  access to high-quality green spaces.
\item
  \textbf{Age demographics}: different age groups might naturally
  gravitate towards or away from green spaces. Younger individuals or
  families with children might prioritize access to parks, unlike other
  demographics.
\item
  \textbf{Mental health}: people with existing mental health issues
  might seek out green spaces for their therapeutic benefits or avoid
  them due to social anxiety or other factors.
\item
  \textbf{Lifestyle choices}: individuals preferring outdoor activities
  might choose to live near green spaces. It becomes challenging to
  determine whether the proximity to green spaces causes improved
  well-being or if it is merely a characteristic of individuals who
  already lead healthier lifestyles.
\item
  \textbf{Personal values and social connections}: the decision to live
  near green spaces might also be influenced by personal values, such as
  environmentalism, or the desire to be part of a community that values
  these spaces. Such values and connections can influence how
  individuals interact with and benefit from each other, while living in
  proximity to green-spaces.
\end{itemize}

These and other unmeasured factors can introduce biases that complicate
the interpretation of causal relationships in observational studies.

\subparagraph{The causal consistency assumption and heterogeneity of
treatments in observational
settings}\label{the-causal-consistency-assumption-and-heterogeneity-of-treatments-in-observational-settings}

Again we consider our interest in quantifying the causal effect of
living near green spaces. The definition of `proximity to green spaces'
itself varies significantly, leading to a diverse range of experiences
classified under the same `treatment'. Again, we set aside the issue
that the proximity is a continuous variable, and that the distance to
the nearest green space influences how often and easily individuals can
access these areas, affecting the impact of this `treatment'. However we
should note that causal inference always requires a contrast of
conditions. When assessing causal contrasts we must specify the points
to be compared on a continuous scale. Arguable such comparisons will
require artificiality and extrapolation. Focussing on the variability of
the green spaces themselves this includes:

\begin{itemize}
\item
  \textbf{Diversity of green spaces}: the biodiversity and aesthetic
  value of these spaces can vary widely. Some might have access to
  well-maintained parks, while others to basic recreational areas with
  limited natural appeal.
\item
  \textbf{Availability of amenities}: amenities such as walking paths,
  benches, and recreational facilities can enhance the experience of
  green spaces, encouraging more frequent and prolonged visits.
\item
  \textbf{Size and type of green space}: the type (e.g., urban park,
  community garden) and size of the green space might affect the
  psychological and physical benefits it offers. Thus, as we convert
  observed outcomes under these heterogenous conditions, it might be
  unclear which interventions it is that we are comparing.
\end{itemize}

\subparagraph{The positivity assumption in observational
settings}\label{the-positivity-assumption-in-observational-settings}

Positivity, the requirement that every individual has a chance of
receiving each level of treatment being compared, can be challenging to
ensure in observational studies. In some urban areas, it might be
practically impossible for certain demographics to have access to green
spaces due to factors like housing prices or availability, limiting
treatment exposure variability within certain strata and complicating
valid causal inference.

Addressing the challenges of observational settings requires a deep
understanding of the context and meticulous application of statistical
methods to mimic the conditions of a randomised experiment. The closer
our data approximates a randomised controlled experiment, the more
confidence we can have in our causal inferences. However, it is crucial
to acknowledge that often, the data may not provide a high level of
confidence. By referencing the gold standard of a randomised experiment,
environmental psychologists can better understand and communicate the
strengths and limitations of observational data in answering causal
questions, many of which cannot be addressed experimentally due to
practical or ethical constraints.

In Section 2, we will examine how causal diagrams can significantly
enhance our understanding of these strengths and limitations. These
diagrams are powerful tools for visualising and analysing relationships
and potential confounding factors within observational data. They
provide a framework for identifying and addressing the assumptions
necessary for causal inference, offering a clearer pathway for
interpreting complex data sets. This approach is instrumental in
bridging the gap between the idealised conditions of a randomised
experiment and the realities of observational studies, ultimately
enriching the environmental psychologist's toolkit for scientific
understanding.

\subsection{Part 2. An Introduction to Causal
Diagrams}\label{part-2.-an-introduction-to-causal-diagrams}

Causal diagrams are powerful tools for evaluating causal inferences
(\citeproc{ref-greenland1999}{Greenland \emph{et al.} 1999};
\citeproc{ref-pearl1995}{Pearl 1995}, \citeproc{ref-pearl2009}{2009}).
However, we begin with a \textbf{warning}, to understand causal diagrams
requires some initial terminology. Our experience is that the
terminology is confusing to those who have yet to encounter causal
diagrams. Hang in there. After presenting the terminology and
conventions, we will work through a series of practical examples.
Importantly, we shall discover that there are all forms of confounding
can be derived from four essential causal structures. Once we understand
these structures, we can then move to straightforwardly applying them in
real world scenarios.

First, causal diagrams employ a set of symbols to represent different
elements involved in causal inference. In the context of our discussion,
the symbols and their corresponding roles used in this article are
presented in Table Table~\ref{tbl-01}.

As show in the Table~\ref{tbl-01}

\begin{itemize}
\tightlist
\item
  \textbf{\(A\)} represents the treatment or exposure variable in the
  study. This could be any intervention or condition whose effect on an
  outcome is being investigated. \textbf{This symbol denotes the cause}.
\item
  \textbf{\(Y\)} represents the outcome variable, which is the effect or
  result that researchers aim to understand and possibly predict based
  on the treatment or exposure.\textbf{This symbol denotes the effect}.
\item
  \textbf{\(L\)} represents measured confounders. These are variables
  that may bias the causal association between \(A\) on \(Y\).
\item
  \textbf{\(U\)} represents unmeasured confounders. These are variables
  that could influence both the treatment and the outcome but are not
  observed or included in the analysis. Their presence can lead to
  biased causal inferences.
\item
  \textbf{\(M\)} represents a mediator variable. Mediators are factors
  through which the treatment exerts its effect on the outcome. Here we
  will primarily focus on identifying the total effect a single
  treatment \(A\) on single outcome \(Y\), however, it will be important
  to understand the role that ``controling for'' mediators plays in
  \emph{biasing} total effect estimates.
\end{itemize}

\begin{table}

\caption{\label{tbl-01}Terminology for causal diagrams. (This table is
adapted from (\citeproc{ref-bulbulia2023}{Bulbulia 2023}))}

\centering{

\terminologylocalconventionssimple

}

\end{table}%

\begin{table}

\caption{\label{tbl-02}Basic conventions for causal diagrams. This table
is adapted from (\citeproc{ref-bulbulia2023}{Bulbulia 2023})}

\centering{

\terminologygeneralbasic

}

\end{table}%

Having defined the meaning of our symbols, Table~\ref{tbl-02} describe
the basic elements of a causal diagram. The key elements are of a causal
diagram are:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Nodes}: nodes represent variables or events within a causal
  system,
\item
  \textbf{Edges} edges represent relationships between these variables.
  In causal diagrams, edges are directed. The define pathways of causal
  influence. Note that causal diagrams are non-parametric. This means
  that we a non-linear arrow is drawn the same a a linear relationship.
\item
  \textbf{Parent and child relationships}: we call a variable an
  ``child'' if an arrow points to it. Any ``parent'' is a variable that
  at the other end of the arrow.
\item
  \textbf{Acyclic}: causal diagrams must be acyclic -- they cannot
  contain feedback loops. More precisely: no variable can be an ancestor
  or descendant of itself. \emph{Therefore, in cases where repeated
  measurements are taken, nodes must be indexed by time.}
\item
  \textbf{Conditioning}: in causal diagrams, we often assess the
  implications for obtaining an unbaised estimate of the relationship
  between the treatment and exposure by controlling for a variable. We
  indicate such ``conditioning'' or ``adjustment'' or ``control'' using
  the boxed symbol.
\item
  \textbf{D-separation}: we call a path ``blocked,'' or ``d-separated,''
  if a node along it prevents the transmission of influence. Two
  variables are considered d-separated if all paths between them are
  blocked; otherwise, they are d-connected
  (\citeproc{ref-pearl1995}{Pearl 1995}). In the next section, we will
  consider the rules of d-separation allow causal graphs to identify
  strategies for identifying causal effects from observational data.
\end{enumerate}

\paragraph{The rules of d-Separation}\label{the-rules-of-d-separation}

\begin{table}

\caption{\label{tbl-03}This table is adapted from
(\citeproc{ref-bulbulia2023}{Bulbulia 2023})}

\centering{

\terminologydirectedgraph

}

\end{table}%

Pearl showed that the principles of d-separation enable us to evaluate
relationships between nodes in a causal diagram
(\citeproc{ref-pearl1995}{Pearl 1995}). To build intuition for moving
from simple to more complex directed acyclic graphs (DAGs) and
understanding Pearl's rules of d-separation consider how causal
relationship can be built up from simple relationships. The elementary
building blocks are presented in Table~\ref{tbl-03}.

\subsubsection{Two Variables with No
Arrows}\label{two-variables-with-no-arrows}

When two variables, \(X_0\) and \(X_1\), have no arrows between them,
this represents a scenario where there is no causal effect assumed
between the two variables. In this context, the variables are considered
to be independent of each other, denoted as \(X_0 \coprod X_1\). This
independence implies that knowing the value of one does not provide any
information about the value of the other.

\subsubsection{Two Variables with a Causal
Arrow}\label{two-variables-with-a-causal-arrow}

Introducing a causal arrow from \(X_0\) to \(X_1\)
(\(X_0 \rightarrow X_1\)) signifies that \(X_0\) causally affects
\(X_1\). This causal relationship implies dependency between \(X_0\) and
\(X_1\), denoted as \(X_0 \cancel\coprod X_1\). Here, knowledge about
\(X_0\) provides information about \(X_1\), reflecting their causal
association.

\subsubsection{Three Variables: Fork, Chain, and Collider
Structures}\label{three-variables-fork-chain-and-collider-structures}

With three variables, the complexity increases, introducing three
fundamental structures: fork, chain, and collider.

\begin{itemize}
\item
  \textbf{Fork Structure}: The fork structure (\(X_0 \rightarrow X_1\),
  \(X_0 \rightarrow X_2\)) implies \(X_0\) is a common cause of \(X_1\)
  and \(X_2\).
\item
  \textbf{Fork Rule}: Here, \(X_1\) and \(X_2\) are conditionally
  independent given \(X_0\) (if \(\boxed{X_0}\), then
  \(X_1 \coprod X_2 | X_0\)). Conditioning on \(X_0\) blocks any
  association between \(X_1\) and \(X_2\) through \(X_0\).
\item
  \textbf{Chain structure}: The chain structure
  (\(X_0 \rightarrow X_1 \rightarrow X_2\)) indicates a causal chain
  where \(X_0\) affects \(X_1\), which in turn affects \(X_2\).
\item
  \textbf{Chain rule}: \(X_0\) and \(X_2\) are conditionally independent
  given \(X_1\) (if \(\boxed{X_1}\), then \(X_0 \coprod X_2 | X_1\)).
  Conditioning on \(X_1\) blocks the indirect path between \(X_0\) and
  \(X_2\).
\item
  \textbf{Collider structure}: The collider structure
  (\(X_0 \rightarrow X_2 \leftarrow X_1\)) shows \(X_2\) as a common
  effect of \(X_0\) and \(X_1\).
\item
  \textbf{Collider rule}: in a collider structure, \(X_0\) and \(X_1\)
  are independent. However, conditioning on \(X_2\) (or a descendant of
  \(X_2\), \(X_3\)) introduces an association between \(X_0\) and
  \(X_1\) (\(X_0 \cancel\coprod X_1 | X_2\)). This is because
  conditioning on the common effect (or its descendant) creates a
  pathway through which \(X_0\) and \(X_1\) can influence each other.
\end{itemize}

Again, for those unfamiliar with causal diagrams, these elementary
structure of causation will sound abstract. For those unfamiliar with
probability theory, writing out conditional dependencies and
independencies is perhaps daunting. However, having stated the rules and
conventions, we shall see that the application of causal diagrams to
concrete problems is not daunting. Indeed our experience is that people
find developing causal diagrams a great deal of fun. Importantly, causal
diagrams require no math. They are purely qualitative tools that encode
assumptions. However, because they allow us to evaluate how variables
become independent or associated, we can use the fork rule, chain rule,
and collider rule to guide us to modelling and data collection decisions
in the pursuit of causal inferences.

\paragraph{The elementary confounding
conditions}\label{the-elementary-confounding-conditions}

\begin{table}

\caption{\label{tbl-04}This describes elementary and complex confounding
scenarios (table is adapted from (\citeproc{ref-bulbulia2023}{Bulbulia
2023}))}

\centering{

\terminologyelconfoundersLONG

}

\end{table}%

\subsubsection{1. The problem of confounding by a common
cause}\label{the-problem-of-confounding-by-a-common-cause}

Table~\ref{tbl-04} row 1 describes the problem of confounding by common
cause. We encountered this problem in the first section. Such
confounding arises when there is a variable or set of variables, denoted
by \(L\), that influence both the exposure, denoted by \(A\), and the
outcome, denoted by \(Y.\) Because \(L\) is a common cause of both \(A\)
and \(Y\), \(L\) may create a statistical association between \(A\) and
\(Y\) that does not reflect a causal association.

For instance, in the context of green spaces, consider people choosing
to live closer to green spaces (exposure \(A\)) and their experience of
improved mental health (outcome \(Y\)). A common cause could be
socioeconomic status (\(L\)). Individuals with higher socioeconomic
status may have the financial capacity to afford housing near green
spaces and simultaneously afford better healthcare and lifestyle
choices, contributing to improved mental health. Thus, while the data
may show a statistical association between living closer to green spaces
(\(A\)) and improved mental health (\(Y\)), this association may not
reflect a direct causal relationship due to the confounding by
socioeconomic status (\(L\)).

How might we obtain balance in this confounder for the treatments to be
compared? Addressing confounding by a common cause involves its
adjustment. This adjustment effectively closes the backdoor path from
the exposure to the outcome. Equivalently, conditioning on \(L\)
d-separates \(A\) and \(Y\). Common adjustment methods include
regression, matching, inverse probability of treatment weighting, and
G-methods (covered in (\citeproc{ref-hernuxe1n2023}{Hern√°n and Monge
2023})). As indicated in Table~\ref{tbl-04} row one, any confounder that
is a common cause of both \(A\) and \(Y\) must precede \(A\) (and hence
\(Y\)). Often it will be possible to known that the a variable precedes
the exposure. For example we may record a persons country of birth at
any point in their life. In other cases, the timing may be unclear. When
we draw our causal diagram, we are asserting that this confounder really
has occured before the exposure. This may often be a strong assertion.
In this case, it may be useful to the scenario in which the confounder
comes later. We next turn to one such scenario.

\subsubsection{2. Mediator bias}\label{mediator-bias}

Let us return to green spaces example, again we consider proximity to
green spaces as the exposure (\(A\)), mental health as the outcome
(\(Y\)), and physical activity as the mediator (\(L\)).

In this scenario, suppose that living close to green spaces (\(A\))
influences physical activity (\(L\)), which subsequently impacts mental
health (\(Y\)). Notice that if we were to condition on physical activity
(\(L\)), assuming it to be a confounder, we would then bias our
estimates of the total effect of proximity to green spaces (\(A\)) on
mental health (\(Y\)). Such a bias arises as a consequence of the chain
rule. Conditioning on \(L\) `d-Separates' the total effect of \(A\) on
\(Y\). This phenomenon is known as mediator bias. Notable Montgomery
\emph{et al.} (\citeproc{ref-montgomery2018}{2018}) finds many dozens of
examples of mediator bias in \emph{experiments} in which control is made
for variables that occur after the treatment. For example, the practice
of obtaining demographic and other information from participants
\emph{after} a study is an invitation to mediator bias. If the treatment
affects these variables, and the variables affect the outcome (as we
assume by controlling for them), then researchers may induce mediator
bias.

To avoid mediator bias we should, of course, avoid conditioning on a
mediator. Importantly, we avoid this problem by ensuring that \(L\)
occurs before the treatment \(A\) and the outcome \(Y\). This solution
is presented Table~\ref{tbl-04} row 2.

\subsubsection{3. Confounding by collider stratification (conditioning
on a common
effect)}\label{confounding-by-collider-stratification-conditioning-on-a-common-effect}

Conditioning on a common effect, also known as collider stratification,
occurs when a variable, denoted by \(L\), is influenced by both the
exposure, denoted by \(A\), and the outcome, denoted by \(Y\).

Imagine, in the context of green spaces, an individual's choice to live
closer to green spaces (exposure \(A\)) and their improved mental health
(outcome \(Y\)) are both influencing the individual's overall
satisfaction with life (common effect \(L\)). Initially, \(A\) and \(Y\)
could be independent, represented as \(A \coprod Y(a)\), suggesting that
the decision to live near green spaces is not directly causing improved
mental health.

However, when we condition on the life satisfaction \(L\) (the common
effect of \(A\) and \(Y\)), a backdoor path between \(A\) and \(Y\) is
opened. This could potentially induce a non-causal association between
living closer to green spaces and improved mental health. The reason
behind this is that the overall life satisfaction \(L\) can provide
information about both the proximity to green spaces \(A\) and the
mental health status \(Y\). Hence, it may appear that there is an
association between \(A\) and \(Y\) even when there may not be a direct
causal relationship.

Causal diagrams point a way to respond to the problem of collider
stratification bias: we should generally ensure that:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  all confounders \(L\) that are common causes of the exposure \(A\) and
  the outcome \(Y\) are measured before \(A\) has occurred, and
\item
  \(A\) is measured before \(Y\) has occurred.
\end{enumerate}

If such temporal order is preserved, \(L\) cannot be an effect of \(A\),
and thus neither of \(Y\).{[}\^{}2{]}

\paragraph{4. Confounding by conditioning on a descendant of a
confounder}\label{confounding-by-conditioning-on-a-descendant-of-a-confounder}

The rules of d-separation also apply to conditioning on descendents of a
confounder. As show in Table~\ref{tbl-04} row five, we should ensure
when conditioning on a measured descendant of an unmeasured collider
because doing so evokes confounding by proxy.

\subsubsection{More complicated forms of confounding are combinations of
elementary
confounding}\label{more-complicated-forms-of-confounding-are-combinations-of-elementary-confounding}

We next turn to more complicated scenarios that combine elements of the
elementary structures of causality.

\subsubsection{M-bias: conditioning on a collider that occurs before the
exposure may introduce
bias}\label{m-bias-conditioning-on-a-collider-that-occurs-before-the-exposure-may-introduce-bias}

Table~\ref{tbl-04} row 5 presents a form of pre-exposure
over-conditioning bias known as ``M-bias''. This bias combines the
collider structure and the fork-structure revealing that it is possible
to induce confounding even if we ensure that all variables have been
measured before the treatment. The collider structure is shown in the
path \(U_Y \to L_0\) and \(U_A \to L_0\). We know from the collider rule
that conditioning on \(L_0\) opens a path between \(U_Y\) and \(U_A\).
What is the result? We find that \(U_Y\) is associated with the outcome
\(Y\) and \(U_A\) is associated with treatment \(A\). Thus the
association that is opened by conditioning on \(L\) creates an open
back-door path linking the treatment to the outcome. We have
confounding. How might such confounding play out in a real world
setting?

In the context of green spaces, consider the scenario where an
individual's level of physical activity (\(L\)) is influenced by an
unmeasured factor related to their propensity to live near green spaces
(\(A\)) and another unmeasured factor linked to their mental health
(\(Y\)). Here, physical activity \(L\) does not directly affect the
decision to live near green spaces \(A\) or mental health status \(Y\),
but is a descendent of unmeasured variables that do. If we condition on
physical activity \(L\) in this scenario, we create the bias just
described, known as ``M-bias.''

How shall we respond? The solution is straightforward. If \(L\) is
neither a common cause of \(A\) and \(Y\) nor the effect of a shared
common cause, then \(L\) should not be included in a causal model. In
terms of the conditional exchangeability principle, we find
\(A \coprod Y(a)\) yet \(A \cancel{\coprod} Y(a)| L\). So we must not
condition on \$L(\citeproc{ref-cole2010}{Cole \emph{et al.}
2010}).\footnote{Note, when we draw a chronologically ordered path from
  left to right the M shape for which ``M-bias'' takes its name changes
  to an E shape We shall avoid proliferating jargon and retain the term
  ``M bias.''}

\subsubsection{6. Conditioning on a descendent may reduce
confounding}\label{conditioning-on-a-descendent-may-reduce-confounding}

We can develop the rules of d-Separation to obtain unexpected strategies
for confounding control. Consider Table~\ref{tbl-04} row six. The causal
diagram describes a setting in which there is an open back-door path
linking the treatment and outcome. We have what appears to be
intractable confounding. Return to our green-space example, consider an
unmeasured confounder \(U\), perhaps a genetic factor, that affects
one's desire to seek out isolation in green spaces \(A\) and also
independently affects one's mental health \(Y\). Were such an unmeasured
confounder to exist we could not obtain an unbiased estimate for the
causal effect of green-space on happiness. However, imagine a variable
\(L^\prime\), that is a trait that is expressed later in life, after
decisions to seek out isolation have been made. If this trait could be
measured, even though it is expressed after the treatment and outcome
has occurred, controlling for it would enable us to close the backdoor
path between the treatment and the outcome. The reason this strategy
work is that a measured effect is a proxy for its cause. By conditioning
on the late-adulthood trait, we partially condition on its cause, in
this case a confounder that occurred before the treatment and leads to a
confounding association between the treatment and outcome in the absence
of causality.

\paragraph{7. Confounding control with three waves of
data}\label{confounding-control-with-three-waves-of-data}

Table~\ref{tbl-04} row 7 presents another setting in which there is
unmeasured confounding. However, we may use the rules of d-separation to
develop strategies for data collection and data modelling that may
greatly reduce the influence of unmeasured confounding on our causal
inferences. As shown the ``response'', but collecting data for the both
the treatment and the outcome at baseline, and controlling for this
information in our statistical models, any unmeasured association
between the treatment \(A_1\) and the outcome \(Y_2\) would need to be
\emph{independent} of these baseline measurements. Thus including the
baseline exposure and outcome, along with other measured covariates that
are measured descendents of unmeasured confounders, we exert
considerable confounding control
(\citeproc{ref-vanderweele2020}{VanderWeele \emph{et al.} 2020}).

A secondary advantage of such data collection is that it allows us to
obtain an incident exposure effect, rather than merely a prevalence
exposure effect. Consider:

\textbf{The prevalence exposure effect} evaluates the association
between the exposure or treatment status at time \(t1\) and the outcome
observed at a later time \(t2\). It is defined by the pathway
\(A_{1} \to Y_{2}\). The prevalent exposure effect does not consider the
initial status of the exposure. It is expressed:

\[
\text{prevalence exposure effect:} \quad A_{1} \to Y_{2}
\]

As such, the prevalence exposure effect describes the effect of current
or ongoing exposures on outcomes. For example imagine that living far
from green spaces makes people so depressed that they never respond to
surveys. When we observe the association between green space and
happiness in the data, we are left with only those people who are so
incurably happy that even living far from green space cannot bring them
down. As such , it may appear in our data that \(A_{1} \to Y_{2}\) is
helpful when, in fact, the treatment is harmful; see: Hern√°n \emph{et
al.} (\citeproc{ref-hernuxe1n2016}{2016}); Danaei \emph{et al.}
(\citeproc{ref-danaei2012}{2012}); VanderWeele \emph{et al.}
(\citeproc{ref-vanderweele2020}{2020}); Bulbulia
(\citeproc{ref-bulbulia2022}{2022}).

\textbf{Incident exposure effect}: evaluates the association between the
exposure or treatment status at time \(t1\) and the outcome observed at
a later time \(t2\) conditional on the baseline exposure:
\(A_{0} \to A_{1} \to Y_{2}\). This model more closely emulates an
experiment because it considers the transition in treatment or exposure
status from \(A_0\) to \(A_1\). The initiation of a treatment provides a
clearer intervention from which to estimate a causal effect at \(Y_2\).
It is expressed:

\[
\text{Incident exposure effect:} \quad \boxed{A_{0}} \to A_{1} \to Y_{2}
\]

Returning to our example, the any association between of \(A_1\) and
\(Y_2\) would require that people initiate a change from the level of
\(A_0\) from baseline. Thus by controlling for the baseline value of the
treatment we may learn about the effect of shifting one's access to
green space status.

As in Table~\ref{tbl-04} row seven, we obtain further control by
including the baseline outcome \(Y_0\) as well as the baseline exposure
\(A_0\) such that:

\[
\boxed{
\begin{aligned}
L_{0} \\
A_{0} \\
Y_{0}
\end{aligned}
}
\to A_{1} \to Y_{2}
\]

Thus the incident exposure effect better emulates a `target trial' or a
the organisation of observational data into a hypothetical experiment in
which there is a `time-zero' initiation of treatment in the data; see:
Hern√°n \emph{et al.} (\citeproc{ref-hernuxe1n2016}{2016}); Danaei
\emph{et al.} (\citeproc{ref-danaei2012}{2012}); VanderWeele \emph{et
al.} (\citeproc{ref-vanderweele2020}{2020}); Bulbulia
(\citeproc{ref-bulbulia2022}{2022}).

Causal diagrams reveal both the possibility and power of panel data
collection, and equally importantly of how to appropriately model model
panel data to obtain valid causal inference (note that a multi-level
model or structural equation ``growth'' model would not obtain the
incident exposure effect.) To obtain the incident exposure effect, we
generally require that events in the data can be accurately classified
into at least three relative time intervals and we must model the
treatments and outcomes as separate elements in our statistical model.

\paragraph{Summary and general rules for creating causal
diagrams}\label{summary-and-general-rules-for-creating-causal-diagrams}

In this chapter, we have reviewed the relevance of the potential
outcomes framework to causal inference environmental psychology. In
Section 1 we encountered three fundamental assumptions required for
obtaining average treatment effects from data:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Causal consistency}: the outcome observed under the treatment
  condition for any unit matches the outcome that would have been
  observed had the unit received the treatment, and similarly for the
  control condition.
\item
  \textbf{Exchangeability}: the allocation of treatment is randomised
  and independent of the potential outcomes, conitional on measured
  covariates.
\item
  \textbf{Positivity}: every unit has a non-zero probability of
  receiving the treatments to be compared.
\end{enumerate}

These assumptions are typically satisfied in the setting of randomised
experiments, which adeptly address the issue of missing potential
outcomes between the groups under comparison. However, in observational
studies, the non-random assignment of treatments complicates causal
inference.

We then turned to casual graphs and discussed their application for
addressing the `no unmeasured confounding' assumption, considering both
elementary and complex confounding scenarios.

We have seen that, at their essence, causal diagrams serve as a
simplified visual language for articulating complex causal relationships
within a given study. In causal diagrams, each symbol encodes specific
assumptions about the causal structure of the variables it represents.
For instance, when we draw an arrow from one variable to another, we
assert that a causal relationship exists based on theoretical or
empirical evidence. This is true for all relationships depicted in the
graph, with the notable exception of the relationship between the
treatment, \(A\), and the outcome, \(Y\).

Importantly, we have seen that when developing a causal diagram, the
objective is to describe only those features of a complex causal setting
that are relevant to assessing bias in the relationship of treatment
\(A\) and the outcome \(Y\). Variables that are not relevant to
evaluating this question should be omitted.

It is worth emphasising that wwe will often want to simplify our causal
diagrams to the minimum level of complexity required to convey the
problem at hand. For example the confounder symbol \(L\) typically
represents a vector of covariates. We may encapsulating numerous
confounders within a single node where all arrows in and out of the node
are functionally equivalent for the assessing bias in the association of
\(A\) and \(Y\). This simplification is crucial as it allows researchers
to focus on key variables and their interconnections without being
overwhelmed by the full complexity of the underlying data.

Finally, the construction of a causal diagram is relative to a specific
research interest and context. The graph is tailored to the particular
causal questions at hand and the specific population under investigation
-- the target population. For example, if the research interest were to
shift to understanding the total effect of a set of covariates \(L\) on
the treatment \(A\), the causal diagram would need be redrawn to reflect
this new focus, altering the direction or presence of arrows to
represent the hypothesised causal pathways accurately.

Here we considered only seven applications of causal diagrams. Of course
there are many more scenarios we would need to consider to evaluate how
observational data might be leveraged to obtain scientific insights in
environmental psychology. In each case, however, our causal diagrams
will consist of combinations of four elementary relationships: the
association of effect with its cause (elementary causality expressed
between two variables), the fork (a common cause of two variables); the
chain (a mediator between two variables); and the collider (the common
effect of two variables). These rules by which variables becomes
associated and disassociated by conditioning on nodes within these
elementary relationships allows researchers to use causal diagrams to
investigate innumerably many practical questions. Causal diagrams may
also inform strategies for data collection, and may help to focus
attention on which information should and should not be included in our
statistical models. We hope the material we presented here will
encourage environmental psychologist to learn more about causal
inference and to integrate causal diagrams into their workflows.

\newpage{}

DON -- if we were to do Part 3 we could use this table

\begin{table}

\caption{\label{tbl-04}This describes elementary and complex confounding
scenarios (table is adapted from (\citeproc{ref-bulbulia2023}{Bulbulia
2023}))}

\centering{

\terminologyelconfoundersexperiments

}

\end{table}%

\newpage{}

\subsection{Funding}\label{funding}

This work is supported by a grant from the Templeton Religion Trust
(TRT0418). JB received support from the Max Planck Institute for the
Science of Human History. The funders had no role in preparing the
manuscript or the decision to publish it.

\subsection{Contributions}\label{contributions}

TBA

\subsection{References}\label{references}

\phantomsection\label{refs}
\begin{CSLReferences}{1}{0}
\bibitem[\citeproctext]{ref-bulbulia2022}
Bulbulia, JA (2022) A workflow for causal inference in cross-cultural
psychology. \emph{Religion, Brain \& Behavior}, \textbf{0}(0), 1--16.
doi:\href{https://doi.org/10.1080/2153599X.2022.2070245}{10.1080/2153599X.2022.2070245}.

\bibitem[\citeproctext]{ref-bulbulia2023}
Bulbulia, JA (2023) Causal diagrams (directed acyclic graphs): A
practical guide.

\bibitem[\citeproctext]{ref-cole2010}
Cole, SR, Platt, RW, Schisterman, EF, \ldots{} Poole, C (2010)
Illustrating bias due to conditioning on a collider. \emph{International
Journal of Epidemiology}, \textbf{39}(2), 417--420.
doi:\href{https://doi.org/10.1093/ije/dyp334}{10.1093/ije/dyp334}.

\bibitem[\citeproctext]{ref-cook2002experimental}
Cook, TD, Campbell, DT, and Shadish, W (2002) \emph{Experimental and
quasi-experimental designs for generalized causal inference}, Vol. 1195,
Houghton Mifflin Boston, MA.

\bibitem[\citeproctext]{ref-danaei2012}
Danaei, G, Tavakkoli, M, and Hern√°n, MA (2012) Bias in observational
studies of prevalent users: lessons for comparative effectiveness
research from a meta-analysis of statins. \emph{American Journal of
Epidemiology}, \textbf{175}(4), 250--262.
doi:\href{https://doi.org/10.1093/aje/kwr301}{10.1093/aje/kwr301}.

\bibitem[\citeproctext]{ref-greenland1999}
Greenland, S, Pearl, J, and Robins, JM (1999) Causal diagrams for
epidemiologic research. \emph{Epidemiology (Cambridge, Mass.)},
\textbf{10}(1), 37--48.

\bibitem[\citeproctext]{ref-hernan2023}
Hernan, MA, and Robins, JM (2023) \emph{Causal inference}, Taylor \&
Francis. Retrieved from
\url{https://books.google.co.nz/books?id=/_KnHIAAACAAJ}

\bibitem[\citeproctext]{ref-hernuxe1n2023}
Hern√°n, MA, and Monge, S (2023) Selection bias due to conditioning on a
collider. \emph{BMJ}, \textbf{381}, p1135.
doi:\href{https://doi.org/10.1136/bmj.p1135}{10.1136/bmj.p1135}.

\bibitem[\citeproctext]{ref-hernuxe1n2016}
Hern√°n, MA, Sauer, BC, Hern√°ndez-D√≠az, S, Platt, R, and Shrier, I (2016)
Specifying a target trial prevents immortal time bias and other
self-inflicted injuries in observational analyses. \emph{Journal of
Clinical Epidemiology}, \textbf{79}, 7075.

\bibitem[\citeproctext]{ref-holland1986}
Holland, PW (1986) Statistics and causal inference. \emph{Journal of the
American Statistical Association}, \textbf{81}(396), 945960.

\bibitem[\citeproctext]{ref-mancuso2018revolutionary}
Mancuso, S (2018) \emph{The revolutionary genius of plants: A new
understanding of plant intelligence and behavior}, Simon; Schuster.

\bibitem[\citeproctext]{ref-mcelreath2020}
McElreath, R (2020) \emph{Statistical rethinking: A {B}ayesian course
with examples in r and stan}, CRC press.

\bibitem[\citeproctext]{ref-montgomery2018}
Montgomery, JM, Nyhan, B, and Torres, M (2018) How conditioning on
posttreatment variables can ruin your experiment and what to do about
It. \emph{American Journal of Political Science}, \textbf{62}(3),
760--775.
doi:\href{https://doi.org/10.1111/ajps.12357}{10.1111/ajps.12357}.

\bibitem[\citeproctext]{ref-neyman1923}
Neyman, JS (1923) On the application of probability theory to
agricultural experiments. Essay on principles. Section 9.(tlanslated and
edited by dm dabrowska and tp speed, statistical science (1990), 5,
465-480). \emph{Annals of Agricultural Sciences}, \textbf{10}, 151.

\bibitem[\citeproctext]{ref-nguyen2021green}
Nguyen, P-Y, Astell-Burt, T, Rahimi-Ardabili, H, and Feng, X (2021)
Green space quality and health: A systematic review. \emph{International
Journal of Environmental Research and Public Health}, \textbf{18}(21),
11028.

\bibitem[\citeproctext]{ref-pearl1995}
Pearl, J (1995) Causal diagrams for empirical research.
\emph{Biometrika}, \textbf{82}(4), 669--688.

\bibitem[\citeproctext]{ref-pearl2009}
Pearl, J (2009) \emph{\href{https://doi.org/10.1214/09-SS057}{Causal
inference in statistics: An overview}}.

\bibitem[\citeproctext]{ref-reyes2021linking}
Reyes-Riveros, R, Altamirano, A, De La Barrera, F, Rozas-V√°squez, D,
Vieli, L, and Meli, P (2021) Linking public urban green spaces and human
well-being: A systematic review. \emph{Urban Forestry \& Urban
Greening}, \textbf{61}, 127105.

\bibitem[\citeproctext]{ref-robins1986}
Robins, J (1986) A new approach to causal inference in mortality studies
with a sustained exposure period---application to control of the healthy
worker survivor effect. \emph{Mathematical Modelling}, \textbf{7}(9-12),
1393--1512.

\bibitem[\citeproctext]{ref-rohrer2018}
Rohrer, JM (2018) Thinking clearly about correlations and causation:
Graphical causal models for observational data. \emph{Advances in
Methods and Practices in Psychological Science}, \textbf{1}(1), 2742.

\bibitem[\citeproctext]{ref-rubin1976}
Rubin, DB (1976) Inference and missing data. \emph{Biometrika},
\textbf{63}(3), 581--592.
doi:\href{https://doi.org/10.1093/biomet/63.3.581}{10.1093/biomet/63.3.581}.

\bibitem[\citeproctext]{ref-rubin2005}
Rubin, DB (2005) Causal inference using potential outcomes: Design,
modeling, decisions. \emph{Journal of the American Statistical
Association}, \textbf{100}(469), 322--331. Retrieved from
\url{https://www.jstor.org/stable/27590541}

\bibitem[\citeproctext]{ref-vanderweele2009}
VanderWeele, TJ (2009) Concerning the consistency assumption in causal
inference. \emph{Epidemiology}, \textbf{20}(6), 880.
doi:\href{https://doi.org/10.1097/EDE.0b013e3181bd5638}{10.1097/EDE.0b013e3181bd5638}.

\bibitem[\citeproctext]{ref-vanderweele2015}
VanderWeele, TJ (2015) \emph{Explanation in causal inference: Methods
for mediation and interaction}, Oxford University Press.

\bibitem[\citeproctext]{ref-vanderweele2018}
VanderWeele, TJ (2018) On well-defined hypothetical interventions in the
potential outcomes framework. \emph{Epidemiology}, \textbf{29}(4), e24.
doi:\href{https://doi.org/10.1097/EDE.0000000000000823}{10.1097/EDE.0000000000000823}.

\bibitem[\citeproctext]{ref-vanderweele2013}
VanderWeele, TJ, and Hernan, MA (2013) Causal inference under multiple
versions of treatment. \emph{Journal of Causal Inference},
\textbf{1}(1), 120.

\bibitem[\citeproctext]{ref-vanderweele2020}
VanderWeele, TJ, Mathur, MB, and Chen, Y (2020) Outcome-wide
longitudinal designs for causal inference: A new template for empirical
studies. \emph{Statistical Science}, \textbf{35}(3), 437466.

\end{CSLReferences}

\subsection{Appendix: Causal Consistency in observational
settings}\label{appendix-causal-consistency-in-observational-settings}

Although we discussed the causal consistency assumption in Part 1, we
did not examine how observational scientists may deal with it.

In observational research, there are typically multiple versions of
treatment. The theory of causal inference under multiple versions of
treatment proves we can consistently estimate causal effects where the
different versions of treatment are conditionally independent of the
outcomes VanderWeele (\citeproc{ref-vanderweele2009}{2009})

Let \(\coprod\) denote independence. Where there are \(K\) different
versions of treatment \(A\) and no confounding for \(K\)'s effect on
\(Y\) given measured confounders \(L\) such that

\[
Y(k) \coprod K | L
\]

Then it can be proved that causal consistency follows. According to the
theory of causal inference under multiple versions of treatment, the
measured variable \(A\) functions as a ``coarsened indicator'' for
estimating the causal effect of the multiple versions of treatment \(K\)
on \(Y(k)\) (\citeproc{ref-vanderweele2009}{VanderWeele 2009},
\citeproc{ref-vanderweele2018}{2018};
\citeproc{ref-vanderweele2013}{VanderWeele and Hernan 2013}).

In the context of green spaces, \(A\) might represent the general action
of moving closer to any green space and \(K\) represents the different
versions of this treatment. For instance, \(K\) could denote moving
closer to different types of green spaces such as parks, forests,
community gardens, or green spaces with varying amenities and features.

Here, the conditional independence implies that, given measured
confounders \(L\) (e.g.~socioeconomic status, age, personal values), the
type of green space one moves closer to (\(K\)) is independent of the
outcomes \(Y(k)\) (e.g.~mental well-being under the \(K\) conditions).
In other words, the version of green space one chooses to live near does
not affect the \(K\) potential outcomes, provided the confounders \(L\)
are properly controlled for in our statistical models.



\end{document}
