---
title: "Causal Inference in Environmental Psychology"
abstract: |
 This chapter offers an introduction to causal inference within environmental psychology, emphasising the utility of causal diagrams (Directed Acyclic Graphs  -- or DAGs) for addressing causal questions. It begins by outlining foundational concepts of causal inference and clarifying assumptions necessary for estimating causal effects. Subsequent sections describe how to construct causal diagrams tailored to identifying causal effects in observational research settings. The chapter is structured into three parts: an introduction to principles of causal inference, a guide on crafting causal diagrams, and a discussion of their practical applications. It aims to be useful to researchers who hope to address causal questions from observational data.
authors: 
  - name: Joseph A. Bulbulia
    orcid: 0000-0002-5861-2056
    email: joseph.bulbulia@vuw.ac.nz
    affiliation: 
      name: Victoria University of Wellington, New Zealand, School of Psychology, Centre for Applied Cross-Cultural Research
      department: Psychology/Centre for Applied Cross-Cultural Research
      city: Wellington
      country: New Zealand
      url: www.wgtn.ac.nz/cacr
  - name: Donald W Hine
    orcid: 0000-0002-3905-7026
    email: donald.hine@canterbury.ac.nz
    affiliation: 
      name: University of Canterbury, School of Psychology, Speech and Hearing
      city: Wellington
      country: New Zealand
      url: www.wgtn.ac.nz/cacr
keywords:
  - DAGS
  - Causal Inference
  - Confounding
  - Environmental
  - Psychology
  - Panel
format:
  pdf:
    sanitize: true
    keep-tex: true
    link-citations: true
    colorlinks: true
    documentclass: article
    classoption: [singlecolumn]
    lof: false
    lot: false
    geometry:
      - top=30mm
      - left=20mm
      - heightrounded
    header-includes:
      - \input{/Users/joseph/GIT/templates/latex/custom-commands.tex}
date: last-modified
execute:
  echo: false
  warning: false
  include: true
  eval: true
fontfamily: libertinus
bibliography: /Users/joseph/GIT/templates/bib/references.bib
csl: ./camb-a.csl
---

```{r}
#| label: load-libraries
#| echo: false
#| include: false
#| eval: false
#  fig-pos: 'htb'
#   html:
#    html-math-method: katex

# Include in YAML for Latex
# sanitize: true
# keep-tex: true
# include-in-header:
#       - text: |
#           \usepackage{cancel}


# for making graphs
library("tinytex")
library(extrafont)
loadfonts(device = "all")


# libraries for jb (when internet is not accessible)
# read libraries
source("/Users/joseph/GIT/templates/functions/libs2.R")

# read functions
source("/Users/joseph/GIT/templates/functions/funs.R")

# read data/ set to path in your computer
pull_path <-
  fs::path_expand(
    "/Users/joseph/v-project\ Dropbox/data/current/nzavs_13_arrow"
  )

# for saving models. # set path fo your computer
push_mods <-
  fs::path_expand(
    "/Users/joseph/v-project\ Dropbox/data/nzvs_mods/00drafts/23-causal-dags"
  )


# keywords: potential outcomes, DAGs, causal inference, evolution, religion, measurement, tutorial
# 10.31234/osf.io/b23k7

```


## Introduction

Causal inference attempts to quantify the magnitude of causal relations from data. In the simplest case, we evaluate whether an intervention or "treatment" on one variable leads, on average, to subsequent changes in another variable, known as the "outcome", and if so, by how much. The objective of causal inference is not only to determine whether cause-effect relationships are present but also to estimate their magnitude using data [@cook2002experimental].

It is often thought that the capacity for evaluating causal relationships is a complex cognitive process unique to humans. However, capacities for causal understanding span the plant and animal kingdoms among organisms that vary in levels of conscious awareness and complexity [@mancuso2018revolutionary]. For example, plants and microorganisms have genetic predispositions to recognise and react to environmental conditions critical for their reproduction and survival. Birds and rodents demonstrate the ability to learn, albeit not as complexly as larger mammals and primates. Over millennia, humans have developed diverse frameworks for causal understanding, ranging from the law of karma in Hindu, Jain, and Buddhist traditions to Aristotle’s four causes, early empirical and experimental approaches by Francis Bacon and John Locke, Newtonian mechanics, and others.

Although the capacity to infer cause and effect is common to many lineages and has evolved to high levels of sophistication in human cultures, methods for quantitatively estimating magnitudes of causal effect from data are much less common. Arguably, such capacities emerged only recently with the development of randomised controlled experiments. Specialist methods for quantifying magnitudes of causal influence from "real-world" data are even more recent. Although such methods are routinely taught in modern epidemiology [@lash2020] and economics [@angrist2009mostly], many social scientists do not routinely employ methods for observational causal inference [@morgan2014]. Thus, in our more specialised sense of "causal inference," that of quantifying magnitudes of effect, causal inference is far from universal. This chapter aims to familiarise environmental psychologists with a fundamental understanding of these newly developed specialist methods.

We believe there is considerable value in understanding these more recently developed methods for causal inference in observational settings and for routinely employing them. Introductory undergraduate psychology courses teach that correlations between variables do not necessarily indicate causation. The question arises: can valid causal inferences ever be drawn from observational data, and if so, how? Although randomised experiments are often considered to be the “gold standard” for generating causal inferencess, they can be costly and ethically challenging. Moreover, even the use of randomised controlled experiments do not always ensure valid causal insights due to factors such as treatment noncompliance, differential attrition patterns across treatment conditions and sampling bias [@hernan2017per; @montgomery2018]. 

Observational data, being more plentiful, offers potentially valuable resources for accelerating knowledge in environmental psychology, provided causal insights can be gleaned from them. However, many researchers continue to attempt to draw causal inferences from such data with statistical models that are not fit for purpose. Indeed routinely employed practices that attempt to "control for" the confounding of cause and effect, such as covariance analysis, can exacerbate bias [@westreich2013; @robins1986; @hernan2023]. Thus, on one side, social scientists assess correlations and state that correlations are not causation. This is unsatisfactory because we typically want to understand causation. On the other hand, we continue to draw hesitant causal conclusions from observed correlations using hedging language [@bulbulia2022]. However, lacking appropriate methods, we typically lack any entitlement to such hedging language. Indeed widespread practices that attempt to "control" point in the opposite direction of truth. In short, the social sciences that have yet to incorporate advances in causal inference are arguably perpetuating what might be called a "causality crisis" [@bulbulia2023a].

The advances in causal inference during the past two decades, which emerged in the disciplines of biostatistics and computer science, offer hope for progress in across all the social sciences including psychology [@vanderweele2015]. Incorporating methods for causal inference into environmental psychology research offers particularly strong potential for scientific progress because many of the questions that environmental psychologists hope to answer cannot be approached through randomised controlled experiments. This chapter aims to lay a foundation of understanding that will inspire environmental psychologists to develop the knowledge and skills to help us obtain the more reliable and robust causal understanding that nearly all of us seek.

## Overview

**Part 1** introduces the potential outcomes framework for causal inference [@hernan2023]. This section describes the three core assumptions underpinning causal inference, grounding these concepts within the familiar context of randomised experiments. Our experience is that building intuitions about causal inference from an understanding of experimental designs helps to demystify the assumptions central to causal analysis and provides clear guidelines for data collection in observational studies. It also reveals that experiments obtain causal inferences by assumptions. Understanding how these assumptions work is not only useful for improving standards of experimental design and inference [@westreich2012berkson; @hernan2017per; @westreich2015; @robins2008estimation], but also for improving design and analysis standards for observational studies.

**Part 2** introduces Directed Acyclic Graphs (DAGs) – or causal diagrams – as powerful tools for examining causal assumptions [@pearl2009a]. Although an exhaustive review of causal diagrams' capabilities is beyond our overview, we present fundamental strategies for constructing and interpreting these diagrams, which we hope will prove valuable for addressing many questions within environmental psychology. We shall see that the elementary building blocks of causal diagrams can be derived from four basic graphical structures.

**Part 3** uses causal diagrams to investigate strategies for causal identification in seven scenarios, revealing the power of causal diagrams to clarify strategies for data collection and modelling. Some of the strategies that causal diagrams suggest accord with intuition. However, some do not.  By drawing our assumptions and evaluating the implications of these structures according to four basic rules, we discover that new prospects for observational data collection and analysis may be disclosed. Causal diagrams take us to places that intuition does not. Although they are grounded in a rigorous system of mathematical proofs, employing them requires no mathematical calculations. This is a great advantage because it makes causal inference widely accessible to a broad bandwidth of scientists and practitioners. 

**Part 4** provides a 'how to' guide for employing Directed Acyclic Graphs in two distinct use cases: (1) cross-sectional designs; (2) repeated-measures longitudinal designs.  

## Part 1: An Overview of the Potential Outcomes Framework for Causal Inference

The potential outcomes framework for causal inference originated in the work of Jerzy Neyman for the purpose of evaluating the effectiveness of agricultural experiments [@neyman1923]. It was later extended by Harvard statistician Donald Rubin, who demonstrated the framework may also facilitate causal inferences in non-experimental settings [@rubin1976]. Jamie Robins further generalised this framework to assess confounding in complex scenarios involving multiple treatments and time-varying factors [@robins1986]. A core concept within this framework is that of a "counterfactual contrast" or "estimand." To quantitatively assess the magnitude of causality requires contrasting how the world would have turned under two or more states, corresponding to different levels of intervention or treatment or sequence of treatments whose effects researchers hope to measure.  Notably, before any intervention, these states remain counterfactual. This is perhaps obvious. We have yet to intervene. However, after any intervention, for every instance of a treatment that is applied, at most only one of the two states of the world to be contrasted is realised. The other state remains counterfactual. Philosophers who have puzzled over the nature of causation have long realised that causality is never directly observed [@hume1902]. In a sense, we may think of causal inference as a form of counterfactual data science [@edwards2015; @bulbulia2023a]

To understand the significance of counterfactual contrasts in causal inference, consider a situation where you are at a crossroads in life, facing a significant decision. You are soon graduating from university and have been accepted into your ideal graduate programme in Environmental Psychology at the University of Canterbury; you are making preparations for a relocation to Christchurch, New Zealand. Concurrently, you receive a compelling job offer from Acme Nuclear Fuels, a pioneer in renewable energy solutions. Each option would lead you on a distinct path, affecting your daily life, income, social circles, romantic relationships, and perhaps ultimately your sense of life's purpose. Which decision will lead to a better overall life for you?

In causal inference, it can be useful to clarify conceptually challenging concepts using mathematical notation. As mentioned, we will see that the practical application of causal diagrams does not rely on mathematical notation or calculations. If you find the mathematics unappealing, you may proceed directly up the elevator of causal inference via the elevator of causal diagrams. However, for some, it can be helpful to express ideas symbolically using elementary mathematics, to climb a ladder slowly, to understand the basis of how causal inference works at each step, and perhaps only then to discard the ladder of mathematical concepts that built this understanding.

Formally, let $D$ denote the decision, where $D = 1$ denotes the decision to attend graduate school and $D=0$ denotes the decision to embark on a career in industry. We denote the two potential outcomes - the counterfactual states of the world under different levels of decision using the notation $Y_{\text{you}}(1)$ and $Y_{\text{you}}(0)$. Again, these outcomes describe the hypothetical scenarios resulting from each decision. We assume there is a fact-of-matter. Were you to choose, your life would turn out one way perhaps similarly, perhaps differently, to the other way. Conceptually, to quantitatively attach a *magnitude* to the causal effect of your choice, we must measure the difference $Y_{\text{you}}(1) - Y_{\text{you}}(0)$. However, this difference is fundamentally unobservable. Once you make a decision, the alternative scenario remains unknowable. We can illustrate this as follows:

$$
(Y_{\text{you}}|D_{\text{you}} = 1) = Y_{\text{you}}(1) \quad \text{implies} \quad Y_{\text{you}}(0)|D_{\text{you}} = 1~ \text{is counterfactual}.
$$

Similarly, the equation applies in reverse when the choice is $A = 0$:

$$
(Y_{\text{you}}|D_{\text{you}} = 0) = Y_{\text{you}}(0) \quad \text{implies} \quad Y_{\text{you}}(1)|D_{\text{you}} = 0~ \text{is counterfactual}.
$$

Of course, you regularly make principled decisions about your life based on past experiences, instincts, and knowledge. Nevertheless, the *data* that you require to quantitatively compare life outcomes under one decision as opposed to the other is not available. Life, as it would have unfolded under the option you do not select, remains counterfactual -- it cannot be directly measured. A quantitative causal contrast here is not a matter of factual data science. This example, although contrived, perhaps resonates with similar crossroads you have encountered in your life. The dilemmas that you faced at these crossroads underscore what is known as "The fundamental problem of causal inference" [@rubin2005; @holland1986]: for any individual case, we cannot observe the potential outcomes that we require to quantify the magnitude of an individual causal contrast.

The fundamental problem of causal inference never goes away. However, by collecting, organising, and aggregating data under certain assumptions, we can obtain valid quantitative causal contrasts from data to estimate *average treatment effects*. To clarify these assumptions, we next consider how experiments attach magnitudes to missing counterfactual outcomes to obtain average treatment effects.

### Understanding Relationships of Cause and Effect Through Intervention Outcomes

Let us transition to an example of relevance to environmental psychology, estimating the average causal effect of easy access to urban green spaces on psychological well-being, with a focus on subjective happiness, hereafter referred to as "happiness." We assume this outcome is measurable and represent it with the letter $Y$. 

For simplicity, we classify the intervention "ample access to green space" as a binary variable. Define $A = 1$ as "having ample access to green space" and $A = 0$ as "lacking ample access to green space." We assume these conditions are mutually exclusive. This simplification does not limit the generality of our conclusions; the points we make about experiments apply to continuous treatments as well. It is crucial in causal inference to specify the population for whom we seek to evaluate causal effects, or the "target population." In this case, our target population is residents of New Zealand in the 2020s.

A preliminary causal question -- defined as a causal contrast or “estimand” might therefore be:

"In New Zealand, does proximity to abundant green spaces increase self-perceived happiness compared to environments lacking such spaces?"

For the sake of argument, suppose that it would be unethical to experimentally randomise individuals into different green-space access conditions, but we choose to overlook this ethical consideration. Assume we could assign people randomly to high and low green space access without objection or harm.

The first point to note in the context of causal inference, as alluded to earlier, is that even well-designed experiments confront the challenge of missing values in the potential outcomes. Once an individual is assigned to one treatment condition, we cannot observe that individual's outcome for the condition not assigned. The fundamental problem of causal inference remains constant: for each individual, we can only observe one of the potential outcomes at any given time. Breaking down the Average Treatment Effect (ATE) into observed and unobserved outcomes yields the following equation:

$$
\text{Average Treatment Effect} = \left(\underbrace{\underbrace{\mathbb{E}[Y(1)|A = 1]}_{\text{observed}} + \underbrace{\mathbb{E}[Y(1)|A = 0]}_{\text{unobserved}}}_{\text{treated}}\right) - \left(\underbrace{\underbrace{\mathbb{E}[Y(0)|A = 0]}_{\text{observed}} + \underbrace{\mathbb{E}[Y(0)|A = 1]}_{\text{unobserved}}}_{\text{untreated}}\right).
$$

In this expression, $\mathbb{E}[Y(1)|A = 1]$ represents the average outcome when the treatment is given, which is observable. However, $\mathbb{E}[Y(1)|A = 0]$ represents the average outcome if the treatment had been given to those who were actually untreated, which remains unobservable. Similarly, the quantity $\mathbb{E}[Y(0)|A = 1]$ also remains unobservable.

It is hopefully evident from this brief application of the potential outcomes framework to experiments that the fundamental problem of causal inference is an ever-present concern even in experiments. For each participant, it is impossible to determine the outcome they would have experienced under an alternative treatment condition, just as you cannot quantitatively describe the life you would have led had you chosen the job at Acme Nuclear Fuels instead of attending the University of Canterbury.

### Causal Inference in Experiments

How do experiments manage to estimate average treatment effects despite the inherent challenges? The solution involves addressing the concept of "confounding." Consider the concept of "confounding by common cause." This occurs when one or more variables causally affect both the intervention under study (the "treatment" or "exposure") and the outcome of interest, leading to a non-causal association between the treatment and outcome. By "non-causal," we mean that were we to intervene in the treatment but not the confounder, the outcome would not change. The common cause creates a misleading or exaggerated relationship that may be mistakenly interpreted as causal. For instance, when assessing the impact of access to green space on happiness, it is possible that the association could be entirely explained by income. If so, then an observed association between access to green space and happiness would be entirely misleading. Were we to relocate low-income individuals to high-access green areas (hopefully not against their will), we might not affect subjective happiness at all. Thus, accurately identifying and adjusting for confounding by common cause is crucial for determining the true causal relationship between two variables, ensuring that the observed association is not merely a result of extraneous influences.

#### Balance of Confounders Removes Confounding

In experimental designs, random assignment of treatment effectively eliminates any systematic relationship between treatment conditions and the distribution of confounders that can affect both the treatment and the outcome. Simply put, randomisation creates a balance in confounders across treatment groups. While factors that might influence the outcome do not vanish, the balance ensured by successful randomisation allows us to disregard the phenomenon of *confounding*. This is because the distribution of confounders will be, on average, identical across the treatment groups, supporting the assumption that any systematic difference in average outcomes between these groups owes to the treatment itself.

We denote the set of all possible confounders by the letter $L$. In causal inference, the absence of confounding can be articulated in one of two equivalent ways. Where the symbol $\coprod$ denotes independence:

1. The potential outcomes, given the treatment and confounders, are conditionally independent: $Y(a) \coprod A \mid L$.
2. The treatment assignment, given the potential outcomes and confounders, is conditionally independent: $A \coprod Y(a) \mid L$.

To some, the mathematical formalism, which involves the symbol $\coprod$ to denote the statistical independence of the term to either side of this symbol, might seem complex. However, this compact notion for describing statistical independence will be helpful later when we explore *how* causal diagrams enable us to confidently develop causal inference strategies for observational data that aim to emulate experimental randomisation. If our treatment $A$ is not independent of the potential outcome $Y(a)$ after controlling for confounders $L$, that is, if $A\cancel\coprod Y(a)|L$, we cannot obtain an unbiased estimate for the causal effect of treatment on outcome.  Moreover if $A\coprod Y(a)$ yet $A\cancel \coprod Y(a)|L$, then we can only obtain consistent causal estimation by *avoiding* controlling for $L$.  The compact mathematical notation allows us to appreciate why causal diagrams work -- how they specifically rely on the assumptions about the relationships of measured and unmeasured variables to the treatment and outcome. Again, the application of causal diagrams to scientific problems does not rely on mastery of probability theory. If you find the notation unfamiliar and confusing you may safely ignore it because your causal diagrams encode these concepts in a purely spatial representation.

However, it is essential to understand that the heart of every strategy for causal inference -- whether experimental or non-experimental -- is the attempt to achieve a balance across treatment conditions in the variables that might influence the outcome. “Under the hood” such balance implies statistical independence of the treatment and the potential outcomes, conditional on covariates $L$. Randomisation ensures $L$ is balanced on the treatments, allowing us to infer that successful randomisation ensures $A \coprod Y(a)$.

Thus, the first key principle for causal inference embodied in randomised experiments may be summarised as follows:

1. **Conditional Exchangeability**: There is statistical independence between potential outcomes and treatment assignment, given all measured confounders, ensuring observed group differences in the effects of the treatment are attributable to the treatment itself. Note that in a randomised controled experiment, we have *unconditional exchangability*. Randomisation ensures balance.

#### Control Ensures Consistent Treatments are Administered Across All Confounders

Two further principles allow researchers to infer average treatment effects without observing individual treatment effects. These principles arise from *control*, not randomisation:

2. **Causal Consistency**: This principle allows us to assume that the observed outcomes in experimental conditions correspond to the potential outcomes under the received treatment. To develop intuition for this, we turn to simple mathematical notation:

For any individual $i$, the observed outcome $Y_i$ given their treatment status $A_i = 1$ is equal to their potential outcome under treatment, denoted as $Y_i(1)$. Similarly, when the treatment status for the same individual $i$ is $A_i = 0$, the observed outcome $Y_i$ is equal to the potential outcome under no treatment, denoted as $Y_i(0)$. More compactly:

$$
\begin{aligned}
Y_{i}(1) &= (Y_{i}|A_{i} = 1) \quad \text{(Potential outcome observed if treated)} \\
Y_{i}(0) &= (Y_{i}|A_{i} = 0) \quad \text{(Potential outcome observed if untreated)}
\end{aligned}
$$

When the exchangeability assumption is satisfied, we can extend our analysis from comparisons of treatment effects observed in the data to comparisons of the average treatment effects as if the entire population had been treated or had been untreated:

$$
\begin{aligned}
\text{Average Treatment Effect (ATE)} &= \mathbb{E}[Y(1)] - \mathbb{E}[Y(0)] \\
&= \mathbb{E}(Y|A=1) - \mathbb{E}(Y|A=0)
\end{aligned}
$$

Here, $\mathbb{E}[Y(1)]$ and $\mathbb{E}[Y(0)]$ denote the expected outcomes if the entire population were treated or untreated, respectively.

Thus, by the assumptions of causal consistency and conditional exchangeability, randomised controlled experiments recover average treatment effects as contrasts in the average of the outcomes in treatment groups (or equivalently as the average of the differences of the treatment outcome averages -- the two quantities are mathematically equivalent.)

Note that the causal consistency assumption is generally satisfied by the *control* that experimentalists exert over randomised controlled experiments. In an experiment, treatment groups receive equivalent treatments. This is almost self-evident. However, as we will see, in real-world data, the assumption of consistent treatments is much harder to satisfy. As it turns out, however, one may consistently estimate causal effects if the many versions of the treatment are conditionally independent of the potential outcomes under these treatments. 

3. **Positivity**: There is one more assumption required to obtain valid causal contrasts from data. We must assume that there is a non-zero probability of receiving each treatment level within covariate-defined subgroups.

$$
P(A = a | L= l) > 0
$$

This assumption is also met by the *control* that experimentalists exert over randomised controlled experiments and is rarely stated explicitly. However, in observational settings, this condition must be verified to avoid extrapolating results beyond observed data.  

### Why Satisfying the Fundamental Assumptions of Causal Inference in Observational Settings is Difficult

In observational research, researchers do not control the treatment allocation. The goal is to replicate a controlled experimental environment as closely as possible. However, obtaining balance in confounders across the treatments to be compared introduces challenges.

##### The Conditional Exchangeability Assumption in Observational Settings

Achieving conditional exchangeability is challenging in observational studies. This condition requires the groups being compared to be similar in every aspect except for the treatment. Consider the example of the effect of living near green spaces on subjective happiness. In real-world data, individuals with access to green spaces may differ from those without access in several ways:

**Socioeconomic status**: there might be a link between an individual's economic background and their proximity to green spaces. More affluent individuals could afford housing in areas with better access to high-quality green spaces.

**Age demographics**: different age groups may have different preferences regarding green spaces. Younger individuals or families with children might prioritise access to parks, unlike other demographics.

**Mental health**: individuals with existing mental health issues might seek out green spaces for therapeutic benefits or avoid them because of social anxiety or other factors.

**Lifestyle choices**: individuals who prefer outdoor activities might choose to live near green spaces. It is difficult to ascertain whether proximity to green spaces directly causes improved well-being or if it is merely a characteristic of individuals who already lead healthier lifestyles.

**Personal values and social connections**: the decision to live near green spaces might also be influenced by personal values, such as environmentalism, or the desire to be part of a community that values these spaces. Such values and connections can affect how individuals interact with and benefit from living in proximity to green spaces.

These and other unmeasured factors can introduce biases, complicating the interpretation of causal relationships in observational studies.

##### The Causal Consistency Assumption and Treatment Heterogeneity

Again we consider our interest in quantifying the causal effect of living near green spaces. The definition of "proximity to green spaces" itself varies significantly, leading to a diverse range of experiences classified under the same "treatment." Again, we set aside the issue that proximity is a continuous variable and that the distance to the nearest green space influences how often and easily individuals can access these areas, affecting the impact of this "treatment." However, we should note that causal inference always requires contrasting conditions. When assessing causal contrasts we must specify the points to be compared on a continuous scale. Arguably, such comparisons will require artificiality and extrapolation.  Focussing on the variability of the green spaces themselves this includes:

**Diversity of green spaces**: the biodiversity and aesthetic value of these spaces can vary widely. Some might have access to well-maintained parks, while others to basic recreational areas with limited natural appeal.

**Availability of amenities**: amenities such as walking paths, benches, and recreational facilities can enhance the experience of green spaces, encouraging more frequent and prolonged visits.

**Size and type of green space**: the type (e.g., urban park, community garden) and size of the green space might affect the psychological and physical benefits it offers.  Thus, as we convert observed outcomes under these heterogenous conditions, it might be unclear which interventions it is that we are comparing.  

##### The Positivity Assumption in Observational Settings 

Positivity, the requirement that every individual has a chance of receiving each level of treatment being compared, can be challenging to ensure in observational studies. In some urban areas, it might be practically impossible for certain demographics to have access to green spaces owing to factors like housing prices or availability, limiting treatment exposure variability within certain strata and complicating valid causal inference.

In observational studies, understanding the context and carefully using statistical methods helps us get close to the conditions of a randomised experiment. The closer we get, the more we can trust our causal conclusions. Yet, it is important to admit that sometimes our data will not give us much confidence. By comparing our work to the ideal of a randomised experiment, environmental psychologists can more clearly see and explain what observational data can and cannot tell us about cause and effect, especially for questions that practical or ethical considerations are not amenable to experimental investigation.

We will set the question of positivity to the side. However, interested readers may consult [@westreich2010; @hernan2023]. The important topic of causal consistency is briefly considered in Appendix B, see also: [@vanderweele2009; @vanderweele2013; @hernan2023].

The primary application of causal diagrams is to evaluate the conditional exchangeability assumption of “no unmeasured confounding.” The next section considers how causal diagrams powerfully assist with evaluating the demands of this assumption, guide data collection, and allow researchers to develop statistical modelling strategies that consistently estimate average magnitudes of causal effects within levels of the interventions they seek to contrast.

## Part 2. Causal Diagrams

This section introduces causal diagrams, starting with essential terminology. Understanding this terminology is not straightforward for newcomers, but it is necessary for applying causal diagrams effectively. After explaining the terms, we will examine practical examples. We will see that various forms of confounding arise from four key causal structures. Recognising these structures is crucial for applying them in practical scenarios. **Appendix A** provides a comprehensive glossary.

Causal diagrams use specific symbols to represent elements important in causal inference, as established in seminal works [@pearl1995; @pearl2009; @greenland1999]. The symbols and their meanings are listed in Table @tbl-01:

- **$A$** is the treatment or exposure variable, the intervention or condition whose effect on an outcome is under investigation. **This symbol represents the cause**.
- **$Y$** is the outcome variable, the effect or result that is being studied. **This symbol represents the effect**.
- **$L$** includes all measured confounders, variables that may affect both the treatment and the outcome.
- **$U$** includes unmeasured confounders, variables not included in the analysis that could influence both the treatment and the outcome, potentially leading to biased conclusions.
- **$M$** is a mediator variable, a factor through which the treatment affects the outcome. The focus here is on identifying the total effect of treatment $A$ on an outcome $Y$, but it is also important to understand how controlling for mediators can affect estimates of this total effect.


::: {#tbl-01}

```{=latex}
\terminologylocalconventionssimple
```
Terminology that is used in this article for causal diagrams. (This table is adapted from [@bulbulia2023])
:::


::: {#tbl-02}

```{=latex}
\terminologygeneralbasic
```
Basic conventions for causal diagrams. (This table is adapted from [@bulbulia2023])
:::


### Elements of Causal Diagrams


Having established the meanings of our symbols, we now turn to the fundamental components of causal diagrams themselves. @tbl-02 describes the basic elements of causal diagrams themselves. Key features are:

1. **Nodes**: representations of variables or events within a causal system. Each node stands for a distinct element that can influence or be influenced within the system.

2. **Edges**: signify the relationships between the variables represented by nodes. In causal diagrams, edges are directed that define pathways of causal influence. Importantly, causal diagrams are non-parametric; thus, the representation of a relationship does not change with its nature—be it linear or non-linear, an arrow is used in both cases.

3. **Parent and child relationships**: a variable is termed a "child" if it receives an arrow from another variable, which is then referred to as its "parent." This terminology helps to describe the direction of causal influence within the diagram.

4. **Acyclic**: causal diagrams must not contain cycles; that is, they cannot have feedback loops where a variable can be both a cause and effect of itself, directly or indirectly. This requirement ensures clarity in the direction of causal influence. In scenarios involving repeated measurements, nodes should be indexed by time to maintain acyclicity.

5. **Conditioning**: a central aspect of causal analysis is determining how controlling for certain variables affects the unbiased estimation of the relationship between the treatment and the outcome. This process of "conditioning" or "adjustment" is visually represented by enclosing the variable in a box within the diagram.


#### The Rules of D-separation 

Pearl proved how rules of d-separation may allow us to evaluate relationships between nodes in a causal diagram [@pearl1995].

When we write $X_1 \cancel\coprod X_2$, it means the probability distributions of  $X_1$ and $X_2$ are intertwined. In simpler terms, what happens with $X_1$ gives us clues about what is happening with $X_2$, indicating a connection or dependence between them.

Conversely, $X_1 \coprod X_2$ means $X_1$ and $X_1$ means the probability distributions of $X_1$ and $X_2$ are independent. Their probability distributions do not mix; knowing about $X_1$ does not help guess anything about $X_2$. 

We call a path "blocked" or "d-separated" if a node along it prevents the transmission of influence.  Two variables are deemed d-separated, denoted as $X_1 \coprod X_2$, if every pathway connecting them is obstructed, indicating no influence or statistical association passes through. Conversely, if at least one path remains unblocked, allowing for the transmission of influence, the variables are considered d-connected, denoted as $X_1 \cancel\coprod X_2$ [@pearl1995].  

Next, we define three basic rules that link causal relationships to statistical associations. All confounding results from these rules.[^notes]

[^notes]: Recall that to ensure balance in confounders across treatments, we must ensure that $A\coprod Y(a)|L$ -- that the potential outcomes are independent within levels of the treatments to be compared, conditional on measured covariates $L$.  We do not need to ensure that the treatment has no effect such that $A\coprod Y |L$. 

##### Two variables with no arrows: absence of causatoin

$$\xorxA$$
If $X_0$ and $X_1$ have no arrows between them, we assert there is no causal effect. Without a causal link, they are independent, denoted mathematically as $X_0 \coprod X_1$. This means knowing one variable tells you nothing about the other.

#### Fundamental causal association: two variables with a causal arrow

$$\xtoxA$$

Adding a causal arrow from $X_0 \to X_1$ indicates $X_0$ causes a change in $X_1$, or that $X_1$ "listens to" $X_0$. Causality creates a *statistical* dependency, denoted mathematically as $X_0 \cancel\coprod X_1$. This means knowing $X_0$ gives information about $X_1$

#### Three Variables: Fork, Chain, and Collider Structures

There are only three fundamental ways in which we may add another causal relationship to the basic two-node structure: the fork structure, the chain structure, and the collider structure. Each structure has a corresponding rule for how the probability distributions of the three nodes become related or unrelated when we condition on them.


##### The Fork Structure

$$\fork$$

The fork structure ($X_0 \rightarrow X_1$, $X_0 \rightarrow X_2$) shows $X_0$ causes both $X_1$ and $X_2$. Given $X_0$, $X_1$ and $X_2$ are independent, written as $X_1 \coprod X_2 | X_0$. This means knowing $X_0$ removes any link between $X_1$ and $X_2$. Thus, if $X_1$ and $X_2$ are statistically related after conditioning on their common cause this statistical association reflects causation. Here, association is causation.

**Fork Rule**:  *if interested in the causal effect of $X_1 \to X_2$, condition on $\boxed{X_1}$**.


##### The Chain Structure

$$\chain$$


The chain structure ($X_0 \rightarrow X_1 \rightarrow X_2$) describes a setting in which $X_0$ affects $X_1$ and $X_1$ affects $X_2$.
In this structure, $X_0$ and $X_2$ are conditionally independent given $X_1$. That is, if $\boxed{X_1}$, then $X_0 \coprod X_2 | X_1$). Hence, conditioning on $X_1$ *blocks* the association of $X_0$ and $X_2$. This implies that if there is a causal association between $X_0$ and $X_2$ we might not detect it after conditioning on $\boxed{X_1}$.

Chain Rule: **if interested in the (total) causal effect of $X_0 \to X_2$, do not condition on $X_1$**.


##### The Collider Structure

$$\immorality$$

The collider structure describes a setting in which both $X_0\to X_2$ and $X_1 \to X_2$.  In a collider structure, $X_0$ and $X_1$ are independent. However, conditioning on $X_2$ -- the common descendant of $X_2$, $X_3$ -- introduces a stastisical association between $X_0$ and $X_1$, such that $X_0 \cancel\coprod X_1 | X_2$. That is conditioning on the common effect (or its descendant) creates a pathway through which $X_0$ and $X_1$ share information about each other. Again, this statistical association between $X_0$ and $X_1$ can arise in the absence of any causal association.

Collider Rule: **if interested in the causal effect of $X_0 \to X_1$, do not condition on $X_2$**.


##### Extensions: all confounding bias results from combinations of basic causal associations and the three elementary structures (forks, chains, colliders)


For example, consider the implications of conditioning on the descendant of a collider:

$$\immoralityChild$$


Because every descendant is statistically associated with its parent (basic causality), a descendant acts as a proxy for its parent. In this scenario, conditioning on $X_3$ (the descendant of $X_2$, which is a collider) inadvertently opens a path between $X_0$ and $X_1$ that would otherwise be blocked by the unconditioned collider $X_2$. This occurs because $X_3$ inherits all statistical associations of $X_2$, here, its statistical associations with $X_0$ and $X_1$. Consequently, when we condition on $X_3$, we indirectly condition on $X_2$, thereby introducing a statistical association between $X_0$ and $X_1$ through $X_3$. 

This principle underscores the nuanced nature of causal inference, demonstrating how conditioning on certain variables, here a descendant of colliders, can inadvertently create statistical dependencies such that association is *not* causation. The implication: do not condition on a descendent of a collider. 

For those new to the causal diagrams, the fundamental structures of causation presented in Pearl's rules of d-separation might seem abstract. Similarly, individuals not versed in probability theory might find the notation of conditional dependencies and independencies challenging. Yet, with the groundwork of rules and conventions laid, applying causal diagrams to real-world problems is need not be intimidating. In fact, based on our experience, many find constructing causal diagrams to be enjoyable conversation starters. Once we know how conditioning affects statistical associations within causal structures, we can work backwards through statistics to make causal inferences.  Put another way, causal diagrams reveal how and when association is causation. This is good news for psychological questions that are inaccessible to experiments. However, we must temper our enthusiasm. We must remember that **all structure relationships in a graph except the focal treatment/outcome relationship must be assumed.** This because most observational datasets are typically compatible with many causal diagrams. Statistically "significant" associations do not in themselves reveal anything about causation. Causal diagrams are powerful aids because they help investigators to understand what follows from their assumptions the data. However, assumptions about the structural relationships in the data cannot be avoided. Because every path except the $A\to Y$ path is assumed, causal diagrams should be created in collaboration with area experts. Where there is expert disagreement, multiple causal diagrams should be proposed to reflect the implications of disagreements for causal inference. 

### How to Create Causal Diagrams 

#### Causal diagrams must address a clearly stated identification problem

The *identification problem* is the task of determining if the effect of a treatment ($A$) on an outcome ($Y$) can be accurately inferred from statistical accociations in the data. Causal diagrams, which incorporate *structural assumptions*, serve as tools for assessing the potential to identify causal effects from the patterns observed in data [@hernán2004a]. These assumptions, fundamental to causal inference, are generally not empirically testable through data alone. The identification challenge encompasses two principal considerations:

1. **In the absence of a causality**, when all known confounders are controlled for, is there statistical independence between the treatment and outcome? This involves identifying all potential confounders, implementing strategies to adjust for these confounders, and then checking if there are any indirect connections, known as *backdoor paths*, between $A$ and $Y$. A *backdoor path* refers to a sequence of links in the causal diagram that could introduce spurious associations between $A$ and $Y$ if not properly adjusted for. (Below we work through seven examples). The goal here is to evaluate whether $A$ and $Y$ are d-separated, indicating that any observed association between $A$ and $Y$ does not stem from these indirect paths.

2. **With an existing causal link**, once all known confounders have been adjusted for, does a direct and unbiased relationship between the treatment and outcome remain observable? This verifies whether the statistical relationship between $A$ and $Y$, as analysed under the specified model, can be interpreted as reflecting a true causal effect, mindful of potential *over-conditioning biases*. *Over-conditioning bias* occurs when the adjustment process inadvertently introduces or magnifies associations that misrepresent the actual causal relationship.

The process breaks down into two tasks:

- **First task**: verify that $A$ and $Y$ are not linked after adjusting for known common causes. This step involves ensuring that no *backdoor paths* -- indirect paths that could falsely suggest a relationship between $A$ and $Y$ -- remain open after an adjustment strategy is applied.
  
- **Second task**: confirm that an assocation between $A$ and $Y$ remains unbiased after the conditioning strategy from the first task. This step is crucial for validating that the observed statistical association accurately reflects the causal relationship, free from distortions caused by the adjustment strategy.


#### Steps in creating causal diagrams that aaddress identification problems

1. **Include all common causes of the exposure and outcome**

There are two main types of common causes: measured and unmeasured. Functionally similar common causes should be grouped under a single variable, where possible. For instance, instead of listing every demographic variable, consider using a single label, such as $L_0$.

2. **Include all ancestors of measured confounders linked with the treatment, the outcome, or both**

Include in your causal diagram any measured variables that are ancestors of unmeasured confounders if this variable is associated with the exposure or outcome or both. This inclusion facilitates identifying strategies where conditioning on a proxy of an unmeasured cause reduces bias from the unmeasured common cause and helps prevent M-bias, a form of over-conditioning bias described below. Again, group functionally similar variables under a common label to simplify the diagram and highlight critical variables affecting the relationship between $A_1$ and $Y_0$.

3. **Explicitly state assumptions about the relative timing of events**

Denote the assumed timing of events with time subscripts (e.g., $L_0$, $A_1$, $Y_2$), where subscripts indicate their relative order in time. This clarifies acyclicity and highlights the temporal nature of causal relationships we assume to hold in our data.

4. **Arrange the temporal order of causality visually**

Organising the assumed temporal order of causality from left to right or top to bottom aids in understanding the causal assertions within the diagram [@bulbulia2023]. As we shall see in Part 3, ensuring appropriate temporal measurement of variables can effectively address common identification problems.

5. **Box variables that are conditioned on**

Generally, the exposure and outcome are not boxed unless measured with error, as they do not act as confounders.

6. **Use conventions to clarify sources of bias and clearly state these conventions**

The creation of a causal diagram lacks a universal approach; therefore we must be clear about our conventions. For instance, drawing open back door paths in red, as we do here, may help highlight and identify bias sources. However, not everyone has colour vision, and indeed not everyone has vision. Clarity demands verbally describing the contents of our causal diagrams.

7. **Represent paths structurally, not parametrically**

Avoid any attempt to graphically describe non-linear relationships between nodes. The purpose of a causal diagram is to evaluate confounding, regardless of the linearity of the paths. Attempting to denote non-linearities can clutter the diagram with assumptions that are not relevant to the task at hand, evaluating sources of bias.

8. **Minimise paths to those necessary for the identification problem**

Avoid unnecessary clutter in causal diagrams. Include only paths essential for addressing a stated identification problem, such as evaluating open back door paths or mediator bias.

9. **Clarify the research question evaluated by the causal diagram**

State the specific question your causal diagram addresses. For example, in causal mediation analysis, distinguishing between the indirect and direct effects of $A$ on $Y$ requires specific paths to be drawn and conditioned on. A strategy that would induce confounding for one question might not be relevant to for a different question.

Next, we shall consider how applying the the rules of forks, chains, and colliders, causal diagrams offer a powerful tool for researchers to obtain consistent causal estimates tailored to their specific questions.

## Part 3. Applying causal diagrams to causal inference: worked examples


::: {#tbl-04}
```{=latex}
\terminologyelconfoundersLONG
```
Worked examples: This table is adapted from [@bulbulia2023].

:::



### 1. The problem of confounding by a common cause

@tbl-04 1 describes the problem of confounding by common cause and its solution. We encountered this problem in Part 1. Such confounding arises when there is a variable or set of variables, denoted by $L$, that influence both the exposure, denoted by $A$, and the outcome, denoted by $Y.$ Because $L$ is a common cause of both $A$ and $Y$, $L$ may create a statistical association between $A$ and $Y$ that does not reflect a causal association.

For instance, in the context of green spaces, consider that people who choose to live closer to green spaces (exposure $A$) and their experience of improved happiness (outcome $Y$). A common cause might be socioeconomic status $L$. Individuals with higher socioeconomic status might have the financial capacity to afford housing near green spaces and simultaneously afford better healthcare and lifestyle choices, contributing to greater happiness. Thus, although the data may show a statistical association between living closer to green spaces $A$ and greater happiness $Y$, this association might not reflect a direct causal relationship owing to confounding by socioeconomic status $L$.

How might we obtain balance in this confounder for the treatments to be compared?  Addressing confounding by a common cause involves adjusting for the confounder in one's statistical model. This may be done through regression, or more complicated methods, such as inverse probability of treatment weighting, marginal structural models, and others see @hernán2023. Such adjustment effectively closes the backdoor path from the exposure to the outcome. Equivalently, conditioning on $L$ d-separates $A$ and $Y$.  

@tbl-04 Row 1, Column 3, emphasises that a confounder by common cause must precede both the exposure and the outcome. While it is often clear that a confounder precedes the exposure (e.g., a person's country of birth), in other cases, the timing might be uncertain. By positioning the confounder before the exposure in our causal diagrams, we assert its temporal precedence. However, when relying on cross-sectional data, such a timing assumption might be strong. In such cases, exploring causal scenarios where the confounder follows the treatment or outcome can be insightful. Causal diagrams are instrumental in examining possible timings and their implications for causal inference. 

Next, we examine the effects of conditioning on a variable that is an effect of the treatment.

### 2. Mediator Bias

Consider again the question of whether proximity to green spaces, $A$, affects happiness, $Y$. Suppose that physical activity is a mediator, $L$.

To fill out the example, imagine that living close to green spaces $A$ influences physical activity $L$, which subsequently affects happiness $Y$. Notice that if we were to condition on physical activity $L$, assuming it to be a confounder, we would then bias our estimates of the total effect of proximity to green spaces $A$ on happiness $Y$. Such a bias arises as a consequence of the chain rule. Conditioning on $L$ "d-separates" the total effect of $A$ on $Y$. This phenomenon is known as mediator bias. Notably, @montgomery2018 finds dozens of examples of mediator bias in *experiments* in which control is made for variables that occur after the treatment.  For example, the practice of obtaining demographic and other information from participants *after* a study is an invitation to mediator bias. If the treatment affects these variables, and the variables affect the outcome (as we assume by controlling for them), then researchers may induce mediator bias. 

To avoid mediator bias when estimating a total causal effect we should, of course, avoid conditioning on a mediator! The surest way to avoid this problem is to ensure that $L$ occurs before the treatment $A$ as well as before the outcome $Y$.  This solution is presented @tbl-04 Row 2 Col 3. 


### 3. Confounding by Collider Stratification (Conditioning on a Common Effect)

Conditioning on a common effect, also known as collider stratification, occurs when a variable, denoted by $L$, is influenced by both the exposure, denoted by $A$, and the outcome, denoted by $Y$.

Imagine, again the context of an access to green space question, that an individual's choice to live closer to green spaces (exposure $A$) and their happiness (outcome $Y$) both affect the individual's overall sense of physical health (common effect $L$). Initially, $A$ and $Y$ could be independent, that is $A \coprod Y(a)$, suggesting that the decision to live near green spaces is not directly a cause of happiness.

However, if we were to condition on physical health $L$ (the common effect of $A$ and $Y$), a backdoor path between $A$ and $Y$ would be opened. That is, by "controlling for" physical health we might *induce* a non-causal association between proximity to green spaces and happiness.

The reason that conditioning on physical health $L$ leads to confounding is that this variable provides information about both the proximity to green spaces $A$ and one's happiness $Y$. Once we learn something about one's physical health, for example, that it is poor, it becomes more probable that a person is happy if they have low access to green spaces (assuming the relationship between green space access and happiness is positive.) 

Causal diagrams point a way to respond to the problem of collider stratification bias: we should generally ensure that:

1.  All confounders $L$ that are common causes of the exposure $A$ and the outcome $Y$ are measured before $A$ has occurred, and

2.  $A$ is measured before $Y$ has occurred.

If such temporal order is preserved, $L$ cannot be an effect of $A$, and thus neither of $Y$.


### 4. Confounding by Conditioning on a Descendant of a Confounder  

The rules of d-separation also apply to conditioning on descendants of a confounder.  As shown in @tbl-04 Row 4, when conditioning on a measured descendant of an unmeasured collider we may unwittingly evoke confounding by proxy. For example, if doctor visits were encoded in our data and doctor visits were an effect of poor health, then conditioning on doctor visits would function in a similar way to conditioning on poor health, introducing collider confounding. 

There are only four elementary forms of confounding. Any confounding scenario we might imagine can be developed from these elementary forms. We next consider how we may combine these elementary causal relationships in causal diagrams to develop effective strategies for confounding control. 

### 5. M-bias: Conditioning on Pre-Exposure Collider

@tbl-04 Row 5 presents a form of pre-exposure over-conditioning confounding known as "M-bias".  This bias combines the collider structure and the fork structure revealing what might not otherwise be obvious: it is possible to induce confounding even if we ensure that all variables have been measured **before** the treatment. The collider structure is evident in the path $U_Y \to L_0$ and $U_A \to L_0$. We know from the collider rule that conditioning on $L_0$ opens a path between $U_Y$ and $U_A$. What is the result? We find that $U_Y$ is associated with the outcome $Y$ and $U_A$ is associated with treatment $A$. This is a fork (common cause) structure. The association between treatment and outcome that is opened by conditioning on $L$ arises from an open back-door path that occurs from the collider structure. We thus have confounding. How might such confounding play out in a real-world setting? 

In the context of green spaces, consider the scenario where an individual's level of physical activity $L$ is influenced by an unmeasured factor related to their propensity to live near green spaces $A$ -- say childhood upbringing. Suppose further that another unmeasured factor -- say a genetic factor -- increases both physical activity $L$ and happiness $Y$. Here, physical activity $L$ does not affect the decision to live near green spaces $A$ or happiness $Y$ but is a descendent of unmeasured variables that do. If we were to condition on physical activity $L$ in this scenario, we would create the bias just described --  "M-bias."  

How shall we respond to this problem? The solution is straightforward. If $L$ is neither a common cause of $A$ and $Y$ nor the effect of a shared common cause, then $L$ should not be included in a causal model. In terms of the conditional exchangeability principle, we find $A \coprod Y(a)$ yet $A \cancel{\coprod} Y(a)| L$. So we should not condition on $L$: do not control for exercise [@cole2010].[^3]

[^3]: Note that when we draw a chronologically ordered path from left to right the M shape for which "M-bias" takes its name changes to an E shape We shall avoid proliferating jargon and retain the term "M bias."

### 6. Conditioning on a Descendent May Sometimes Reduce Confounding

Consider how we may use the rules of d-separation to obtain unexpected strategies for confounding control. In @tbl-04 Row 6, we encounter a causal diagram in which an unmeasured confounder opens a back-door path that links the treatment and outcome. We have what appears to be intractable confounding.  Return to our green space example. Suppose an unmeasured genetic factor $U$ affects one's desire to seek out isolation in green spaces $A$ and also independently affects one's happiness $Y$.  Were such an unmeasured confounder to exist, we could not obtain an unbiased estimate for the causal effect of green space access on happiness. However, imagine a variable $L^\prime$ that is a trait that is expressed later in life, which arises from this genetic factor. If such a trait could be measured, even though the trait $L'$ is expressed after the treatment and outcome have occurred, controlling for $L'$ would enable investigators to close the backdoor path between the treatment and the outcome. The reason this strategy works is that a measured effect is a *proxy* for its cause $U$, the unmeasured confounder.  By conditioning on the late-adulthood trait, $L'$, we partially condition on its cause, $U$, the confounder of $A \to Y$. Thus, not all effective confounding control strategies need to rely on measuring pre-exposure variables. 

### 7. Confounding Control with Three Waves of Data is Powerful and Reveals Possibilities for Estimating an "Incident Exposure" Effect

@tbl-04 row 7 presents another setting in which there is unmeasured confounding. In response to this problem, we use the rules of d-separation to develop a strategy for data collection and modelling that may greatly reduce the influence of unmeasured confounding.  @tbl-04 row 7 col 3, by collecting data for both the treatment and the outcome at baseline and controlling for baseline values of the treatment and outcome, any unmeasured association between the treatment $A_1$ and the outcome $Y_2$ would need to be *independent* of their baseline measurements. As such, including the baseline treatment and outcome, along with other measured covariates that might be measured descendants of unmeasured confounders, is a strategy that exerts considerable confounding control [@vanderweele2020]. 

Furthermore, the graph makes evident a second benefit of this strategy. Consider that an ordinary regression would estimate what is called a "prevalence exposure effect." The prevalence exposure effect evaluates the association between the
exposure or treatment status at time $t1$ and the outcome observed at a later time $t2$. It is defined by the pathway $A_{1} \to Y_{2}$. It is expressed as:

$$
\text{Prevalence exposure effect:} \quad A_{1} \to Y_{2}
$$

Note that a prevalence exposure effect estimate does not consider the initial status of the exposure. As such, a prevalence exposure effect estimate describes the effect of current or ongoing exposures on outcomes. This is often *not* the effect of theoretical interest.

For example, imagine that living far from green spaces makes people so depressed that they never respond to surveys. When we observe the association between green space and happiness in the data, we are left with only those people who are so incurably happy that even living far from green space cannot bring them down. As such , it might appear in our data that $A_{1} \to Y_{2}$ is helpful when,
in fact, the treatment is initially harmful; see: @hernán2016; @danaei2012; @vanderweele2020; @bulbulia2022.

By contrast, adjusting for the baseline exposure and outcome enables us to recover an incident exposure effect. The incident exposure effect evaluates the causal association between the exposure or treatment status at time $t1$ and the outcome observed at a later time $t2$ conditional on the baseline exposure: $A_{0} \to A_{1} \to Y_{2}$. 

By including the baseline exposure, then, we consider the *transition* in treatment or exposure status from $A_0$ to $A_1$. The initiation of a treatment provides a clearer intervention from which to estimate a causal effect at $Y_2$, and we more closely emulate an experiment. It is expressed:

$$
\text{Incident exposure effect:} \quad \boxed{A_{0}} \to A_{1} \to Y_{2}
$$

Returning to our example, a model that controls for baseline exposure would require that people initiate a change from the level of $A_0$ observed baseline. Put differently, by controlling for the baseline value of the treatment, we may learn about the causal effect of shifting one's access to green space status. The incident exposure effect better emulates a "target trial" or the
the organisation of observational data into a hypothetical experiment in which there is a "time-zero" initiation of treatment in the data; see @hernán2016; @danaei2012; @vanderweele2020; @bulbulia2022.  

Finally, we obtain still further control for unmeasured confounding by including, in addition to  the baseline exposure $A_0$, the baseline outcome, $Y_0$, such that:

$$
\boxed{
\begin{aligned}
L_{0} \\
A_{0} \\
Y_{0}
\end{aligned}
}
\to A_{1} \to Y_{2}
$$


In this example, we discover that causal diagrams may novel insights both in data collection and data modelling. To obtain the incident exposure effect, we generally require that events in the data can be accurately classified into at least three relative time intervals and we must model the treatments and outcomes as separate elements in our statistical model. 


## Part 4. Putting Causal Diagrams to Use in Cross-Sectional and Longitudinal Designs

### Cross-sectional designs

In environmental psychology, researchers often grapple with whether causal inferences can be drawn from cross-sectional data, especially when longitudinal data are not available. The challenge is not unique to cross-sectional designs; even longitudinal studies require careful assumption-management. We next discuss how causal diagrams can guide inference in both data types, with examples relevant to environmental psychologists.

1. **Assumption mmnagement**: the crux of causal inference, regardless of data type, is the management of assumptions. Although cross-sectional analyses typically demand stronger assumptions owing to the snapshot nature of data, these assumptions, when transparently articulated, do not always bar causal analysis. For instance, if investigating the effect of green spaces on mental health, longitudinal data might allow discovery of effects owing to changes in access to green spaces, however cross-sectional data might still offer insights if we control for known confounders such as age, socioeconomic status, and urban vs. rural living environments.

2. **Stable confounders**: in cross-sectional studies, some confounders are inherently stable over time, such as ethnicity, year and place of birth, and biological gender. For environmental psychologists examining the relationship between access to natural environments and psychological well-being, these stable confounders can be adjusted for without concern for introducing bias from mediators or colliders. For example, conditioning on year of birth can help to isolate the effect of recent urban development on mental health, independent of generational differences in attitudes toward green spaces.

3. **Invariable confounders**: other confounders, while not immutable, are less likely to be influenced by the treatment. Variables such as sexual orientation, educational attainment, and often income level fall into this category. For instance, the effect of exposure to polluted environments on cognitive outcomes can be analysed by conditioning on education level, assuming that recent exposure to pollution is unlikely to retroactively change someone's educational history.

4. **Timing and reverse causation**: the sequence of treatment and outcome is crucial. In some cases, the temporal order is clear, reducing concerns about reverse causation. Mortality is a definitive outcome where the timing issue is unambiguous. If researching the effects of air quality on mortality, the causal direction (poor air quality leading to higher mortality rates) is straightforward.

5. **Multiple causal diagrams**: given the complexity of environmental influences on psychological outcomes, it's prudent to construct multiple causal diagrams to cover various hypothetical scenarios. For example, when studying the effect of community green space on stress reduction, one diagram might assume direct benefits of green space on stress, while another might include potential mediators like physical activity. By analysing and reporting findings based on multiple diagrams, researchers can explore the robustness of their conclusions across different theoretical frameworks.



@tbl-cs describes ambigious confounding control setting arising from cross-sectional data. Suppose again we are interested in the causal effect of access to greenspace denoted by $A$ on 'happiness', denoted by $Y$.   We are uncertain whether excercise, denoted by $L$, is a common cause of $A$ and $Y$ and thus a confounder, or whether excercise is a mediator along the path from $A$ to $Y$. We may use causal diagrams to investigate the consequences of such ambiguity. 

**Assumption 1: Exercise is a common cause of $A$ and $Y$**, this scenario is presented in @tbl-cs row 1. Here, our strategy for confounding control is to estimate the effect of $A$ on $Y$ conditioning on $L$. 


**Assumption 2: Exercise is a mediator of $A$ and $Y$**, this scenario is presented in @tbl-cs row 2. Here, our strategy for confounding control is to simply estimate the effect of $A$ on $Y$ without including $L$ (assuming there are no other common causes of the treatment and outcome). 



::: {#tbl-cs}

```{=latex}
\examplecrosssection
```
This table is adapted from [@bulbulia2023]
:::



To clarify how answer may differ we can simulate data and run separate regressions, reflecting the different conditioning strategies embedded in the different assumptions. The following simulation generates data from a process in which exercise is a mediator (Scenario 2). (See Appendix C)



```{r}
#| label: simulation_cross_sectional
#| tbl-cap: "Code for a simulation of a data generating process in which the effect of excercise (L) fully mediates the effect of greenspace (A) on happiness (Y)."
#| out-width: 80%
#| echo: false


# load libraries
library(gtsummary) # gtsummary: nice tables
library(kableExtra) #  tables in latex/markdown
library(clarify) # simulate ATE

# simulation seed
set.seed(123) #  reproducibility

# define the parameters 
n = 1000 # Number of observations
p = 0.5  # Probability of A = 1 (access to greenspace)
alpha = 0 # Intercept for L (excercise)
beta = 2  # Effect of A on L 
gamma = 1 # Intercept for Y 
delta = 1.5 # Effect of L on Y
sigma_L = 1 # Standard deviation of L
sigma_Y = 1.5 # Standard deviation of Y

# simulate the data: fully mediated effect 
A = rbinom(n, 1, p) # binary exposure variable
L = alpha + beta*A + rnorm(n, 0, sigma_L) # continuous mediator
Y = gamma + delta*L + rnorm(n, 0, sigma_Y) # continuous outcome

# make the data frame
data = data.frame(A = A, L = L, Y = Y)

# fit regression in which L is assume to be a mediator
fit_1 <- lm( Y ~ A + L, data = data)

# fit regression in which L is assume to be a mediator
fit_2 <- lm( Y ~ A, data = data)

# create gtsummary tables for each regression model
table1 <- tbl_regression(fit_1)
table2 <- tbl_regression(fit_2)

# merge the tables for comparison
table_comparison <- tbl_merge(
  list(table1, table2),
  tab_spanner = c("Model: Exercise assumed confounder", 
                  "Model: Exercise assumed to be a mediator")
)
# make latex table
markdown_table_0 <- as_kable_extra(table_comparison, 
                                   format = "latex", 
                                   booktabs = TRUE)
markdown_table_0
```


This table presents us the conditional treatment effect. Where an outcome is continuous and there are no interactions, the coeficient for the treatment ($A_1$) reflect the average treatment effect. With covariates and interactions, to estimate an average treatment effects requires additional steps.  We present code for obtaining marginal treatment effects in Appendix C 

```{r}
#| label: ate_simulation_cross_sectional
#| fig-cap: ""
#| out-width: 100%
#| echo: false

# use `clarify` package to obtain ATE
library(clarify)
# simulate fit 1 ATE
set.seed(123)
sim_coefs_fit_1 <- sim(fit_1)
sim_coefs_fit_2 <- sim(fit_2)

# marginal risk difference ATE, simulation-based: model 1 (L is a confounder)
sim_est_fit_1 <-
  sim_ame(
    sim_coefs_fit_1,
    var = "A",
    subset = A == 1,
    contrast = "RD",
    verbose = FALSE
  )

# marginal risk difference ATE, simulation-based: model 2 (L is a mediator)
sim_est_fit_2 <-
  sim_ame(
    sim_coefs_fit_2,
    var = "A",
    subset = A == 1,
    contrast = "RD",
    verbose = FALSE
  )

# obtain summaries
summary_sim_est_fit_1 <- summary(sim_est_fit_1, null = c(`RD` = 0))
summary_sim_est_fit_2 <- summary(sim_est_fit_2, null = c(`RD` = 0))

# get coefficients for reporting
# ate for fit 1, with 95% CI
ATE_fit_1 <- glue::glue(
  "ATE =
                        {round(summary_sim_est_fit_1[3, 1], 2)},
                        CI = [{round(summary_sim_est_fit_1[3, 2], 2)},
                        {round(summary_sim_est_fit_1[3, 3], 2)}]"
)
# ate for fit 2, with 95% CI
ATE_fit_2 <-
  glue::glue(
    "ATE = {round(summary_sim_est_fit_2[3, 1], 2)},
                        CI = [{round(summary_sim_est_fit_2[3, 2], 2)},
                        {round(summary_sim_est_fit_2[3, 3], 2)}]"
  )
```


On the assumptions outlined in , in which we *assert* that exercise is a confounder, the average treatment effect of access to green space on happiness is `r ATE_fit_1`.

However, on the assumptions outline in DAG , in which we *assert* that exercise is a mediator, the average treatment effect of access to green space on happiness is `r ATE_fit_2`.

These findings illustrate the role that assumptions about the relative timing of exercise as a confounder or as a mediator plays. 

### Recommendations for conducting and reporting causal analyses with cross-sectional data.

We offer the following recommendations for analysing and reporting analyses with cross-sectional data

1. **Draw multiple causal diagrams**: as the previous example illustrates, we may wish to draw a number of causal diagrams that represent different theoretical assumptions about the both the relationships and timing in the occurrance of variables relevant to an identification problem. This practice allows for a comprehensive exploration of potential causal pathways and the roles variables may play —- be they confounders, mediators, or colliders. For instance, when examining the effect of urban green spaces on mental health, consider diagrams that account for direct effects, as well as those that include mediators like physical activity or social interaction.

2. **Perform and report analyses for each assumption**: conduct separate analyses based on the different scenarios outlined by your causal diagrams. This approach ensures that the analytical strategy aligns with the theoretical underpinnings of each model. Transparently reporting the results from each analysis, including the assumptions and statistical methods employed, enhances balance in a study. Editors and reviewers will not be accustomed to this practice of conditioning inferences on different assumptions. However, for meaningful progress, it essential to persevere. The problem inherent to cross-sectional analyses must be addressed by broadining the scope of our imaginations beyond testing one or another favoured hypothesis. Rather, we should graph the structural assumptions encoded by different theories, and condition on specific combinations in the occurance of events.

3. **Interpret findings with attention to ambiguities**: Ccrefully interpret the results, paying close attention to any ambiguities or inconsistencies that emerge across the different analyses. Describe how different assumptions about structural relationships and timing can lead to different conclusions. For example, if the effect of green space access on mental health appears positive when treating exercise as a mediator but negative when treated as a confounder -- as in the present simulation -- discuss the theoretical and practical implications of these findings.

4. **Recommend caution when findings diverge**: wherever findings lead to different practical conclusions, embrace caution about drawing firm conclusions. 

5. **Suggest specific avenues for future research**: identify gaps in the current understanding that arise from ambiguities in your findings. Recommend specific, targeted data collection that might clarify the nature of the relationships among variables. For instance, longitudinal studies or experiments.

6. **Supplement observational data with simulated data**: data simulation is a powerful tool for understanding the complexities of causal inference in environmental psychology. By simulating data based on various theoretical models, researchers can explore how different assumptions about confounders, mediators, and the structure of causal relationships might influence their findings. Simulation studies allow for the testing of analytical strategies under controlled conditions, providing insights into the robustness of methods against violations of assumptions or the presence of unobserved confounders. Appendices C and D provides example code.

7. **Conduct sensitivity analyses to assess robustness**: sensitivity analyses are essential for evaluating the extent to which conclusions are dependent on specific assumptions or parameters within the causal model.  Data simulation can be a powerful tool for evaluating the sensitivity of results to assumptions.  In the next section, we consider a simple sensitivity analysis for assessing robustness to unmeasured confounding. 


### Longitudinal Designs

Causation occurs in time. Longitudinal designs offer a substantial advantage over cross-sectional designs for causal inference because sequential measurements allow us to capture causation, and quantify its magnitude. We typically do not need to assert timing as we do in cross-sectional data settings.  Despite this advantage, longitudinal studies neverhteless rely on assumptions. These must be stated, and carefully managed. 

1. **Temporal sequencing**: the core strength of longitudinal designs is their ability to capture temporal sequencing, reducing ambiguity about the directionality of causal relationships. For instance, tracking changes in "happiness" following changes in access to green spaces over time can more definitively suggest causation than cross-sectional snapshots.

2. **Managing assumptions**: although longitudinal data reduces the need to assert timing , researchers still face assumptions regarding the absence of unmeasured confounders and the stability of over time. These assumptions must be explicitly stated.  As with cross sectional designs, wherever assumptions differ, researchers should draw different causal diagrams that reflect these assumptions, and subsequently conduct and report separate analyses. 

3. **Conditioning on baselines**: In longitudinal designs, conditioning on baseline measures of both the treatment and outcome can significantly address problems of unmeasured confounding. As mentioned before, by conditioning on baseline levels of access to green spaces and baseline mental health, researchers can more accurately estimate the *incident effect* of changes in green space access on changes in mental health.




::: {#tbl-lg}

```{=latex}
\examplelongitudinal
```
This table is adapted from [@bulbulia2023]
:::


### Recommendations for conducting and reporting causal analyses with cross-sectional data.

We offer the following recommendations for analysing and reporting analyses with cross-sectional data


1. **Draw two causal diagrams: one that describes the identification problem and the other for its solutions**: not only is it beneficial to draw separate causal diagrams to represent distinct structural assumptions, it is often helpful to draw a distinct causal diagram -- or several -- for describing the  identification problem and a distinct causal diagram for describing its proposed solutions. As illustrated in @tbl-lg, we recommend constructing at least two causal diagrams: first, draft an initial causal diagram that depicts assumed relationships among variables, including potential confounders and mediators. Second, draft a subsequent causal diagram that depicts a strategy for addressing these relationships, for example by highlighting variables on which to condition to reveal the causal effect of interest.

2. *Use at least three-wave of data*: a three-wave longitudinal design offers a refined approach by incorporating data from three time points. This design enables the examination of temporal precedence and lagged effects, providing stronger evidence of causality. For instance, if increased access to green spaces (time 0) is followed by increased exercise (time 1), which then precedes improvements in happiness (time 2), the temporal pattern can clarify a causal pathway from green space access through exercise to mental health. @tbl-lg describe the core of a three-wave longitudinal problem together with a strategy for addressing it.

3. *Simulate data*

4. *Conduct sensitivity analyses such as the E-value* 

5. *Calculate average treatment effects for the entire population* 

6. *Effect-modification* 






## Summary 

This chapter has offered an introduction to the potential outcomes framework for causal inference and directed acylic graphs to environmental psychology. Part 1 introduced three crucial assumptions for estimating magnitudes average treatment effects from data:


1. **Conditional Exchangeability**: the allocation of treatment is randomised and independent of the potential outcomes, conditional on measured covariates.

2. **Causal Consistency**: the outcome observed under the treatment condition matches the outcome that would have been observed if the unit had received the treatment, and vice versa for the control condition.

3. **Positivity**: every unit has a non-zero probability of receiving the treatments under comparison.

Although (perfectly) randomised controlled experiments naturally satisfy these assumptions by design-- randomisation ensures exchangeability, control ensures consistency and positivity -- observational studies generally do not. To obtain consistent causal estimates from observational data we must evaluate whether and how these assumptions may be satisfied.

Part 2 discussed causal diagrams and their role in addressing the conditional exchangeability assumption, or the assumption of "no unmeasured confounders." We discovered four elementary structures from which all causal relationships are build. And within these structures we discovered elementary rules for evaluating the implications that conditioning on elements within these structures entail about statistical associations that are observable in data. As such, causal diagrams offer a simplified visual vocabulary for mapping complex causal relationships onto observations in data. However, the relationships encoded in graphs are assertions that are generally not themselves verifiable from data. The only causal relationships that are not asserted are those between treatments and outcomes. We use causal diagrams to evaluate structural sources of bias in the statistical associations of treatments and outcomes that causal diagrams that may arise from causal relationships in the world that we assume might statistically associate treatments with outcomes irrespective of their causal associations.

In Part 3, we applied causal diagrams to various confounding scenarios. We observed that a causal diagram need highlight only those aspects of a causal setting relevant to assessing structural sources of bias that link the treatment and outcome in the absence of causality. Throughout we omitted nodes and paths that were not strictly necessary for evaluating our stated identification problem. Again we demonstrated that causal diagrams are not merely related to context-dependent questions but also to the assumptions we make about the causal structure of the world -- structural assumptions. 

Although here we have only discussed seven specific applications of causal diagrams, their utility spans far beyond. The simple rules by which variables become associated and disassociated by conditioning on nodes within four elementary structures allow researchers to apply causal diagrams to quantitatively investigate causality across innumerably many questions. We discovered that such applications are not limited to the production of effective modelling strategies but may also inform strategies for data collection, such as repeated measures data collection.

We hope the material we present here will encourage environmental psychologists to learn more about causal inference and to integrate causal diagrams into their workflow. The tools for assessing causation from correlation have been developed. There is no longer any excuse to ignore them. 


{{< pagebreak >}}

## Funding

This work is supported by a grant from the Templeton Religion Trust (TRT0418).
JB received support from the Max Planck Institute for the Science of Human History.
The funders had no role in preparing the manuscript or the decision to publish it.

## Contributions

DH proposed the chapter. JB developed the approach and wrote the first draft. Both authors contributed substantially to the final work.@appendix_a


## References

::: {#refs}
:::



## Appendix A: Glossary {.appendix}


**Acyclic**: a causal diagram cannot contain feedback loops. More precisely, no variable can be an ancestor or descendant of itself. If variables are repeatedly measured here, it is especially important to index nodes by the relative timing of the nodes.

**Adjustment set**: a collection of variables we must either condition upon or deliberately avoid conditioning upon to obtain a consistent
causal estimate for the effect of interest [@pearl2009].

**Ancestor (parent)**: a node with a direct or indirect influence on others, positioned upstream in the causal chain.

**Arrow**: denotes a causal relationship linking nodes.

**Backdoor path**: a "backdoor path" between a treatment variable, $A$, and an outcome variable, $Y$, is a sequence of links in a causal diagram that starts with an arrow into $A$ and reaches $Y$ through common causes, introducing potential confounding bias such that stastical association does not reflect causality. To estimate the causal effect of $A$ on $Y$ without bias, these paths must be blocked by adjusting for confounders. The backdoor criterion guides the selection of variables for adjustment to ensure unbiased causal inference.

**Conditioning**: the process of explicitly accounting for a variable in our statistical analysis to address the identification problem. In
causal diagrams, we usually represent conditioning by drawing a box around a node of the conditioned variable, for example,
$\boxed{L_{0}}\to A_{1} \to L_{2}$. We do not box exposures and outcomes, because we assume they are included in a model by default.
Depending on the setting, we may condition by regression stratification, inverse-probability of treatment weighting, g-methods, doubly robust
machine learning algorithms, or other methods. We do not cover such methods in this tutorial, however see @hernan2023.

**Counterfactual**: a hypothetical outcome that would have occurred for the same individuals under a different treatment condition than the one they actually experienced.

**Direct effect**: the portion of the total effect of a treatment on an outcome that is not mediated by other variables within the causal pathway.

**Collider**: a variable in a causal diagram at which two incoming
paths meet head-to-head. For example if
$A \rightarrowred \boxed{L} \leftarrowred Y$, then $L$ is a collider. If
we do not condition on a collider (or its descendants), the path between
$A$ and $Y$ remains closed. Conditioning on a collider (or its
descendants) will induce an association between $A$ and $Y$.

**Confounder**: a member of an adjustment set. Notice a variable is a
'confounder' in relation to a specific adjustment set. 'Confounder' is a
relative concept [@lash2020].

**D-separation**: in a causal diagram, a path is 'blocked' or 'd-separated' if a node along it interrupts causation. Two variables are
d-separated if all paths connecting them are blocked, making them conditionally independent. Conversely, unblocked paths result in
'd-connected' variables, implying potential dependence [@pearl1995].

**Descendant (child)**: a node influenced, directly or indirectly, by upstream nodes (parents).


**Effect-modifier**: a variable is an effect-modifier, or 'effect-measure modifier' if its presence changes the magnitude or direction of the effect of an exposure or treatment on an outcome across the levels or values of this variable. In other words, the effect of the exposure is different at different levels of the effect-modifier. 

**External validity**: the extent to which causal inferences can be generalizsd to other populations, settings, or times, also called "Target Validity."

**Identification problem**: the challenge of estimating the causal effect of a variable using by adjusting for measured variables on units
in a study. Causal diagrams were developed to address the identification problem by application of the rules of d-separation to a causal diagram.

**Indirect effect (Mediated effect)**: The portion of the total effect that is transmitted through a mediator variable.

**Internal validity**: the degree to which the design and conduct of a study are likely to have prevented bias, ensuring that the causal relationship observed can be confidently attributed to the treatment and not to other factors.

**Instrumental variable**: an ancestor of the exposure but not of the outcome. An instrumental variable affects the outcome only through its effect on the exposure and not otherwise. Whereas conditioning on a variable causally associated with the outcome but not with the exposure will generally increase modelling precision, we should avoid conditioning on instrumental variables [@cinelli2022].  Second, when an instrumental variable is the descendant of an unmeasured confounder, we should generally condition the instrumental variable to provide a partial adjustment for a confounder.

**Mediator**: a variable that transmits the effect of the treatment variable on the outcome variable, part of the causal pathway between treatment and outcome.

**Modified Disjunctive Cause Criterion**: @vanderweele2019 recommends obtaining a maximally efficient adjustment which he calls a 'confounder set' A member of this set is any set of variables that can reduce or remove a structural sources of bias. The strategy is as follows:

a.  Control for any variable that causes the exposure, the outcome, or
    both.
b.  Control for any proxy for an unmeasured variable that is a shared
    cause of both the exposure and outcome.
c.  Define an instrumental variable as a variable associated with the
    exposure but does not influence the outcome independently, except
    through the exposure. Exclude any instrumental variable that is not
    a proxy for an unmeasured confounder from the confounder set
    [@vanderweele2019].

Note that the concept of a 'confounder set' is broader than that of an
'adjustment set.' Every adjustment set is a member of a confounder set.
Hence, the Modified Disjunctive Cause Criterion will eliminate bias when
the data permit. However, a confounder set includes variables that will
reduce bias in cases where confounding cannot be eliminated.

**Node**: characteristic or features of units in a population ('variable') represented on a causal diagram. In a causal diagram, nodes are drawn with reference to variables defomed for the target population.

**Randomisation**: The process of randomly assigning subjects to different treatments or control groups, aiming to eliminate selection bias in experimental studies.

**Reverse Causation**: $\atoyassert$, but in reality $\ytoa$

**Statistical model:** a mathematical representation of the
relationships between variables in which we quantify covariances and
their corresponding uncertainties in the data. Statistical models
typically correspond to multiple causal structures [@pearl2018;
@vanderweele2022b; @hernan2023]. That is, the causes of such covariances
cannot be identified without assumptions.

**Structural model:** defines assumptions about causal relationships.
Causal diagrams graphically encode these assumptions [@hernan2023],
leaving out the assumption about whether the exposure and outcome are
causally associated. Outside of randomised experiments, we cannot
compute causal effects in the absence of structural models. A structural
model is needed to interpret the statistical findings in causal terms.
Structural assumptions should be developed in consultation with experts.
The role of structural assumptions when interpreting statistical results
remains poorly understood across many human sciences and forms the
motivation for my work here.

**Time-varying confounding:** occurs when a confounder that changes over
time also acts as a mediator or collider in the causal pathway between
exposure and outcome. Controlling for such a confounder can introduce
bias. Not controlling for it can retain bias.



## Appendix B: Causal Consistency in observational settings {.appendix}

In observational research, there are typically multiple versions of treatment. The theory of causal inference under multiple versions of treatment proves we can consistently estimate causal effects where the different versions of treatment are conditionally independent of the outcomes [@vanderweele2009, @vanderweele2009; @vanderweele2013; @vanderweele2018] 

Let $\coprod$ denote independence.
Where there are $K$ different versions of treatment $A$ and no confounding for $K$'s effect on $Y$ given measured confounders $L$ such that

$$
Y(k) \coprod K | L
$$

Then it can be proved that causal consistency follows. According to the theory of causal inference under multiple versions of treatment, the measured variable $A$ functions as a "coarsened indicator" for estimating the causal effect of the multiple versions of treatment $K$ on $Y(k)$ [@vanderweele2009; @vanderweele2013; @vanderweele2018].  

In the context of green spaces, let $A$ represent the general action of moving closer to any green space and $K$ represent the different versions of this treatment. For instance, $K$ could denote moving closer to different types of green spaces such as parks, forests, community gardens, or green spaces with varying amenities and features.

Here, the conditional independence implies that, given measured confounders $L$ (e.g. socioeconomic status, age, personal values), the type of green space one moves closer to ($K$) is independent of the outcomes $Y(k)$ (e.g. mental well-being under the $K$ conditions). In other words, the version of green space one chooses to live near does not affect the $K$ potential outcomes, provided the confounders $L$ are properly controlled for in our statistical models.

Put simply, strategies for confounding control and for consistently estimating causal effects when there are multiple versions of treatment converge. However, the quantities we estimate under multiple versions of treatment might lack any clear interpretations.  For example, we cannot readily determine which of the many versions of treatment is most causal efficacious and which lack any causal effect, or are harmful.  


## Appendix C: Computing the Average Treatment Effect {.appendix}


```{r}
#| label: simulation_cross_sectionaltwo
#| tbl-cap: ""
#| out-width: 100%
#| echo: true
#| eval: false

# load libraries
library(gtsummary) # gtsummary: nice tables
library(kableExtra) #  tables in latex/markdown
library(clarify) # simulate ATE

# simulation seed
set.seed(123) #  reproducibility

# define the parameters 
n = 1000 # Number of observations
p = 0.5  # Probability of A = 1 (access to greenspace)
alpha = 0 # Intercept for L (excercise)
beta = 2  # Effect of A on L 
gamma = 1 # Intercept for Y 
delta = 1.5 # Effect of L on Y
sigma_L = 1 # Standard deviation of L
sigma_Y = 1.5 # Standard deviation of Y

# simulate the data: fully mediated effect 
A = rbinom(n, 1, p) # binary exposure variable
L = alpha + beta*A + rnorm(n, 0, sigma_L) # continuous mediator
Y = gamma + delta*L + rnorm(n, 0, sigma_Y) # continuous outcome

# make the data frame
data = data.frame(A = A, L = L, Y = Y)

# fit regression in which L is assume to be a mediator
fit_1 <- lm( Y ~ A + L, data = data)

# fit regression in which L is assume to be a mediator
fit_2 <- lm( Y ~ A, data = data)

# create gtsummary tables for each regression model
table1 <- tbl_regression(fit_1)
table2 <- tbl_regression(fit_2)

# merge the tables for comparison
table_comparison <- tbl_merge(
  list(table1, table2),
  tab_spanner = c("Model: Exercise assumed confounder", 
                  "Model: Exercise assumed to be a mediator")
)
# make latex table
markdown_table_0 <- as_kable_extra(table_comparison, 
                                   format = "latex", 
                                   booktabs = TRUE)
# print table
markdown_table_0
```




Next, we present the code for calculating an average treatment effect.  


```{r}
#| label: ate-sim-crosstwo
#| tbl-cap: "Code for calculating the average treatment effect as contrasts between simulated outcomes for the entire population."
#| echo: true
#| eval: false

# use `clarify` package to obtain ATE
library(clarify)
# simulate fit 1 ATE
set.seed(123)
sim_coefs_fit_1 <- sim(fit_1)
sim_coefs_fit_2 <- sim(fit_2)

# marginal risk difference ATE, simulation-based: model 1 (L is a confounder)
sim_est_fit_1 <-
  sim_ame(
    sim_coefs_fit_1,
    var = "A",
    subset = A == 1,
    contrast = "RD",
    verbose = FALSE
  )

# marginal risk difference ATE, simulation-based: model 2 (L is a mediator)
sim_est_fit_2 <-
  sim_ame(
    sim_coefs_fit_2,
    var = "A",
    subset = A == 1,
    contrast = "RD",
    verbose = FALSE
  )

# obtain summaries
summary_sim_est_fit_1 <- summary(sim_est_fit_1, null = c(`RD` = 0))
summary_sim_est_fit_2 <- summary(sim_est_fit_2, null = c(`RD` = 0))

# get coefficients for reporting
# ate for fit 1, with 95% CI
ATE_fit_1 <- glue::glue(
  "ATE =
                        {round(summary_sim_est_fit_1[3, 1], 2)},
                        CI = [{round(summary_sim_est_fit_1[3, 2], 2)},
                        {round(summary_sim_est_fit_1[3, 3], 2)}]"
)
# ate for fit 2, with 95% CI
ATE_fit_2 <-
  glue::glue(
    "ATE = {round(summary_sim_est_fit_2[3, 1], 2)},
                        CI = [{round(summary_sim_est_fit_2[3, 2], 2)},
                        {round(summary_sim_est_fit_2[3, 3], 2)}]"
  )
```



## Appendix D: Simulation of Different Confounding Control Strategies {.appendix}


Here are data illustrating how different confounding control strategies perform against the "ground truth" of simulated data. This simulation reveals that (1) standard control for confounding may not perform optimally when there is unmeasured confounding and (2) including the baseline treatment and outcome, if these variables are strongly correlated with the unmeasured confounder, may substantially reduce the effect of unmeasured confounding


In this simulation:

- The interaction between $A_1$ (treatment) and $L_0$ (baseline confounders) is specified by the coefficient $\theta_{A1L0} = 0.5$, indicating that the effect of $A_1$ on $Y_2$ varies depending on the value of $L_0$.
- The inclusion of this interaction allows us to examine how the baseline level of a confounder ($L_0$) modifies the effect of the treatment ($A_1$) on the outcome ($Y_2$).

```{r}
#| label: codelg
#| echo: true
#| eval: true

library(kableExtra)
library(gtsummary)
if(!require(grf)){install.packages("grf")} # causal forest
set.seed(123) # Ensure reproducibility

n <- 10000 # Number of observations

# Baseline covariates
U <- rnorm(n) # Unmeasured confounder
A_0 <- rbinom(n, 1, prob = plogis(U)) # Baseline exposure
Y_0 <- rnorm(n, mean = U, sd = 1) # Baseline outcome
L_0 <- rnorm(n, mean = U, sd = 1) # Baseline confounders

# Coefficients for treatment assignment
beta_A0 = 0.25
beta_Y0 = 0.3
beta_L0 = 0.2
beta_U = 0.1

# Simulate treatment assignment
A_1 <- rbinom(n, 1, prob = plogis(-0.5 + 
                                    beta_A0 * A_0 +
                                    beta_Y0 * Y_0 + 
                                    beta_L0 * L_0 + 
                                    beta_U * U))

# Coefficients for continuous outcome
delta_A1 = 0.3
delta_Y0 = 0.9
delta_A0 = 0.1
delta_L0 = 0.3
theta_A0Y0L0 = 0.5 # Interaction effect between A_1 and L_0
delta_U = 0.05

# Simulate continuous outcome including interaction
Y_2 <- rnorm(n,
             mean = 0 +
               delta_A1 * A_1 + 
               delta_Y0 * Y_0 + 
               delta_A0 * A_0 + 
               delta_L0 * L_0 + 
               theta_A0Y0L0 * Y_0 * 
               A_0 * L_0 + 
               delta_U * U,
             sd = .5)

# Data frame
data <- data.frame(Y_2, A_0, A_1, L_0, Y_0, U)

# models
# no control
fit_no_control <- lm(Y_2 ~ A_1, data = data)
#summary(fit_no_control)

# standard covariate control
fit_standard <- lm(Y_2 ~ A_1 + L_0, data = data)
#summary(fit_standard)

# interaction
fit_interaction  <- lm(Y_2 ~ A_1 + L_0 + A_0 + Y_0 + A_0:L_0:Y_0, data = data)
#summary(fit_interaction)


# create gtsummary tables for each regression model
tbl_fit_no_control<- tbl_regression(fit_no_control)  
tbl_fit_standard <- tbl_regression(fit_standard)
tbl_fit_interaction <-
  tbl_regression(fit_interaction)


# get only the treatment variable
tbl_list_modified <- lapply(list(
  tbl_fit_no_control,
  tbl_fit_standard,
  tbl_fit_interaction),
function(tbl) {
  tbl %>%
    modify_table_body(~ .x %>% dplyr::filter(variable == "A_1"))
})


# merge the tables
table_comparison <- tbl_merge(
  tbls = tbl_list_modified,
  tab_spanner = c(
    "No Control",
    "Standard",
    "Interaction")
) |>
  modify_table_styling(
    column = c(p.value_1, p.value_2, p.value_3),
    hide = TRUE
  )

#create latex table for publication
markdown_table <-
  as_kable_extra(table_comparison, format = "latex", booktabs = TRUE) |>
  kable_styling(latex_options = "scale_down")
# print it
markdown_table
```


Next we simulate average treatment effect

```{r}
#| label: ate-sim-long
#| tbl-cap: "Code for calculating the average treatment effect."
#| echo: true
#| eval: true

# use `clarify` package to obtain ATE
library(clarify)
# simulate fit 1 ATE
set.seed(123)
sim_coefs_fit_std <- sim(fit_standard)
sim_coefs_fit_int <- sim(fit_interaction)

# marginal risk difference ATE, simulation-based: model 1 (L is a confounder)
sim_est_fit_std <-
  sim_ame(
    sim_coefs_fit_std,
    var = "A_1",
    subset = A_1 == 1,
    contrast = "RD",
    verbose = FALSE
  )

# marginal risk difference ATE, simulation-based: model 2 (L is a mediator)
sim_est_fit_int <-
  sim_ame(
    sim_coefs_fit_int,
    var = "A_1",
    subset = A_1 == 1,
    contrast = "RD",
    verbose = FALSE
  )

# obtain summaries
summary_sim_est_fit_std <-
  summary(sim_est_fit_std, null = c(`RD` = 0))
summary_sim_est_fit_int <-
  summary(sim_est_fit_int, null = c(`RD` = 0))

# get coefficients for reporting
# ate for fit 1, with 95% CI
ATE_fit_std <- glue::glue(
  "ATE = {round(summary_sim_est_fit_std[3, 1], 2)}, 
  CI = [{round(summary_sim_est_fit_std[3, 2], 2)},
  {round(summary_sim_est_fit_std[3, 3], 2)}]"
)
# ate for fit 2, with 95% CI
ATE_fit_int <-
  glue::glue(
    "ATE = {round(summary_sim_est_fit_int[3, 1], 2)},
    CI = [{round(summary_sim_est_fit_int[3, 2], 2)},
    {round(summary_sim_est_fit_int[3, 3], 2)}]"
  )
# ATE_fit_std
# ATE_fit_int
```

Using the `clarify` package, we infer the ATE for the standard model is `r ATE_fit_std`.

Using the `clarify` package, we infer the ATE for the model that conditions on the baseline exposure and baseline outcome to be:  `r ATE_fit_int`, which is close to the values supplied to the data-generating mechanism. 

Because the baseline exposure and baseline outcome are often the most important variables to include when estimating an incident effect, we should endevour to obtain at least three waves of data such that these variables along with other baseline confounders are included in at time 0, the exposure is included at time 1, and the outcome is included at time 2. 

## Appendix E: Non-parametric Estimation of Average Treatment Effects Using Causal Forests

Semi-parametric and non-parametric estimators have many advantages, most especially the ability to fit non-restrictive statistical models. Here we include an example of such a model using the causal forest package. 

```{r}
#| label: causal_forest
#| echo: true

# load causal forest library 
library(grf) # estimate conditional and average treatment effects
library(glue) # reporting 

#  'data' is our data frame with columns 'A_1' for treatment, 'L_0' for a covariate, and 'Y_2' for the outcome
#  we also have the baseline exposure 'A_0' and 'Y_0'
#  ensure W (treatment) and Y (outcome) are vectors
W <- as.matrix(data$A_1)  # Treatment
Y <- as.matrix(data$Y_2)  # Outcome
X <- as.matrix(data[, c("L_0", "A_0", "Y_0")])

# fit causal forest model 
fit_causal_forest <- causal_forest(X, Y, W)

# estimate the average treatment effect (ATE)
ate <- average_treatment_effect(fit_causal_forest)

# make data frame for reporting using "glue' 
ate<- data.frame(ate)

# obtain ate for report
ATE_fit_causal_forest <-
  glue::glue(
    "ATE = {round(ate[1, 1], 2)}, se = {round(ate[2, 1], 2)}"
  )
```

Causal forest estimates the average treatment effect as `r ATE_fit_causal_forest`. This converges to to the true value supplied to the generating mechanism of 0.3

