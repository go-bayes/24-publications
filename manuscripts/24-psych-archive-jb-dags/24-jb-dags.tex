% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames,x11names}{xcolor}
%
\documentclass[
  single column]{article}

\usepackage{amsmath,amssymb}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage[]{libertinus}
\ifPDFTeX\else  
    % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\usepackage[top=30mm,left=25mm,heightrounded,headsep=22pt,headheight=11pt,footskip=33pt,ignorehead,ignorefoot]{geometry}
\setlength{\emergencystretch}{3em} % prevent overfull lines
\setcounter{secnumdepth}{-\maxdimen} % remove section numbering
% Make \paragraph and \subparagraph free-standing
\makeatletter
\ifx\paragraph\undefined\else
  \let\oldparagraph\paragraph
  \renewcommand{\paragraph}{
    \@ifstar
      \xxxParagraphStar
      \xxxParagraphNoStar
  }
  \newcommand{\xxxParagraphStar}[1]{\oldparagraph*{#1}\mbox{}}
  \newcommand{\xxxParagraphNoStar}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
  \let\oldsubparagraph\subparagraph
  \renewcommand{\subparagraph}{
    \@ifstar
      \xxxSubParagraphStar
      \xxxSubParagraphNoStar
  }
  \newcommand{\xxxSubParagraphStar}[1]{\oldsubparagraph*{#1}\mbox{}}
  \newcommand{\xxxSubParagraphNoStar}[1]{\oldsubparagraph{#1}\mbox{}}
\fi
\makeatother


\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
% definitions for citeproc citations
\NewDocumentCommand\citeproctext{}{}
\NewDocumentCommand\citeproc{mm}{%
  \begingroup\def\citeproctext{#2}\cite{#1}\endgroup}
\makeatletter
 % allow citations to break across lines
 \let\@cite@ofmt\@firstofone
 % avoid brackets around text for \cite:
 \def\@biblabel#1{}
 \def\@cite#1#2{{#1\if@tempswa , #2\fi}}
\makeatother
\newlength{\cslhangindent}
\setlength{\cslhangindent}{1.5em}
\newlength{\csllabelwidth}
\setlength{\csllabelwidth}{3em}
\newenvironment{CSLReferences}[2] % #1 hanging-indent, #2 entry-spacing
 {\begin{list}{}{%
  \setlength{\itemindent}{0pt}
  \setlength{\leftmargin}{0pt}
  \setlength{\parsep}{0pt}
  % turn on hanging indent if param 1 is 1
  \ifodd #1
   \setlength{\leftmargin}{\cslhangindent}
   \setlength{\itemindent}{-1\cslhangindent}
  \fi
  % set entry spacing
  \setlength{\itemsep}{#2\baselineskip}}}
 {\end{list}}
\usepackage{calc}
\newcommand{\CSLBlock}[1]{\hfill\break\parbox[t]{\linewidth}{\strut\ignorespaces#1\strut}}
\newcommand{\CSLLeftMargin}[1]{\parbox[t]{\csllabelwidth}{\strut#1\strut}}
\newcommand{\CSLRightInline}[1]{\parbox[t]{\linewidth - \csllabelwidth}{\strut#1\strut}}
\newcommand{\CSLIndent}[1]{\hspace{\cslhangindent}#1}

\usepackage{booktabs}
\usepackage{longtable}
\usepackage{array}
\usepackage{multirow}
\usepackage{wrapfig}
\usepackage{float}
\usepackage{colortbl}
\usepackage{pdflscape}
\usepackage{tabu}
\usepackage{threeparttable}
\usepackage{threeparttablex}
\usepackage[normalem]{ulem}
\usepackage{makecell}
\usepackage{xcolor}
\input{/Users/joseph/GIT/latex/latex-for-quarto.tex}
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\AtBeginDocument{%
\ifdefined\contentsname
  \renewcommand*\contentsname{Table of contents}
\else
  \newcommand\contentsname{Table of contents}
\fi
\ifdefined\listfigurename
  \renewcommand*\listfigurename{List of Figures}
\else
  \newcommand\listfigurename{List of Figures}
\fi
\ifdefined\listtablename
  \renewcommand*\listtablename{List of Tables}
\else
  \newcommand\listtablename{List of Tables}
\fi
\ifdefined\figurename
  \renewcommand*\figurename{Figure}
\else
  \newcommand\figurename{Figure}
\fi
\ifdefined\tablename
  \renewcommand*\tablename{Table}
\else
  \newcommand\tablename{Table}
\fi
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Listing}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}
\makeatother
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\@ifpackageloaded{subcaption}{}{\usepackage{subcaption}}
\makeatother
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\usepackage{bookmark}

\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same} % disable monospaced font for URLs
\hypersetup{
  pdftitle={Causal Directed Acyclic Graphs (DAGs): A Practical Guide},
  pdfauthor={Joseph A. Bulbulia},
  colorlinks=true,
  linkcolor={blue},
  filecolor={Maroon},
  citecolor={Blue},
  urlcolor={Blue},
  pdfcreator={LaTeX via pandoc}}

\title{Causal Directed Acyclic Graphs (DAGs): A Practical Guide}

\usepackage{academicons}
\usepackage{xcolor}

  \author{Joseph A. Bulbulia}
            \affil{%
             \small{     Victoria University of Wellington, New Zealand
          ORCID \textcolor[HTML]{A6CE39}{\aiOrcid} ~0000-0002-5861-2056 }
              }
      


\date{2024-06-10}
\begin{document}
\maketitle
\begin{abstract}
Causal inference requires contrasting counterfactual states of the world
under pre-specified interventions. Obtaining counterfactual contrasts
from data relies on explicit assumptions and careful, multi-step
workflows. Causal diagrams are powerful tools for clarifying whether and
how the counterfactual contrasts we seek can be identified from data.
Here, I explain how to use causal directed acyclic graphs (causal DAGs)
to determine whether and how causal effects can be identified from
`real-world' non-experimental observational data. I offer practical tips
for reporting and suggest ways to avoid common pitfalls.

\textbf{KEYWORDS}: \emph{Causal Inference}; \emph{Culture}; \emph{DAGs};
\emph{Evolution}; \emph{Human Sciences}; \emph{Longitudinal}
\end{abstract}

\subsection{Introduction}\label{id-sec-introduction}

Human research begins with two fundamental questions:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  What do I want to know?
\item
  For which population does this knowledge generalise?
\end{enumerate}

In the human sciences, our questions are typically causal. We aim to
understand the effects of interventions on certain variables. However,
many researchers report non-causal associations, collecting data,
applying complex regressions, and reporting coefficients. We often speak
of covariates as `predicting' outcomes. Yet, even when our models
predict well, it remains unclear how these predictions relate to the
scientific questions that sparked our interest. Our predictions lack
meaning and fail to address our core scientific questions.

Some say that association cannot imply causation. However, our
experimental traditions reveal that when interventions are controlled
and randomised, the coefficients we recover from statistical models can
permit causal interpretations.

Despite familiarity with experimental protocols, many researchers
struggle to emulate randomisation and control with non-experimental or
`real-world' data. Though we use terms such as `control' and employ
sophisticated adjustment strategies, such as multilevel modelling and
structural equation models, our practices are not systematic. We often
overlook that what we take as control can undermine our ability to
consistently estimate causal effects
(\citeproc{ref-montgomery2018}{Montgomery \emph{et al.} 2018}). Although
the term `crisis' is overused, the state of causal inference across many
human sciences, including experimental sciences, has much headroom for
improvement. `Room for headroom' applies to poor experimental designs
that unintentionally weaken causal claims
(\citeproc{ref-bulbulia_2024_experiments}{Bulbulia 2024c};
\citeproc{ref-hernan2017per}{Hern√°n \emph{et al.} 2017}). Fortunately,
recent decades have seen considerable progress in causal data science,
commonly called `causal inference.' The progress has transformed those
areas of health science, economics, and computer science that have
adopted it. Causal inference provides methods for obtaining valid causal
inferences from data through careful, systematic workflows.

Within the workflows of causal inference, causal diagrams---or causal
graphs---are powerful tools for evaluating whether and how causal
effects can be identified from data. My purpose here is to explain where
these tools fit within causal inference workflows and to illustrate
their practical applications. I focus on causal directed acyclic graphs
(DAGs) because they are relatively easy to use and clear for most
applications. However, DAGs can be misused. I will also explain common
pitfalls and how to avoid them.

In \hyperref[id-sec-1]{Part 1}, I review the conceptual foundations of
causal inference. The basis of all causal inference lies in
counterfactual contrasts. Although there are different philosophical
approaches to counterfactual reasoning, they are largely similar in
practice. The overview here builds on the Neyman-Rubin potential
outcomes framework, extended for longitudinal data by epidemiologist
James Robins. Although this is not the framework within which causal
directed acyclic graphs were developed, the potential outcomes framework
is easier to interpret. (I discuss Pearl's non-parametric structural
equation approach in \hyperref[id-app-d]{Appendix D}).

In \hyperref[id-sec-2]{Part 2}, I describe how causal directed acyclic
graphs help identify causal effects. I outline five elementary graphical
structures that encode all causal relations, forming the building blocks
of all causal directed acyclic graphs. I then examine five rules that
clarify whether and how investigators can identify causal effects from
data.

In \hyperref[id-sec-3]{Part 3}, I apply causal directed acyclic graphs
to practical problems, showing how repeated measures data collection can
solve seven common identification issues. Timing is critical but not
sufficient alone. I also use causal diagrams to explain the limitations
of repeated-measures data collection for identifying causal effects,
tempering enthusiasm for easy solutions. Indeed, I will review how many
statistical structural equation models and sophisticated multi-level
models are not well-calibrated for identifying causal effects.

In \hyperref[id-sec-4]{Part 4}, I offer practical suggestions for
creating and reporting causal directed acyclic graphs in scientific
research. These graphs represent investigator assumptions about causal
(or structural) relationships in nature. These relationships cannot
typically be derived from data alone and must be developed with
scientific specialists. Where there is ambiguity or debate,
investigators should report multiple causal diagrams and conduct
distinct analyses for each.

\subsection{Part 1: Causal Inference as Counterfactual Data
Science}\label{id-sec-1}

The first step in answering a causal question is to ask it
(\citeproc{ref-hernuxe1n2016}{Hern√°n \emph{et al.} 2016a}).

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  What causal quantity do I want to consistently estimate?
\item
  For which population does this knowledge generalise?
\end{enumerate}

Causal diagrams come after we have stated a causal question and
clarified the population for whom we hope to obtain valid causal
inferences, the `target population'. We begin by considering what is
required to state a causal question and define a target population
precisely.

\subsubsection{The Fundamental Problem of Causal Inference: Missing
Counterfactual
Observations}\label{the-fundamental-problem-of-causal-inference-missing-counterfactual-observations}

To ask a causal question, we must consider the concept of causality
itself. Consider an intervention, \(A\), and its effect, \(Y\). We say
that \(A\) causes \(Y\) if altering \(A\) would lead to a change in
\(Y\) (\citeproc{ref-hume1902}{Hume 1902};
\citeproc{ref-lewis1973}{Lewis 1973}). If altering \(A\) would not
change \(Y\), we say that \(A\) has no causal effect on \(Y\).

In causal inference, we aim to quantitatively contrast the potential
outcomes in response to different levels of a well-defined intervention.
Commonly, we refer to such interventions as `exposures' or `treatments;'
we refer to the possible effects of interventions as `potential
outcomes.'

Consider a binary treatment variable \(A \in \{0,1\}\). For each unit
\(i\) in the set \(\{1, 2, \ldots, n\}\), when \(A_i\) is set to 0, the
potential outcome under this condition is denoted \(Y_i(0)\).
Conversely, when \(A_i\) is set to 1, the potential outcome is denoted
\(Y_i(1)\). We refer to the terms \(Y_i(1)\) and \(Y_i(0)\) as
`potential outcomes' because, until realised, the effects of
interventions describe counterfactual states.

Suppose that each unit \(i\) receives either \(A_i = 1\) or \(A_i = 0\).
The corresponding outcomes are realised as \(Y_i|A_i = 1\) or
\(Y_i|A_i = 0\). For now, we assume that each realised outcome under
that intervention is equivalent to one of the potential outcomes
required for a quantitative causal contrast, such that
\([(Y_i(a)|A_i = a)] = (Y_i|A_i = a)\). Thus, when \(A_i = 1\),
\(Y_i(1)|A_i = 1\) is observed. However, when \(A_i = 1\), it follows
that \(Y_i(0)|A_i = 1\) is not observed:

\[
Y_i|A_i = 1 \implies Y_i(0)|A_i = 1~ \text{is counterfactual}
\]

Conversely:

\[
Y_i|A_i = 0 \implies Y_i(1)|A_i = 0~ \text{is counterfactual}
\]

We define \(\delta_i\) as the individual causal effect for unit \(i\)
and express the individual causal effect as:

\[
\delta_i = Y_i(1) - Y_i(0)
\]

An individual causal effect is a contrast between treatments one of
which is excluded by the other. That individual causal effects cannot be
identified from observations is known as `\emph{the fundamental problem
of causal inference}' (\citeproc{ref-holland1986}{Holland 1986};
\citeproc{ref-rubin1976}{Rubin 1976}).

\subsubsection{Identifying Causal Effects Using Randomised
Experiments}\label{identifying-causal-effects-using-randomised-experiments}

Although it is not typically feasible to compute individual causal
effects, under certain assumptions, it may be possible to estimate
\emph{average} treatment effects, also called `marginal effects'. We
define an average treatment effect (ATE) as the difference between the
expected or average outcomes under treatment and contrast conditions for
a pre-specified population.

Consider a binary treatment, \(A \in \{0,1\}\):

\[
\text{Average Treatment Effect} = \mathbb{E}[Y(1)] - \mathbb{E}[Y(0)]
\]

This is our pre-specified estimand for our target population. A
challenge remains in computing these treatment-group averages, given
that individual causal effects are unobservable. We can frame the
problem by referring to the \emph{full data} required to compute this
estimand --- that is, in terms of the complete counterfactual dataset
where the missing potential outcomes, inherent in observational data,
were somehow available. The text highlighted in red denotes inherently
missing responses over the joint distribution of the full counterfactual
dataset. We find that for each treatment condition, half the
observations over the joint distribution of the counterfactual data are
inherently unobserved.

\[
\text{Average Treatment Effect} = \left(\underbrace{\underbrace{\mathbb{E}[Y(1)|A = 1]}_{\text{observed for } A = 1} + \underbrace{\textcolor{red}{\mathbb{E}[Y(1)|A = 0]}}_{\textcolor{red}{\text{unobserved for } A = 0}}}_{\text{effect among treated}}\right) - \left(\underbrace{\underbrace{\mathbb{E}[Y(0)|A = 0]}_{\text{observed for } A = 0} + \underbrace{\textcolor{red}{\mathbb{E}[Y(0)|A = 1]}}_{\textcolor{red}{\text{unobserved for } A = 1}}}_{\text{effect among untreated}}\right)
\]

Randomisation allows investigators to recover the treatment group
averages despite the inherently missing observations within these
groups. We do not require the joint distribution over the full data
(i.e., the counterfactual data) to obtain these averages. When
investigators randomise units into treatment conditions, ensuring full
adherence and a sufficiently large sample to rule out chance differences
in group composition, we can generally attribute differences in
treatment group averages to the treatment itself. Randomisation implies:

\[
\mathbb{E}[Y(0) | A = 1] = \mathbb{E}[Y(0) | A = 0]
\]

and

\[
\mathbb{E}[Y(1) | A = 1] = \mathbb{E}[Y(1) | A = 0]
\]

If we assume:

\[ 
\mathbb{E}[Y(1) | A = 1] = \mathbb{E}[Y | A = 1]
\]

and

\[
\mathbb{E}[Y(0) | A = 0] = \mathbb{E}[Y | A = 0]
\]

it follows that the average treatment effect of a randomised experiment
can be computed:

\[
\text{Average Treatment Effect} = \widehat{\mathbb{E}}[Y | A = 1] - \widehat{\mathbb{E}}[Y | A = 0]
\]

There are four critical aspects of how ideally randomised experiments
enable the estimation of average treatment effects worth highlighting.

First, we must specify a population for whom they seek to generalise
their results. We refer to this population as the \emph{target
population}. If the study population differs from the target population
in the distribution of covariates that interact with the treatment, we
will have no guarantees our results will generalise (for discussions of
sample/target population mismatch, refer to: Imai \emph{et al.}
(\citeproc{ref-imai2008misunderstandings}{2008}); Westreich \emph{et
al.} (\citeproc{ref-westreich2019target}{2019}); Westreich \emph{et al.}
(\citeproc{ref-westreich2017}{2017}); Pearl and Bareinboim
(\citeproc{ref-pearl2022}{2022}); Bareinboim and Pearl
(\citeproc{ref-bareinboim2013general}{2013}); Stuart \emph{et al.}
(\citeproc{ref-stuart2018generalizability}{2018}); Webster-Clark and
Breskin (\citeproc{ref-webster2021directed}{2021})).

Second, because the units in the study sample at randomisation may
differ from the units in the study after randomisation, we must be
careful to avoid biases that arise from sample/population mismatch over
time. If there is sample attrition or non-response, the treatment effect
we obtain for the sample may differ from the treatment effect in the
target population.

Third, a randomised experiment recovers the causal effect of random
treatment assignment, not of the treatment itself, which may differ if
some participants do not adhere to their treatment (even if they remain
in the study). The effect of randomised assignment is called the
`intent-to-treat effect' or equivalently the `intention-to-treat
effect'. The effect of perfect adherence is called the `per-protocol
effect' (\citeproc{ref-hernan2017per}{Hern√°n \emph{et al.} 2017};
\citeproc{ref-lash2020}{Lash \emph{et al.} 2020}). To obtain the
per-protocol effect for randomised experiments, methods for causal
inference in observational settings must be applied
(\citeproc{ref-bulbulia_2024_experiments}{Bulbulia 2024c}).

Fourth, I have presented the average treatment effect on the difference
scale, that is, as a difference in average potential outcomes for the
target population under two distinct levels of treatment. However,
depending on the scientific question at hand, investigators may wish to
estimate causal effects on the risk-ratio scale, the rate-ratio scale,
the hazard-ratio scale, or another scale. Where there are interactions
such that treatment effects vary across different strata of the
population, an estimate of the causal effect on the risk difference
scale will differ in at least one stratum to be compared from the
estimate on the risk ratio scale
(\citeproc{ref-greenland2003quantifying}{Greenland 2003};
\citeproc{ref-vanderweele2012}{VanderWeele 2012}). The sensitivity of
treatment effects in the presence of interactions to the scale of
contrast underscores the importance of pre-specifying a scale for the
causal contrast investigators hope to obtain.

Fifth, investigators may unintentionally spoil randomisation by
adjusting for indicators that might be affected by the treatment,
outcome, or both, by excluding participants using attention checks, by
collecting covariate data that might be affected by the experimental
conditions, by failing to account for non-response and
loss-to-follow-up, and by committing any number of other self-inflicted
injuries. Unfortunately, such practices are widespread
(\citeproc{ref-montgomery2018}{Montgomery \emph{et al.} 2018}). Notably,
causal graphical methods are useful for describing causal identification
in experiments (refer to Hern√°n \emph{et al.}
(\citeproc{ref-hernan2017per}{2017})), a topic we consider elsewhere
(\citeproc{ref-bulbulia_2024_experiments}{Bulbulia 2024c}).

In observational studies, investigators might wish to describe the
target population of interest as a restriction of the study sample
population. For example, investigators might wish to estimate the
average treatment effect only in the population that received the
treatment. This treatment effect is sometimes called the average
treatment effect in the treated (ATT) and may be expressed as:

\[
\text{Average Treatment Effect in the Treated} = \mathbb{E}[Y(1) - Y(0) \mid A = 1]
\]

Consider that if investigators are interested in the average treatment
effect in the treated, counterfactual comparisons are deliberately
restricted to the sample population that was treated. That is, the
investigators will seek to obtain the average of the missing
counterfactual outcomes for the treated population were they not
treated, without necessarily obtaining the counterfactual outcomes for
the untreated population were they treated. This difference in focus may
imply different assumptions and analytic workflows. Appendix B describes
an example for which the assumptions required to estimate the average
treatment effect may be preferred. In what follows, we will use the term
ATE as a placeholder to mean the average treatment effect, or
equivalently the `marginal effect', for a target population on a
pre-specified scale of causal contrast.

Setting aside the important detail that the `average treatment effect'
requires considerable care in its specification, it is worth pausing to
marvel at how an ideally conducted randomised controlled experiment
provides a means for identifying inherently unobservable
counterfactuals. It does so by using a Sherlock-Holmes method of
inference by elimination of confounders, which randomisation balances
across treatments.

When experimenters observe a difference in average treatment effects,
and all else goes right, they may infer that the distribution of
potential outcomes differs by treatment because randomisation exhausts
every other explanation except that of the treatment. They are entitled
to this inference because randomisation balances the distribution of
potential confounders across the treatment groups to be compared.

However, we lack guarantees for balance in the confounders outside of
randomised experiments. For this reason, we should prefer developing
sound randomised experiments for addressing every causal question that
experiments can address. Unfortunately, randomised experiments cannot
address many scientifically important questions. This bitter constraint
is familiar to evolutionary human scientists. We typically confront
`What if?' questions that are rooted in the unidirectional nature of
human history. However, understanding how randomisation obtains the
missing counterfactual outcomes that we require to consistently estimate
average treatment effects clarifies the tasks of causal inference in
non-experimental settings (\citeproc{ref-hernuxe1n2008a}{Hern√°n \emph{et
al.} 2008a}; \citeproc{ref-hernuxe1n2022}{Hern√°n \emph{et al.} 2022};
\citeproc{ref-hernuxe1n2006}{Hern√°n and Robins 2006a}).

Next, we examine these identification assumptions in greater detail
because using causal diagrams without understanding these assumptions is
unsafe.

\subsubsection{Fundamental Assumptions Required for Causal Inference in
the Potential Outcomes
Framework}\label{fundamental-assumptions-required-for-causal-inference-in-the-potential-outcomes-framework}

There are three fundamental identification assumptions that must be
satisfied to consistently estimate causal effects with data. These
assumptions are typically satisfied in randomised controlled trials but
not in real-world studies where randomised treatment assignment is
absent.

\paragraph{Assumption 1: Causal
Consistency}\label{assumption-1-causal-consistency}

We satisfy the causal consistency assumption when, for each unit \(i\)
in the set \(\{1, 2, \ldots, n\}\), the observed outcome corresponds to
one of the specific counterfactual outcomes to be compared such that:

\[
Y_i^{observed}|A_i = 
\begin{cases} 
Y_i(a^*) & \text{if } A_i = a^* \\
Y_i(a) & \text{if } A_i = a
\end{cases}
\]

The causal consistency assumption implies that the observed outcome at a
specific treatment level equates to the counterfactual outcome for that
individual at the observed treatment level. Although it seems
straightforward, treatment conditions vary, and treatment heterogeneity
poses considerable challenges to satisfying this assumption. Refer to
\hyperref[id-app-c]{Appendix C} for further discussion on how
investigators may satisfy causal consistency in real-world settings.

\paragraph{Assumption 2: Positivity}\label{assumption-2-positivity}

We satisfy the positivity assumption if there is a non-zero probability
of receiving each treatment level for every combination of covariates
that occurs in the population. Where \(A\) is the treatment and \(L\) is
a vector of covariates sufficient for ensuring no unmeasured
confounding, we say positivity is achieved if:

\[
0 < Pr(A = a | L = l) < 1, \quad \text{for all } a, l \text{ with } Pr(L = l) > 0
\]

There are two types of positivity violation:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Random non-positivity}: When a treatment is theoretically
  possible but specific treatment levels are not represented in the
  data, random non-positivity is the only identifiability assumption
  verifiable with data.
\item
  \textbf{Deterministic non-positivity}: When the treatment is
  implausible by nature, such as a hysterectomy in biological males.
\end{enumerate}

Satisfying the positivity assumption can present considerable data
challenges (\citeproc{ref-bulbulia2023a}{Bulbulia \emph{et al.} 2023};
\citeproc{ref-westreich2010}{Westreich and Cole 2010}). For instance, if
we wanted to estimate a one-year causal effect of weekly religious
service attendance on charitable donations, controlling for baseline
attendance, and the natural transition rate to weekly service attendance
is low, the effective sample size for the treatment condition may be
insufficient. Where the positivity assumption is violated, causal
diagrams will be of limited utility because observations in the data do
not support valid causal inferences. (\hyperref[id-app-b]{Appendix B}
presents a worked example illustrating this difficulty in a cultural
evolutionary context.)

\paragraph{Assumption 3: Conditional Exchangeability (also `No
Unmeasured Confounding', `Conditional Ignorability',
`d-separation')}\label{assumption-3-conditional-exchangeability-also-no-unmeasured-confounding-conditional-ignorability-d-separation}

We satisfy the conditional exchangeability assumption if the treatment
groups are conditionally balanced in the variables that could affect the
potential outcomes. In experimental designs, random assignment
facilitates this assumption. In observational studies, more effort is
required to control for any covariate that could account for observed
correlations between \(A\) and \(Y\) without a causal effect of \(A\) on
\(Y\).

Let \(\coprod\) denote independence, and let \(L\) denote the set of
covariates necessary to ensure this conditional independence.
Conditional exchangeability is satisfied when:

\[
Y(a) \coprod A | L \quad \text{or equivalently} \quad A \coprod Y(a) | L
\]

If we assume that positivity and consistency assumptions also, we may
compute the average treatment effect (ATE) on the difference scale:

\[
\text{Average Treatment Effect} = \mathbb{E}[Y(1) | L] - \mathbb{E}[Y(0) | L]
\]

In randomised controlled experiments, exchangeability is unconditional.
Although adjusting by interacting the treatment with pre-treatment
variables may improve efficiency and diminish threats to randomisation
from chance imbalances, it is confusing to think of such an adjustment
as `control.'

In real-world observational studies, where measured confounders are
sufficient to ensure conditional exchangeability, we obtain estimates
for the average treatment effect by conditioning on the densities of
measured confounders by treatment group. Where \(A = a\) and \(A = a^*\)
are the treatment levels we seek to contrast:

\[
\widehat{\text{ATE}} =  \sum_l \big( \mathbb{E}[Y(a^*) \mid L] - \mathbb{E}[Y(a) \mid L] \big) \times Pr(L)
\]

By causal consistency, we obtain:

\[
\widehat{\text{ATE}} =  \sum_l \big( \mathbb{E}[Y \mid A = a^*, L] - \mathbb{E}[Y \mid A = a, L] \big) \times Pr(L)
\]

For continuous covariates \(L\), we have:

\[
\widehat{\text{ATE}} = \int \big( \mathbb{E}[Y \mid A = a^*, L] - \mathbb{E}[Y \mid A = a, L] \big) dP(L)
\]

The primary function of a causal directed acyclic graph is to identify
sources of bias that may lead to an association between an exposure and
outcome in the absence of causation. These graphs visually encode
features of a causal order necessary to evaluate the assumptions of
conditional exchangeability---also called `no-unmeasured confounding',
`ignorability', and `d-separation' (explained below). Although causal
directed acyclic graphs can also be useful for addressing broader
threats and opportunities for causal inferences, it is important to
understand that causal directed acyclic graphs are designed to evaluate
the assumptions of conditional exchangeability.

Finally, without randomisation, we cannot fully ensure the no-unmeasured
confounding assumption necessary to recover the missing counterfactuals
required to consistently estimate causal effects from data
(\citeproc{ref-greifer2023}{Greifer \emph{et al.} 2023};
\citeproc{ref-stuart2015}{Stuart \emph{et al.} 2015}). Because
unmeasured confounding is almost always a concern, causal data science
workflows typically include sensitivity analyses to determine how much
unmeasured confounding would be required to compromise a study's
findings (\citeproc{ref-vanderweele2019}{VanderWeele 2019}).

\subsubsection{Summary of Part 1}\label{summary-of-part-1}

Causal data science is distinct from ordinary data science. The initial
step involves formulating a precise causal question that clearly
identifies the exposure, outcome, and population of interest. We must
then satisfy the three fundamental assumptions required for causal
inference, implicit in the ideal of a randomised experiment:

\begin{itemize}
\tightlist
\item
  \textbf{Causal consistency}: Outcomes at a specific exposure level
  must align with their counterfactual counterparts.
\item
  \textbf{Positivity}: Each exposure level must have a non-zero
  probability across all covariates.
\item
  \textbf{Conditional exchangeability}: There should be no unmeasured
  confounding, meaning treatment assignment is `ignorable' conditional
  on measured confounders.
\end{itemize}

\newpage{}

\subsection{Part 2: How Causal Directed Acyclic Graphs Clarify the
Conditional Exchangeability Assumption}\label{id-sec-2}

We introduce causal directed acyclic graphs by describing the meaning of
our symbols.

\subsubsection{Variable Naming
Conventions}\label{variable-naming-conventions}

\begin{table}

\caption{\label{tbl-terminology}Variable naming conventions}

\centering{

\terminologylocalconventionssimple

}

\end{table}%

\begin{itemize}
\item
  \textbf{\(X\)}: Denotes a random variable without reference to its
  role.
\item
  \textbf{\(A\)}: Denotes the `treatment' or `exposure'---a random
  variable. This is the variable for which we seek to understand the
  effect of intervening on it. It is the `cause.'
\item
  \textbf{\(A=a\)}: Denotes a fixed `treatment' or `exposure.' The
  random variable \(A\) is set to level \(A=a\).
\item
  \textbf{\(Y\)}: Denotes the outcome or response of an intervention. It
  is the `effect.'
\item
  \textbf{\(Y(a)\)}: Denotes the counterfactual or potential state of
  \(Y\) in response to setting the level of the treatment to a specific
  level, \(A=a\). The outcome \(Y\) as it would be observed when,
  perhaps contrary to fact, treatment \(A\) is set to level \(A=a\).
  Different conventions exist for expressing a potential or
  counterfactual outcome, such as \(Y^a\), \(Y_a\).
\item
  \textbf{\(L\)}: Denotes a measured confounder or set of confounders.
  This set, if conditioned upon, ensures that any differences between
  the potential outcomes under different levels of the treatment are the
  result of the treatment and not the result of a common cause of the
  treatment and the outcome. Mathematically, we write this independence:
\end{itemize}

\[
Y(a) \coprod A \mid L
\]

\begin{itemize}
\tightlist
\item
  \textbf{\(U\)}: Denotes an unmeasured confounder or confounders. \(U\)
  is a variable or set of variables that may affect both the treatment
  and the outcome, leading to an association in the absence of
  causality, even after conditioning on measured covariates:
\end{itemize}

\[
Y(a) \cancel{\coprod} A \mid L \quad \text{[because of unmeasured } U]
\]

\begin{itemize}
\item
  \textbf{\(F\)}: Denotes a modifier of the treatment effect. \(F\)
  alters the magnitude or direction of the effect of treatment \(A\) on
  an outcome \(Y\).
\item
  \textbf{\(M\)}: Denotes a mediator, a variable that transmits the
  effect of treatment \(A\) on an outcome \(Y\).
\item
  \textbf{\(\bar{X}\)}: Denotes a sequence of variables, for example, a
  sequence of treatments.
\item
  \textbf{\(\mathcal{R}\)}: Denotes a randomisation to treatment
  condition.
\item
  \textbf{\(\mathcal{G}\)}: Denotes a graph, here, a causal directed
  acyclic graph.
\end{itemize}

Note that investigators use a variety of different symbols. There is no
unique right way to create a causal directed acyclic graph, except that
meaning must be clear and the graph must be capable of identifying
relationships of conditional and unconditional independence between the
treatment and outcome. Although directed acyclic graphs are accessible
tools, general graphical models such as `Single World Intervention
Graphs,' which allow for the explicit representation of counterfactual
dependencies, may be preferable for investigators to estimate causal
effects under multiple interventions
(\citeproc{ref-bulbulia2024swigstime}{Bulbulia 2024b};
\citeproc{ref-richardson2013}{Richardson and Robins 2013a}).

\subsubsection{Conventions We Use in This Article to Create Causal
Directed Acyclic
Graphs}\label{conventions-we-use-in-this-article-to-create-causal-directed-acyclic-graphs}

The conventions we use to describe components of our causal graphs are
given in Table~\ref{tbl-general}.

\begin{table}

\caption{\label{tbl-general}Nodes, Edges, Conditioning Conventions.}

\centering{

\terminologygeneraldags

}

\end{table}%

\begin{itemize}
\item
  \textbf{Node}: a node or vertex represents characteristics or features
  of units within a population on a causal diagram -- that is a
  `variable.' In causal directed acyclic graphs, we draw nodes with
  respect to the \emph{target population}, which is the population for
  whom investigators seek causal inferences
  (\citeproc{ref-suzuki2020}{Suzuki \emph{et al.} 2020}). Time-indexed
  node: \(X_t\) denotes relative chronology; \(X_{\customphi{t}}\) is
  our convention for indicating that timing is assumed, perhaps
  erroneously.
\item
  \textbf{Edge without an Arrow} (\(\association\)): path of
  association, causality not asserted.
\item
  \textbf{Red Edge without an Arrow} (\(\associationred\)): confounding
  path: ignores arrows to clarify statistical dependencies.
\item
  \textbf{Arrow} (\(\rightarrowNEW\)): denotes causal relationship from
  the node at the base of the arrow (a parent) to the node at the tip of
  the arrow (a child). We typically refrain from drawing an arrow from
  treatment to outcome to avoid asserting a causal path from \(A\) to
  \(Y\) because the function of a causal directed acyclic graph is to
  evaluate whether causality can be identified for this path.
\item
  \textbf{Red Arrow} (\(\rightarrowred\)): path of non-causal
  association between the treatment and outcome. Path is associational
  and may run against arrows.
\item
  \textbf{Dashed Arrow} (\(\rightarrowdotted\)): denotes a true
  association between the treatment and outcome that becomes partially
  obscured when conditioning on a mediator, assuming \(A\) causes \(Y\).
\item
  \textbf{Dashed Red Arrow} (\(\rightarrowdottedred\)): highlights
  over-conditioning bias from conditioning on a mediator.
\item
  \textbf{Open Blue Arrow} (\(\rightarrowblue\)): Highlights effect
  modification, occurring when the treatment effect levels vary within
  covariate levels. We do not assess the causal effect of the effect
  modifier on the outcome, recognising that intervening on the effect
  modifier may be incoherent. This is an off-label convention we use to
  clarify our interest in effect modification within strata of a
  covariate when there is a true treatment effect. However, it is
  possible to replace these open blue arrows with ordinary nodes and
  explain that the edges are drawn not for identification but for
  evaluating generalisations (see
  \citeproc{ref-bulbulia2024swigstime}{Bulbulia 2024b}).
\item
  \textbf{Boxed Variable} \(\boxed{X}\): conditioning or adjustment for
  \(X\).
\item
  \textbf{Red-Boxed Variable} \(\boxedred{X}\): highlights the source of
  confounding bias from adjustment.
\item
  \textbf{Dashed Circle} \(\circledotted{X}\): no adjustment is made for
  a variable (implied for unmeasured confounders.)
\item
  \textbf{\(\mathbf{\mathcal{R}}\)} randomisation, for example,
  randomisation into treatment: \(\mathcal{R} \rightarrow A\).
\item
  \textbf{Presenting Temporal Order}: Causal directed acyclic graphs
  must be --- as truth in advertising implies--- \emph{acyclic.}
  Directed edges or arrows define ancestral relations. No descendant
  node can cause an ancestor node. Therefore causal diagrams are, by
  default, sequentially ordered.
\end{itemize}

Nevertheless, to make our causal graphs more readable, we adopt the
following conventions:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  The layout of a causal diagram is structured from left to right to
  reflect the assumed sequence of causality as it unfolds.
\item
  We often index our nodes using \(X_t\) to indicate their relative
  timing and chronological order, where \(t\) represents the time point
  or sequence in the timeline of events.
\item
  Where temporal order is uncertain or unknown, we use the notation
  \(X_{\phi t}\) to propose a temporal order that is uncertain.
\end{enumerate}

Typically, the timing of unmeasured confounders is unknown, except that
they occur before the treatments of interest; hence, we place
confounders to the left of the treatments and outcomes they are assumed
to affect, but without any time indexing.

Again, temporal order is implied by the relationship of nodes and edges.
However, explicitly representing the order in the layout of one's causal
graph often makes it easier to evaluate, and the convention representing
uncertainty is useful, particularly when the data do not ensure the
relative timing of the occurrence of the variable in a causal graph.

More generally, investigators use various conventions to convey causal
structures on graphs. Whichever convention we adopt must be clear.

Finally, note that all nodes and paths on causal graphs---including the
absence of nodes and paths---are asserted. Constructing causal diagrams
requires expert judgment of the scientific system under investigation.
It is a great power given to those who construct causal graphs, and with
great power comes great responsibility to be transparent. When
investigators are unclear or there is debate about which graphical model
fits reality, they should present multiple causal graphs. Where
identification is possible, they should perform and report multiple
analyses.

\subsubsection{How Causal Directed Acyclic Graphs Relate Observations to
Counterfactual
Interventions}\label{how-causal-directed-acyclic-graphs-relate-observations-to-counterfactual-interventions}

\paragraph{Ancestral Relations in Directed Acyclic
Graphs}\label{ancestral-relations-in-directed-acyclic-graphs}

We define the relation of `parent' and `child' on a directed acyclic
graph as follows:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Node \(A\) is a \textbf{parent} of node \(B\) if there is a directed
  edge from \(A\) to \(B\), denoted \(A \rightarrow B\).
\item
  Node \(B\) is a \textbf{child} of node \(A\) if there is a directed
  edge from \(A\) to \(B\), denoted \(A \rightarrow B\).
\end{enumerate}

It follows that a parent and child are \textbf{adjacent nodes} connected
by a directed edge.

We denote the set of all parents of a node \(B\) as \(\text{pa}(B)\).

In a directed acyclic graph, the directed edge \(A \rightarrow B\)
indicates a statistical dependency where \(A\) may provide information
about \(B\). In a causal directed acyclic graph, the directed edge
\(A \rightarrow B\) is interpreted as a causal relationship, meaning
\(A\) is a direct cause of \(B\).

We further define the relations of \textbf{ancestor} and
\textbf{descendant} on a directed acyclic graph as follows:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Node \(A\) is an \textbf{ancestor} of node \(C\) if there exists a
  directed path from \(A\) to \(C\). Formally, \(A\) is an ancestor of
  \(C\) if there exists a sequence of adjacent nodes
  \((A, B_1, B_2, \ldots, B_t, C)\) such that
  \(A \rightarrow B_1 \rightarrow B_2 \rightarrow \cdots \rightarrow B_t \rightarrow C\).
\item
  Node \(C\) is a \textbf{descendant} of node \(A\) if there exists a
  directed path from \(A\) to \(C\). Formally, \(C\) is a descendant of
  \(A\) if there exists a sequence of adjacent nodes
  \((A, B_1, B_2, \ldots, B_t, C)\) such that
  \(A \rightarrow B_1 \rightarrow B_2 \rightarrow \cdots \rightarrow B_t \rightarrow C\).
\end{enumerate}

It follows that a node can have multiple ancestors and descendants.

\paragraph{Markov Factorisation and the Local Markov
Assumption}\label{markov-factorisation-and-the-local-markov-assumption}

Pearl (\citeproc{ref-pearl2009a}{2009}) p 52 asks us to imagine the
following. Suppose we have a distribution \(P\) defined on n discrete
variables, \(X_1, X_2, \dots, X_n\). By the chain rule, the joint
distribution for variables \(X_1, X_2, \dots, X_n\) on a graph can be
decomposed into the product of \(n\) conditional distributions such that
we may obtain the following factorisation:

\[
\Pr(x_1, \dots, x_n) = \prod_{j=1}^n \Pr(x_j \mid x_1, \dots, x_{j-1})
\]

We translate nodes and edges on a graph into a set of conditional
independences that a graph implies over statistical distributions.

According to \textbf{the local Markov assumption}, given its parents in
a directed acyclic graph, a node is said to be independent of all its
non-descendants. Under this assumption, we obtain what Pearl calls
Bayesian network factorisation, such that:

\[
\Pr(x_j \mid x_1, \dots, x_{j-1}) = \Pr(x_j \mid \text{pa}_j)
\]

This factorisation greatly simplifies the calculation of the joint
distributions encoded in the directed acyclic graph (causal or
non-causal) by reducing complex factorisations of the conditional
distributions in \(\mathcal{P}\) to simpler conditional distributions in
the set \(\text{PA}_j\), represented in the structural elements of a
directed acyclic graph (\citeproc{ref-lauritzen1990}{Lauritzen \emph{et
al.} 1990}; \citeproc{ref-pearl1988}{Pearl 1988},
\citeproc{ref-pearl1995}{1995}, \citeproc{ref-pearl2009a}{2009}).

\paragraph{Minimality Assumption}\label{minimality-assumption}

The minimality assumption combines (a) the local Markov assumption with
(b) the assumption that adjacent nodes on the graph are dependent. This
is needed for causal directed acyclic graphs because the local Markov
assumption permits that adjacent nodes may be independent
(\citeproc{ref-neal2020introduction}{Neal 2020}).

\paragraph{Causal Edges Assumption}\label{causal-edges-assumption}

The \textbf{causal edges assumption} states that every parent is a
direct cause of their children. Given minimalism, the causal edges
assumption allows us to read causal dependence in directed acyclic
graphs. In Pearl's formalism, we use non-parametric structural equations
to evaluate causal assumptions using statistical distributions (refer to
Appendix E; Neal (\citeproc{ref-neal2020introduction}{2020})).

\paragraph{Compatibility Assumption}\label{compatibility-assumption}

The compatibility assumption ensures that the joint distribution of
variables aligns with the conditional independencies implied by the
causal graph. This assumption requires that the probabilistic model
conforms to the graph's structural assumptions. Demonstrating
compatibility directly from data is challenging, as it involves
verifying that all conditional independencies specified by the causal
directed acyclic graph (DAG) are present in the data. Therefore, we
typically assume compatibility rather than empirically proving it
(\citeproc{ref-pearl2009a}{Pearl 2009}).

\paragraph{Faithfulness}\label{faithfulness}

\textbf{Faithfulness} complements Markov factorisation in causal
diagrams. A causal diagram is considered faithful to a given set of data
if all the conditional independencies present in the data are accurately
depicted in the graph. Conversely, the graph is faithful if every
dependency implied by the graph's structure can be observed in the data
(\citeproc{ref-hernan2024WHATIF}{Hernan and Robins 2024}). This concept
ensures that the graphical representation of relationships between
variables aligns with empirical evidence
(\citeproc{ref-pearl2009a}{Pearl 2009}).

The distinction between \textbf{weak faithfulness} and \textbf{strong
faithfulness} addresses the nature of observed independencies:

\begin{itemize}
\item
  \textbf{Weak faithfulness} allows for the possibility that some
  observed independencies might occur by chance, such as through
  cancellation of effects among multiple causal paths.
\item
  \textbf{Strong faithfulness} assumes that all observed statistical
  relationships directly reflect the underlying causal structure, with
  no difference left to chance.
\end{itemize}

The faithfulness assumption, whether weak or strong, is not directly
testable from observed data (\citeproc{ref-pearl2009a}{Pearl 2009}).

\paragraph{The Rules of d-separation}\label{the-rules-of-d-separation}

\textbf{d-separation}: in a causal diagram, a path is `blocked' or
`d-separated' if a node along it interrupts causation. Two variables are
d-separated if all paths connecting them are blocked, making them
conditionally independent. Conversely, unblocked paths result in
`d-connected' variables, implying potential dependence
(\citeproc{ref-pearl1995}{Pearl 1995}, \citeproc{ref-pearl2009a}{2009}).
(Note that `d' stands for `directional'.)

The rules of d-separation are as follows:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Fork rule} (\(B \leftarrowNEW \boxed{A} \rightarrowNEW C\)):
  \(B\) and \(C\) are independent when conditioning on \(A\)
  (\(B \coprod C \mid A\)).
\item
  \textbf{Chain rule} (\(A \rightarrowNEW \boxed{B} \rightarrowNEW C\)):
  Conditioning on \(B\) blocks the path between \(A\) and \(C\)
  (\(A \coprod C \mid B\)).
\item
  \textbf{Collider rule}
  (\(A \rightarrowNEW \boxed{C} \leftarrowNEW B\)): \(A\) and \(B\) are
  independent until conditioning on \(C\), which introduces dependence
  (\(A \cancel{\coprod} B \mid C\)).
\end{enumerate}

Judea Pearl proved these theorems in the 1990s
(\citeproc{ref-pearl1995}{Pearl 1995}, \citeproc{ref-pearl2009a}{2009}).
It follows that:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  An open path (no variables conditioned on) is blocked only if two
  arrows point to the same node: \(A \rightarrowred C \leftarrowred B\).
  The node of common effect (here \(C\)) is called a \emph{collider}.
\item
  Conditioning on a collider does not block a path, such that
  \(A \rightarrowred \boxed{C} \leftarrowred B\) can lead to an
  association between \(A\) with \(B\) in the absence of causation.
\item
  Conditioning on a descendant of a collider does not block a path, such
  that if \(C \rightarrowNEW \boxed{C'}\), then
  \(A \rightarrowred \boxed{C'} \leftarrowred B\) is open.
\item
  If a path does not contain a collider, any variable conditioned along
  the path is blocked, such that
  \(A \rightarrowNEW \boxed{B} \rightarrowNEW C\) blocks the path from
  \(A\) to \(C\) (\citeproc{ref-hernan2023}{Hernan and Robins 2023 p.
  78}; \citeproc{ref-pearl2009a}{Pearl 2009}).
\end{enumerate}

\paragraph{Backdoor Adjustment}\label{backdoor-adjustment}

Pearl's general identification algorithm is known as the `back door
adjustment theorem' (\citeproc{ref-pearl2009a}{Pearl 2009}).

Let us shift to the general notation that we will use in the following
examples. Where \(A\) denotes the treatment, \(Y\) denotes the outcome,
and \(L\) denotes a set (or subset) of measured covariates. In a causal
directed acyclic graph (causal DAG), we say that a set of variables
\(L\) satisfies the backdoor adjustment theorem relative to the
treatment \(A\) and the outcome \(Y\) if \(L\) blocks every path between
\(A\) and \(Y\) that contains an arrow pointing into \(A\) (a backdoor
path). Formally, \(L\) must satisfy two conditions:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  No element of \(L\) is a descendant of \(A\).
\item
  \(L\) blocks all backdoor paths from \(A\) to \(Y\).
\end{enumerate}

If \(L\) satisfies these conditions, the causal effect of \(A\) on \(Y\)
can be estimated by conditioning on \(\boxed{L}\)
(\citeproc{ref-pearl2009a}{Pearl 2009}).

Pearl also proves a `front-door adjustment' criterion, which is rarely
used, refer to \hyperref[id-app-e]{Appendix E}.

\paragraph{Comment on Pearl's Do-Calculus versus the Potential Outcomes
Framework}\label{comment-on-pearls-do-calculus-versus-the-potential-outcomes-framework}

Here, we have developed counterfactual contrasts using the potential
outcomes framework. Pearl develops counterfactual contrasts using
operations on structural functionals, referred to as `do-calculus'. In
Pearl's framework, we obtain counterfactual inference by assuming that
the nodes in a causal directed acyclic graph correspond to a system of
structural equation models, refer to \hyperref[id-app-d]{Appendix D}.

Mathematically, potential outcomes and counterfactual interventions are
equivalent, such that:

\[
\Pr(Y(a) = y) \equiv \Pr(Y = y \mid do(A = a))
\]

where the left-hand side of the equivalence is the potential outcomes
framework formalisation of a potential outcome recovered by causal
consistency, and the right-hand side is given by Pearl's do-calculus,
which formalises interventional distributions on nodes of a graph that
correspond to structural causal models or, equivalently, to
non-parametric structural equation models with independent errors.

In practice, whether one uses the do-calculus or the potential outcomes
framework to interpret causal inferences is often irrelevant to
identification results. However, there are theoretically interesting
debates about edge cases. For example, Pearl's structural causal models
permit the identification of contrasts that cannot be falsified under
any experiment (\citeproc{ref-richardson2013}{Richardson and Robins
2013a}). Because advocates of non-parametric structural equation models
treat causality as primitive, they are less concerned with the
requirement for falsification (\citeproc{ref-diaz2021nonparametric}{Dƒ±ÃÅaz
\emph{et al.} 2021}, \citeproc{ref-Diaz2023}{2023};
\citeproc{ref-pearl2009a}{Pearl 2009};
\citeproc{ref-rudolph2024mediation}{Rudolph \emph{et al.} 2024})

I have presented the potential outcomes framework because it is easier
to interpret, more general, and---to my mind---clearer and more
intellectually compelling (moreover, one does not need to be a
verificationist to adopt it). However, for nearly every practical
purpose, the do-calculus and `po-calculus' (potential outcomes
framework, refer to Shpitser and Tchetgen
(\citeproc{ref-shpitser2016causal}{2016})) are both mathematically and
practically equivalent.

\subsubsection{The Five Elementary Structures of
Causality}\label{the-five-elementary-structures-of-causality}

\begin{table}

\caption{\label{tbl-fiveelementary}The five elementary structures of
causality from which all causal directed acyclic graphs can be built.}

\centering{

\terminologydirectedgraph

}

\end{table}%

Table~\ref{tbl-fiveelementary} presents five elementary structures of
causality from which all causal directed acyclic graphs are built. These
elementary structures can be assembled in different combinations to
clarify the causal relationships presented in a causal directed acyclic
graph.

\newpage{}

\subsubsection{The Five Elementary Rules for Causal
Identification}\label{the-five-elementary-rules-for-causal-identification}

Table~\ref{tbl-terminologyconfounders} describes five elementary rules
for identifying conditional independence using directed acyclic causal
diagrams.

\begin{table}

\caption{\label{tbl-terminologyconfounders}Five elementary rules for
causal identification.}

\centering{

\terminologyelconfounders

}

\end{table}%

There are no shortcuts to reasoning about causality. Each causal
question must be asked in the context of a specific scientific question,
and each causal graph must be built under the best lights of domain
expertise. However, the following five elementary rules for confounding
control are implied by the theorems that underpin causal directed
acyclic graphs. They may be a useful start for evaluating the prospects
for causal identification across a broad range of settings.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Ensure That Treatments Precede Outcomes}: This rule is a
  logical consequence of our assumption that causality follows the arrow
  of time and that a causal directed acyclic graph is faithful to this
  ordering. However, the assumption that treatments precede outcomes may
  be easily violated where investigators cannot ensure the relative
  timing of events from their data.
\end{enumerate}

Note that this assumption does not raise concerns in settings where past
outcomes may affect future treatments. Indeed, an effective strategy for
confounding control in such settings is to condition on past outcomes,
and where relevant, on past treatments as well. For example, if we wish
to identify the causal effect of \(A_1\) on \(Y_2\), and
repeated-measures time series data are available, it may be useful to
condition such that
\(\boxed{A_{-1}} \to \boxed{Y_0} \to A_1 \rightarrow Y_2\). Critically,
the relations of variables must be arranged sequentially without cycles.

To estimate a causal effect of \(Y\) on \(A\), we would focus on:
\(\boxed{Y_{-1}} \to \boxed{A_0} \to Y_1 \rightarrow A_2\). Departing
from convention, here \(Y\) denotes the treatment and \(A\) denotes the
outcome. Causal directed acyclic graphs must be acyclic. Yet most
processes in nature include feedback loops. However, there is no
contradiction as long as we represent these loops as sequential events.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\item
  \textbf{Condition on Common Causes or Their Proxies}: This rule
  applies to settings in which the treatment \(A\) and the outcome \(Y\)
  share common causes. By conditioning on these common causes, we block
  the open backdoor paths that could introduce bias into our causal
  estimates. Controlling for these common causes (or their proxies)
  helps to isolate the specific effect of \(A\) on \(Y\). Note that we
  do not draw a path from \(A \to Y\) in this context because it
  represents an interventional distribution. In a causal directed
  acyclic graph, conditioning does not occur on interventional
  distributions. We do not box \(A\) and \(Y\).
\item
  \textbf{Do Not Condition on a Mediator when Estimating Total Effects}:
  This rule applies to settings in which the variable \(L\) is a
  mediator of \(A \to Y\). Recall Pearl's backdoor path criterion
  requires that we do not condition on a descendant of the treatment.
  Here, conditioning on \(L\) violates the backdoor path criterion,
  risking bias for a total causal effect estimate. We must not condition
  on a mediator if we are interested in total effect estimates. Note we
  draw the path from \(A \to Y\) to underscore that this specific
  overconditioning threat occurs in the presence of a true treatment
  effect. Over-conditioning bias can operate in the absence of a true
  treatment effect. This is important because conditioning on a mediator
  might create associations without causation. In many settings,
  ensuring accuracy in the relative timing of events in our data will
  prevent the self-inflicted injury of conditioning on a common effect
  of the treatment.
\item
  \textbf{Do Not Condition on a Collider}: This rule applies to settings
  in which \(L\) is a common effect of \(A\) and \(Y\). Conditioning on
  a collider may invoke a spurious association. Again, the backdoor path
  criterion requires that we do not condition on a descendant of the
  treatment. We would not be tempted to condition on \(L\) if we knew
  that it was an effect of \(A\). In many settings, ensuring accuracy in
  the relative timing of events in our data will prevent the
  self-inflicted injury of conditioning on a common effect of the
  treatment and outcome.
\item
  \textbf{Proxy Rule: Conditioning on a Descendant Is Akin to
  Conditioning on Its Parent}: This rule applies to settings where
  \(L'\) is an effect from another variable \(L\). The graph considers
  when \(L'\) is downstream of a collider. Here again, in many settings,
  ensuring accuracy in the relative timing of events in our data will
  prevent the self-inflicted injury of conditioning on a common effect
  of the treatment and outcome.
\end{enumerate}

\subsubsection{Summary Part 2}\label{summary-part-2}

We use causal directed acyclic graphs to represent and evaluate
structural sources of bias. We do not use these causal graphs to
represent the entirety of the causal system in which we are interested,
but rather only those features necessary to evaluate conditional
exchangeability, or equivalently to evaluate d-separation. Moreover,
causal directed acyclic graphs should not be confused with the
structural equation models employed in the statistical structural
equation modelling traditions (see also Rohrer \emph{et al.}
(\citeproc{ref-rohrer2022PATH}{2022})). Although Pearl's formalism is
built upon `Non-Parametric Structural Equation Models,' the term
`Structural Equation Model' can be misleading. Causal graphs are
structural models that represent assumptions about reality, they are not
statistical models. We use structural causal models to evaluate
identifiability. We create causal graphs before we embark on statistical
modelling. They aim to clarify how to write statistical models by
elucidating which variables we must include in our statistical models
and, equally important, which variables we must exclude to avoid
invalidating our causal inferences. All causal graphs are grounded in
our assumptions about the structures of causation. Although it is
sometimes possible to use causal diagrams for causal discovery, their
purpose is to clarify d-separation.

This distinction between structural and statistical models is
fundamental because absent clearly defined causal contrasts and
carefully evaluated assumptions about structural sources of bias, the
statistical structural equation modelling tradition offers no guarantees
that the coefficients investigators recover are interpretable.
Misunderstanding this difference between structural and statistical
models has led to considerable confusion across the human sciences
(\citeproc{ref-bulbulia2022}{Bulbulia 2022};
\citeproc{ref-vanderweele2015}{VanderWeele 2015};
\citeproc{ref-vanderweele2022}{VanderWeele 2022};
\citeproc{ref-vanderweele2022b}{\textbf{vanderweele2022b?}}).

\subsection{Part 3. How Causal Directed Acyclic Graphs Clarify the
Importance of Timing of Events Recorded in Data}\label{id-sec-3}

\begin{table}

\caption{\label{tbl-elementary-chronological-hyg}}

\centering{

\captionsetup{labelsep=none}

\terminologychronologicalhygeine

}

\end{table}%

As hinted at in the previous section, the five elementary rules of
confounding control reveal the importance of ensuring accurate timing in
the occurrence of the variables whose structural features a causal
directed acyclic graph encodes. We begin by considering seven examples
of confounding problems resolved when such accuracy is ensured.

The first seven case studies illustrate the focus that causal directed
acyclic graphs bring to the fundamental imperative to ensure accurate
timing in the chronology of events recorded in data. These illustrations
refer to causal graphs in Table~\ref{tbl-elementary-chronological-hyg}.
The symbol \(\mathcal{G}\) denotes a graph. We use the convention:
\(\mathcal{G}_{\{\text{row}\}\{.\}\{1 = \text{problem}; 2 = \text{solution}\}}\)
to identify a causal directed acyclic graph in the table. The context
will also make it clear to understand which graph we are discussing.

\subsubsection{Example 1: Reverse
Causation}\label{example-1-reverse-causation}

Table~\ref{tbl-elementary-chronological-hyg} \(\mathcal{G}_{3.1}\)
illustrates bias from reverse causation. Suppose we are interested in
the causal effect of marriage on well-being. If we observe that married
people are happier than unmarried people, we might erroneously infer
that marriage causes happiness, or happiness causes marriage (refer to
McElreath (\citeproc{ref-mcelreath2020}{2020})).

Table~\ref{tbl-elementary-chronological-hyg} \(\mathcal{G}_{3.2}\)
clarifies a response. Ensure that the treatment is observed before the
outcome is observed. Note further that the treatment, in this case, is
not clearly specified because `marriage' is unclear. There are at least
four causal contrasts we might consider when thinking of `marriage',
namely:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \(Y(0, 0)\): The potential outcome when there is no marriage.
\item
  \(Y(0, 1)\): The potential outcome when there is marriage.
\item
  \(Y(1, 0)\): The potential outcome when there is divorce.
\item
  \(Y(1, 1)\): The potential outcome from marriage prevalence.
\end{enumerate}

Each of these outcomes allows for a specific contrast. There are
\(\binom{4}{2}\) unique contrasts. Which of the six unique contrasts do
we wish to consider? It is important that we can order our data in time.
However, even before thinking about data, we must recognise that the
question `What is the causal effect of marriage on happiness?' is
ill-defined. This question does not uniquely indicate which of the six
causal contrasts to consider. The first step is to state this question.
(For a worked example refer to Bulbulia
(\citeproc{ref-bulbulia2024swigstime}{2024b})).

\subsubsection{Example 2: Confounding by Common
Cause}\label{example-2-confounding-by-common-cause}

Table~\ref{tbl-elementary-chronological-hyg} \(\mathcal{G}_{3.2}\)
illustrates confounding by common cause. Suppose there is a common
cause, \(L\), of the treatment, \(A\), and outcome, \(Y\). In this
setting, \(L\) may create a statistical association between \(A\) and
\(Y\), implying causation in its absence. Most human scientists will be
familiar with the threat to inference in this setting: a `third
variable' leads to association without causation.

Consider an example where smoking, \(L\), is a common cause of both
yellow fingers, \(A\), and cancer, \(Y\). Here, \(A\) and \(Y\) may show
an association without causation. If investigators were to scrub the
hands of smokers, this would not affect cancer rates.

Table~\ref{tbl-elementary-chronological-hyg} \(\mathcal{G}_{3.2}\)
clarifies a response. Condition on the common cause, smoking. Within
strata of smokers and non-smokers, there will be no association between
yellow fingers and cancer.

\subsubsection{Example 3: Mediator Bias}\label{example-3-mediator-bias}

Table~\ref{tbl-elementary-chronological-hyg} \(\mathcal{G}_{3.1}\)
illustrates mediator bias. Conditioning on the effect of a treatment in
this graph partially blocks the flow of information from treatment to
outcome, biasing the total effect estimate.

Suppose investigators are interested in whether cultural `beliefs in big
Gods' \(A\) affect social complexity \(Y\). Suppose that `economic
trade', \(L\), is both a common cause of the treatment and outcome. To
address confounding by a common cause, we must condition on economic
trade. However, timing matters. If we condition on measurements that
reflect economic trade after the emergence of beliefs in big Gods, we
may bias our total effect estimate.

Table~\ref{tbl-elementary-chronological-hyg} \(\mathcal{G}_{3.2}\)
clarifies a response. Ensure that measurements of economic trade are
obtained for cultural histories before big Gods arise. Do not condition
on post-treatment instances of economic trade.

\subsubsection{Example 4: Collider Bias}\label{example-4-collider-bias}

Table~\ref{tbl-elementary-chronological-hyg} \(\mathcal{G}_{4.1}\)
illustrates collider bias. Imagine a randomised experiment investigating
the effects of different settings on individuals' self-rated health. In
this study, participants are assigned to either civic settings (e.g.,
community centres) or religious settings (e.g., places of worship). The
treatment of interest, \(A\), is the type of setting, and the outcome,
\(Y\), is self-rated health. Suppose there is no effect of setting on
self-rated health. However, suppose both setting and rated health
independently influence a third variable: cooperativeness. Specifically,
imagine religious settings encourage cooperative behaviour, and at the
same time, individuals with better self-rated health are more likely to
engage cooperatively. Now suppose the investigators decide to condition
on cooperativeness, which in reality is the common effect of \(A\) and
the outcome \(Y\). Their rationale might be to study the effects of
setting on health among those who are more cooperative or perhaps to
`control for' cooperation in the health effects of religious settings.
By introducing such `control', the investigators would inadvertently
introduce collider bias, because the control variable is a common effect
of the treatment and the outcome.

Table~\ref{tbl-elementary-chronological-hyg} \(\mathcal{G}_{4.2}\)
clarifies a response. If the worry is that cooperativeness is a
confounder, ensure that cooperativeness is measured before the
initiation of exposure to religious settings.

\subsubsection{Example 5: Collider Proxy
Bias}\label{example-5-collider-proxy-bias}

Table~\ref{tbl-elementary-chronological-hyg} \(\mathcal{G}_{5.1}\)
illustrates bias from conditioning on the proxy of a collider. Consider
again the scenario described in \(\sec 3.4\), but instead of controlling
for cooperativeness, investigators control for charitable donations, a
proxy for cooperativeness. Here, because the control variable is a
descendant of a collider, conditioning on the proxy is akin to
conditioning on the collider.

Table~\ref{tbl-elementary-chronological-hyg} \(\mathcal{G}_{5.2}\)
clarifies a response. Do not condition on charitable donations, an
effect of treatment.

\subsubsection{Example 6: Post-Treatment Collider Stratification
Bias}\label{example-6-post-treatment-collider-stratification-bias}

Table~\ref{tbl-elementary-chronological-hyg} \(\mathcal{G}_{6.1}\)
illustrates post-treatment collider stratification bias. Consider again
an experiment investigating the effect of religious service on
self-rated health. Suppose we measure `religiosity' after the
experiment, along with other demographic data. Suppose further that
religious setting affects religiosity, as does an unmeasured confounder,
such as childhood deprivation. Suppose that childhood deprivation
affects self-reported health. Although our experiment ensured
randomisation of the treatment and thus ensured no unmeasured common
causes of the treatment and outcome, conditioning on the post-treatment
variable `religiosity' opens a back-door path from the treatment to the
outcome. This path is
\(A_0 \associationred L_1 \associationred U \associationred Y_2\). We
introduced confounding into our randomised experiment.

Table~\ref{tbl-elementary-chronological-hyg} \(\mathcal{G}_{6.2}\)
clarifies a response. Do not condition on a variable that the treatment
may affect (refer to Cole \emph{et al.} (\citeproc{ref-cole2010}{2010})
for a discussion of theoretical examples; refer to Montgomery \emph{et
al.} (\citeproc{ref-montgomery2018}{2018}) for evidence of the
widespread prevalence of post-treatment adjustment in published
political science experiments).

\subsubsection{Example 7: Conditioning on Past Treatments and Past
Outcomes to Control for Unmeasured
Confounders}\label{example-7-conditioning-on-past-treatments-and-past-outcomes-to-control-for-unmeasured-confounders}

Table~\ref{tbl-elementary-chronological-hyg} \(\mathcal{G}_{7.1}\)
illustrates the threat of unmeasured confounding. In `real world'
studies, this threat is ubiquitous.

Table~\ref{tbl-elementary-chronological-hyg} \(\mathcal{G}_{7.2}\)
clarifies a response. With at least three repeated measurements,
investigators may greatly reduce unmeasured confounding by controlling
for past measurements of the treatment as well as past measurements of
the outcome. With such control, any unmeasured confounder must be
orthogonal to its effects at baseline (refer to VanderWeele \emph{et
al.} (\citeproc{ref-vanderweele2020}{2020})). Moreover, controlling for
past treatments allows investigators to estimate an incident exposure
effect over a prevalence exposure effect. The prevalence exposure effect
describes the effect of current or ongoing exposures (treatments) on
outcomes. This effect risks leading to erroneous conclusions. The
incident exposure effect targets initiation into treatment, which is
typically the effect we obtain from experiments. To obtain the incident
exposure effect, we generally require that events in the data can be
accurately classified into at least three relative time intervals (refer
to Hern√°n \emph{et al.} (\citeproc{ref-hernuxe1n2016}{2016a}); Danaei
\emph{et al.} (\citeproc{ref-danaei2012}{2012}); VanderWeele \emph{et
al.} (\citeproc{ref-vanderweele2020}{2020}); Bulbulia
(\citeproc{ref-bulbulia2022}{2022})).

\subsection{Summary Part 3}\label{summary-part-3}

The examples in Part 3 reveal that the ability to order treatments,
outcomes, and their common causes on a timeline is necessary for
obtaining valid inferences. When timing is ensured, we can use Pearl's
backdoor path adjustment algorithm to evaluate identification, subject
to the assumptions encoded in a causal directed acyclic graph.

\newpage{}

\subsection{Part 4 How Causal Directed Acyclic Graphs Clarify The
Insufficiency of Ensuring The Timing of Events Recorded in Data For
Causal Identification}\label{id-sec-4}

We next present a series of illustrations that clarify ordering
variables in time is insufficient insurance against confounding biases.
All graphs in \hyperref[id-sec-4]{Part 4} refer to
Table~\ref{tbl-chronology-notenough}.

\begin{table}

\caption{\label{tbl-chronology-notenough}Common confounding scenarios in
which chronology is not enough.}

\centering{

\terminologychronologicalhygeineNOTENOUGH

}

\end{table}%

\subsubsection{Example 1: M-bias}\label{example-1-m-bias}

Table~\ref{tbl-chronology-notenough} \(\mathcal{G}_{1.1}\) illustrates
the threat of over-conditioning on pre-treatment variables -- `M-bias'.
Suppose we want to estimate the effect of religious service attendance
on charitable donations. We obtain time-series data and include a rich
set of covariates, including baseline measures of religious service and
charity. Suppose there is no treatment effect. Suppose further that we
condition on loyalty measures, yet loyalty neither affects religious
service attendance nor charitable giving. However, imagine that loyalty
is affected by two unmeasured confounders. Furthermore, imagine that
one's childhood upbringing (an unmeasured variable) affects both loyalty
and inclinations to religious service but not charitable giving. \(U_A\)
denotes this unmeasured confounder. Furthermore, suppose wealth affects
loyalty and charitable giving but not religious service. \(U_Y\) denotes
this unmeasured confounder. In this setting, because loyalty is a
collider of the unmeasured confounders, conditioning on loyalty opens a
path between treatment and outcome. This path is
\(A \associationred U_A \associationred U_Y \associationred Y\).

Table~\ref{tbl-chronology-notenough} \(\mathcal{G}_{1.2}\) clarifies a
response. If we are confident that \(\mathcal{G}_{1.1}\) describes the
structural features of confounding, we should not condition on loyalty.

\subsubsection{Example 2: M-bias Where the Pre-treatment Collider is a
Confounder}\label{example-2-m-bias-where-the-pre-treatment-collider-is-a-confounder}

Table~\ref{tbl-chronology-notenough} \(\mathcal{G}_{2.1}\) illustrates
the threat of incorrigible confounding. Imagine the scenario in
\(\mathcal{G}_{1.1}\) and \(\mathcal{G}_{1.2}\) but with one change.
Loyalty is indeed a common cause of religious service attendance (the
treatment) and charitable giving (the outcome). If we do not condition
on loyalty, we have unmeasured confounding. This is bad. If we condition
on loyalty, as we have just considered, we also have unmeasured
confounding. This is also bad.

Table~\ref{tbl-chronology-notenough} \(\mathcal{G}_{2.2}\) clarifies a
response. Suppose that although we have not measured wealth, we have
measured a surrogate of wealth, say neighbourhood deprivation.
Conditioning on this surrogate is akin to conditioning on the unmeasured
confounder; we should adjust for neighbourhood deprivation.

\subsubsection{Example 3: Opportunities for Post-treatment Conditioning
for Confounder
Control}\label{example-3-opportunities-for-post-treatment-conditioning-for-confounder-control}

Table~\ref{tbl-chronology-notenough} \(\mathcal{G}_{3.1}\) illustrates
the threat of unmeasured confounding. Suppose we are interested in
whether curiosity affects educational attainment. The effect might be
unclear. Curiosity might increase attention but it might also increase
distraction. Consider an unmeasured genetic factor \(U\) that influences
both curiosity and educational attainment, say anxiety. Suppose we do
not have early childhood measures of anxiety in our dataset. We have
unmeasured confounding. This is bad.

Table~\ref{tbl-chronology-notenough} \(\mathcal{G}_{3.2}\) clarifies a
response. Suppose \(U\) also affects melanin production in hair
follicles. If grey hair is an effect of a cause of curiosity, and if
grey hair cannot be an effect of educational attainment, we could
diminish unmeasured confounding by adjusting for grey hair in adulthood.
This example illustrates how conditioning on a variable that occurs
after the treatment has occurred, or even after the outcome has been
observed, may prove useful for confounding control. When considering
adjustment strategies, it is sometimes useful to consider adjustment on
post-treatment confounders.

\subsubsection{Example 4: Residual Confounding After Conditioning on
Past Treatments and Past
Outcomes}\label{example-4-residual-confounding-after-conditioning-on-past-treatments-and-past-outcomes}

Table~\ref{tbl-chronology-notenough} \(\mathcal{G}_{4}\) illustrates the
threat of confounding even after adjusting for baseline measures of the
treatment and the outcome. Imagine that childhood deprivation, an
unmeasured variable, affects both religious service attendance and
charitable giving. Despite adjusting for religious status and charitable
giving at baseline, childhood deprivation might influence changes in one
or both variables over time. This can create a longitudinal association
between religious service attendance and charitable giving without a
causal relationship. Strictly speaking, the causal effect cannot be
identified. We may estimate an effect and perform sensitivity analyses
to check how much unmeasured confounding would be required to explain
way an effect (refer to Linden \emph{et al.}
(\citeproc{ref-linden2020EVALUE}{2020})); we may also seek negative
controls (refer to Hernan and Robins
(\citeproc{ref-hernan2024WHATIF}{2024})).

\subsubsection{Example 5: Intermediary Confounding in Causal
Mediation}\label{example-5-intermediary-confounding-in-causal-mediation}

Table~\ref{tbl-chronology-notenough} \(\mathcal{G}_5\) illustrates the
threat of treatment confounding in causal mediation. Imagine that the
treatment is randomised; there is no treatment-outcome confounding. Nor
is there treatment-mediator confounding. \(\mathcal{R} \to A\) ensures
that backdoor paths from the treatment to the outcome are closed. We may
obtain biased results despite randomisation because the mediator is not
randomised. Suppose we are interested in whether the effects of COVID-19
lockdowns on psychological distress were mediated by levels of
satisfaction with the government. Suppose that assignment to COVID-19
lockdowns was random, and that time series data taken before COVID-19
provides comparable population-level contrasts. Despite random
assignment to treatment, assume that there are variables that may affect
both satisfaction with the government and psychological distress. For
example, job security or relationship satisfaction might plausibly
function as common causes of the mediator (government satisfaction) and
the outcome (psychological distress). To obtain valid inference for the
mediator-outcome path, we must control for these common causes.

Table~\ref{tbl-chronology-notenough} \(\mathcal{G}_5\) reveals the
difficulty in decomposing the total effect of COVID-19 on psychological
distress into the direct effect of COVID-19 that is not mediated by
satisfaction with the government and the indirect effect that is
mediated. Let us assume that confounders of the mediator-outcome path
are themselves potentially affected by the treatment. In this example,
imagine that COVID-19 lockdowns affect relationship satisfaction because
couples are trapped in ``captivity.'' Imagine further that COVID-19
lockdowns affect job security, which is reasonable if one owns a
street-facing business. If we adjust for these intermediary variables
along the path between the treatment and outcome, we will partially
block the treatment-mediator path. This means that we will not be able
to obtain a natural indirect effect estimate that decomposes the effect
of the treatment into that part that goes through the intermediary path
\(A \associationred V \associationred M \associationred Y\) and that
part that goes through the mediated path independently of \(V\), namely
\(A \associationred V \associationred M \associationred Y\). However, it
may be possible to estimate controlled direct effects---that is, direct
effects when the mediator is fixed to different levels
(\citeproc{ref-greenland1999}{Greenland \emph{et al.} 1999};
\citeproc{ref-shpitser2022multivariate}{Shpitser \emph{et al.} 2022};
\citeproc{ref-vanderweele2015}{VanderWeele 2015}), or to obtain
approximations of the natural direct effect (refer to Dƒ±ÃÅaz \emph{et al.}
(\citeproc{ref-Diaz2023}{2023}); Stensrud \emph{et al.}
(\citeproc{ref-stensrud2023conditional}{2023}); Bulbulia
(\citeproc{ref-bulbulia2024swigstime}{2024b}){]}.

\subsubsection{Example 6: Treatment Confounder Feedback in Sequential
Treatments}\label{example-6-treatment-confounder-feedback-in-sequential-treatments}

Table~\ref{tbl-chronology-notenough} \(\mathcal{G}_6\) illustrates the
threat of treatment confounder feedback in sequential treatment regimes.
Suppose we are interested in whether beliefs in big Gods affect social
complexity. Suppose that beliefs in big Gods affect economic trade and
that economic trade may affect beliefs in big Gods and social
complexity. Suppose the historical record is fragmented such that there
are unmeasured variables that affect both trade and social complexity.
Even if these unmeasured variables do not affect the treatment,
conditioning on the \(L\) (a confounder) and sequential treatment opens
a backdoor path
\(A \associationred L \associationred U \associationred Y\). We have
confounding.

Table~\ref{tbl-chronology-notenough} \(\mathcal{G}_6\) reveals the
difficulty of sequentially estimating causal effects. To estimate an
effect requires special estimators under the assumption of sequential
randomisation for fixed treatments and the assumption of strong
sequential randomisation for time-varying treatments---that is, for
treatments whose levels depend on past treatments and confounders (refer
to Robins (\citeproc{ref-robins1986}{1986}); Hern√°n \emph{et al.}
(\citeproc{ref-hernan2004STRUCTURAL}{2004}); Van Der Laan and Rose
(\citeproc{ref-vanderlaan2011}{2011}); Van Der Laan and Rose
(\citeproc{ref-vanderlaan2018}{2018}); Haneuse and Rotnitzky
(\citeproc{ref-haneuse2013estimation}{2013}); Young \emph{et al.}
(\citeproc{ref-young2014identification}{2014}); Rotnitzky \emph{et al.}
(\citeproc{ref-rotnitzky2017multiply}{2017}); Richardson and Robins
(\citeproc{ref-richardson2013}{2013a}); Dƒ±ÃÅaz \emph{et al.}
(\citeproc{ref-diaz2021nonparametric}{2021}); Williams and D√≠az
(\citeproc{ref-williams2021}{2021}); Hoffman \emph{et al.}
(\citeproc{ref-hoffman2023}{2023})).

Importantly, we have six potential contrasts for the two sequential
treatments: beliefs in big Gods at both time points vs.~beliefs in big
Gods at neither time point; beliefs in big Gods first, then lost
vs.~never believing in big Gods at both:

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.1333}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.5467}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3200}}@{}}
\caption{Table outlines four fixed treatment regimens and six causal
contrasts in time-series data where treatments vary over
time.}\label{tbl-regimens}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Type
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Description
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Counterfactual Outcome
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Type
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Description
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Counterfactual Outcome
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Regime & Always believe in big Gods & \(Y(1,1)\) \\
Regime & Never believe in big Gods & \(Y(0,0)\) \\
Regime & Believe once first, then scepticism & \(Y(1,0)\) \\
Regime & Start with scepticism, then believe & \(Y(0,1)\) \\
Contrast & Always believe vs.~Never believe & \(E[Y(1,1) - Y(0,0)]\) \\
Contrast & Always believe vs.~Treat once first &
\(E[Y(1,1) - Y(1,0)]\) \\
Contrast & Always believe vs.~Treat once second &
\(E[Y(1,1) - Y(0,1)]\) \\
Contrast & Never believe vs.~Treat once first &
\(E[Y(0,0) - Y(1,0)]\) \\
Contrast & Never believe vs.~Treat once second &
\(E[Y(0,0) - Y(0,1)]\) \\
Contrast & Believe once first vs.~Believe once second &
\(E[Y(1,0) - Y(0,1)]\) \\
\end{longtable}

We can compute six causal contrasts for these four fixed regimens, as
shown in Table~\ref{tbl-regimens}.

A limitation of directed acyclic causal diagrams is that we do not
project factorisations of the counterfactual contrasts onto the graphs
themselves. To evaluate counterfactual identification, using Single
World Intervention Graphs (refer to Robins and Richardson
(\citeproc{ref-robins2010alternative}{2010}); Richardson and Robins
(\citeproc{ref-richardson2013swigsprimer}{2013b}); Richardson and Robins
(\citeproc{ref-richardson2023potential}{2023})) can be helpful. We
consider intermediate confounding in more detail in Bulbulia
(\citeproc{ref-bulbulia2024swigstime}{2024b}).

\subsubsection{Example 7: Collider Stratification Bias in Sequential
Treatments}\label{example-7-collider-stratification-bias-in-sequential-treatments}

Table~\ref{tbl-chronology-notenough} \(\mathcal{G}_7\) illustrates the
threat of confounding bias in sequential treatments even without
treatment-confounder feedback. Assume the setting is \(\mathcal{G}_6\)
with two differences. First, assume that the treatment, beliefs in big
Gods, does not affect trade networks. However, assume that an unmeasured
confounder affects both the beliefs in big Gods and the confounder,
trade networks. Such a confounder might be openness to outsiders, a
feature of ancient cultures for which no clear measures are available.
We need not imagine that treatment affects future states of confounders
for time-varying confounding. It would be sufficient to induce bias for
an unmeasured confounder to affect the treatment and the confounder, in
the presence of another confounder that affects both the confounder and
the outcome.

Table~\ref{tbl-chronology-notenough} \(\mathcal{G}_7\) reveals the
challenges of sequentially estimating causal effects. Yet again, to
estimate causal effects here requires special estimators, under the
assumption of sequential randomisation for fixed treatments, and the
assumption of strong sequential randomisation for time-varying
treatments (refer to Robins (\citeproc{ref-robins1986}{1986}); Hern√°n
\emph{et al.} (\citeproc{ref-hernan2004STRUCTURAL}{2004}); Van Der Laan
and Rose (\citeproc{ref-vanderlaan2011}{2011}); Van Der Laan and Rose
(\citeproc{ref-vanderlaan2018}{2018}); Haneuse and Rotnitzky
(\citeproc{ref-haneuse2013estimation}{2013}); Young \emph{et al.}
(\citeproc{ref-young2014identification}{2014}); Rotnitzky \emph{et al.}
(\citeproc{ref-rotnitzky2017multiply}{2017}); Richardson and Robins
(\citeproc{ref-richardson2013}{2013a}); Dƒ±ÃÅaz \emph{et al.}
(\citeproc{ref-diaz2021nonparametric}{2021}); Williams and D√≠az
(\citeproc{ref-williams2021}{2021}); Hoffman \emph{et al.}
(\citeproc{ref-hoffman2023}{2023})). We note again that a specific
causal contrast must be stated, and we must ask, for which cultures do
causal effects generalise.

Readers should be aware that merely applying currently popular tools of
time-series data analysis---multi-level models and structural equation
models---will not overcome the threats of confounding in sequential
treatments. Applying models to data will not recover consistent causal
effect estimates. Again, space constraints prevent us from discussing
statistical estimands and estimation here (refer to Bulbulia
(\citeproc{ref-bulbulia2024PRACTICAL}{2024a})).

\subsubsection{Summary Part 4}\label{summary-part-4}

Directed acyclic graphs reveal that ensuring the timing of events in
one's data does not ensure identification. In some cases, certain
mediated effects cannot be identified by any data, as we discussed in
the context of mediation analysis with intermediate confounding.
However, across the human sciences, we often apply statistical models to
data and interpret the outputs as meaningful. Causal diagrams
demonstrate that standard approaches, no matter how sophisticated, are
often hazardous and can lead to misleading conclusions.

\subsection{Part 5. Creating Causal Diagrams: Pitfalls and
Tips}\label{id-sec-5}

The primary interest of causal diagrams is to address
\textbf{identification problems}. Pearl's backdoor adjustment theorem
proves that if we adopt an adjustment set such that \(A\) and \(Y\) are
d-separated, and furthermore do not condition on a variable along the
path from \(A\) to \(Y\), association is causation.

Here is how investigators may construct safe and effective directed
acyclic graphs.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Clarify the causal question and target population}
\end{enumerate}

An identification strategy is relative to the question at hand. The
adjustment criteria for estimating an effect of A on Y will generally
differ from those for estimating an effect of \(Y\) on \(A\). Before
attempting to draw any causal diagram, state the problem your diagram
addresses and the population to whom it applies. Additionally, when
adopting a specific identification strategy for a treatment or set of
treatments, the coefficients we obtain for the other variables in the
model will often be biased causal effect estimates for those variables.

Moreover, the coefficients obtained from statistical models developed to
estimate causal effects will typically not have a marginal
interpretation (\citeproc{ref-chatton2020}{Chatton \emph{et al.} 2020};
\citeproc{ref-cole2008}{Cole and Hern√°n 2008};
\citeproc{ref-vanderweele2009a}{VanderWeele 2009b}). This structural
implication has wide-ranging consequences for scientific reporting. For
example, if regression coefficients are reported at all, they should
come with clear warnings against interpreting them as having any causal
meaning or interpretation (refer to Westreich and Greenland
(\citeproc{ref-westreich2013}{2013}); McElreath
(\citeproc{ref-mcelreath2020}{2020}); Bulbulia
(\citeproc{ref-bulbulia2023}{2023})). Powerful machine learning
algorithms treat these parameters as a nuisance, and in many cases,
coefficients cannot be obtained. Referees of human science journals need
to be alerted to this fact and retrained accordingly.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\item
  \textbf{Consider whether the three fundamental assumptions for causal
  inference may be satisfied}

  Merely possessing data, even if the data are richly detailed
  time-series data, does not mean our causal questions will find
  answers. Along with identification, we must also consider the causal
  consistency and positivity assumptions, refer to
  \hyperref[id-sec-1]{Part 1}.
\item
  \textbf{Clarify the meanings of symbols and conventions}

  It is fair to say that the state of terminology in causal inference is
  a dog's breakfast. Meanings and conventions vary not only for
  terminology but also for causal graphical conventions. For example,
  whereas we have denoted unmeasured confounders using the variable
  \(U\), those who follow Pearl will often draw a bi-directional arrow.
  Although investigators will have their preferences, there is generally
  little substantive interest in one's conventions, only that they are
  made clear, frequently repeated (as I have done for each graph table),
  and applied correctly.
\item
  \textbf{Include all common causes of the treatment and outcome}

  Once we have stated our causal question, we are ready to create a
  draft of our causal graph. This graph should incorporate the most
  recent common causes (parents) of both the treatment and the outcome,
  or, where measures are not available, measures for available proxies.

  Where possible, aggregate functionally similar common causes and label
  them with a single node. For example, all baseline confounders that
  are a common cause of the treatment and outcome might be labelled
  \(L_0\). Time-varying confounders might be labelled
  \(L_1, L_2, \dots L_{\tau -1}\), where \(Y_\tau\) is the outcome at
  the end of study.

  How do we determine whether a variable is a common cause of the
  treatment and the outcome? We might not always be in a position to
  know. Remember that a causal directed acyclic graph (DAG) asserts
  structural assumptions. Expertise in crafting causal diagrams does not
  guarantee expertise in encoding plausible structural assumptions.
  Therefore, creating and revising causal DAGs should involve topic
  specialists. Additionally, the decision-making processes should be
  thoroughly documented in published research, even if this
  documentation is placed in supplementary materials.
\item
  \textbf{Consider potential unmeasured confounders}

  We leverage domain expertise not only to identify measured sources of
  confounding but also---and perhaps most importantly---to identify
  potential unmeasured confounders. These should be included in our
  causal diagrams. Because we cannot guard against all unmeasured
  confounding, it is essential to perform sensitivity analyses and to
  consider developing multiple analytic strategies to provide multiple
  channels of evidence for the question at hand, such as instrumental
  variables, negative control treatments, negative control outcomes, and
  mendelian randomisation (refer to
  \citeproc{ref-angrist2009mostly}{Angrist and Pischke 2009};
  \citeproc{ref-smith2022combining}{Smith \emph{et al.} 2022}).
\item
  \textbf{Ensure the causal directed acyclic graph is acyclic}

  Although not strictly necessary, it may be useful to annotate the
  temporal sequence of events using subscripts (e.g., \(L_0\), \(A_1\),
  \(Y_2\)), as we have done here. Moreover, spatially ordering your
  directed acyclic graph to reflect the progression of causality in
  time---either left-to-right or top-to-bottom---will often enhance the
  graph's comprehensibility.
\item
  \textbf{Represent paths structurally, not parametrically}

  Whether a path is linear is unimportant for causal
  identification---and remember causal diagrams are tools for causal
  identification. Focus on whether paths exist, not their functional
  form (linear, non-linear, etc.).

  Consider a subway map of Paris. We do not include all the streets on
  this map, all noteworthy sites, or a detailed overview of the holdings
  by room in the Louvre. We use other maps for these purposes. Remember,
  the primary function of a causal diagram is to ensure d-separation. If
  a causal diagram is to be useful, it must remove almost every detail
  about the reality it assumes.
\item
  \textbf{Minimise paths to those necessary for addressing an
  identification problem}

  Reduce clutter; only include paths critical for a specific question
  (e.g., backdoor paths, mediators). For example, in
  Table~\ref{tbl-chronology-notenough} \(\mathcal{G} 6\) and
  Table~\ref{tbl-chronology-notenough} \(\mathcal{G} 7\), I did not draw
  arrows from the first treatment to the second treatment. Although I
  assume that such arrows exist, drawing them was not, in these
  examples, relevant to evaluating the identification problem at hand.
\item
  \textbf{When Temporal Order is Unknown, Explicitly Represent This
  Uncertainty on Your Causal Diagram}

  In many settings, the relevant timing of events cannot be ascertained
  with confidence. To address this, we adopt the convention of indexing
  nodes with uncertain timing using \(X_{\phi t}\) notation. Although
  there is no widely adopted convention for representing uncertainty in
  timing, our primary obligation is to be clear.
\item
  \textbf{Create, Report, and Deploy Multiple Graphs}
\end{enumerate}

Causal inference hinges on assumptions, and experts might disagree. When
the structure of reality encoded in a causal graph is uncertain or
debated, investigators should produce multiple causal diagrams that
reflect these uncertainties and debates.

By stating different assumptions and adopting multiple modelling
strategies that align with these assumptions, we might find that our
causal conclusions are robust despite differences in structural
assumptions. Even when different structural assumptions lead to opposing
causal inferences, this knowledge can guide future data collection to
resolve these differences. The primary goal of causal inference, as with
all science, is to truthfully advance empirical understanding.
Assertions are poor substitutes for honesty. Rather than asserting a
single causal directed graph, investigators should follow the
implications of several.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{10}
\tightlist
\item
  \textbf{Use Automated Identification Algorithms such as
  \texttt{daggity} with Care}
\end{enumerate}

Automated software can assist with identification tasks, such as
factorising complex conditional independencies. However, automated
software may not converge on identifying the optimal set of confounders
in the presence of intractable confounding.

Consider Tyler VanderWeele's \textbf{modified disjunctive cause
criterion}. VanderWeele (\citeproc{ref-vanderweele2019}{2019})
recommends obtaining a maximally efficient adjustment, termed a
`confounder set.' A member of this set is any variable that can reduce
or remove structural sources of bias. The strategy is as follows:

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\item
  Control for any variable that causes the treatment, the outcome, or
  both.
\item
  Control for any proxy of an unmeasured variable that is a shared cause
  of both the treatment and outcome.
\item
  Define an instrumental variable as a variable associated with the
  treatment but not influencing the outcome independently, except
  through the treatment. Exclude any instrumental variable that is not a
  proxy for an unmeasured confounder from the confounder set
  (\citeproc{ref-vanderweele2019}{VanderWeele 2019}).
\end{enumerate}

VanderWeele's modified disjunctive cause criterion is an excellent
strategy for selecting an optimal confounder set. However, this set
might not remove all structural sources of confounding bias in most
observational settings. As such, an automated algorithm might reject it.
This rejection could be unwise because, in non-randomised treatment
assignments, we should often include relations of unmeasured confounding
in our causal graphs. Rejecting causal inferences in observational
settings entirely would be imprudent, as many examples closely
approximate randomised control trials
(\citeproc{ref-hernan2008aObservationalStudiesAnalysedLike}{Hern√°n
\emph{et al.} 2008b}; \citeproc{ref-hernan2016}{Hern√°n \emph{et al.}
2016b}; \citeproc{ref-hernan2006estimating}{Hern√°n and Robins 2006b}).

For example, consider Table~\ref{tbl-chronology-notenough}
\(\mathcal{G}_{2.1}\), where we encountered intractable confounding.
What if there were no proxy for an unmeasured confounder? Should we
condition on the measured confounder and induce M-bias, leave the
backdoor path from the measured confounder open, or not attempt causal
inferences at all? The answer depends on assumptions about the relative
strength of confounding in the causal diagram. Rather than relying on a
generic strategy, we require subject-specialist expertise, sensitivity
analyses, and multiple causal identification strategies
(\citeproc{ref-smith2022combining}{Smith \emph{et al.} 2022}).

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{11}
\tightlist
\item
  \textbf{Clarify Assumptions about Structural Bias from Measurement
  Error and Target Population Restriction (also known as `Selection
  Bias')}
\end{enumerate}

Space constraints prevented us from examining how causal directed
acyclic graphs can clarify structural biases from measurement error and
restrictions of the target population in the sample population at the
start and end of the study. We can (and should) examine structural
features of bias in these settings. For an overview, refer to Bulbulia
(\citeproc{ref-bulbulia2024wierd}{2024d}).

\subsection{Conclusions}\label{id-sec-6}

\paragraph{Limitations}\label{limitations}

\textbf{First, I have focused on the application of causal diagrams to
confounding bias; however, there are other biases that threaten causal
inference besides confounding biases.} Causal directed acyclic graphs
can also be extended to evaluate measurement-error biases and some
features of target population restriction bias (also called `selection
restriction bias'). Valid causal inferences require addressing all
structural sources of bias. This work does not aim for complete coverage
of how causal diagrams may be useful for off-label applications other
than assessing d-separation, but it hopes to stimulate curiosity
(\citeproc{ref-bulbulia2024wierd}{Bulbulia 2024d};
\citeproc{ref-hernan2024WHATIF}{Hernan and Robins 2024};
\citeproc{ref-hernan2017SELECTIONWITHOUTCOLLIDER}{Hern√°n 2017};
\citeproc{ref-hernan2009MEASUREMENT}{Hern√°n and Cole 2009};
\citeproc{ref-liu2023application}{Liu \emph{et al.} 2023};
\citeproc{ref-vanderweele2012MEASUREMENT}{VanderWeele and Hern√°n 2012}).

\textbf{Second, I have not reviewed other graphical tools for
identification, such as Single World Intervention Graphs}. Although
causal directed acyclic graphs are powerful tools for addressing
identification problems, they are not the only graphical tools
researchers use to investigate causality. For example, Robins
(\citeproc{ref-robins1986}{1986}) developed the `finest fully randomized
causally interpreted structured tree graph (FFRCISTG),' which has been
more recently revived and simplified in Single World Intervention Graphs
(refer to Richardson and Robins
(\citeproc{ref-richardson2013swigsprimer}{2013b})). These graphs
explicitly factorise counterfactual states, which can be helpful for
identification in complex longitudinal settings. For some, representing
counterfactual states on a graph is more satisfying, as it allows
inspection of the conditional independence of expectations over
\(Y(a^*)\) and \(Y(a)\) separately. Refer to Bulbulia
(\citeproc{ref-bulbulia2024swigstime}{2024b}) for use cases.

\textbf{Third, I have not reviewed workflows downstream of causal
identification}. This article does not cover statistical estimands,
statistical estimation, and the interpretation and reporting of causal
inferences, which come downstream of causal graphs in causal inference
workflows. Rapid developments in machine learning offer applied
researchers new tools for handling model misspecification (refer to Van
Der Laan and Rose (\citeproc{ref-vanderlaan2018}{2018}); Laan and Gruber
(\citeproc{ref-van2012targeted}{2012}); Dƒ±ÃÅaz \emph{et al.}
(\citeproc{ref-diaz2021nonparametric}{2021}); Williams and D√≠az
(\citeproc{ref-williams2021}{2021}); Hoffman \emph{et al.}
(\citeproc{ref-hoffman2023}{2023})) and assessing treatment effect
heterogeneity (refer to Athey \emph{et al.}
(\citeproc{ref-athey2019}{2019}); Athey and Wager
(\citeproc{ref-athey2021}{2021}); Wager and Athey
(\citeproc{ref-wager2018}{2018}); Vansteelandt and Dukes
(\citeproc{ref-vansteelandt2022a}{2022})). Those interested in workflows
for causal inference in panel studies might consier VanderWeele \emph{et
al.} (\citeproc{ref-vanderweele2020}{2020}). The workflows in my
research group can be found here: Bulbulia
(\citeproc{ref-bulbulia2024PRACTICAL}{2024a}). For general approaches I
recommend: \href{https://tlverse.org/tmle3/}{https://tlverse.org/tmle3/,
accessed 10 June 2024}. However, readers should be aware that workflows
for statistical designs and estimation are quickly evolving.

Nevertheless, after precisely stating our causal question, the most
difficult and important challenge is considering whether and how it
might be identified in the data. The `statistical models first' approach
routinely applied in most human sciences is soon ending. This approach
has been attractive because it is relatively easy to implement---the
methods do not require extensive training---and because the application
of statistical models to data appears rigorous. However, if the
coefficients we recover from these methods have meaning, this is
typically accidental. Without a causal framework, these coefficients are
not just uninformative about what works and why; they lack any meaning
(\citeproc{ref-ogburn2021}{Ogburn and Shpitser 2021}).

There are many good resources available for learning causal directed
acyclic graphs (\citeproc{ref-barrett2021}{Barrett 2021};
\citeproc{ref-cinelli2022}{Cinelli \emph{et al.} 2022};
\citeproc{ref-greenland1999}{Greenland \emph{et al.} 1999},
\citeproc{ref-greenland1999}{1999}; \citeproc{ref-hernan2023}{Hernan and
Robins 2023}, \citeproc{ref-hernan2024WHATIF}{2024};
\citeproc{ref-major2023exploring}{Major-Smith 2023};
\citeproc{ref-mcelreath2020}{McElreath 2020};
\citeproc{ref-morgan2014}{Morgan and Winship 2014};
\citeproc{ref-pearl2009a}{Pearl 2009}; \citeproc{ref-rohrer2018}{Rohrer
2018}; \citeproc{ref-suzuki2020}{Suzuki \emph{et al.} 2020}). This work
aims to add to these resources, first by providing additional conceptual
orientation to the frameworks and workflows of causal data science,
highlighting the risks of applying causal graphs without this
understanding; second, by using causal diagrams to emphasise the
importance of ensuring relative timing for the variables whose causal
relationships are represented on the graph; third, by employing causal
diagrams to clarify the limitations of longitudinal data for certain
questions in causal mediation and time-varying confounding under
time-varying treatments, which remain topics of confusion in many human
sciences (see Bulbulia (\citeproc{ref-bulbulia2024swigstime}{2024b}) for
a detailed explanation).

For those just getting started on causal diagrams, I recommend Miguel
Hernan's free course here:
\href{https://www.edx.org/learn/data-analysis/harvard-university-causal-diagrams-draw-your-assumptions-before-your-conclusions}{https://www.edx.org/learn/data-analysis/harvard-university-causal-diagrams-draw-your-assumptions-before-your-conclusions,
accessed 10 June 2024}

For those seeking a slightly more technical but still accessible
introduction to causal inference and causal DAGs, I recommend Brady
Neal's introduction to causal inference course and textbook, both freely
available here
\href{https://www.bradyneal.com/causal-inference-course}{https://www.bradyneal.com/causal-inference-course,
accessed 10 June 2024}.

\subsubsection{Neurath's Boat: On the Priority of Assumptions in
Science}\label{neuraths-boat-on-the-priority-of-assumptions-in-science}

We might wonder, if not from the data, where do our assumptions about
causality come from? We have said that our assumptions must come from
expert knowledge. Our reliance on expert knowledge might seem
counterintuitive for building scientific knowledge. Shouldn't we use
data to build scientific knowledge, not the other way around? Isn't
scientific history a record of expert opinions being undone?

The Austrian philosopher Otto Neurath famously described scientific
progress using the metaphor of a ship that must be rebuilt at sea:

\begin{quote}
\ldots{} every statement about any happening is saturated with
hypotheses of all sorts and these in the end are derived from our whole
world-view. We are like sailors who on the open sea must reconstruct
their ship but are never able to start afresh from the bottom. Where a
beam is taken away a new one must at once be put there, and for this the
rest of the ship is used as support. In this way, by using the old beams
and driftwood, the ship can be shaped entirely anew, but only by gradual
reconstruction. (\citeproc{ref-neurath1973}{Neurath 1973 p. 199})
\end{quote}

Neurath emphasises the iterative process of accumulating scientific
knowledge; new insights are formed from the foundation of existing
knowledge (\citeproc{ref-godfrey2006strategy}{Godfrey-Smith 2006},
\citeproc{ref-godfrey2009theory}{2009};
\citeproc{ref-quine1981theories}{Quine 1981}).

Causal diagrams are at home in Neurath's boat. We should resist the
tradition of science that believes knowledge develops solely from the
results of statistical tests applied to data. The data have never fully
contained the answers we seek. When reconstructing knowledge, we have
always relied on assumptions. Causal graphs enable us to make these
assumptions explicit and to understand what we obtain based on them.

\newpage{}

\subsection{Funding}\label{funding}

This work is supported by a grant from the Templeton Religion Trust
(TRT0418) and RSNZ Marsden 3721245, 20-UOA-123; RSNZ 19-UOO-090. I also
received support from the Max Planck Institute for the Science of Human
History. The Funders had no role in preparing the manuscript or deciding
to publish it.

\subsection{Acknowledgements}\label{acknowledgements}

I am grateful to Dr Inkuk Kim for checking previous versions of this
manuscript and offering feedback.

I am also grateful to two anonymous reviewers and the editor, Charles
Efferson, of Evolutionary Human Sciences, for their constructive
feedback, which improved this manuscript.

Any remaining errors are my own.

\newpage{}

\subsection{Appendix A: Glossary}\label{id-app-a}

\begin{table}

\caption{\label{tbl-experiments}Glossary}

\centering{

\glossaryTerms

}

\end{table}%

\newpage{}

\subsection{Appendix B: Causal Inference in History: The Difficulty in
Satisfying the Three Fundamental Assumptions for Causal
Inference}\label{id-app-b}

Consider the Protestant Reformation of the 16th century, which initiated
religious change throughout much of Europe. Historians have argued that
Protestantism caused social, cultural, and economic changes in those
societies where it took hold (see: Weber
(\citeproc{ref-weber1905}{1905}); Weber
(\citeproc{ref-weber1993}{1993}); Swanson
(\citeproc{ref-swanson1967}{1967}); Swanson
(\citeproc{ref-swanson1971}{1971}); Basten and Betz
(\citeproc{ref-basten2013}{2013}), and for an overview, see: Becker
\emph{et al.} (\citeproc{ref-becker2016}{2016})).

Suppose we want to estimate the Protestant Reformation's `Average
Treatment Effect'. Let \(A = a^*\) denote the adoption of Protestantism.
We compare this effect with that of remaining Catholic, represented as
\(A = a\). We assume that both the concepts of `adopting Protestantism'
and `economic development' are well-defined (e.g., GDP +1 century after
a country has a Protestant majority contrasted with remaining Catholic).
The causal effect for any individual country is \(Y_i(a^*) - Y_i(a)\).
Although we cannot identify this effect, if the basic assumptions of
causal inference are met, we can estimate the average or marginal effect
by conditioning on the confounding effects of \(L\):

\[
ATE_{\textnormal{economic~development}} = \mathbb{E}[Y(\textnormal{Became~Protestant}|L) - Y(\textnormal{Remained~Catholic}|L)]
\]

When asking causal questions about the economic effect of adopting
Protestantism versus remaining Catholic, several challenges arise
regarding the three fundamental assumptions required for causal
inference.

\textbf{Causal Consistency}: This requires that the outcome under each
level of treatment to be compared is well-defined. In this context,
defining what `adopting Protestantism' and `remaining Catholic' mean may
present challenges. The practices and beliefs of each religion might
vary significantly across countries and time periods, making it
difficult to create a consistent, well-defined treatment. Furthermore,
the outcome---economic development---may also be challenging to measure
consistently across different countries and time periods.

There is undoubtedly considerable heterogeneity in the `Protestant
treatment.' In England, Protestantism was closely tied to the monarchy
(\citeproc{ref-collinson2003}{Collinson 2003}). In Germany, Martin
Luther's teachings emphasised individual faith in scripture, which, it
has been claimed, supported economic development by promoting literacy
(\citeproc{ref-gawthrop1984}{Gawthrop and Strauss 1984}). In England,
King Henry VIII abolished Catholicism
(\citeproc{ref-collinson2003}{Collinson 2003}). The Reformation, then,
occurred differently in different places. The treatment needs to be
better defined.

There is also ample scope for interference: 16th-century societies were
interconnected through trade, diplomacy, and warfare. Thus, the
religious decisions of one society were unlikely to have been
independent from those of other societies.

\textbf{Exchangeability}: This requires that given the confounders, the
potential outcomes are independent of the treatment assignment. It might
be difficult to account for all possible confounders in this context.
For example, historical, political, social, and geographical factors
could influence both a country's religious affiliations and its economic
development.

\textbf{Positivity}: This requires that there is a non-zero probability
of every level of treatment for every stratum of confounders. If we
consider various confounding factors such as geographical location,
historical events, or political circumstances, some countries might only
ever have the possibility of either remaining Catholic or becoming
Protestant, but not both. For example, it is unclear under which
conditions 16th-century Spain could have been randomly assigned to
Protestantism (\citeproc{ref-nalle1987}{Nalle 1987};
\citeproc{ref-westreich2010}{Westreich and Cole 2010}).

Perhaps a more credible measure of effect in the region of our interests
is the Average Treatment Effect in the Treated (ATT) expressed:

\[
ATT_{\textnormal{economic~development}} = \mathbb{E}[(Y(a^*) - Y(a))|A = a^*,L]
\]

Where \(Y(a^*)\) represents the potential outcome if treated, and
\(Y(a)\) represents the potential outcome if not treated. The
expectation is taken over the distribution of the treated units (i.e.,
those for whom \(A = a^*\)). \(L\) is a set of covariates on which we
condition to ensure that the potential outcomes \(Y(a^*)\) and \(Y(a)\)
are independent of the treatment assignment \(A\), given \(L\). This
accounts for any confounding factors that might bias the estimate of the
treatment effect.

Here, the ATT defines the expected difference in economic success for
cultures that became Protestant compared with the expected economic
success if those cultures had not become Protestant, conditional on
measured confounders \(L\), among the exposed (\(A = a^*\)). To estimate
this contrast, our models would need to match Protestant cultures with
comparable Catholic cultures effectively. By estimating the ATT, we
avoid the assumption of non-deterministic positivity for the untreated.
However, whether matching is conceptually plausible remains debatable.
Ostensibly, it would seem that assigning a religion to a culture is not
as easy as administering a pill (\citeproc{ref-watts2018}{Watts \emph{et
al.} 2018}).

\newpage{}

\subsection{Appendix C: Causal Consistency Under Multiple Versions of
Treatment}\label{id-app-c}

To better understand how the causal consistency assumption might fail,
consider a question discussed in the evolutionary human science
literature about whether a society's beliefs in big Gods affect its
development of social complexity (\citeproc{ref-beheim2021}{Beheim
\emph{et al.} 2021}; \citeproc{ref-johnson2015}{Johnson 2015};
\citeproc{ref-norenzayan2016}{Norenzayan \emph{et al.} 2016};
\citeproc{ref-sheehan2022}{Sheehan \emph{et al.} 2022};
\citeproc{ref-slingerland2020coding}{Slingerland \emph{et al.} 2020};
\citeproc{ref-watts2015}{Watts \emph{et al.} 2015};
\citeproc{ref-whitehouse2023}{Whitehouse \emph{et al.} 2023}).
Historians and anthropologists report that such beliefs vary over time
and across cultures in intensity, interpretations, institutional
management, and rituals (\citeproc{ref-bulbuliaj.2013}{Bulbulia, J.
\emph{et al.} 2013}; \citeproc{ref-decoulanges1903}{De Coulanges 1903};
\citeproc{ref-geertz2013}{Geertz \emph{et al.} 2013};
\citeproc{ref-wheatley1971}{Wheatley 1971}). This variation in content
and settings could influence social complexity. Moreover, the treatments
realised in one society might affect those realised in other societies,
resulting in \emph{spill-over} effects in the exposures (`treatments')
to be compared (\citeproc{ref-murray2021a}{Murray \emph{et al.} 2021};
\citeproc{ref-shiba2023uncovering}{Shiba \emph{et al.} 2023}).

The theory of causal inference under multiple versions of treatment,
developed by VanderWeele and Hern√°n, formally addresses this challenge
of treatment-effect heterogeneity
(\citeproc{ref-vanderweele2009}{VanderWeele 2009a},
\citeproc{ref-vanderweele2018}{2018};
\citeproc{ref-vanderweele2013}{VanderWeele and Hernan 2013}). The
authors proved that if the treatment variations, \(K\), are
conditionally independent of the potential outcomes, \(Y(k)\), given
covariates \(L\), then conditioning on \(L\) allows us to consistently
estimate causal effects over the heterogeneous treatments
(\citeproc{ref-vanderweele2009}{VanderWeele 2009a}).

Where \(\coprod\) denotes independence, we may assume causal consistency
where the interventions to be compared are independent of their
potential outcomes, conditional on covariates, \(L\):

\[
K \coprod Y(k) | L
\]

According to the theory of causal inference under multiple versions of
treatment, we may think of \(K\) as a `coarsened indicator' for \(A\).
Although the theory of causal inference under multiple versions of
treatment provides a formal solution to the problem of treatment-effect
heterogeneity and treatment-effect dependencies (also known as SUTVA -
the `stable unit treatment value assumption', refer to Rubin
(\citeproc{ref-rubin1980randomization}{1980})), computing and
interpreting causal effect estimates under this theory can be
challenging.

Consider the question of whether a reduction in Body Mass Index (BMI)
affects health (\citeproc{ref-hernuxe1n2008}{Hern√°n and Taubman 2008}).
Weight loss can occur through various methods, each with different
health implications. Specific methods, such as regular exercise or a
calorie-reduced diet, benefit health. However, weight loss might result
from adverse conditions such as infectious diseases, cancers,
depression, famine, or accidental amputations, which are generally not
beneficial to health. Hence, even if causal effects of `weight loss'
could be consistently estimated when adjusting for covariates \(L\), we
might be uncertain about how to interpret the effect we are consistently
estimating. This uncertainty highlights the need for precise and
well-defined causal questions. For example, rather than stating the
intervention vaguely as `weight loss', we could state the intervention
clearly and specifically as `weight loss achieved through aerobic
exercise over at least five years, compared with no weight loss.' This
specificity in the definition of the treatment, along with comparable
specificity in the statement of the outcomes, helps ensure that the
causal estimates we obtain are not merely unbiased but also
interpretable; for discussion, see Hern√°n \emph{et al.}
(\citeproc{ref-hernuxe1n2022}{2022}); Murray \emph{et al.}
(\citeproc{ref-murray2021a}{2021}); Hern√°n and Taubman
(\citeproc{ref-hernuxe1n2008}{2008}).

Beyond uncertainties for the interpretation of heterogeneous treatment
effect estimates, there is the additional consideration that we cannot
fully verify from data whether the measured covariates \(L\) suffice to
render the multiple versions of treatment independent of the
counterfactual outcomes. This problem is acute when there is
\emph{interference}, which occurs when treatment effects are relative to
the density and distribution of treatment effects in a population. Scope
for interference will often make it difficult to warrant the assumption
that the potential outcomes are independent of the many versions of
treatment that have been realised, dependently, on the administration of
previous versions of treatments across the population
(\citeproc{ref-bulbulia2023a}{Bulbulia \emph{et al.} 2023};
\citeproc{ref-ogburn2022}{Ogburn \emph{et al.} 2022};
\citeproc{ref-vanderweele2013}{VanderWeele and Hernan 2013}).

In short, although the theory of causal inference under multiple
versions of treatment provides a formal solution for consistent causal
effect estimation in observational settings, \emph{treatment
heterogeneity} remains a practical threat. Generally, we should assume
that causal consistency is unrealistic unless proven innocent.

For now, we note that the causal consistency assumption provides a
theoretical starting point for recovering the missing counterfactuals
required for computing causal contrasts. It identifies half of these
missing counterfactuals directly from observed data. The concept of
conditional exchangeability, which we examine next, offers a means for
recovering the remaining half.

\newpage{}

\subsection{Appendix D Pearl's Do-Calculus and Structural Causal
Models}\label{id-app-d}

In the potential outcomes framework, we represent interventions by
setting variables to specific levels, e.g., setting the treatment to a
specific value \(A = \tilde{a}\). Counterfactual outcomes are conceived
as the outcomes that would occur if, perhaps contrary to fact, the
treatment were set to a specific level. We use the convention \(Y_i(a)\)
or equivalently \(Y_i^{a}\) to denote the counterfactual or `potential'
outcome for individual \(i\) when that individual's treatment is set to
\(A_i = a\). Because we assume individual treatments to be independent
and identically distributed (i.i.d.), we drop the subscripts when
describing the potential outcomes for multiple individuals under
specific levels of treatment. Thus, we write \(Y(a)\) or \(Y^a\) as
shorthand for the average potential outcome:

\[
Y(a) = \frac{1}{n} \sum_{i=1}^n Y_i(a)
\]

We say that the conditional exchangeability is satisfied if the
potential outcomes are independent of the treatments assigned
conditional on measured covariates:

\[
A \coprod Y(\tilde{a}) \mid L
\]

Causal directed acyclic graphs do not directly represent counterfactual
outcomes. Instead, they evaluate whether causality can be identified
from hypothetical interventions on the variables represented in a graph.
Formally, causal directed acyclic graphs rely on Judea Pearl's
do-calculus (\citeproc{ref-pearl2009a}{Pearl 2009}), which relies on the
concept of an `interventional distribution'. The idea is that any node
in a graph can be intervened upon. Nodes and edges in a causal diagram
correspond to non-parametric structural equations. Note that
non-parametric structural equations are causal-structural models. They
are fundamentally different from statistical structural equation models
that are employed in may human sciences.

In a causal directed acyclic graph, non-parametric structural equations
represent the underlying causal mechanisms without making specific
parametric assumptions about the functional forms of relationships. For
example, if \(L\) denotes the common causes of treatment \(A\) and
outcome \(Y\), then:

\begin{itemize}
\tightlist
\item
  The node \(L\) corresponds to the non-parametric structural equation
  \(L = f_L(U_L)\).
\item
  The treatment is expressed \(A = f_A(L, U_A)\).
\item
  And the outcome is expressed \(Y = f_Y(A, L, U_Y)\).
\end{itemize}

In Pearl's formalism, we assume that \(U_L, U_A, U_Y\) are independent
exogenous random variables. That is, there are no arrows linking \(A\)
to \(Y\) except those from the parent node \(L\). Causal diagrams allow
us to factorise the joint observations of \(L\), \(A\) and \(Y\). Define
\(O\) as a distribution of independent identically distributed
observations such that \(O = (L, A, Y)\). The true distribution \(P_O\)
is factorised as:

\[
P_O = P_O(Y \mid A, L) P_O(A \mid L) P_O(L)
\]

Where: - \(P_O(L)\) is the marginal distribution of the covariates
\(L\). - \(P_O(A \mid L)\) is the conditional distribution of the
treatment given the covariates. - \(P_O(Y \mid A, L)\) is the
conditional expectation of the outcome given the treatment and
covariates.

Pearl's do-calculus allows us to evaluate the consequences of
intervening on variables represented in a causal DAG to interpret
probabilistic dependencies and independencies in the conditional and
marginal associations presented on a graph. By contrast, the potential
outcomes framework considers potential outcomes to be fixed and real
(even if assigned non-deterministically).

It is not inherently problematic that causal DAGs do not explicitly
represent potential outcomes. The theory of counterfactual inference on
which causal DAGs are based does not require this explicit
representation. Judea Pearl's rules of d-separation and the backdoor
adjustment criterion allow us to interpret conditional independencies
based on hypothetical interventions on the nodes of a causal DAG
(\citeproc{ref-neal2020introduction}{Neal 2020};
\citeproc{ref-pearl2009a}{Pearl 2009}). However, as we consider in
\hyperref[id-sec-4]{Part 4} and \hyperref[id-app-f]{Appendix F},
presenting counterfactual histories under specific interventions on
Single World Intervention Graphs can help clarify structural
relationships that are not well-represented on a causal DAG.

Again, mathematically the potential outcomes framework is mathematically
equivalent to the structural causal model framework when it is assumed
the the exogenous error structures are independent. However, the
potential outcomes framework is more general and clarifies how
identification is possible even when the error structures are not
independent. See Robins (\citeproc{ref-robins1986}{1986}), Richardson
and Robins (\citeproc{ref-richardson2013}{2013a}).

\newpage{}

\subsection{Appendix E: Front Door Path Criterion}\label{id-app-e}

To obtain an unbiased estimate for the causal effect of \(A\) on \(C\)
using the front door criterion, we need to identify a set of variables
\(M\) that mediates the effect of \(A\) on \(C\).

Pearl defines the front door criterion more generally as follows: a set
of variables \(B\) satisfies the front door criterion relative to
variables \(A\) and \(C\) in a causal directed acyclic graph
\(\mathcal{G}\) if:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \(B\) is affected by \(A\).
\item
  \(B\) affects \(C\).
\item
  There are no backdoor paths from \(A\) to \(B\).
\item
  All backdoor paths from \(B\) to \(C\) are blocked by conditioning on
  \(A\).
\end{enumerate}

In other words, \(B\) must be an intermediate variable that captures the
entire causal effect of \(A\) on \(C\), with no confounding paths
remaining between \(A\) and \(B\), and any confounding between \(B\) and
\(C\) must be blocked by \(A\).

The frontdoor criterion is less widely used than the backdoor criterion
because it requires the identification of an appropriate mediator that
fully captures the causal effect (\citeproc{ref-pearl2009a}{Pearl
2009}).

\newpage{}

\subsection{References}\label{references}

\phantomsection\label{refs}
\begin{CSLReferences}{1}{0}
\bibitem[\citeproctext]{ref-angrist2009mostly}
Angrist, JD, and Pischke, J-S (2009) \emph{Mostly harmless econometrics:
An empiricist's companion}, Princeton University Press.

\bibitem[\citeproctext]{ref-athey2019}
Athey, S, Tibshirani, J, and Wager, S (2019) Generalized random forests.
\emph{The Annals of Statistics}, \textbf{47}(2), 1148--1178.
doi:\href{https://doi.org/10.1214/18-AOS1709}{10.1214/18-AOS1709}.

\bibitem[\citeproctext]{ref-athey2021}
Athey, S, and Wager, S (2021) Policy Learning With Observational Data.
\emph{Econometrica}, \textbf{89}(1), 133--161.
doi:\href{https://doi.org/10.3982/ECTA15732}{10.3982/ECTA15732}.

\bibitem[\citeproctext]{ref-bareinboim2013general}
Bareinboim, E, and Pearl, J (2013) A general algorithm for deciding
transportability of experimental results. \emph{Journal of Causal
Inference}, \textbf{1}(1), 107--134.

\bibitem[\citeproctext]{ref-barrett2021}
Barrett, M (2021) \emph{Ggdag: Analyze and create elegant directed
acyclic graphs}. Retrieved from
\url{https://CRAN.R-project.org/package=ggdag}

\bibitem[\citeproctext]{ref-basten2013}
Basten, C, and Betz, F (2013) Beyond work ethic: Religion, individual,
and political preferences. \emph{American Economic Journal: Economic
Policy}, \textbf{5}(3), 67--91.
doi:\href{https://doi.org/10.1257/pol.5.3.67}{10.1257/pol.5.3.67}.

\bibitem[\citeproctext]{ref-becker2016}
Becker, SO, Pfaff, S, and Rubin, J (2016) Causes and consequences of the
protestant reformation. \emph{Explorations in Economic History},
\textbf{62}, 1--25.

\bibitem[\citeproctext]{ref-beheim2021}
Beheim, B, Atkinson, QD, Bulbulia, J, \ldots{} Willard, AK (2021)
Treatment of missing data determined conclusions regarding moralizing
gods. \emph{Nature}, \textbf{595}(7866), E29--E34.
doi:\href{https://doi.org/10.1038/s41586-021-03655-4}{10.1038/s41586-021-03655-4}.

\bibitem[\citeproctext]{ref-bulbulia2024PRACTICAL}
Bulbulia, J (2024a) A practical guide to causal inference in three-wave
panel studies. \emph{PsyArXiv Preprints}.
doi:\href{https://doi.org/10.31234/osf.io/uyg3d}{10.31234/osf.io/uyg3d}.

\bibitem[\citeproctext]{ref-bulbulia2024swigstime}
Bulbulia, J (2024b) Causal inference: Interaction, mediation, and
time-varying treatments. \emph{PsyArXiv}.
doi:\href{https://doi.org/10.31234/osf.io/vr268}{10.31234/osf.io/vr268}.

\bibitem[\citeproctext]{ref-bulbulia_2024_experiments}
Bulbulia, J (2024c) Confounding in experiments. \emph{PsyArXiv}.
doi:\href{https://doi.org/10.31234/osf.io/6rnj5}{10.31234/osf.io/6rnj5}.

\bibitem[\citeproctext]{ref-bulbulia2024wierd}
Bulbulia, J (2024d) The weirdest causal inferences: Why comparative
cultural research requires a causal understanding of measurement error
bias. \emph{PsyArXiv}.
doi:\href{https://doi.org/10.31234/osf.io/kj7rv}{10.31234/osf.io/kj7rv}.

\bibitem[\citeproctext]{ref-bulbulia2022}
Bulbulia, JA (2022) A workflow for causal inference in cross-cultural
psychology. \emph{Religion, Brain \& Behavior}, \textbf{0}(0), 1--16.
doi:\href{https://doi.org/10.1080/2153599X.2022.2070245}{10.1080/2153599X.2022.2070245}.

\bibitem[\citeproctext]{ref-bulbulia2023}
Bulbulia, JA (2023) Causal diagrams (directed acyclic graphs): A
practical guide.

\bibitem[\citeproctext]{ref-bulbulia2023a}
Bulbulia, JA, Afzali, MU, Yogeeswaran, K, and Sibley, CG (2023)
Long-term causal effects of far-right terrorism in {N}ew {Z}ealand.
\emph{PNAS Nexus}, \textbf{2}(8), pgad242.

\bibitem[\citeproctext]{ref-bulbuliaj.2013}
Bulbulia, J., Geertz, AW, Atkinson, QD, \ldots{} Wilson, DS (2013) The
cultural evolution of religion. In P. J. Richerson and M. Christiansen,
eds., Cambridge, MA: MIT press, 381--404.

\bibitem[\citeproctext]{ref-chatton2020}
Chatton, A, Le Borgne, F, Leyrat, C, \ldots{} Foucher, Y (2020)
G-computation, propensity score-based methods, and targeted maximum
likelihood estimator for causal inference with different covariates
sets: a comparative simulation study. \emph{Scientific Reports},
\textbf{10}(1), 9219.
doi:\href{https://doi.org/10.1038/s41598-020-65917-x}{10.1038/s41598-020-65917-x}.

\bibitem[\citeproctext]{ref-cinelli2022}
Cinelli, C, Forney, A, and Pearl, J (2022) A Crash Course in Good and
Bad Controls. \emph{Sociological Methods \&Research}, 00491241221099552.
doi:\href{https://doi.org/10.1177/00491241221099552}{10.1177/00491241221099552}.

\bibitem[\citeproctext]{ref-cole2008}
Cole, SR, and Hern√°n, MA (2008) Constructing inverse probability weights
for marginal structural models. \emph{American Journal of Epidemiology},
\textbf{168}(6), 656--664.

\bibitem[\citeproctext]{ref-cole2010}
Cole, SR, Platt, RW, Schisterman, EF, \ldots{} Poole, C (2010)
Illustrating bias due to conditioning on a collider. \emph{International
Journal of Epidemiology}, \textbf{39}(2), 417--420.
doi:\href{https://doi.org/10.1093/ije/dyp334}{10.1093/ije/dyp334}.

\bibitem[\citeproctext]{ref-collinson2003}
Collinson, P (2003) \emph{The reformation: A history}, Weidenfeld;
Nicholson; London, England.

\bibitem[\citeproctext]{ref-danaei2012}
Danaei, G, Tavakkoli, M, and Hern√°n, MA (2012) Bias in observational
studies of prevalent users: lessons for comparative effectiveness
research from a meta-analysis of statins. \emph{American Journal of
Epidemiology}, \textbf{175}(4), 250--262.
doi:\href{https://doi.org/10.1093/aje/kwr301}{10.1093/aje/kwr301}.

\bibitem[\citeproctext]{ref-decoulanges1903}
De Coulanges, F (1903) \emph{La cit√© antique: √âtude sur le culte, le
droit, les institutions de la gr√®ce et de rome}, Hachette.

\bibitem[\citeproctext]{ref-diaz2021nonparametric}
Dƒ±ÃÅaz, I, Hejazi, NS, Rudolph, KE, and Der Laan, MJ van (2021)
Nonparametric efficient causal mediation with intermediate confounders.
\emph{Biometrika}, \textbf{108}(3), 627--641.

\bibitem[\citeproctext]{ref-Diaz2023}
Dƒ±ÃÅaz, I, Williams, N, and Rudolph, KE (2023) \emph{Journal of Causal
Inference}, \textbf{11}(1), 20220077.
doi:\href{https://doi.org/doi:10.1515/jci-2022-0077}{doi:10.1515/jci-2022-0077}.

\bibitem[\citeproctext]{ref-gawthrop1984}
Gawthrop, R, and Strauss, G (1984) Protestantism and literacy in early
modern germany. \emph{Past \& Present}, (104), 31--55.

\bibitem[\citeproctext]{ref-geertz2013}
Geertz, AW, Atkinson, QD, Cohen, E, \ldots{} Wilson, DS (2013) The
cultural evolution of religion. In P. J. Richerson and M. Christiansen,
eds., Cambridge, MA: MIT press, 381--404.

\bibitem[\citeproctext]{ref-godfrey2006strategy}
Godfrey-Smith, P (2006) The strategy of model-based science.
\emph{Biology and Philosophy}, \textbf{21}, 725--740.

\bibitem[\citeproctext]{ref-godfrey2009theory}
Godfrey-Smith, P (2009) \emph{Theory and reality: An introduction to the
philosophy of science}, University of Chicago Press.

\bibitem[\citeproctext]{ref-greenland2003quantifying}
Greenland, S (2003) Quantifying biases in causal models: Classical
confounding vs collider-stratification bias. \emph{Epidemiology},
300--306.

\bibitem[\citeproctext]{ref-greenland1999}
Greenland, S, Pearl, J, and Robins, JM (1999) Causal diagrams for
epidemiologic research. \emph{Epidemiology (Cambridge, Mass.)},
\textbf{10}(1), 37--48.

\bibitem[\citeproctext]{ref-greifer2023}
Greifer, N, Worthington, S, Iacus, S, and King, G (2023) \emph{Clarify:
Simulation-based inference for regression models}. Retrieved from
\url{https://iqss.github.io/clarify/}

\bibitem[\citeproctext]{ref-haneuse2013estimation}
Haneuse, S, and Rotnitzky, A (2013) Estimation of the effect of
interventions that modify the received treatment. \emph{Statistics in
Medicine}, \textbf{32}(30), 5260--5277.

\bibitem[\citeproctext]{ref-hernan2023}
Hernan, MA, and Robins, JM (2023) \emph{Causal inference}, Taylor \&
Francis. Retrieved from
\url{https://books.google.co.nz/books?id=/_KnHIAAACAAJ}

\bibitem[\citeproctext]{ref-hernan2024WHATIF}
Hernan, MA, and Robins, JM (2024) \emph{Causal inference: What if?},
Taylor \& Francis. Retrieved from
\url{https://www.hsph.harvard.edu/miguel-hernan/causal-inference-book/}

\bibitem[\citeproctext]{ref-hernan2017SELECTIONWITHOUTCOLLIDER}
Hern√°n, MA (2017) Invited commentary: Selection bias without colliders
\textbar{} american journal of epidemiology \textbar{} oxford academic.
\emph{American Journal of Epidemiology}, \textbf{185}(11), 1048--1050.
Retrieved from \url{https://doi.org/10.1093/aje/kwx077}

\bibitem[\citeproctext]{ref-hernan2008aObservationalStudiesAnalysedLike}
Hern√°n, MA, Alonso, A, Logan, R, \ldots{} Robins, JM (2008b)
Observational studies analyzed like randomized experiments: An
application to postmenopausal hormone therapy and coronary heart
disease. \emph{Epidemiology}, \textbf{19}(6), 766.
doi:\href{https://doi.org/10.1097/EDE.0b013e3181875e61}{10.1097/EDE.0b013e3181875e61}.

\bibitem[\citeproctext]{ref-hernuxe1n2008a}
Hern√°n, MA, Alonso, A, Logan, R, \ldots{} Robins, JM (2008a)
Observational studies analyzed like randomized experiments: An
application to postmenopausal hormone therapy and coronary heart
disease. \emph{Epidemiology}, \textbf{19}(6), 766.
doi:\href{https://doi.org/10.1097/EDE.0b013e3181875e61}{10.1097/EDE.0b013e3181875e61}.

\bibitem[\citeproctext]{ref-hernan2009MEASUREMENT}
Hern√°n, MA, and Cole, SR (2009) Invited commentary: Causal diagrams and
measurement bias. \emph{American Journal of Epidemiology},
\textbf{170}(8), 959--962.
doi:\href{https://doi.org/10.1093/aje/kwp293}{10.1093/aje/kwp293}.

\bibitem[\citeproctext]{ref-hernan2004STRUCTURAL}
Hern√°n, MA, Hern√°ndez-D√≠az, S, and Robins, JM (2004) A structural
approach to selection bias. \emph{Epidemiology}, \textbf{15}(5),
615--625. Retrieved from \url{https://www.jstor.org/stable/20485961}

\bibitem[\citeproctext]{ref-hernuxe1n2006}
Hern√°n, MA, and Robins, JM (2006a) Estimating causal effects from
epidemiological data. \emph{Journal of Epidemiology \& Community
Health}, \textbf{60}(7), 578--586.
doi:\href{https://doi.org/10.1136/jech.2004.029496}{10.1136/jech.2004.029496}.

\bibitem[\citeproctext]{ref-hernan2006estimating}
Hern√°n, MA, and Robins, JM (2006b) Estimating causal effects from
epidemiological data. \emph{Journal of Epidemiology \& Community
Health}, \textbf{60}(7), 578--586.
doi:\href{https://doi.org/10.1136/jech.2004.029496}{10.1136/jech.2004.029496}.

\bibitem[\citeproctext]{ref-hernan2017per}
Hern√°n, MA, Robins, JM, et al. (2017) Per-protocol analyses of pragmatic
trials. \emph{N Engl J Med}, \textbf{377}(14), 1391--1398.

\bibitem[\citeproctext]{ref-hernuxe1n2016}
Hern√°n, MA, Sauer, BC, Hern√°ndez-D√≠az, S, Platt, R, and Shrier, I
(2016a) Specifying a target trial prevents immortal time bias and other
self-inflicted injuries in observational analyses. \emph{Journal of
Clinical Epidemiology}, \textbf{79}, 70--75.

\bibitem[\citeproctext]{ref-hernan2016}
Hern√°n, MA, Sauer, BC, Hern√°ndez-D√≠az, S, Platt, R, and Shrier, I
(2016b) Specifying a target trial prevents immortal time bias and other
self-inflicted injuries in observational analyses. \emph{Journal of
Clinical Epidemiology}, \textbf{79}, 70--75.

\bibitem[\citeproctext]{ref-hernuxe1n2008}
Hern√°n, MA, and Taubman, SL (2008) Does obesity shorten life? The
importance of well-defined interventions to answer causal questions.
\emph{International Journal of Obesity (2005)}, \textbf{32 Suppl 3},
S8--14.
doi:\href{https://doi.org/10.1038/ijo.2008.82}{10.1038/ijo.2008.82}.

\bibitem[\citeproctext]{ref-hernuxe1n2022}
Hern√°n, MA, Wang, W, and Leaf, DE (2022) Target trial emulation: A
framework for causal inference from observational data. \emph{JAMA},
\textbf{328}(24), 2446--2447.
doi:\href{https://doi.org/10.1001/jama.2022.21383}{10.1001/jama.2022.21383}.

\bibitem[\citeproctext]{ref-hoffman2023}
Hoffman, KL, Salazar-Barreto, D, Rudolph, KE, and D√≠az, I (2023)
Introducing longitudinal modified treatment policies: A unified
framework for studying complex exposures.
doi:\href{https://doi.org/10.48550/arXiv.2304.09460}{10.48550/arXiv.2304.09460}.

\bibitem[\citeproctext]{ref-holland1986}
Holland, PW (1986) Statistics and causal inference. \emph{Journal of the
American Statistical Association}, \textbf{81}(396), 945--960.

\bibitem[\citeproctext]{ref-hume1902}
Hume, D (1902) \emph{Enquiries Concerning the Human Understanding: And
Concerning the Principles of Morals}, Clarendon Press.

\bibitem[\citeproctext]{ref-imai2008misunderstandings}
Imai, K, King, G, and Stuart, EA (2008) Misunderstandings between
experimentalists and observationalists about causal inference.
\emph{Journal of the Royal Statistical Society Series A: Statistics in
Society}, \textbf{171}(2), 481--502.

\bibitem[\citeproctext]{ref-johnson2015}
Johnson, DD (2015) Big gods, small wonder: Supernatural punishment
strikes back. \emph{Religion, Brain \& Behavior}, \textbf{5}(4),
290--298.

\bibitem[\citeproctext]{ref-van2012targeted}
Laan, MJ van der, and Gruber, S (2012) Targeted minimum loss based
estimation of causal effects of multiple time point interventions.
\emph{The International Journal of Biostatistics}, \textbf{8}(1).

\bibitem[\citeproctext]{ref-lash2020}
Lash, TL, Rothman, KJ, VanderWeele, TJ, and Haneuse, S (2020)
\emph{Modern epidemiology}, Wolters Kluwer. Retrieved from
\url{https://books.google.co.nz/books?id=SiTSnQEACAAJ}

\bibitem[\citeproctext]{ref-lauritzen1990}
Lauritzen, SL, Dawid, AP, Larsen, BN, and Leimer, H-G (1990)
Independence properties of directed {M}arkov fields. \emph{Networks},
\textbf{20}(5), 491--505.

\bibitem[\citeproctext]{ref-lewis1973}
Lewis, D (1973) Causation. \emph{The Journal of Philosophy},
\textbf{70}(17), 556--567.
doi:\href{https://doi.org/10.2307/2025310}{10.2307/2025310}.

\bibitem[\citeproctext]{ref-linden2020EVALUE}
Linden, A, Mathur, MB, and VanderWeele, TJ (2020) Conducting sensitivity
analysis for unmeasured confounding in observational studies using
e-values: The evalue package. \emph{The Stata Journal}, \textbf{20}(1),
162--175.

\bibitem[\citeproctext]{ref-liu2023application}
Liu, Y, Schnitzer, ME, Herrera, R, Dƒ±ÃÅaz, I, O'Loughlin, J, and
Sylvestre, M-P (2023) The application of target trials with longitudinal
targeted maximum likelihood estimation to assess the effect of alcohol
consumption in adolescence on depressive symptoms in adulthood.
\emph{American Journal of Epidemiology}, kwad241.

\bibitem[\citeproctext]{ref-major2023exploring}
Major-Smith, D (2023) Exploring causality from observational data: An
example assessing whether religiosity promotes cooperation.
\emph{Evolutionary Human Sciences}, \textbf{5}, e22.

\bibitem[\citeproctext]{ref-mcelreath2020}
McElreath, R (2020) \emph{Statistical rethinking: A {B}ayesian course
with examples in {R} and {S}tan}, CRC press.

\bibitem[\citeproctext]{ref-montgomery2018}
Montgomery, JM, Nyhan, B, and Torres, M (2018) How conditioning on
posttreatment variables can ruin your experiment and what to do about
It. \emph{American Journal of Political Science}, \textbf{62}(3),
760--775.
doi:\href{https://doi.org/10.1111/ajps.12357}{10.1111/ajps.12357}.

\bibitem[\citeproctext]{ref-morgan2014}
Morgan, SL, and Winship, C (2014) \emph{Counterfactuals and causal
inference: Methods and principles for social research}, 2nd edn,
Cambridge: Cambridge University Press.
doi:\href{https://doi.org/10.1017/CBO9781107587991}{10.1017/CBO9781107587991}.

\bibitem[\citeproctext]{ref-murray2021a}
Murray, EJ, Marshall, BDL, and Buchanan, AL (2021) Emulating target
trials to improve causal inference from agent-based models.
\emph{American Journal of Epidemiology}, \textbf{190}(8), 1652--1658.
doi:\href{https://doi.org/10.1093/aje/kwab040}{10.1093/aje/kwab040}.

\bibitem[\citeproctext]{ref-nalle1987}
Nalle, ST (1987) Inquisitors, priests, and the people during the
catholic reformation in spain. \emph{The Sixteenth Century Journal},
557--587.

\bibitem[\citeproctext]{ref-neal2020introduction}
Neal, B (2020) Introduction to causal inference from a machine learning
perspective. \emph{Course Lecture Notes (Draft)}. Retrieved from
\url{https://www.bradyneal.com/Introduction_to_Causal_Inference-Dec17_2020-Neal.pdf}

\bibitem[\citeproctext]{ref-neurath1973}
Neurath, O (1973) Anti-spengler. In M. Neurath and R. S. Cohen, eds.,
\emph{Empiricism and sociology}, Dordrecht: Springer Netherlands,
158--213.
doi:\href{https://doi.org/10.1007/978-94-010-2525-6_6}{10.1007/978-94-010-2525-6\_6}.

\bibitem[\citeproctext]{ref-norenzayan2016}
Norenzayan, A, Shariff, AF, Gervais, WM, \ldots{} Henrich, J (2016) The
cultural evolution of prosocial religions. \emph{Behavioral and Brain
Sciences}, \textbf{39}, e1.
doi:\href{https://doi.org/10.1017/S0140525X14001356}{10.1017/S0140525X14001356}.

\bibitem[\citeproctext]{ref-ogburn2021}
Ogburn, EL, and Shpitser, I (2021) Causal modelling: The two cultures.
\emph{Observational Studies}, \textbf{7}(1), 179--183.
doi:\href{https://doi.org/10.1353/obs.2021.0006}{10.1353/obs.2021.0006}.

\bibitem[\citeproctext]{ref-ogburn2022}
Ogburn, EL, Sofrygin, O, D√≠az, I, and Laan, MJ van der (2022) Causal
inference for social network data. \emph{Journal of the American
Statistical Association}, \textbf{0}(0), 1--15.
doi:\href{https://doi.org/10.1080/01621459.2022.2131557}{10.1080/01621459.2022.2131557}.

\bibitem[\citeproctext]{ref-pearl1988}
Pearl, J (1988) \emph{Probabilistic reasoning in intelligent systems:
Networks of plausible inference}, Morgan kaufmann.

\bibitem[\citeproctext]{ref-pearl1995}
Pearl, J (1995) Causal diagrams for empirical research.
\emph{Biometrika}, \textbf{82}(4), 669--688.

\bibitem[\citeproctext]{ref-pearl2009a}
Pearl, J (2009) \emph{Causality}, Cambridge University Press.

\bibitem[\citeproctext]{ref-pearl2022}
Pearl, J, and Bareinboim, E (2022) External validity: From do-calculus
to transportability across populations. In, 1st edn, Vol. 36, New York,
NY, USA: Association for Computing Machinery, 451--482. Retrieved from
\url{https://doi.org/10.1145/3501714.3501741}

\bibitem[\citeproctext]{ref-quine1981theories}
Quine, WVO (1981) \emph{Theories and things}, Harvard University Press.

\bibitem[\citeproctext]{ref-richardson2013}
Richardson, TS, and Robins, JM (2013a) Single world intervention graphs:
A primer. In, Citeseer. Retrieved from
\url{https://core.ac.uk/display/102673558}

\bibitem[\citeproctext]{ref-richardson2013swigsprimer}
Richardson, TS, and Robins, JM (2013b) Single world intervention graphs:
A primer. In \emph{Second UAI workshop on causal structure learning,
{B}ellevue, {W}ashington}, Citeseer. Retrieved from
\url{https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=07bbcb458109d2663acc0d098e8913892389a2a7}

\bibitem[\citeproctext]{ref-richardson2023potential}
Richardson, TS, and Robins, JM (2023) Potential outcome and decision
theoretic foundations for statistical causality. \emph{Journal of Causal
Inference}, \textbf{11}(1), 20220012.

\bibitem[\citeproctext]{ref-robins1986}
Robins, J (1986) A new approach to causal inference in mortality studies
with a sustained exposure period---application to control of the healthy
worker survivor effect. \emph{Mathematical Modelling}, \textbf{7}(9-12),
1393--1512.

\bibitem[\citeproctext]{ref-robins2010alternative}
Robins, JM, and Richardson, TS (2010) Alternative graphical causal
models and the identification of direct effects. \emph{Causality and
Psychopathology: Finding the Determinants of Disorders and Their Cures},
\textbf{84}, 103--158.

\bibitem[\citeproctext]{ref-rohrer2018}
Rohrer, JM (2018) Thinking clearly about correlations and causation:
Graphical causal models for observational data. \emph{Advances in
Methods and Practices in Psychological Science}, \textbf{1}(1), 27--42.

\bibitem[\citeproctext]{ref-rohrer2022PATH}
Rohrer, JM, H√ºnermund, P, Arslan, RC, and Elson, M (2022) That's a lot
to process! Pitfalls of popular path models. \emph{Advances in Methods
and Practices in Psychological Science}, \textbf{5}(2).
doi:\href{https://doi.org/10.1177/25152459221095827}{10.1177/25152459221095827}.

\bibitem[\citeproctext]{ref-rotnitzky2017multiply}
Rotnitzky, A, Robins, J, and Babino, L (2017) On the multiply robust
estimation of the mean of the g-functional. Retrieved from
\url{https://arxiv.org/abs/1705.08582}

\bibitem[\citeproctext]{ref-rubin1976}
Rubin, DB (1976) Inference and missing data. \emph{Biometrika},
\textbf{63}(3), 581--592.
doi:\href{https://doi.org/10.1093/biomet/63.3.581}{10.1093/biomet/63.3.581}.

\bibitem[\citeproctext]{ref-rubin1980randomization}
Rubin, DB (1980) Randomization analysis of experimental data: The fisher
randomization test comment. \emph{Journal of the American Statistical
Association}, \textbf{75}(371), 591--593.
doi:\href{https://doi.org/10.2307/2287653}{10.2307/2287653}.

\bibitem[\citeproctext]{ref-rudolph2024mediation}
Rudolph, KE, Williams, NT, and Diaz, I (2024) {Practical causal
mediation analysis: extending nonparametric estimators to accommodate
multiple mediators and multiple intermediate confounders}.
\emph{Biostatistics}, kxae012.
doi:\href{https://doi.org/10.1093/biostatistics/kxae012}{10.1093/biostatistics/kxae012}.

\bibitem[\citeproctext]{ref-sheehan2022}
Sheehan, O, Watts, J, Gray, RD, \ldots{} Atkinson, QD (2022) Coevolution
of religious and political authority in austronesian societies.
\emph{Nature Human Behaviour}.
doi:\href{https://doi.org/10.1038/s41562-022-01471-y}{10.1038/s41562-022-01471-y}.

\bibitem[\citeproctext]{ref-shiba2023uncovering}
Shiba, K, Daoud, A, Hikichi, H, \ldots{} Kawachi, I (2023) Uncovering
heterogeneous associations between disaster-related trauma and
subsequent functional limitations: A machine-learning approach.
\emph{American Journal of Epidemiology}, \textbf{192}(2), 217--229.

\bibitem[\citeproctext]{ref-shpitser2022multivariate}
Shpitser, I, Richardson, TS, and Robins, JM (2022) Multivariate
counterfactual systems and causal graphical models. In
\emph{Probabilistic and causal inference: The works of {J}udea {P}earl},
813--852.

\bibitem[\citeproctext]{ref-shpitser2016causal}
Shpitser, I, and Tchetgen, ET (2016) Causal inference with a graphical
hierarchy of interventions. \emph{Annals of Statistics}, \textbf{44}(6),
2433.

\bibitem[\citeproctext]{ref-slingerland2020coding}
Slingerland, E, Atkinson, QD, Ember, CR, \ldots{} Gray, RD (2020) Coding
culture: Challenges and recommendations for comparative cultural
databases. \emph{Evolutionary Human Sciences}, \textbf{2}, e29.

\bibitem[\citeproctext]{ref-smith2022combining}
Smith, GD, Richmond, RC, and Pingault, J-B (2022) \emph{Combining human
genetics and causal inference to understand human disease and
development}, Cold Spring Harbor Laboratory Press.

\bibitem[\citeproctext]{ref-stensrud2023conditional}
Stensrud, MJ, Robins, JM, Sarvet, A, Tchetgen Tchetgen, EJ, and Young,
JG (2023) Conditional separable effects. \emph{Journal of the American
Statistical Association}, \textbf{118}(544), 2671--2683.

\bibitem[\citeproctext]{ref-stuart2018generalizability}
Stuart, EA, Ackerman, B, and Westreich, D (2018) Generalizability of
randomized trial results to target populations: Design and analysis
possibilities. \emph{Research on Social Work Practice}, \textbf{28}(5),
532--537.

\bibitem[\citeproctext]{ref-stuart2015}
Stuart, EA, Bradshaw, CP, and Leaf, PJ (2015) Assessing the
Generalizability of Randomized Trial Results to Target Populations.
\emph{Prevention Science}, \textbf{16}(3), 475--485.
doi:\href{https://doi.org/10.1007/s11121-014-0513-z}{10.1007/s11121-014-0513-z}.

\bibitem[\citeproctext]{ref-suzuki2020}
Suzuki, E, Shinozaki, T, and Yamamoto, E (2020) Causal Diagrams:
Pitfalls and Tips. \emph{Journal of Epidemiology}, \textbf{30}(4),
153--162.
doi:\href{https://doi.org/10.2188/jea.JE20190192}{10.2188/jea.JE20190192}.

\bibitem[\citeproctext]{ref-swanson1967}
Swanson, GE (1967) Religion and regime: A sociological account of the
{R}eformation.

\bibitem[\citeproctext]{ref-swanson1971}
Swanson, GE (1971) Interpreting the reformation. \emph{The Journal of
Interdisciplinary History}, \textbf{1}(3), 419--446. Retrieved from
\url{http://www.jstor.org/stable/202620}

\bibitem[\citeproctext]{ref-vanderlaan2011}
Van Der Laan, MJ, and Rose, S (2011) \emph{Targeted Learning: Causal
Inference for Observational and Experimental Data}, New York, NY:
Springer. Retrieved from
\url{https://link.springer.com/10.1007/978-1-4419-9782-1}

\bibitem[\citeproctext]{ref-vanderlaan2018}
Van Der Laan, MJ, and Rose, S (2018) \emph{Targeted Learning in Data
Science: Causal Inference for Complex Longitudinal Studies}, Cham:
Springer International Publishing. Retrieved from
\url{http://link.springer.com/10.1007/978-3-319-65304-4}

\bibitem[\citeproctext]{ref-vanderweele2009}
VanderWeele, TJ (2009a) Concerning the consistency assumption in causal
inference. \emph{Epidemiology}, \textbf{20}(6), 880.
doi:\href{https://doi.org/10.1097/EDE.0b013e3181bd5638}{10.1097/EDE.0b013e3181bd5638}.

\bibitem[\citeproctext]{ref-vanderweele2009a}
VanderWeele, TJ (2009b) Marginal structural models for the estimation of
direct and indirect effects. \emph{Epidemiology}, 18--26.

\bibitem[\citeproctext]{ref-vanderweele2012}
VanderWeele, TJ (2012) Confounding and Effect Modification: Distribution
and Measure. \emph{Epidemiologic Methods}, \textbf{1}(1), 55--82.
doi:\href{https://doi.org/10.1515/2161-962X.1004}{10.1515/2161-962X.1004}.

\bibitem[\citeproctext]{ref-vanderweele2015}
VanderWeele, TJ (2015) \emph{Explanation in causal inference: Methods
for mediation and interaction}, Oxford University Press.

\bibitem[\citeproctext]{ref-vanderweele2018}
VanderWeele, TJ (2018) On well-defined hypothetical interventions in the
potential outcomes framework. \emph{Epidemiology}, \textbf{29}(4), e24.
doi:\href{https://doi.org/10.1097/EDE.0000000000000823}{10.1097/EDE.0000000000000823}.

\bibitem[\citeproctext]{ref-vanderweele2019}
VanderWeele, TJ (2019) Principles of confounder selection.
\emph{European Journal of Epidemiology}, \textbf{34}(3), 211--219.

\bibitem[\citeproctext]{ref-vanderweele2022}
VanderWeele, TJ (2022) Constructed measures and causal inference:
Towards a new model of measurement for psychosocial constructs.
\emph{Epidemiology}, \textbf{33}(1), 141.
doi:\href{https://doi.org/10.1097/EDE.0000000000001434}{10.1097/EDE.0000000000001434}.

\bibitem[\citeproctext]{ref-vanderweele2013}
VanderWeele, TJ, and Hernan, MA (2013) Causal inference under multiple
versions of treatment. \emph{Journal of Causal Inference},
\textbf{1}(1), 1--20.

\bibitem[\citeproctext]{ref-vanderweele2012MEASUREMENT}
VanderWeele, TJ, and Hern√°n, MA (2012) Results on differential and
dependent measurement error of the exposure and the outcome using signed
directed acyclic graphs. \emph{American Journal of Epidemiology},
\textbf{175}(12), 1303--1310.
doi:\href{https://doi.org/10.1093/aje/kwr458}{10.1093/aje/kwr458}.

\bibitem[\citeproctext]{ref-vanderweele2020}
VanderWeele, TJ, Mathur, MB, and Chen, Y (2020) Outcome-wide
longitudinal designs for causal inference: A new template for empirical
studies. \emph{Statistical Science}, \textbf{35}(3), 437--466.

\bibitem[\citeproctext]{ref-vansteelandt2022a}
Vansteelandt, S, and Dukes, O (2022) Assumption-lean inference for
generalised linear model parameters. \emph{Journal of the Royal
Statistical Society Series B: Statistical Methodology}, \textbf{84}(3),
657--685.

\bibitem[\citeproctext]{ref-wager2018}
Wager, S, and Athey, S (2018) Estimation and inference of heterogeneous
treatment effects using random forests. \emph{Journal of the American
Statistical Association}, \textbf{113}(523), 1228--1242.
doi:\href{https://doi.org/10.1080/01621459.2017.1319839}{10.1080/01621459.2017.1319839}.

\bibitem[\citeproctext]{ref-watts2015}
Watts, J, Greenhill, SJ, Atkinson, QD, Currie, TE, Bulbulia, J, and
Gray, RD (2015) \emph{Broad supernatural punishment but not moralizing
high gods precede the evolution of political complexity in
{A}ustronesia} \emph{Proceedings of the Royal Society B: Biological
Sciences}, Vol. 282, The Royal Society, 20142556.

\bibitem[\citeproctext]{ref-watts2018}
Watts, J, Sheehan, O, Bulbulia, Joseph A, Gray, RD, and Atkinson, QD
(2018) Christianity spread faster in small, politically structured
societies. \emph{Nature Human Behaviour}, \textbf{2}(8), 559--564.
doi:\href{https://doi.org/gdvnjn}{gdvnjn}.

\bibitem[\citeproctext]{ref-weber1905}
Weber, M (1905) \emph{The protestant ethic and the spirit of capitalism:
And other writings}, Penguin.

\bibitem[\citeproctext]{ref-weber1993}
Weber, M (1993) \emph{The sociology of religion}, Beacon Press.

\bibitem[\citeproctext]{ref-webster2021directed}
Webster-Clark, M, and Breskin, A (2021) Directed acyclic graphs, effect
measure modification, and generalizability. \emph{American Journal of
Epidemiology}, \textbf{190}(2), 322--327.

\bibitem[\citeproctext]{ref-westreich2010}
Westreich, D, and Cole, SR (2010) Invited commentary: positivity in
practice. \emph{American Journal of Epidemiology}, \textbf{171}(6).
doi:\href{https://doi.org/10.1093/aje/kwp436}{10.1093/aje/kwp436}.

\bibitem[\citeproctext]{ref-westreich2019target}
Westreich, D, Edwards, JK, Lesko, CR, Cole, SR, and Stuart, EA (2019)
Target validity and the hierarchy of study designs. \emph{American
Journal of Epidemiology}, \textbf{188}(2), 438--443.

\bibitem[\citeproctext]{ref-westreich2017}
Westreich, D, Edwards, JK, Lesko, CR, Stuart, E, and Cole, SR (2017)
Transportability of trial results using inverse odds of sampling
weights. \emph{American Journal of Epidemiology}, \textbf{186}(8),
1010--1014.
doi:\href{https://doi.org/10.1093/aje/kwx164}{10.1093/aje/kwx164}.

\bibitem[\citeproctext]{ref-westreich2013}
Westreich, D, and Greenland, S (2013) The table 2 fallacy: Presenting
and interpreting confounder and modifier coefficients. \emph{American
Journal of Epidemiology}, \textbf{177}(4), 292--298.

\bibitem[\citeproctext]{ref-wheatley1971}
Wheatley, P (1971) \emph{The pivot of the four quarters : A preliminary
enquiry into the origins and character of the ancient chinese city},
Edinburgh University Press. Retrieved from
\url{https://cir.nii.ac.jp/crid/1130000795717727104}

\bibitem[\citeproctext]{ref-whitehouse2023}
Whitehouse, H, Francois, P, Savage, PE, \ldots{} Turchin, P (2023)
Testing the big gods hypothesis with global historical data: A review
and retake. \emph{Religion, Brain \& Behavior}, \textbf{13}(2),
124--166.

\bibitem[\citeproctext]{ref-williams2021}
Williams, NT, and D√≠az, I (2021) \emph{{l}mtp: Non-parametric causal
effects of feasible interventions based on modified treatment policies}.
doi:\href{https://doi.org/10.5281/zenodo.3874931}{10.5281/zenodo.3874931}.

\bibitem[\citeproctext]{ref-young2014identification}
Young, JG, Hern√°n, MA, and Robins, JM (2014) Identification, estimation
and approximation of risk under interventions that depend on the natural
value of treatment using observational data. \emph{Epidemiologic
Methods}, \textbf{3}(1), 1--19.

\end{CSLReferences}



\end{document}
