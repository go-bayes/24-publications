---
title: "An Invitation to Causal Inference in Environmental Psychology"
abstract: |
  This chapter introduces causal inference within environmental psychology, underscoring its fundamental differences from traditional statistical analysis. The content is organised into four main sections:
  1. **Non-technical introduction**: stating a causal question requires defining pre-specified contrasts between interventions experienced by an entire population. Because each individual can experience only one intervention, causal effects must be estimated using assumptions. We build intuitions for these assumptions by clarifying their satisfaction in randomised controlled experiments.
  2. **Causal Directed Acyclic Graphs (DAGs) tutorial**. Causal DAGs are potent tools for clarifying whether and how causal effects may be identified from data. We explain how they work. 
  3. **Practical examples**: we apply causal DAGs to common scenarios in observational environmental psychology.
  4. **Guidelines**: the main aim of this chapter is to motivate broader adoption of causal workflows so that environmental psychologists may systematically address causal questions that animate the interests but which presently remain elusive.
authors: 
  - name: Joseph A Bulbulia
    orcid: 0000-0002-5861-2056
    email: joseph.bulbulia@vuw.ac.nz
    affiliation: 
      name: Victoria University of Wellington, New Zealand, School of Psychology, Centre for Applied Cross-Cultural Research
      department: Psychology/Centre for Applied Cross-Cultural Research
      city: Wellington
      country: New Zealand
      url: www.wgtn.ac.nz/cacr
  - name: Donald W Hine
    orcid: 0000-0002-3905-7026
    email: donald.hine@canterbury.ac.nz
    affiliation: 
      name: University of Canterbury, School of Psychology, Speech and Hearing
      city: Canterbury
      country: New Zealand
      url: https://profiles.canterbury.ac.nz/Don-Hine
keywords:
  - DAGS
  - Causal Inference
  - Confounding
  - Environmental
  - Longitudinal
  - Psychology
format:
  pdf:
    sanitize: true
    keep-tex: true
    link-citations: true
    colorlinks: true
    documentclass: article
    classoption: [singlecolumn]
    lof: false
    lot: false
    number-sections: false
    number-depth: 4
    highlight-style: github
    geometry:
      - top=30mm
      - left=20mm
      - heightrounded
    template-partials: 
      - /Users/joseph/GIT/templates/quarto/title.tex
    header-includes:
      - \input{/Users/joseph/GIT/latex/latex-for-quarto.tex}
date: last-modified
execute:
  echo: false
  warning: false
  include: true
  eval: true
fontfamily: libertinus
bibliography: /Users/joseph/GIT/templates/bib/references.bib
csl: ./camb-a.csl
---

```{r}
#| label: load-libraries
#| echo: false
#| include: false
#| eval: false
#  fig-pos: 'htb'
#   html:
#    html-math-method: katex

# Include in YAML for Latex
# sanitize: true
# keep-tex: true
# include-in-header:
#       - text: |
#           \usepackage{cancel}


# for making graphs
library("tinytex")
library(extrafont)
loadfonts(device = "all")


# libraries for jb (when internet is not accessible)
# read libraries
source("/Users/joseph/GIT/templates/functions/libs2.R")

# read functions
source("/Users/joseph/GIT/templates/functions/funs.R")

# read data/ set to path in your computer
pull_path <-
  fs::path_expand(
    "/Users/joseph/v-project\ Dropbox/data/current/nzavs_13_arrow"
  )

# for saving models. # set path fo your computer
push_mods <-
  fs::path_expand(
    "/Users/joseph/v-project\ Dropbox/data/nzvs_mods/00drafts/23-causal-dags"
  )


# keywords: potential outcomes, DAGs, causal inference, evolution, religion, measurement, tutorial
# 10.31234/osf.io/b23k7

```

Psychological scientists are taught that "correlation does not imply causation." By "correlation," we refer to statistical measures of association between variables. Most statistical techniques —from t-tests to structural equation models- estimate associations from data. Although we know that measuring associations does not imply causation, in observational settings, we often persist in reporting statistical associations as if they are meaningful and, perhaps, even as tentative evidence for causation. The purpose of this chapter is to clarify why such reporting is confused and misleading, and to guide you toward better practices.

What do we mean by "causation?" Causation has been a topic of extensive interest and debate in philosophy [@lewis1973]. Here, we narrow our focus. We consider the assumptions under which it is possible to estimate causal effects from data and how to estimate them. In causal-effect estimation, or "causal inference," investigators seek to quantify the average differences across a specified population or sub-population (the "target population") that interventions would produce on well-defined outcomes. This requires comparing at least two states of the world: one where the population experiences a treatment and another where they do not or experience a different level of treatment. While the concept of causation in causal inference is more narrowly defined than causation itself, it draws intellectual inspiration from David Hume, who, in his *Enquiries Concerning Human Understanding* (1751), characterises the cause-effect relationship as follows:

  > "If the first object had not been, *the second never would have existed*." [@hume1902] (emphasis added).

This conceptualisation aligns closely with the counterfactual approach in causal inference, which considers what would have happened to an outcome in both the presence and the absence of a treatment.

Hume's definition relies on counterfactual thinking—specifically, comparing two mutually exclusive states of the world: one where an event occurs and one where it does not. For Hume, assessing causation requires not just observing events as they happen but also considering how the world might have differed had those events not occurred. Such comparisons, where we consider scenarios where treatments did not take place, are known as "counterfactual contrasts;" such contrasts are fundamental to causal-effect estimation. In modern causal inference, these contrasts are formalised within the potential outcomes framework, which estimates the average difference in outcomes between treated and control conditions had the entire population of interest been treated at different levels of the intervention.

Importantly, although statistically evaluating associations from data is essential for estimating average treatment effects, causation estimation cannot be derived from the study of associations alone. Every realisation in the data provides information about only one of the two potential outcomes necessary for a causal contrast. We may only obtain causal contrasts under assumptions that are not testable from the data. Moreover, a careful and systematic workflow is required. By the end of this chapter, you will understand why, without such a workflow, common analytic techniques—such as linear regression, correlation, and structural equation modelling—lack causal interpretations and may mislead investigators.

[**Part 1**](#section-part1) introduces the counterfactual framework of causal inference, focusing on the three fundamental assumptions necessary for estimating average causal effects. We build intuition for these concepts by considering randomised controlled trials, where these assumptions are met through enforced randomisation [@westreich2012berkson; @hernan2017per; @westreich2015; @robins2008estimation].

[**Part 2**](#section-part2) introduces causal Directed Acyclic Graphs (DAGs), powerful tools for visualising and addressing the assumption of conditional exchangeability (also known as the "no unmeasured confounders" assumption). Here, we introduce Judea Pearl's rules of d-separation [@pearl1995], which allow investigators to identify appropriate variables to adjust for confounding. Although most psychological scientists are aware that regression adjustment is commonly used to control confounding, they may not be fully aware of the formal criteria required to select appropriate adjustment variables. However, understanding the formal criteria is essential because over-adjustment may introduce bias or reduce statistical power, leading to misleading conclusions.


[**Part 3**](#section-part3) provides seven practical examples where causal diagrams address real-world causal questions. These examples clarify three objectives: (1) constructing causal diagrams that accurately represent hypothesised relationships between variables; (2) identifying and evaluating the assumptions required to estimate causal effects from observational data, ensuring they align with the underlying causal structure; and (3) using DAGs to guide the estimation of causal effects, from identifying necessary adjustment sets to applying appropriate statistical methods for valid inference. These examples serve as practical guides for translating causal questions into causal identification models, clarifying the assumptions needed to estimate causal relationships from data. It is only after these identification assumptions are stated that that we can develop statistical estimators and perform statistical analysis [@vansteelandt2012;@wager2018]

[**Part 4**](#section-part4) offers practical guidelines for environmental psychologists aiming to infer causal effects from observational data. Given that assumptions about causal relationships are often uncertain or subject to debate, we recommend developing multiple causal diagrams and reporting their corresponding analyses to evaluate different plausible causal pathways. This approach enhances transparency in how causal relationships are inferred.

We conclude by suggesting further readings and resources for those interested in learning more about causal inference.

## Part 1: An Overview of the Counterfactual Framework for Causal Inference {#section-part1}

### The Origins of Causal Inference

Causal inference began in the early 20$^{th}$ century with Jerzy Neyman's invention of the potential outcomes framework, initially developed for agricultural experiments. Neyman realised that understanding the causal effect of an experimental treatment required comparing potential outcomes under different treatment conditions, even though only one outcome could be observed for each unit [@neyman1923]. This framework laid the foundation for modern causal inference.

Donald Rubin extended this approach to observational settings into what is now known as the Neyman-Rubin Causal Model [@rubin1976; @holland1986]. James Robins advanced causal inference by introducing the mathematical and conceptual framework for understanding causal effects in settings where there are two or more sequential treatments over time [@robins1986]. Robins and his colleagues also developed statistical tools such as marginal structural models and structural nested models to enable quantitative assessment of time-varying treatments and time-varying confounding [@hernan2024WHATIF]. The computer scientist Judea Pearl developed causal Directed Acyclic Graphs (DAGs), and proved that these graphical models may be applied to evaluate one of the three fundamental assumptions of causal inference, namely "no unmeasured confounding", also known as "conditional exchangeability", "selection on observables" and "ignorability", or in Pearl's formalism, "d-separation" or equivalently, "no open backdoor path"[@pearl1995a; @pearl2009]. The jargon that has evolved in causal inference is not restricted to the concept of no unmeasured confounding. In [**Part 2**](#section-part2) we encounter the concept of "causal identification," which is unrelated to the concept of "statistical identification."  

Although terminology differs across the subfields of causal inference, the mathematical basis of causal inference evolved nearly independently in the fields of biostatistics, economics, and computer science. This shared foundation, anchored in proofs, has enabled a remarkable agreement across disciplinary lines despite terminological differences. In every approach to causal inference, a causal effect is conceptualised as a contrast between two states of the world, only one of which may be observed on any individual -- a "counterfactual contrast" also known as a "causal estimand" [@hernan2024WHATIF]. Notably, prior to intervention, these scenarios are purely hypothetical. Post-intervention, only one scenario is actualised for each realised treatment, leaving the alternative as a non-observed counterfactual. For any individual unit to be treated, that only one of the two possible outcomes is realised underscores a critical property of causality: causality is **not directly observable** [@hume1902]. Causal inference, therefore, can only quantify causal effects by combining data with counterfactual simulation [@edwards2015; @bulbulia2023a]. The concept of a counterfactual data science --  may sound strange. However, anyone who has encountered a randomised experiment has encountered counterfactual data science. Before building intuitions for causal inference from the familiar example of experiments, let's first build intuitions for the idea that causal quantities are never directly observed. 

## The Fundamental Problem of Causal Inference: Counterfactual Comparisons in Environmental Psychology

Imagine you are faced with a significant life decision: enrolling in a graduate programme in environmental psychology in New Zealand or accepting a job offer from a leading renewable energy company. This choice will shape your future, influencing your lifestyle, income, and social network. Which option is best for you?

The challenge is that, once you choose one path, you cannot observe how your life would have unfolded on the other. If you go to graduate school, you will experience that outcome, but the outcome of taking the job remains unknown—and vice versa. This is **the fundamental problem of causal inference**: we can never observe both potential outcomes for the same individual, so the path not taken remains an unobservable "what if?" — a counterfactual that cannot be measured [@holland1986].

Again, counterfactual comparisons lie at the heart of causal inference, as they involve contrasting what actually happened with what would have happened under a different scenario. In environmental psychology, computing counterfactual contrasts is essential for understanding the effects of psychological and behavioural interventions on well-defined outcomes. However, the full data required to make such contrasts are inevitably partially missing and can only be recovered by assumptions [@edwards2015; @westreich2015].

### Causal Inference in Experiments: The Problem of Missing Counterfactuals

Consider a question relevant to environmental psychologists: What is the causal effect of access to green spaces on subjective happiness? Denote happiness by $Y$, where $Y_i$ represents the happiness of individual $i$. Assume that "subjective happiness" is a coherent concept. For now, assume that errors in its measurement are not systematically linked to access to green spaces. Suppose further that "ample access to green space" is represented as a binary variable: $A = 1$ for "ample access" and $A = 0$ for "lack of ample access." These conditions are mutually exclusive. While we simplify the treatment to a binary variable, the concepts apply to more complex or continuous treatments. Estimating causal effects always requires a contrast between well-defined treatment conditions. Importantly, defining clear causal questions is essential but often neglected in psychological science outside experimental work.

Imagine our aim is to compare potential outcomes under different treatment conditions. Specifically, we contrast the happiness of individuals with access to green space ($A = 1$) against those without ($A = 0$). The target population for this contrast should be explicit—for example, all New Zealand residents in 2024.

A clear causal question, framed as a counterfactual contrast—also known as a "causal estimand"—might be:

> *"Among New Zealand residents, does access to abundant green space increase self-perceived happiness compared to environments without such spaces?"*

Now, imagine—hypothetically and ethically—that we could randomise individuals to high or low green space access. Notice that even if such an experimental setup were possible, causal inference would face missing data in the potential outcomes. For each person assigned to treatment, only one potential outcome is observed, depending on the treatment they receive. The outcome they would have experienced under the alternative treatment is unobserved -- it remains counterfactual. Such missingness is the fundamental problem of causal inference, raised to the level of treatment groups. We only observe $Y_i(1)$ for individuals with $A_i = 1$ and $Y_i(0)$ for individuals with $A_i = 0$. The other potential outcome for each individual remains unobserved. However, although this missingness in the "full data" poses a challenge at the individual level, we can estimate the average treatment effect within the sampled population without needing to obtain individual-level causal effects.

Although individual causal effects are generally not recoverable from data, we can recover causal effect estimates by changing our causal question. For example, although randomised controlled experiments do not solve the fundamental problem of causal inference at the level of individuals, they may obtain consistent causal effect estimates for average treatment effects at the level of populations. Randomised experiments solve the missing data problem at the heart of causal inference by satisfying three fundamental assumptions required for obtaining average treatment effects. These are the (1) Conditional Exchangeability Assumption, (2) The Causal Consistency Assumption, and (3) The Positivity Assumption. We call an "observational study" one in which the data have not been obtained by randomised treatment assignment and controlled administration of treatment. We then consider how, when observational studies satisfy the three fundamental assumptions of causal inference, investigators may obtain consistent causal effect estimates for average treatment effects.

Before proceeding, we offer the following clarification: the concept of within-subject effects as understood in traditional statistical analyses does not directly apply in causal inference and may even appear incoherent within this framework. In conventional analyses, within-subject effects refer to changes observed within the same individual across different conditions or over time, using repeated measures to control for individual-specific variability. However, causal inference is fundamentally concerned with estimating causal effects based on hypothetical interventions and potential outcomes—scenarios that cannot be simultaneously realized for a single individual. Since an individual cannot both receive and not receive a treatment at the same time, we cannot observe both potential outcomes required to define the causal effect at the individual level. Instead, causal inference relies on comparisons of population averages under distinct treatments or treatment regimes. Therefore, what is often termed a 'within-subject effect" is more accurately understood as a within-subject research design in the context of causal inference. In causal inference, there is no coherent distinction for a within-person/between-person causal effect [@rohrer2023withinbetween]

#### Assumption 1: Conditional Exchangeability

First, we define the expected value of a treatment $A=a$ as the sum of individual counterfactual (or equivalently) "potential" outcomes for individuals within a specified population:


$$
\mathbb{E}[Y(a)] \equiv \frac{1}{n} \sum_{i=1}^n Y_i(a)
$$

Note that in causal inference, we assume these potential outcomes to be real, even if determined stochastically.

We next define the average treatment effect (ATE) as a contrast between the averages of the potential outcomes in each treatment condition:


$$
\text{ATE} = \mathbb{E}[Y(1)] - \mathbb{E}[Y(0)]
$$

Suppose that the individuals in a sample are representative of the population of interest—the target population. In this setting, randomisation ensures there is no common cause of the treatment and the potential outcomes that would be observed under treatment. That is, if treatment assignment is determined by chance, any difference in the average response within treatment groups is best explained by the treatment itself.

Mathematically, we express the absence of a common cause of treatment assignment and the potential outcomes under treatment:

$$
Y(a) \coprod A
$$

This notation means that the potential outcomes $Y(a)$ are independent of ($\coprod$) the treatment assignment $A$. Importantly, the observed outcomes need not be independent of treatment. Note that we do not require that exposure is independent of the actual outcomes under exposure ($Y|A$), which would be equivalent to requiring there can be no causal effects for treatment! Randomisation achieves unconditional independence, allowing us to estimate average causal effects because treatments remain independent of potential outcomes under treatment. Put differently, randomisation ensures that average treatment differences (or equivalently, differences in treatment averages) are the result of the treatments themselves rather than the effects of another variable. 

Importantly, we can relax the requirement for unconditional independence and allow randomisation to occur conditional on certain measurable features of the sampled population. For example, suppose we randomise treatment within different age cohorts such that older individuals have a greater probability of receiving the treatment than younger individuals. Assume that the treatment effect is constant across all ages. In this scenario, we would expect to see higher average treatment effects in older cohorts simply because a larger proportion of older individuals receive the treatment.

When randomisation occurs conditional on a measurable feature such as age, and the treatment effect is constant, differences in treatment probabilities across groups can lead to variations in observed average treatment effects. However, if we adjust for this difference in the probability of receiving treatment, or simply compare treatment effects within the different age strata, the bias from differential treatment group assignment disappears.

We use the symbol $L$ to denote measured variables that might be common causes of the treatment ($A$) and the treatment effect ($Y(a)$). We write that the potential or counterfactual outcome $Y(a)$ is independent of $A$, conditional on $L$ as follows:

This assumption means that, within levels of $L$, the treatment assignment is as good as random. In experiments, randomisation ensures unconditional exchangeability ($Y(a) \coprod  A$). However, to compute causal effects from data in which the treatments are not randomised, we must believe that within levels of measured covariates $L$, the treatment is as good as random. Practically, this involves measuring all common causes of $A$ and $Y$.

##### Challenges in Observational Settings

In observational studies, achieving conditional exchangeability is challenging because treatment assignment is not controlled. Individuals with access to green spaces may differ from those without in various ways. We have noted that income might be a common cause of both access to green space and happiness. However, there are other common causes, such as age, health status, and past happiness. Different age groups may have varying access to green spaces and different happiness baselines. Healthier individuals might choose to live near green spaces and also report higher happiness. Furthermore, happiness might cause people to seek green spaces, and happiness in one's past might be a cause of happiness in one's future.

Merely observing statistical associations between access to green space and happiness does not itself resolve the causal question of whether intervening on access across a population would affect happiness levels, and if so, in which direction and by how much.

#### Assumption 2: Causal Consistency

Causal consistency requires that the potential outcome under the treatment actually received equals the observed outcome:

$$
Y_i = Y_i(a) \quad \text{if } A_i = a
$$

Note that in causal inference, we compare potential outcomes under at least two different treatments, say, $Y(a = 1)$ and $Y(a = 0)$ (in causal inference, we use a lowercase variable to note that the random variable $A$ is fixed to a certain level ($A = a$). To compute causal contrasts, the counterfactual or potential outcomes under treatment must be observable. The causal consistency assumption allows investigators to link counterfactual or potential outcomes to observed outcomes. It might seem obvious that if one receives a treatment, we can say that the observation of the outcome following treatment is no longer counterfactual—it is actual. However, for the causal consistency assumption to hold across a population, we must assume that the treatments are well-defined and consistently administered. Thus, the causal consistency assumption is a two-edged sword. On the one hand, we may use it—if other assumptions are satisfied—to compute causal contrasts. Simply put, causal consistency puts the factual into counterfactual. 

In controlled experiments, we may typically take causal consistency for granted. The investigators administer consistent treatments. However, in observational settings, no such control is ensured. For example, one reason it has been difficult to investigate the causal effects of weight loss from massive observational medical datasets is that there are many ways to lose weight—eating less and healthfully and exercising causes people to lose weight. However, one may also lose weight from smoking, psychological distress, stomach stapling, amputation, cancer, and famine. The mechanisms of the latter forms of weight loss are unhealthy. Thus, stating a causal contrast, for example, as the expected difference in all-cause mortality after five successive years of weight loss, is an invitation for confusion. The treatments that lead to weight loss in medical data are not comparable across all cases (refer to @vanderweele2013, @bulbulia2023).

##### Challenges in Observational Settings

Similarly, in observational environmental psychology, the treatments of interest may not be standardised. For green space access, we might worry about variability in the green spaces we have measured: not all green spaces are equal—differences in size, quality, and amenities can affect outcomes. Moreover, there is likely considerable variability in exposure levels; the amount of time that individuals spend in green spaces can vary widely. These variations can violate causal consistency because the "treatment" is not the same across individuals on which treatments are to be compared. Supplemental materials S-Appendix-B address the causal consistency assumption in detail.

#### Assumption 3: Positivity

Positivity requires that every individual has a non-zero probability of receiving each level of the treatment, given their covariates $L$:

$$
P(A = a \mid L = l) > 0 \quad \text{for all } a, l
$$

This assumption ensures that we have data to compare treatment effects across all levels of $L$. For example, suppose we are interested in the causal effects of vasectomy on happiness.  It would not make sense to include biological females in this study because biological females cannot have vasectomies [@westreich2010; @hernan2023].

##### Challenges in Observational Settings

In practice, some individuals may have zero probability of receiving certain treatments. For example, some regions may lack green spaces entirely owing to geographic constraints. Economic barriers may mean that low-income individuals do not have access to areas with ample green spaces. Policy restrictions, such as zoning laws, may also prevent individuals from accessing specific environments.

### Summary

Causal inference fundamentally relies on statistical inference; however, statistics is only a part of a larger workflow that begins with defining a causal quantity. This involves specifying contrasts that would be observed if we had complete data for the population of interest, treated at different levels of the intervention [@ogburn2021]. The next step is to determine how these contrasts can be estimated from the available data.

Standard statistical practices, such as regression adjustment and structural equation modelling, often fail to accurately estimate causal effects. This is largely because these models typically do not explicitly define the treatments being compared, nor do they properly account for the causal structures that can bias treatment-outcome associations.

In the following section, we introduce causal directed acyclic graphs (causal DAGs) -- intuitive graphical tools that help researchers evaluate the complex conditional dependencies needed to satisfy the "no unmeasured confounders" assumption. This is done by inspecting relationships in a graph using a straightforward algorithm known as the rules of d-separation [@pearl1995; @pearl2009a]. Causal DAGs also reveal how standard practices in statistical modelling, such as over-adjustment in regression or mediation analysis in structural equation modelling, may inadvertently introduce bias through inappropriate variable adjustment. Such over-adjustment can obscure or distort true causal relationships.

In causal inference, non-causal associations are termed "spurious" because they lack coherent interpretation. It is often unclear how to interpret such associations, as they might imply no causal effect, a causal effect of a different magnitude, or even a causal effect in the opposite direction. By explicitly representing the causal structure and carefully selecting which variables to adjust for, causal DAGs can help environmental psychologists more effectively estimate causal effects within their research.

## Part 2: An Introduction to Causal Diagrams {#section-part2}


We introduce causal DAGs by defining essential terminology. Refer to [**Appendix A**](#appendix-a) for a detailed glossary. 

### Elements of Causal Diagrams

A causal directed acyclic graph (DAG) is a diagram that clarifies whether and how the assumption of conditional exchangeability (no unmeasured confounders) may be satisfied from data. A causal DAG is constructed with the aim of evaluating this assumption for a specific treatment variable (or set of treatment variables) and a specific outcome (or set of outcomes). When there are multiple treatments, the assumptions required for causal identification become increasingly complex. Here, we restrict our focus to the setting in which there is one treatment variable. The difficulty of ensuring identification for multiple treatments will become evident from this restricted setting.

It is important to note at the outset that in a causal DAG, we do not attempt to represent all causal relationships within a system, but only those that are relevant to the causal question of interest. This requires a conceptual shift from the use of graphs in the structural equation modelling (SEM) tradition, where the goal is to represent a system of statistical relationships obtained from data. Instead, causal DAGs focus on the minimal set of relationships necessary to address the causal question and assess whether pre-specified causal contrasts can be identified from data.


#### 1. **Nodes**

Nodes represent variables — or states of the world relevant to our research question. 

#### 2. **Arrows (also known as Edges)**

Arrows indicate the direction and presence of causal relationships between nodes. They represent the assumed flow of causal influence from a “parent” (originating variable) to a “child” (receiving variable). For example, an arrow from $A \rightarrow Y$ suggests that $A$ causally influences $Y$.

The goal of a causal DAG is to determine whether consistent causal effect estimates can be obtained from data by inspecting the relationships within the diagram.

#### 3. **Conditioning**

Recall that $Y(a) \coprod A \mid L$ is the conditional exchangeability assumption—or the assumption of "no unmeasured confounders." To infer causal effects, we must ensure that, conditional on To infer causal effects we must ensure that conditional on $L$, treatment assignment is "as good as random." That is, given $L$, the treatment assignment is essentially random, and any observed association between $A$ and $Y$ can be interpreted causally.

We denote variables that we "control for," "condition on," or "adjust for" by enclosing them in a box: $\boxed{L}$.

### d-Separation

In causal diagrams, d-separation is a graphical criterion used to determine whether a set of variables is independent of another set, given a third set, based on the structure of the graph. A path between two variables is considered blocked or d-separated if, given the variables we condition on, the path does not transmit any association between the variables. If all paths between two variables are blocked, the variables are d-separated and are conditionally independent. Conversely, if any path is unblocked (d-connected), the variables may be dependent [@pearl1995; @pearl2009a]. Note that the “d” in d-separation stands for "directional."

The rules of d-separation may be briefly stated as follows:

#### 1.	Fork (Common Cause):
Diagram: $B \leftarrow \boxed{A} \rightarrow C$

Implication: conditioning on the common cause  $A$  blocks the association between  $B$  and  $C$ . Therefore,  $B$  and  $C$  are independent given  $A$ :
 
 $$
 B \coprod C \mid A
 $$ 

#### 2.	Chain (Mediator):
Diagram: $A \rightarrow \boxed{B} \rightarrow C$

Implication: conditioning on the mediator  $B$  blocks the association between  $A$  and  $C$ . Thus,  $A$ and  $C$  are independent given  $B$ :
 $$
 A \coprod C \mid B
 $$

#### 3.	Collider (Common Effect):

Diagram: $A \rightarrow \boxed{C} \leftarrow B$

Implication: conditioning on the common effect  $C$  (or any of its descendants) opens the path between  $A$  and  $B$ , introducing dependence. Hence,  $A$  and  $B$  are dependent given $C$:

$$
A \cancel \coprod B \mid C
$$

We can restate the rules d-separation as follows:

1.	**Colliders block paths when unconditioned:** an open path (with no variables conditioned on) is blocked if it contains a collider— a node where two arrows converge. If $A \rightarrow \boxed{C} \leftarrow B$, then $A$ and $B$ are blocked from being associated with each other when $C$ is not conditioned upon.
2.	**Conditioning on a collider opens a path:** conditioning on a collider opens a previously blocked path, potentially introducing an association between variables that are not causally related. If $A \rightarrow \boxed{C} \leftarrow B$, conditioning on $C$ allows information to flow between $A$ and $B$, leading to a possible association in the absence of a direct causal association.
3.	**Conditioning on a descendant of a collider also opens the path:** conditioning on a descendant of a collider has a similar effect. If $A \rightarrow \boxed{C} \leftarrow B$, and $C \to D$ then conditioning on $C^{\prime}$ is akin to conditioning on $C$, the path $A \rightarrow C \rightarrow \boxed{C^\prime} \leftarrow B$ becomes unblocked. Thus, conditioning on $C^\prime$ opens an association between $A$ and $B$.
4.	**Conditioning on non-collider nodes blocks the path:** if a path does not contain a collider, conditioning on any variable along that path blocks it. For instance, in the diagram $A \rightarrow \boxed{B} \rightarrow C$ conditioning on $B$ blocks the path from $A$ to $C$, rendering $A$ and $C$ conditionally independent given $B$ [@pearl2009a; @hernan2024WHATIF, p. 78].

Pearl proved these the rules of d-separation in the 1990s, demonstrating that we may use causal directed acyclic graphs to evaluate the conditions under which consistent causal effect estimate are possible from data [@pearl1995; @pearl2009a]. Note again that the structures in a graph must be assumed and cannot generally be verified by the data alone.

#### Absence of Causality: Two Variables with No Arrows

When no arrows connect $A$ and $B$, we assume they do not share a causal relationship and are statistically independent. This means that knowing something about $A$ does not give us any information about $B$ and vice versa. Graphically, we represent this relationship as:

$$
\xorxALARGE
$$

That is, as two nodes without any linking arrow.

#### Causal Structure 1: Direct Causation Between Two Variables

A causal arrow ($A \to B$) signifies that changes in $A$ directly cause changes in $B$, creating statistical dependence between them. This direct causal link is graphically depicted as:

$$
\xtoxALARGE
$$

That is, as two nodes linked by an arrow.

#### **Rule 1: Ensure That the Treatment Precedes the Outcome** {#sec-four-rules}

Causality follows the arrow of time; an outcome occurs after the intervention that causes it. Therefore, the treatment ($A$) must occur before the outcome ($B$) in order to claim causality. This underscores the importance of temporal precedence in causal inference. Where the relative timing of events is unknown—as it often is in observational data—we cannot ensure that the treatment precedes the outcome unless we make additional assumptions. Such assumptions may be implausible or untestable. Where this is the case, we cannot rule out the possibility that the outcome precedes the treatment.


##### Motivating Example

Suppose we find an association between conservation behaviours and happiness. We might infer that conservation behaviours cause happiness. However, it is also possible that happy people are simply more likely to engage in conservation behaviours. If we do not have longitudinal data that shows treatment preceding the outcome, we cannot rule out this alternative explanation.

#### Causal Structure 2: The Fork Structure—Common Cause Scenario

The fork structure occurs when a variable $A$ influences both $B$ and $C$ ($A \rightarrow B$ and $A \rightarrow C$). This makes $A$ a common cause of $B$ and $C$. Graphically:


$$
\forkLARGE
$$

#### Rule 2: Condition on the Common Cause to Block Confounding of The Treatment/Outcome Association
To estimate the causal effect between $B$ and $C$, we need to adjust for the common cause $A$ (indicated by $\boxed{A}$). This adjustment blocks the confounding path and removes any non-causal association between $B$ and $C$.

##### Fork Structure: Motivating Example

Suppose data reveals that areas with higher rates of bicycle commuting also have lower levels of psychological distress. Does bicycle commuting directly reduce distress? Not necessarily. Sunshine hours ($A$) could be a common cause:

- Sunshine ($A$) encourages bicycle commuting ($B$).
- Sunshine ($A$) reduces psychological distress ($C$).

By adjusting for sunshine ($\boxed{A}$), the apparent link between bicycle commuting ($B$) and psychological distress ($C$) disappears. This is an example of a spurious association -- one that that does not arise from a direct causal relationship between $B$ and $C$ but rather from their common cause, $A$


#### Causal Structure 3: The Chain Structure (Mediators)

The chain structure ($A \rightarrow B \rightarrow C$) illustrates a scenario where $A$ affects $B$, and $B$ in turn affects $C$. In this case, $B$ acts as a mediator in the relationship between $A$ and $C$. Graphically:

$$
\chainLARGE
$$

#### Rule 3: Avoid Conditioning on the Mediator for Total Effect of the Treatment and Outcome

If we are interested in estimating the total causal effect of $A$ on $C$, we should not condition on the mediator $B$. Conditioning on $B$ would **block** the path from $A$ to $C$, making $A$ and $C$ appear independent, even though there is a causal relationship. 

Many psychological scientists are taught to adjust for a rich array of confounders without attending to hazards of over-adjustment. Consider the following example of how over-adjustment can distort causal pathways, potentially masking true relationships.


##### Conditioning on a Mediator: Motivating Example

Suppose we examine whether renovating green spaces ($A$) in urban areas reduces crime rates ($C$). The renovation improves community engagement ($B$), which then reduces crime. If we adjust for community engagement, it might look like the green space renovation has no effect on crime, when in fact it does, but indirectly through community engagement. This example illustrates the risks of **over-conditioning bias.** Over-conditioning bias occurs when adjusting for variables that either lie on the causal path between treatment and outcome or act as mediators or colliders. Such adjustment can distort the causal relationships, potentially blocking the effect of interest or introducing spurious associations.

Let us denote the relative timing of events using subscripts. Here, we assume that $A_1 \to B_2 \to C_3$. It is clear from the rules of d-separation that $A_{1} \rightarrow \boxed{B_2} \rightarrow C_{3}$ (conditioning on $B_2$) blocks the path from $A_1$ to $C_3$.  However, what if $B_0 \to A_1$ and $B_0 \to C_3$. The rules of d-separation tell us that to obtain an unbiased causal effect estimate of $A_1 \to C_3$ we must condition on $B_0$.  With cross-sectional data we cannot typically ensure the relative timing of confounders, treatments, and outcomes.  For this reason, we should collect time-series data to ensure that the relative timing of events assumed in our model is appropriate. 

In some cases, investigators may want to distinguish between direct and indirect effects, as is done in causal mediation analysis. However, performing causal mediation analysis requires making additional assumptions about the relationships among variables and about unmeasured confounding. These assumptions are often difficult to verify. Causal mediation analysis goes beyond the scope of our current discussion, however interested readers may refer to @vanderweele2015; @bulbulia2024swigstime.

#### Causal Structure 4: The Collider Structure: Conditioning on the Common Effect of a Treatment and Outcome.

The collider structure ($A \to C$, $B \to C$) occurs when two variables $A$ and $B$ independently cause a common effect $C$. Initially, $A$ and $B$ are independent. However, conditioning on the collider $C$ introduces a non-causal or "spurious" association between $A$ and $B$. Graphically:

$$
\immoralityLARGE
$$

This gives us two rules: 

#### Rule 4: Do Not Condition on a Collider of the Treatment and Outcome

When assessing the causal effect of $A$ on $B$, avoid conditioning on a collider ($C$) or its descendants, as it may introduce a misleading association between $A$ and $B$.


#### Rule 5: Do Not Condition on Descendants of Colliders of the Treatment and Outcome

Causal relationships induce associations. Conditioning on a descendant of a collider is similar to conditioning on its parent, as the descendant serves as a proxy for its parent. Therefore, we should avoid conditioning on a descendant of a collider (or mediator).

#### Conditioning on a Collider: Motivating Example

Consider a scenario where we want to determine whether access to green spaces ($A$) makes people happier ($B$). If we condition on health status ($C$), which is influenced by both $A$ and $B$, we may introduce a spurious association between $A$ and $B$. This occurs because conditioning on $C$ creates an artificial link between $A$ and $B$, even if no direct causal relationship exists between them.

For instance, among individuals with poor health (low $C$), those with greater access to green spaces ($A$) may appear less happy ($B$). When we stratify by $C$, a negative association between green space access and happiness might emerge, even if $A$ and $B$ are not causally related. Similarly, among healthy individuals (high $C$), those with less access to green spaces ($A$) may seem happier ($B$), again suggesting a negative association upon stratifying by $C$.

This example demonstrates the risks of over-conditioning bias. By ensuring that confounders precede both the treatment and the outcome, we can often avoid collider bias, though not always. In our upcoming discussion of M-bias, we will explore how adjusting for pre-treatment confounders may sometimes induce collider bias. Additionally, we will discuss when conditioning on post-treatment proxies for pre-treatment confounders ($L^\prime$) may be a practical strategy for confounding control.

For now, it is essential to recognise that collider bias is a form of over-conditioning bias. Its threat to valid causal inference can be understood by examining the basic relationships in a causal Directed Acyclic Graph (DAG). Over-conditioning bias has not been fully appreciated in traditional structural equation modelling approaches. Pearl’s rules of d-separation show that when two arrows converge on the same node, their parent nodes may become spuriously associated. To estimate any causal path, it must remain unblocked by other nodes and free from unmeasured common causes.


### Complex Causal Relationships Are Composed of Five Elementary Structures Whose Features We Assume


All forms of confounding bias stem from combinations of the basic causal structures outlined above (absence of cause, causality, forks, chains, and colliders). Understanding these elements allows us to identify potential confounders based on the assumptions encoded in a causal diagram.

There are three further observations readers should note. 

First, causal directed acyclic graphs (DAGs) provide a framework for structuring assumptions about causal relationships. They cannot prove causation on their own, as multiple causal diagrams are typically consistent with the data. The strength of causal DAGs lies in helping researchers understand the implications of their assumptions. Therefore, causal DAGs should be created in collaboration with domain experts, and when experts disagree, multiple causal diagrams should be proposed to reflect differing assumptions.

Second, causal inference relies on the assumption that the treatment precedes the outcome in time. Cross-sectional data, that is, data in which information is collected at a single point in time, typically lacks the temporal resolution that investigators require to establish temporal ordering. In the next section we consider problems that temporal ambiguity bring beyond the obvious problem of reverse causality (it is the outcome that causes the treatment).

Third, longitudinal data, which follow the same subjects over multiple time points, allows for a clear temporal ordering of events, however longitudinal data are insufficient to establish causality. Of course, by tracking when a treatment occurs relative to the outcome, longitudinal data enables researchers to establish the sequence necessary to support causal claims. Therefore, in studies aiming to make causal inferences, longitudinal data should always be sought to confirm that the confounders precede the treatment, and the treatment precedes the outcome. Yet although longitudinal data are desirable they are not a panacea. We have seen that to obtain a causal effect estimate requires a workflow that begins by stating a clearly defined causal question, with reference to a counterfactual contrast between clearly specified treatments applied to a target population. We must then ensure that all assumptions required for causal inference are satisfied, most especially that treatment effects may be identified from data. Only after we have stated our causal question and checked that our data support the causal assumptions required to identify our targetted causal estimand may we proceed to statistical analysis.

## Part 3. How to Use Causal Diagrams for Causal Identification Tasks- Worked Examples {#section-part3}

### Notation

Causal diagrams use specific symbols to represent elements essential in causal inference [@pearl1995; @pearl2009; @greenland1999].  We review our notation.

- **Treatment ($A$)**: an intervention or exposure, such as giving people access to green space.
- **Outcome ($Y$)**: the variable of interest affected by the treatment, such as subjective well-being. Recall that $Y(a)$ represents the effect on $Y$ when $A$ is set to a specific value, $a$.
- **Measured Confounders ($L$)**: variables that may influence both the treatment and outcome, such as age, gender, or income.
- **Measured Descendants of Confounders Not Affected By Treatment or Outcome: ($L^\prime$)**: we shall see that by conditioning on measured descendants of confounders not affected by the treatment, we may adjust for the effects of confounders. That is, descendants of confounders function as surrogates for the confounders.
- **Unmeasured Confounders ($U$)**: variables not directly observed but potentially influence the treatment and outcome, leading to a non-causal or "spurious" association between treatment and outcome.
@tbl-04 provides seven worked examples that put causal diagrams to work.  Our example will focus on the question of whether access to green space affects happiness and approach this question by focusing on how different assumptions about (i) the structure of the world and (ii) the observational data that have been collected may affect strategies for confounding control and the confidence in our results.  Each example refers to a row in the table. 


::: {#tbl-04}
```{=latex}
\terminologyelconfoundersLONG
```
Putting causal directed acyclic graphs (DAGs) to work.

:::

### 1. Confounding by a Common Cause

@tbl-04 Row 1 describes the confounding problem of a common cause. We encountered this problem in **Part 2**(#section-part2). Such confounding arises when there is a variable or set of variables, denoted by $L$, that influence both the exposure, denoted by $A$, and the outcome, denoted by $Y$ Because $L$ is a common cause of both $A$ and $Y$, $L$ may create a statistical association between $A$ and $Y$ that does not reflect a causal association.

For instance, in the context of green spaces, consider people who live closer to green spaces (exposure $A$) and their experience of improved happiness (outcome $Y$). A common cause might be socioeconomic status $L$. Individuals with higher socioeconomic status might have the financial capacity to afford housing near green spaces and simultaneously afford better healthcare and lifestyle choices, contributing to greater happiness. Thus, although the data may show a statistical association between living closer to green spaces $A$ and greater happiness $Y$, this association might not reflect a direct causal relationship owing to confounding by socioeconomic status $L$.

Addressing confounding by a common cause involves adjusting for the confounder in one's statistical model. We may adjust through regression, or more complicated methods, such as the inverse probability of treatment weighting, marginal structural models, and others see @hernan2024WHATIF.  Such adjustment effectively closes the backdoor path from the exposure to the outcome. Equivalently, conditioning on $L$ leads to d-separation of $A$ and $Y$ because $\boxed{L}$ closes the backdoor path from $A$ to $Y$.

@tbl-04 Row 1, Column 3, emphasises that a confounder by common cause must precede both the exposure and the outcome. Although in cross-sectional data it is sometimes clear whether a confounder precedes the treatment (e.g., a person's country of birth reported in cross-sectional data) often the relative timing of events is unclear. We must draw several causal diagrams to consider the implications of different assumptions.  Below we return to idea that investigators should draw multiple causal DAGs and perform multiple analyses. Next, we consider scenarios in which investigators condition on effects of the treatment. 

### 2. Mediator Bias

 @tbl-04 Row 1 presents a problem of mediator bias. Consider again whether proximity to green spaces, $A$, affects happiness, $Y$. Suppose that physical activity is a mediator, $L$.

Imagine living close to green spaces ($A$) increases physical activity ($L$), which then improves happiness ($Y$). If we treat physical activity ($L$) as a confounder and adjust for it, we will bias our estimate of the total effect of green space proximity ($A$) on happiness ($Y$). This bias happens because adjusting for $L$ breaks the link between $A$ and $Y$. This is called mediator bias. Notably, experiments are not immune to such over-conditioning biases. For example @montgomery2018 found that over half of all published research in political science experiments were guilty of conditioning on a post-treatment variable (see also @bulbulia_2024_experiments).

To avoid mediator bias when estimating a total causal effect, we should never condition on a mediator. The surest way to prevent this problem is to ensure that data collection for covariates we assume to be confounders $L$ occurs before data collection of the treatment $A$, which in turn occurs before data collection of the outcome $Y$ [@vanderweele2020].  We present this solution in @tbl-04 Row 2 Col 3. 


### 3. Confounding by Collider Stratification (Conditioning on a Common Effect)


 @tbl-04 Row 1 presents a problem of collider bias.  Conditioning on a common effect, or collider stratification, occurs when a variable thought to be a confounder, denoted by $L$, is in fact influenced by both the exposure, denoted by $A$, and the outcome, denoted by $Y$.

Suppose that the decision to live closer to green spaces (exposure $A$) and states of subjective happiness (outcome $Y$) are independent such that $A \coprod Y(a)$. Furthermore, assume physical health $L$ is an effect of both access to green space and states of subjective happiness. That is, assume that $L$ is an effect of $A$ and $Y$. If we were to condition on $L$ in this setting, we would introduce *collider stratification bias*. When we control for the common effect $L$ (physical health), we may inadvertently introduce confounding. This happens because knowing something about $L$ gives us information about both $A$ and $Y$. If someone were high on physical health but low an access to greenspace, this would imply that they are higher in happiness. Likewise, if someone were low in physical health but high in access to green space, this would imply lower happiness. As a result of our conditioning strategy, it would appear that access to green space and happiness are negatively associated. However, if we were to avoid conditioning on the common outcome, we would find that the treatment and outcome are not associated.  

How can we avoid collider bias, the temporal sequence of measurement affords a powerful strategy:

Ensure all common causes of $A$ and $Y$  -- call them $L$ -- are measured before the treatment $A$ occurs. Ensure further that $Y$ occurs after $A$ occurs.  If the confounder $L$ is not measured, ensure that conditioning on its downstream proxy, $L'$ does not induce collider or mediator biases.

By adhering to this sequence, we can mitigate the risk of collider stratification bias and better understand the causal relationships between exposure, outcome, and their common effects.

### 4. Confounding by Conditioning on a Descendant of a Confounder  

 @tbl-04 Row 4 presents a problem of collider bias by decent. Recall the rules of d-separation also apply to conditioning on descendants of a confounder.  Thus, we may unwittingly evoke confounding by proxy when conditioning on a measured descendant of an unmeasured collider. 
 
For example, if doctor visits were encoded in our data, and doctor visits were an effect of poor health, conditioning on doctor visits would function similarly to conditioning on poor health in the previous example, introducing collider confounding. 


### 5. M-bias: Conditioning on Pre-Exposure Collider {#section-mbias}

There are only five elementary structures of causality. Every confounding scenario can be developed from these five elementary structures. We next consider how we may combine these elementary causal relationships in causal diagrams to create effective strategies for confounding control. 


@tbl-04 Row 5 presents a form of pre-exposure over-conditioning confounding known as "M-bias".  This bias combines the collider structure and the fork structure, revealing what might not otherwise be obvious: it is possible to induce confounding even if we ensure that all variables have been measured **before** the treatment. The collider structure is evident in the path $U_Y \to L_0$ and $U_A \to L_0$. The collider rule shows that conditioning on $L_0$ opens a path between $U_Y$ and $U_A$. What is the result? We find that $U_Y$ is associated with the outcome $Y$ and $U_A$ is associated with treatment $A$. This is a fork (common cause) structure. The association between treatment and outcome opened by conditioning on $L$ arises from an open back-door path that occurs from the collider structure. We thus have confounding. How might such confounding play out in a real-world setting? 

In the context of green spaces, consider the scenario where an individual's level of physical activity $L$ is influenced by an unmeasured factor related to their propensity to live near green spaces $A$ -- say childhood upbringing. Suppose further that another unmeasured factor -- say a genetic factor -- increases both physical activity $L$ and happiness $Y$. Here, physical activity $L$ does not affect the decision to live near green spaces $A$ or happiness $Y$ but is a descendent of unmeasured variables that do. If we were to condition on physical activity $L$ in this scenario, we would create the bias just described --  "M-bias."  

How shall we respond to this problem? The solution is straightforward. If $L$ is neither a common cause of $A$ and $Y$ nor the effect of a shared common cause, then $L$ should not be included in a causal model. In terms of the conditional exchangeability principle, we find $A \coprod Y(a)$ yet $A \cancel{\coprod} Y(a)| L$. So we should not condition on $L$: do not control for exercise [@cole2010].[^3]

[^3]: Note that when we draw a chronologically ordered path from left to right, the M shape for which "M-bias" takes its name changes to an E shape. We shall avoid proliferating jargon and retain the term "M bias."

### 6. Conditioning on a Descendent May Sometimes Reduce Confounding {#section-conditioning-on-descendents}

 In @tbl-04 Row 6, we encounter a causal diagram in which an unmeasured confounder opens a back-door path that links the treatment and outcome.  Here, we consider how we may use the rules of d-separation to obtain unexpected strategies for confounding control. 
 
 Returning to our green space example, suppose an unmeasured genetic factor $U$ affects one's desire to seek out isolation in green spaces $A$ and independently affects one's happiness $Y$.  Were such an unmeasured confounder to exist we could not obtain an unbiased estimate for the causal effect of green space access on happiness. We have, it seems, intractable confounding.  
 
However, imagine a variable $L^\prime$, a trait expressed later in life that arises from this genetic factor. If such a trait could be measured, even though the trait $L'$ is expressed after the treatment and outcome have occurred, controlling for $L'$ would enable investigators to close the backdoor path between the treatment and the outcome. This strategy works because a measured effect is a *proxy* for its cause $U$, the unmeasured confounder.  By conditioning on the late-adulthood trait, $L'$, we partially condition on its cause, $U$, the confounder of $A \to Y$. Thus, not all effective confounding control strategies need to rely on measuring pre-exposure variables. Thus, the elementary causal structures reveal a possibility for confounding control by condition on a post-outcome variable.  This strategy is not intuitive. Although a common cause must occur before a treatment (and outcome), its proxy need not! If we have a measure for the latter but not the former, we should condition on the post-treatment proxy of a pre-treatment common cause.

### 7. Confounding Control with Three Waves of Data is Powerful and Reveals Possibilities for Estimating an "Incident Exposure" Effect

@tbl-04 row 7 presents another setting in which there is unmeasured confounding. In response to this problem, we use the rules of d-separation to develop a data collection and modelling strategy that may greatly reduce the influence of unmeasured confounding.  @tbl-04 row 7 col 3, by collecting data for both the treatment and the outcome at baseline and controlling for baseline values of the treatment and outcome, any unmeasured association between the treatment $A_1$ and the outcome $Y_2$ would need to be *independent* of their baseline measurements. As such, including the baseline treatment and outcome, along with other measured covariates that might be measured descendants of unmeasured confounders, is a strategy that exerts considerable confounding control [@vanderweele2020]. 

Furthermore, this causal graph makes evident a second benefit of this strategy.  Returning to our example, a model that controls for baseline exposure would require that people initiate a change from the $A_0$ observed baseline level. Thus, by controlling for the baseline value of the treatment, we may learn about the causal effect of shifting one's access to green space status. This effect is called the "incident exposure effect." The incident exposure effect better emulates a "target trial" or the organisation of observational data into a hypothetical experiment in which there is a "time-zero" initiation of treatment in the data; see @hernán2016; @danaei2012; @vanderweele2020; @bulbulia2022.  Without controlling for the baseline treatment, we could only estimate a "prevalent exposure effect." If the initial exposure caused people some people to be miserable, we would not be able to track this outcome. The prevalent exposure effect would mask it, distorting causal inferences for the quantity of interest, namely, what would happen, on average, if people were to shift to having greater greenspace access. 

Finally, we obtain further control for unmeasured confounding by controlling for both the baseline treatment and the baseline outcome. For an unmeasured confounder to affect both the treatment and the outcome (and unmeasured fork structure), it would need to do so independently of the baseline measures of the treatment and exposure [@vanderweele2020]. Note again that we generally require repeated measures on the same unit over time intervals to obtain an incident exposure effect and exert more robust control for unmeasured confounding using past states of the treatment and outcome.


## Part 4.  Practical Guide For Constructing Causal Diagrams and Reporting Results When Causal Structure is Unclear {#section-part4}


#### 1. Clarify the Research Question and Target Population

Before drawing a causal diagram, we must state the problem it addresses and the population to whom it applies. Causal identification strategies may vary by question. We have seen that if $A\to B\to C; A\to B; A\to C$  the confounding control strategy for evaluating $L \to Y$ differs from that for $A \to Y$. Again, reporting coefficients other than the association between $A \to Y$ is typically ill-advised; see @westreich2013; @mcelreath2020; @bulbulia2023.

### 2 Evaluate Bias in the Absence of a Treatment Effect

Before attributing any statistical association to causality, we must eliminate non-causal sources of correlation. We do this by, first, by drawing the treatment ($A$) and outcome ($Y$) on our causal diagram with no arrow linking them. We do not draw an arrow between $A$ and $Y$ because we are evaluating bias in the absence of a treatment effect. Second, we identify all common causes of $A$ and $Y$, and consider whether they are measured or unmeasured. Third, we use the rules of d-separation to evaluate whether conditioning on measured common causes of $A$ and $Y$ will block all backdoor paths that create indirect, non-causal associations between $A$ and $Y$. That is, we consider whether $A$ and $Y$ are d-separated by the measured common causes of $A$ and $Y$. If they are, then we have successfully blocked all backdoor paths and can proceed to evaluate bias in the presence of a treatment effect. If they are not, then we have failed to block all backdoor paths and our estimate of the causal effect of $A$ on $Y$ will be biased. 


#### 3. Draw the Most Recent Common Causes of Exposure and Outcome

Include all common causes (confounders) of both the exposure and the outcome in your diagram, whether measured or unmeasured. Where possible, group functionally similar common causes into a single variable (e.g., $L_0$ for demographic variables).

#### 4. Include All Ancestors of Measured Confounders

Add any ancestors (precursors) of measured confounders associated with the treatment, the outcome, or both. Simplify the causal diagram by grouping similar variables. For example, suppose we believe that both age and income are common causes of both the treatment and the outcome. We may represent this belief by grouping age and income into a single variable, $L_0 = \{age,~income\}$.

#### 5. Explicitly State Assumptions About Relative Timing

Annotate the temporal sequence of events using subscripts (e.g., $L_0$, $A_1$, $Y_2$). 

Note that it is imperative that causal DAGs are acyclic. The graph: $A \to Y \to A \to Y$ is not acyclic. **However, the demand for an acyclic graph does not imply that dynamic causal relationships are precluded from causal analysis.** For example $L_0 \to Y_1 \to  A_1 \to L_2 \to A_3 \to Y_5$ is an acyclic graph. Here, past states of the confounders, treatment, and outcome are depicted as affecting their future states.  By indexing the relative timing of states we ensure that the causal diagram is acyclic. 

Note this example further illustrates the potential difficulties when investigating multiple treatments. Suppose we are interested in causal contrasts for multiple treatments over time, for example the contrast between $A_1 = 0, A_3 = 0$ and $A_1 = 1, A_3 = 1$, with the outcome $Y_5$ measured at the end of study. Notice that $L_2$ is a common cause of $A_3$ and $Y_5$. We must condition on $L_2$ to block the back-door path from $A_3$ to $Y_5$. However, were we to condition on $L_2$, we would induce mediator bias because $L_2$ blocks the path from $A_1$ to $Y_5$.  We cannot use standard regression, multi-level regression, or structural equation modelling to estimate the time-varying treatment regime of interest. We instead require special methods (refer to @hernan2024WHATIF; @bulbulia2024swigstime).

#### 6. Arrange Temporal Order Visually

Arrange your diagram to reflect the temporal progression of causality, either left-to-right or top-to-bottom. This enhances comprehension of causal relations. Establishing temporal ordering is vital for evaluating identification problems, as discussed in [**Part 3**](#sec-part3).

#### 7. Box Variables Adjusted for Confounding

Mark variables for adjustment (e.g., confounders) with boxes or another easy to understand convention. Be clear about this and other conventions. 

#### 8. Present Paths Structurally, Not Parametrically

Focus on whether paths exist, not their functional form (linear, non-linear, etc.). Parametric descriptions are not relevant for bias evaluation in a causal diagram. For an explanation of causal interaction and diagrams, see @bulbulia2023.

#### 9. Minimise Paths to Those Necessary for the Identification Problem

Reduce clutter by including only paths critical for the specific question (e.g., back-door paths, mediators).

#### 10. Consider Potential Unmeasured Confounders

Use domain expertise to identify potential unmeasured confounders and represent them in your diagram. This proactive step helps anticipate and address *all* possible sources of confounding bias.

#### 11. State Your Graphical Conventions

Establish and explain the graphical conventions used in your diagram (e.g., using red to highlight open back-door paths). Consistency in symbol use enhances interpretability, while explicit descriptions improve accessibility and understanding.

#### 12. Prepare Sensitivity Analyses. 

Because unmeasured confounding cannot be ruled out, investigators should implement sensitivity analyses to determine how dependent conclusions are on specific assumptions or parameters within your causal model. A relatively simple sensitivity analysis is VanderWeele's E-value [@vanderweele2017]

### Specific Advice for Causal Analysis and Reporting in Cross-Sectional Designs


::: {#tbl-cs}

```{=latex}
\examplecrosssection
```
Cross-sectional designs typically require multiple causal DAGS where the temporal order of variables cannot be ensured.
:::



### Recommendations for Conducting and Reporting Causal Analyses with Cross-Sectional Data

When analysing and reporting analyses with cross-sectional data, researchers face the challenge of making causal inferences without the benefit of temporal information. 

The following recommendations aim to guide researchers in navigating these challenges effectively:

#### 1. **Draw multiple causal diagrams**

As shown in @tbl-cs, draw multiple causal diagrams to represent different theoretical assumptions about the relationships and timing of variables relevant to an identification problem. If some causal pathways cannot be ruled out, clarify the implications of assigning variables the roles for which consensus or which the time ordering of the data do not resolve. 

#### 2. **Perform and report analyses for each assumption**

Conduct and transparently report separate analyses for each scenario your causal diagrams depict. This practice ensures that your study is theoretically grounded for each model. Presenting results from each analytical approach and the underlying assumptions and statistical methods promotes a balanced interpretation of findings. 

#### 3. **Report divergent findings**

Approach conclusions with caution, especially when findings suggest differing practical implications. Acknowledge the limitations of cross-sectional data in establishing causality and the potential for alternative explanations. Do not over-sell.

#### 4. **Identify avenues for future research**

Target future research that might clarify ambiguities. Consider the design of longitudinal studies or experiments capable of clarifying lingering uncertainties.

#### 5. **Supplement observational data with simulated data** 

Leverage data simulation to understand the complexities of causal inference. Simulating data based on various theoretical models allows researchers to examine the effect of different assumptions on their findings. This method tests analytical strategies under controlled conditions, assessing the robustness of conclusions against assumption violations or unobserved confounders.



### Specific Advice for Causal Analysis and Reporting in Longitudinal Designs


 Longitudinal designs offer a substantial advantage over cross-sectional designs for causal inference because sequential measurements allow us to capture causation and quantify its magnitude. We typically do not need to assert timing as in cross-sectional data settings. Because we know when variables have been measured, we can reduce ambiguity about the directionality of causal relationships. For instance, tracking changes in "happiness" following changes in access to green spaces over time can more definitively suggest causation than cross-sectional snapshots.

Despite this advantage, longitudinal researchers nevertheless face assumptions regarding the absence of unmeasured confounders or the stability of measured confounders over time. These assumptions must be explicitly stated.  As with cross-sectional designs, wherever assumptions differ, researchers should draw different causal diagrams that reflect these assumptions and subsequently conduct and report separate analyses.  Supplementary materials D and E provide examples of how to conduct and report analyses for multiple causal diagrams. The following summarises our advice. 



#### 1. Draw multiple causal diagrams

   - **Identification problem diagram**: begin as usual by stating a causal question and population of interest. Then constructing a causal diagram that outlines your assumptions about the relationships among variables relevant for ensuring conditional exchangeability.

   - **Solution diagram**: next, create a separate causal diagram that proposes solutions to the identified problems. Having distinct diagrams for the problem and its proposed solutions clarifies your study's analytic strategy and theoretical underpinning.

@tbl-lg provides an example of a table with multiple causal diagrams clarifying potential sources of confounding threats and reports strategies for addressing them. 

::: {#tbl-lg}

```{=latex}
\examplelongitudinal
```
Use causal DAGs to report both the causal identification problem and its solution.
:::


#### 2. Attempt longitudinal designs with at least three waves of data

Incorporating repeated measures data from at least three time intervals considerably enhances your ability to infer causal relationships. For example, by adjusting for physical activity measured before the treatment, we can ensure that physical activity does not result from a new initiation to green spaces, which we establish by measuring green space access at baseline. Establishing chronological order allows us to avoid confounding problems 1-4 in @tbl-04. 


#### 3. Where causality is unclear, report results for multiple causal graphs

Given that the true causal structure may be complex and partially unknown, analysing and reporting results under each plausible causal diagram is prudent. 

#### 4. Address missing data at baseline and study attrition

Longitudinal studies often need help with missing data and attrition, which can introduce bias and affect the validity of causal inferences. Implement and report strategies for handling missing data, such as multiple imputation or sensitivity analyses that assess the bias arising from missing responses at the study's conclusion. (For more about addressing missing data, see: [@bulbulia2024PRACTICAL]). 

By following these recommendations, you will more effectively navigate the inherent limitations of observational longitudinal data, improving the quality of your causal inferences.


### On the Differences Between Methods for Causal Inference And The Statistical Modelling Tradition 

In traditional statistical methods such as regression analysis and structural equation modelling (SEM), the focus often lies in estimating associations among variables, sometimes treating all variables symmetrically within the model. However, causal inference shifts the focus to estimating pre-specified causal contrasts, defined explicitly in terms of interventions and outcomes measured on a target population. This approach takes us beyond measuring associations in the data to understanding the effects of hypothetical and actual interventions. 

Here, we have focussed the place of counterfactual contrasts between  well-defined outcomes experienced by a target population under different levels of treatment.  It is important to emphasise that the other variables in a model are typically treated as nuisance variables—variables that are not of direct interest, but must be accounted for to obtain valid causal effect estimates. In causal inference, nuisance parameters often include confounders that need to be adjusted to block non-causal associations between a treatment of interest and the outcomes of interest. However, the coefficients associated with these nuisance variables generally lack a causal interpretation. For example, consider a causal pathway where $L \to A \to Y$, and we are interested in estimating the effect of $A$ on $Y$ while controlling for $L$. Although controlling for $L$ is necessary to obtain an unbiased estimate of the causal effect of $A$ on $Y$, the coefficient of $L$ in the model does not have a straightforward causal interpretation and reporting it may be misleading, especially if $A$ mediates the effect of $L$ on $Y$.

By concentrating on pre-specified causal contrasts and appropriately handling nuisance parameters, causal inference methods enable clarity for pre-specified causal contrasts of interest. This approach departs from traditional statistical methods by emphasising the estimation of specific causal effects rather than modelling the entire system of associations. It acknowledges that not all parameters in a model can simultaneously be of equal interest; some parameters primarily serve to adjust for confounding or other biases and should not be interpreted causally.

Understanding and properly addressing nuisance parameters are critical steps in a causal inference workflow. The observation that nuisance parameters typically lack causal interpretation suggests that the standard practice of reporting all model coefficients may be unwise or even misleading (see @westreich2013; @mcelreath2020; @bulbulia2023; @Lonati_Lalive_Efferson_2024). When multiple treatments are of interest, it is essential to ensure that the identification assumptions required for each causal effect are satisfied—a task that can be more challenging than it appears (see @vanderweele2015; @hernan2008aObservationalStudiesAnalysedLike; @robins1986; @robins1992; @robins2008estimation; @bulbulia2024swigstime).

Our questions are nearly always causal. We want to know what would happen, on average, if we were to intervene in the world. In the observational psychological sciences, obtaining such understanding requires a paradigm shift from traditional statistical modelling. Such a paradigm shift has become established in economics, computer science, and epidemiology, and they are making strong inroads in political science. However, in observational psychological sciences, including environmental psychology, methods for causal inference remain rare. We hope this chapter inspires readers to begin developing robust causal workflows capable of addressing their causal questions and those that have long animated the field's scientific interests but which have yet to be coherently addressed. 



## Where to Go Next?

There are many good resources available for learning causal directed acyclic graphs [@rohrer2018; @greenland1999; @glymour2008causal; @hernan2024WHATIF; @cinelli2022; @barrett2021; @mcelreath2020; @greenland1999; @suzuki2020; @pearl2009a; @pearl_greenland_2007; @major2023exploring; @greenland1999; @morgan2014].  For those just getting started on causal diagrams, we recommend Miguel Hernan's free course here: [https://www.edx.org/learn/data-analysis/harvard-university-causal-diagrams-draw-your-assumptions-before-your-conclusions, accessed 10 June 2024](https://www.edx.org/learn/data-analysis/harvard-university-causal-diagrams-draw-your-assumptions-before-your-conclusions). For those seeking a slightly more technical but still accessible introduction to causal inference and causal DAGs, we recommend Brady Neal's introduction to causal inference course and textbook, both freely available here [https://www.bradyneal.com/causal-inference-course, accessed 10 June 2024](https://www.bradyneal.com/causal-inference-course). For those interested in causal inference in experiments refer to @hernan2017per, @montgomery2018; @bulbulia_2024_experiments. For those interested in causal mediation analysis and time-varying treatments refer to @robins1986; @robins1992; @robins2008estimation; @vanderlaanRobins2003CensoringLongitudinal; @diaz2021nonparametric; @williams2021; @hoffman2023;@bulbulia2024swigstime. Moreover, the fifth segment of Miguel Hernan's free online course covers time-varying treatments:  [https://www.edx.org/learn/data-analysis/harvard-university-causal-diagrams-draw-your-assumptions-before-your-conclusions, accessed 10 June 2024](https://www.edx.org/learn/data-analysis/harvard-university-causal-diagrams-draw-your-assumptions-before-your-conclusions). On the pitfalls of traditional path modelling refer to @rohrer2022PATH. We have set aside complications arising from measurement error. For more on this topic see @hernan2009MEASUREMENT; @hernan2024WHATIF; @bulbulia2024wierd; @vanderweele2022; @vanderweele2022a.


{{< pagebreak >}}

## Funding

This work is supported by a grant from the Templeton Religion Trust (TRT0418). JB received support from the Max Planck Institute for the Science of Human History. The funders had no role in preparing the manuscript or deciding to publish it.

## Contributions

DH proposed the chapter. JB developed the approach and wrote the first draft. Both authors contributed substantially to the final work.



<!-- 

### The Five Elementary Rules For Evaluating Confounding

To review, causal diagrams allow researchers to visualise and systematically identify potential confounders and strategies for adjusting for them. There are five basic graphical structures:

#### 1. **Causality Absent**  $A$ does not cause $B$: absent any common causes, there is no statistical association between them.

$$\xorxA$$ 

#### 2. **Causality Present**  $A$ causes $B$: absent conditioning that blocks them, $A$ and $B$ will be statistically associated.

$$\xtoxA$$

#### 3. **The Fork Structure** $A$ causes $B$ and $A$ causes $C$: absent conditioning on $A$, $B$ and $C$ will be statistically associated. Conditional on $A$, $B$ and $C$ will be independent.

$$\forkTINY$$

#### 4. **The Chain Structure**  $A$ causes $B$ and $B$ causes $C$: absent conditioning on $B$, $A$ and $C$ will be statistically associated. Conditioning on $B$, $A$ and $C$ will be independent. 

$$\chainTINY$$ 

#### 5. **A Collider Structure**  $A$ causes $C$ and $B$ causes $C$: absent conditioning on $C$, $A$ and $B$ will be statistically independant. Conditioning on $C$, $A$ and $B$ will be statistically associated. 

$$\immoralityTINY$$

From these five elementary structures, we discovered four rules that allow us to use these structures to evaluate confounding and its control:

#### 1. **The Fork Rule** 

When a common cause influences treatment and outcome, condition on the common cause to avoid bias.

#### 2. **The Chain Rule**

  (i)  For total effect estimates, avoid conditioning on mediators within the causal path.
  (ii) For mediation analysis, ensure potential confounders do not introduce bias. )(Note: mediation analysis is complex [@vanderweele2015; @vansteelandt2012; @bulbulia2023].)

#### 3. **The Collider Rule**

Conditioning on a common effect opens a path between the two variables that cause it.  

#### 4. **The Proxy Rule**

Conditioning on a descendant is a proxy for conditioning on its parent.  -->

{{< pagebreak >}}

## References

::: {#refs}
:::


