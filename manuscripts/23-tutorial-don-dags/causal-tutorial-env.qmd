---
title: "Causal Inference in Environmental Psychology"
abstract: |
  This chapter covers causal inference in environmental psychology, focussing on how to understand and use causal diagrams, also known as Directed Acyclic Graphs (DAGs). We start with the basics of causal inference to help readers get a clear idea of the assumptions required to estimate causal effects. Then, we explain how to create simple causal diagrams, and apply the lessons scenarios that environmental psychologists may face. The chapter is divided into three parts: the first introduces causal inference principles, the second explains causal diagrams, and the third discusses causal inference in experiments. It is designed for researchers interested in making stronger causal claims from both observational and experimental data.
authors: 
  - name: Donald W Hine
    orcid: 0000-0002-3905-7026
    email: donald.hine@canterbury.ac.nz
    affiliation: 
      name: University of Canterbury, School of Psychology, Speech and Hearing
      city: Wellington
      country: New Zealand
      url: www.wgtn.ac.nz/cacr
  - name: Joseph A. Bulbulia
    orcid: 0000-0002-5861-2056
    email: joseph.bulbulia@vuw.ac.nz
    affiliation: 
      name: Victoria University of Wellington, New Zealand, School of Psychology, Centre for Applied Cross-Cultural Research
      department: Psychology/Centre for Applied Cross-Cultural Research
      city: Wellington
      country: New Zealand
      url: www.wgtn.ac.nz/cacr
keywords:
  - DAGS
  - Causal Inference
  - Confounding
  - Environmental
  - Psychology
  - Panel
format:
  pdf:
    sanitize: true
    keep-tex: true
    link-citations: true
    colorlinks: true
    documentclass: article
    classoption: [singlecolumn]
    lof: false
    lot: false
    geometry:
      - top=30mm
      - left=20mm
      - heightrounded
    header-includes:
      - \input{/Users/joseph/GIT/templates/latex/custom-commands.tex}
date: last-modified
execute:
  echo: false
  warning: false
  include: true
  eval: true
fontfamily: libertinus
bibliography: /Users/joseph/GIT/templates/bib/references.bib
csl: ./camb-a.csl
---

```{r}
#| label: load-libraries
#| echo: false
#| include: false
#| eval: false
#  fig-pos: 'htb'
#   html:
#    html-math-method: katex

# Include in YAML for Latex
# sanitize: true
# keep-tex: true
# include-in-header:
#       - text: |
#           \usepackage{cancel}


# for making graphs
library("tinytex")
library(extrafont)
loadfonts(device = "all")


# libraries for jb (when internet is not accessible)
# read libraries
source("/Users/joseph/GIT/templates/functions/libs2.R")

# read functions
source("/Users/joseph/GIT/templates/functions/funs.R")

# read data/ set to path in your computer
pull_path <-
  fs::path_expand(
    "/Users/joseph/v-project\ Dropbox/data/current/nzavs_13_arrow"
  )

# for saving models. # set path fo your computer
push_mods <-
  fs::path_expand(
    "/Users/joseph/v-project\ Dropbox/data/nzvs_mods/00drafts/23-causal-dags"
  )


# keywords: potential outcomes, DAGs, causal inference, evolution, religion, measurement, tutorial
# 10.31234/osf.io/b23k7

```


# Introduction

Causal inference is the process of using empirical observations to identify the causal relationships between variables or events. In this simplist case, this involves assessing whether and intervention or "treatments" on one variable leads to subsequent changes in another variable, the "outcome." The goal of causal inference is to determine whether cause-effect relationships are present, and to quantify their magnitude [@cook2002experimental].

It is often thought that causal inference is a complex cognitive process unique to humans. Yet, it may be more accurately viewed as a phenomenon that spans across the plant and animal kingdoms, varying in levels of conscious awareness and complexity [@mancuso2018revolutionary]. For instance, plants and microorganisms have genetic predispositions to recognise and react to environmental conditions critical for their reproduction and survival. Birds and rodents demonstrate the ability to learn, albeit not as complexly as larger mammals and primates. Over millennia, humans have developed diverse frameworks for causal inference, ranging from the law of karma in Hindu, Jain, and Buddhist traditions, to Aristotle’s four causes, early empirical and experimental approaches by Francis Bacon and John Locke, Newtonian mechanics, and more recent theories based on counterfactual reasoning.  

Although the capacity to infer cause and effect is common to many lineages and has evolved to high levels of sophistication with many human cultures, methods for quantitatively estimating causal effects from data are much less common, and have emerged only relatively recently with the development of randomised controlled experiments. More recent and specialised still are capacities for quantifying the magnitudes of causal effects from what is sometimes called "real-world" data or "observational data" -- that is, data that have not been collect through random assignment to controlled treatments.  This chapter aims to familiarise environmental psychologists with powerful methods for analysing observational data to draw causal inferences.

We believe there is considerable value in understanding these more recently developed methods. 
Introductory undergraduate psychology courses teach that correlations between variables do not necessarily indicate causality. The question arises: Can valid causal inferences be drawn from observational data, and if so, how? Although randomised experiments provide causal insights, they can be costly and ethically challenging. Moreover, even randomised controlled experiments do not ensure valid causal inferences. Observational data, being more plentiful, offers potentially valuable resources for accelerating knowledge in environmental psychology, provided causal insights can be gleaned from them.  However, the manner in which many researchers draw inferences from such data has yet to be developed. On the one side, we assess correlations, and state that correlations are not causation. This is unsatisfactory because we typically want to understand causation.  On the other hand, we continue to draw hesitant causal conclusions using hedging language. However, lacking a appropriate methods, we often have no entitlement or guarantee that such speculation is misleading [@bulbulia2022; bulbulia@2023a].

There are reasons to hope we may do better. Significant advances in estimating causation from observational data over the past two decades offer hope [@vanderweele2015]. While most progress has been outside psychology, psychological scientists are increasingly interested [@mcelreath2020;@rohrer2018]. Incorporating methods of causal inference into environmental psychology research offers significant potential. This chapter aims to lay a foundational understanding, aspiring to motivate environmental psychologists towards crafting more robust and insightful causal inferences.

## Overview

**Part 1** introduces the "potential outcomes" framework for causal inference [@hernan2023]. This section describes the three core assumptions underpinning causal inference, grounding these concepts within the context of randomized experiments, which are likely familiar to many readers. By deriving intuitions about causal inference from an understanding of experimental designs, we can demystify the assumptions central to causal analysis and provide clear guidelines for data collection.

**Part 2** introduces Directed Acyclic Graphs (DAGs) – or causal diagrams – as powerful tools for examining causal assumptions. Although an exhaustive review of causal diagrams' capabilities is beyond this overview, we present fundamental strategies for constructing and interpreting these diagrams, which should prove valuable for addressing many questions within environmental psychology.

**Part 3** examines how experimental designs may benefit from causal methods developed for observational settings. Although discussions about the use of experiments often revolve around concerns of internal and external validity, we use causal diagrams to explore how experimental designs might be susceptible to internal validity threats, independent of external validity considerations. We show how the methodologies for causal inference in observational studies are equally relevant for experimenters and argue that causal inference techniques should be an integral component of experimental methodology education.

## Part 1: Introducing the Potential Outcomes Framework of Causal Inference

The potential outcomes framework originated in the work of Jerzy Neyman for evaluating agricultural experiments [@neyman1923]. It was later extended by Harvard statistician Donald Rubin to facilitate causal inference in non-experimental settings [@rubin1976]. Jamie Robins further generalised this framework to assessing confounding in complex scenarios involving multiple treatments and time-varying factors [@robins1986]. A core concept within this framework is the concept of 'counterfactual outcomes.'

To grasp the significance of counterfactual outcomes in causal analysis, consider yourself at the cross-roads of a monumental life decision. Imagine you are soon graduating university. You are accepted into your ideal graduate program at the University of Canterbury; you make preparations for a relocation to Christchurch, New Zealand. Concurrently, you receive a compelling job offer from Acme Nuclear Fuels, a pioneer in renewable energy solutions. Each option would set you on a distinct path, affecting your daily life, income, social circles, romantic relationships, and perhaps your life's purpose. Which life will be better? 

Formally, let $A$ denote the choice to attend graduate school ($A = 1$) or to embark on a career in industry ($A = 0$). The potential outcomes, $Y_{\text{you}}(1)$ and $Y_{\text{you}}(0)$, symbolize the hypothetical scenarios resulting from each decision. To evaluate the causal effect of your choice, we examine the difference $Y_{\text{you}}(1) - Y_{\text{you}}(0)$. However, this differential is fundamentally unobservable, as once a decision is made, the alternative scenario remains unknowable.

$$
(Y_{\text{you}}|A_{\text{you}} = 1) = Y_{\text{you}}(1) \quad \text{implies} \quad Y_{\text{you}}(0)|A_{\text{you}} = 1~ \text{is counterfactual}.
$$

Similarly, the counterfactual applies in reverse when the choice is $A = 0$. 

Consider the implications. Although you can, of course, make principled decisions about which life to choose that are based on past experiences, the data you require to compare life outcomes under one decision as opposed to the are not available. Life as it would have unfolding under the option you do not select remains forever counterfactual. This example underscores 'the fundamental problem of causal inference' as articulated by Rubin and Holland [@rubin2005; @holland1986]: the impossibility of observing both potential outcomes for an individual simultaneously.


### Understanding Relationships of Cause and Effect Through Intervention Outcomes

Let's shift our example to a question in environmental psychology. Suppose we we want to examine the causal effect of easy access to urban green spaces on psychological well-being [@nguyen2021green; @reyes2021linking]. For simplicity, suppose our interest is "subjective happiness," hereafter referred to as "happiness", and assume this quantity is measurable.  We will represent this outcome with the symbol $Y$

For simplicity, let's assume that intervention "ample access to green space" is a binary variable. Define $A = 1$ as "having ample access to green space" and $A = 0$ as "lacking ample access to green space."" Assume these conditions are mutually exclusive. The observations we make here apply to continuous treatments so this simplification does not come at the loss of generality. In causal inference is it important to specify the population for whom we seek to evaluate causal effects. We call this object the "target population." Here, we will take the target population to be residents of New Zealander in the 2020s.  **Note that in causal inference, before analysing any data we must state a clearly defined causal question with respect to the target population.**

 Our preliminary causal question might be:

'In New Zealand, does proximity to abundant green spaces increase self-perceived happiness compared to environments lacking such spaces?'

Next, suppose we agree it would be unethical to experimentally randomise individuals into different green-space access conditions. Let's ignore this issue. Assume we had the means to assign people randomly to high and low greenspace-access, and that no one objected. 

The first point to notice is that in the context of causal inference, even well-designed experiments face the challenge of missing values in the potential outcomes. Once an individual is assigned to one treatment condition, we cannot observe that individual's outcome for the treatment condition that was not assigned. This issue is inherent to the nature of causal inference where, for each individual, we can only observe one of the potential outcomes. Breaking down the ATE into observed and unobserved outcomes gives us: 

$$
\text{Average Treatment Effect} = \left(\underbrace{\underbrace{\mathbb{E}[Y(1)|A = 1]}_{\text{observed for } A = 1} + \underbrace{\mathbb{E}[Y(1)|A = 0]}_{\text{unobserved for } A = 0}}_{\text{effect among treated}}\right) - \left(\underbrace{\underbrace{\mathbb{E}[Y(0)|A = 0]}_{\text{observed for } A = 0} + \underbrace{\mathbb{E}[Y(0)|A = 1]}_{\text{unobserved for } A = 1}}_{\text{effect among untreated}}\right).
$$

In this expression, $\mathbb{E}[Y(1)|A = 1]$ represents the average outcome when the treatment is given, which is observable. However, $\mathbb{E}[Y(1)|A = 0]$ represents the average outcome if the treatment had been given to those who were actually untreated. This quantity remains unobservable. Similarly, the quantity $\mathbb{E}[Y(0)|A = 1]$ remains unobservable.

Clearly, the fundamental problem of causal inference lurks in the background of experiments. For each individual we cannot know how the intervention would have turned out had they received the alternative treatment to what they in fact received, any more than you can quantitatively determine how life would have turned out had you decided to take the job at Acme Nuclear Fuels after deciding in fact to attend the University of Canterbury. 

Indeed, the fundamental challenge of causal inference is also present in experimental research. For each participant, it's impossible to ascertain the outcome they would have experienced under an alternative treatment condition, just as you cannot quantitatively predict the life you would have led had you chosen the job at Acme Nuclear Fuels instead of attending the University of Canterbury.

### Causal inference in Experiments

So, how do experiments manage to estimate average treatment effects despite this challenge? The key lies in addressing the concept of 'confounding.' Causal inference is complicated by various forms of confounding, which we will delve into more thoroughly in Part 2. For now, consider a specific type of confounding that is crucial for understanding how experiments yield unbiased average treatment effects: "confounding by common cause."

Confounding by common cause occurs when an external variable, known as a confounder, influences both the treatment (or intervention) and the outcome of interest, potentially leading to a spurious association between them. This confounder creates a false or exaggerated relationship that may be mistakenly interpreted as causal. For example, when assessing the role of access to green space in happiness, perhaps the association is explained entirely by income. Notice the association between green space and happiness would not merely be biased, it would be entirely misleading. Were we to shift low-income people to high-access green regions we might do nothing to affect subjective happiness. Therefore, accurately identifying and adjusting for confounding by common cause is critical for determining the true causal relationship between two variables, ensuring that the observed association is not merely a result of extraneous influences.

#### Balance of confounders removes confounding 

Notice that in experimental designs, random assignment of treatment eliminates any systematic relationship between treatment conditions and the distribution of variables that can affect the outcomes under treatment, creating balance in confounders across treatment groups. Confounders, of course, do not disappear. However the balance that randomisation (ideally) brings ensures that confounding disappears. This is because the distribution of confounders is, on average, identical across the treatment groups. This equilibrium permits the assumption that the any systematic difference between the average outcomes in these groups arises from treatment itself.

Let us denote the set of all possible confounders by the letter $L$. In causal inference, the absence of confounding can be articulated in one of two equivalent ways:

1. The potential outcomes, given the treatment and confounders, are conditionally independent: $Y(a) \coprod A \mid L$.
2. The treatment assignment, given the potential outcomes and confounders, is conditionally independent: $A \coprod Y(a) \mid L$.

Note this mathematical formalism might appear excessive, but it will assist later when we assess causal inference strategies for observational data that are aimed at emulating experimental randomisation. 

At the core of these strategies is an endeavour to ensure that we obtain a balance across treatment conditions in the factors that might influence the outcome, and balance implies independences of the treatment and the potential outcomes, conditional on covariates $L$. Because which randomisation (if successful) ensures $L$ is balanced on the treatments, we may drop the $L$ and state that randomisation ensures $A \coprod Y(a)$.

Randomisation in experiments, ideally, achieves balance in variables associated with treatment and outcome, eliminating confounding and enabling specific treatment effect inferences. 

Thus, the first key principle for causal inference embodied in randomised experiments is:

1. **Conditional exchangeability**: we may assume statistical independence between potential outcomes and treatment assignment, given confounders, ensuring observed group differences are attributable to the treatment, not pre-existing differences.

#### Control ensures consistent treatments are administered across all confounders 

There are two further principles embodied in experiments that allow researchers to infer average treatment effects without observing individual treatment effects. These are:

2. **Causal consistency**: we may assume that the observed outcomes in experimental conditions correspond to the potential outcomes under the received treatment. That is for every individual we may assume that:  
For any individual $i$, the observed outcome $Y_i$ given their treatment status $A_i = 1$ is equal to their potential outcome under treatment, denoted as $Y_i(1)$. Similarly, when the treatment status for the same individual $i$ is $A_i = 0$, the observed outcome $Y_i$ is equal to the potential outcome under no treatment, denoted as $Y_i(0)$. This encapsulates the concept of causal consistency, which asserts that the observed outcome for an individual under the treatment condition they actually received aligns with their corresponding potential outcome. This can be formally expressed as:

$$
\begin{aligned}
Y_{i}(1) &= (Y_{i}|A_{i} = 1) \quad \text{(Potential outcome observed if treated)} \\
Y_{i}(0) &= (Y_{i}|A_{i} = 0) \quad \text{(Potential outcome observed if untreated)}
\end{aligned}
$$
 
When conditional exchangeability is satisfied, we can extend our analysis from comparisons of treatment effects observed in the data to comparisons of the average treatment effects as if the entire population had been treated or had been untreated. This is mathematically represented as:


$$
\begin{aligned}
\text{Average Treatment Effect (ATE)} &= \mathbb{E}[Y(1)] - \mathbb{E}[Y(0)] \\
&= \mathbb{E}(Y|A=1) - \mathbb{E}(Y|A=0)
\end{aligned}
$$

Here, $\mathbb{E}[Y(1)]$ and $\mathbb{E}[Y(0)]$ denote the expected outcomes if the entire population were treated or untreated, respectively. $\mathbb{E}(Y|A=1)$ and $\mathbb{E}(Y|A=0)$ denote the observed average outcomes among those who were treated ($A=1$) and untreated ($A=0$), under the condition of exchangeability.  By the causal consistency and conditional exchangeability assumptions, then, we may estimate the ATE by comparing the average outcomes between treated and untreated groups within the *observed data*.

Note that causal consistency assumption is generally satisfied by the *control* that experimentalists exert over randomised controlled experiments. In an experiment, treatment groups get equivalent treatments.  However, as we shall see that in real-world data, the assumption of consistent treatments is much harder to satisfy.

3. **Positivity**: we may assume that there is a non-zero probability of receiving each treatment level within covariate-defined subgroups. 

$$ P(A = a | L= l) > 0 $$

This assumption is also satisfied by the *control* that experimentalists exert over randomised controlled experiments. However, in observational settings, this condition must be verified to avoid extrapolating results beyond observed data.

#### The three fundamental principles of causal inference

Thus we have seen that three principles -- Exchangeability, Consistency, and Positivity -- address the core challenge of causal inference: making valid inferences about unobservable potential outcomes. In Section 3, we shall understand that even randomised controlled experiments can produce biased causal effect estimates, and that methods for causal inference in observational settings offer powerful tools for clarifying sources of bias. For now, it is these three principles that define the ideal of a randomised controlled experiment, bringing focus to the demands required for causal inference in observational 'real-world' settings. We next turn to this topic.

### Why the fundamental assumptions of causal inference in observational settings are hard

In observational research, where researchers do not control the allocation of treatments, the objective is to emulate a controlled experimental environment as closely as possible. However, this emulation introduces unique challenges, particularly in defining and consistently applying treatments across the study population.

##### The conditional exchangeability assumption in observational settings

Achieving conditional exchangeability, where the groups being compared are similar in all aspects except for the treatment, poses a significant challenge in observational studies. Return to our example about the effect of living near green spaces on subjective happiness. When observing real-world data, it is evident that individuals who live with access to green spaces might systematically differ from those who do not, in several respects:

- **Socioeconomic status**: there might be a correlation between an individual's economic background and their proximity to green spaces. More affluent individuals could afford housing in areas with better access to high-quality green spaces.

- **Age demographics**: different age groups might naturally gravitate towards or away from green spaces. Younger individuals or families with children might prioritize access to parks, unlike other demographics.

- **Mental health**: people with existing mental health issues might seek out green spaces for their therapeutic benefits or avoid them due to social anxiety or other factors.

- **Lifestyle choices**: individuals preferring outdoor activities might choose to live near green spaces. It becomes challenging to determine whether the proximity to green spaces causes improved well-being or if it is merely a characteristic of individuals who already lead healthier lifestyles.

- **Personal values and social connections**: the decision to live near green spaces might also be influenced by personal values, such as environmentalism, or the desire to be part of a community that values these spaces. Such values and connections can influence how individuals interact with and benefit from each other, while living in proximity to green-spaces.

These and other unmeasured factors can introduce biases that complicate the interpretation of causal relationships in observational studies.

##### The causal consistency assumption and heterogeneity of treatments in observational settings

Again we consider our interest in quantifying the causal effect of living near green spaces. The definition of 'proximity to green spaces' itself varies significantly, leading to a diverse range of experiences classified under the same 'treatment'. Again, we set aside the issue that the proximity is a continuous variable, and that the distance to the nearest green space influences how often and easily individuals can access these areas, affecting the impact of this 'treatment'. However we should note that causal inference always requires a contrast of conditions. When assessing causal contrasts we must specify the points to be compared on a continuous scale. Arguable such comparisons will require artificiality and extrapolation.  Focussing on the variability of the green spaces themselves this includes:

- **Diversity of green spaces**: the biodiversity and aesthetic value of these spaces can vary widely. Some might have access to well-maintained parks, while others to basic recreational areas with limited natural appeal.

- **Availability of amenities**: amenities such as walking paths, benches, and recreational facilities can enhance the experience of green spaces, encouraging more frequent and prolonged visits.

- **Size and type of green space**: the type (e.g., urban park, community garden) and size of the green space might affect the psychological and physical benefits it offers.  Thus, as we convert observed outcomes under these heterogenous conditions, it might be unclear which interventions it is that we are comparing.  

##### The positivity assumption in observational settings 

Positivity, the requirement that every individual has a chance of receiving each level of treatment being compared, can be challenging to ensure in observational studies. In some urban areas, it might be practically impossible for certain demographics to have access to green spaces due to factors like housing prices or availability, limiting treatment exposure variability within certain strata and complicating valid causal inference.

Addressing the challenges of observational settings requires a deep understanding of the context and meticulous application of statistical methods to mimic the conditions of a randomised experiment. The closer our data approximates a randomised controlled experiment, the more confidence we can have in our causal inferences. However, it is crucial to acknowledge that often, the data may not provide a high level of confidence. By referencing the gold standard of a randomised experiment, environmental psychologists can better understand and communicate the strengths and limitations of observational data in answering causal questions, many of which cannot be addressed experimentally due to practical or ethical constraints.

In Section 2, we will examine how causal diagrams can significantly enhance our understanding of these strengths and limitations. These diagrams are powerful tools for visualising and analysing relationships and potential confounding factors within observational data. They provide a framework for identifying and addressing the assumptions necessary for causal inference, offering a clearer pathway for interpreting complex data sets. This approach is instrumental in bridging the gap between the idealised conditions of a randomised experiment and the realities of observational studies, ultimately enriching the environmental psychologist's toolkit for scientific understanding.

## Part 2. An Introduction to Causal Diagrams

Causal diagrams are powerful tools for evaluating causal inferences [@pearl1995; @pearl2009; @greenland1999].  However, we begin with a **warning**, to understand causal diagrams requires some initial terminology.  Our experience is that the terminology is confusing to those who have yet to encounter causal diagrams. Hang in there.  After presenting the terminology and conventions, we will work through a series of practical examples.  Importantly, we shall discover that there are all forms of confounding can be derived from four essential causal structures.  Once we understand these structures, we can then move to straightforwardly applying them in real world scenarios. 

First, causal diagrams employ a set of symbols to represent different elements involved in causal inference. In the context of our discussion, the symbols and their corresponding roles used in this article are presented in Table @tbl-01. 

As show in the @tbl-01

- **$A$** represents the treatment or exposure variable in the study. This could be any intervention or condition whose effect on an outcome is being investigated. **This symbol denotes the cause**.
- **$Y$** represents the outcome variable, which is the effect or result that researchers aim to understand and possibly predict based on the treatment or exposure.**This symbol denotes the effect**.
- **$L$** represents measured confounders. These are variables that may bias the causal association between $A$ on $Y$.
- **$U$** represents unmeasured confounders. These are variables that could influence both the treatment and the outcome but are not observed or included in the analysis. Their presence can lead to biased causal inferences.
- **$M$** represents a mediator variable. Mediators are factors through which the treatment exerts its effect on the outcome. Here we will primarily focus on identifying the total effect a single treatment $A$ on single outcome $Y$, however, it will be important to understand the role that "controling for" mediators plays in *biasing* total effect estimates.



::: {#tbl-01}

```{=latex}
\terminologylocalconventionssimple
```
Terminology for causal diagrams. (This table is adapted from [@bulbulia2023])

:::


::: {#tbl-02}

```{=latex}
\terminologygeneralbasic
```
Basic conventions for causal diagrams. This table is adapted from [@bulbulia2023]

:::

Having defined the meaning of our symbols, @tbl-02 describe the basic elements of a causal diagram. The key elements are of a causal diagram are:

1.  **Nodes**: nodes represent variables or events within a causal system, 

2.  **Edges** edges represent relationships between these variables. In causal diagrams, edges are directed. The define pathways of causal influence. Note that causal diagrams are non-parametric. This means that we a non-linear arrow is drawn the same a a linear relationship.

3.  **Parent and child relationships**: we call a variable an "child" if an arrow points to it. Any "parent" is a variable that at the other end of the arrow.  

4.  **Acyclic**: causal diagrams must be acyclic -- they cannot contain feedback loops.
More precisely: no variable can be an ancestor or descendant of itself. *Therefore, in cases where repeated measurements are taken, nodes must be indexed by time.* 

5.  **Conditioning**: in causal diagrams, we often assess the implications for obtaining an unbaised estimate of the relationship between the treatment and exposure by controlling for a variable. We indicate such "conditioning" or "adjustment" or "control" using the boxed symbol. 

5.  **D-separation**: we call a path "blocked," or "d-separated," if a node along it prevents the transmission of influence. Two variables are considered d-separated if all paths between them are blocked; otherwise, they are d-connected [@pearl1995]. In the next section, we will consider the rules of d-separation allow causal graphs to identify strategies for identifying causal effects from observational data. 


#### The rules of d-Separation 

::: {#tbl-03}

```{=latex}
\terminologydirectedgraph
```
This table is adapted from [@bulbulia2023]

:::


Pearl showed that the principles of d-separation enable us to evaluate relationships between nodes in a causal diagram [@pearl1995]. To build intuition for moving from simple to more complex directed acyclic graphs (DAGs) and understanding Pearl's rules of d-separation consider how causal relationship can be built up from simple relationships. The elementary building blocks are presented in @tbl-03.

### Two Variables with No Arrows

When two variables, $X_0$ and $X_1$, have no arrows between them, this represents a scenario where there is no causal effect assumed between the two variables. In this context, the variables are considered to be independent of each other, denoted as $X_0 \coprod X_1$. This independence implies that knowing the value of one does not provide any information about the value of the other.

### Two Variables with a Causal Arrow

Introducing a causal arrow from $X_0$ to $X_1$ ($X_0 \rightarrow X_1$) signifies that $X_0$ causally affects $X_1$. This causal relationship implies dependency between $X_0$ and $X_1$, denoted as $X_0 \cancel\coprod X_1$. Here, knowledge about $X_0$ provides information about $X_1$, reflecting their causal association.

### Three Variables: Fork, Chain, and Collider Structures

With three variables, the complexity increases, introducing three fundamental structures: fork, chain, and collider.

- **Fork Structure**: The fork structure ($X_0 \rightarrow X_1$, $X_0 \rightarrow X_2$) implies $X_0$ is a common cause of $X_1$ and $X_2$. 

- **Fork Rule**: Here, $X_1$ and $X_2$ are conditionally independent given $X_0$ (if $\boxed{X_0}$, then $X_1 \coprod X_2 | X_0$). Conditioning on $X_0$ blocks any association between $X_1$ and $X_2$ through $X_0$.

- **Chain structure**: The chain structure ($X_0 \rightarrow X_1 \rightarrow X_2$) indicates a causal chain where $X_0$ affects $X_1$, which in turn affects $X_2$. 

- **Chain rule**: $X_0$ and $X_2$ are conditionally independent given $X_1$ (if $\boxed{X_1}$, then $X_0 \coprod X_2 | X_1$). Conditioning on $X_1$ blocks the indirect path between $X_0$ and $X_2$.

- **Collider structure**: The collider structure ($X_0 \rightarrow X_2 \leftarrow X_1$) shows $X_2$ as a common effect of $X_0$ and $X_1$. 

- **Collider rule**: in a collider structure, $X_0$ and $X_1$ are independent. However, conditioning on $X_2$ (or a descendant of $X_2$, $X_3$) introduces an association between $X_0$ and $X_1$ ($X_0 \cancel\coprod X_1 | X_2$). This is because conditioning on the common effect (or its descendant) creates a pathway through which $X_0$ and $X_1$ can influence each other.


Again, for those unfamiliar with causal diagrams, these elementary structure of causation will sound abstract.  For those unfamiliar with probability theory, writing out conditional dependencies and independencies is perhaps daunting.  However, having stated the rules and conventions, we shall see that the application of causal diagrams to concrete problems is not daunting.  Indeed our experience is that people find developing causal diagrams a great deal of fun. Importantly, causal diagrams require no math. They are purely qualitative tools that encode assumptions.  However, because they allow us to evaluate how variables become independent or associated, we can use the fork rule, chain rule, and collider rule to guide us to modelling and data collection decisions in the pursuit of causal inferences. 



#### The elementary confounding conditions 


::: {#tbl-04}

```{=latex}
\terminologyelconfoundersLONG
```
This describes elementary and complex confounding scenarios (table is adapted from [@bulbulia2023])

:::



### 1. The problem of confounding by a common cause

@tbl-04 row 1 describes the problem of confounding by common cause. We encountered this problem in the first section. Such confounding arises when there is a variable or set of variables, denoted by $L$, that influence both the exposure, denoted by $A$, and the outcome, denoted by $Y.$ Because $L$ is a common cause of both $A$ and $Y$, $L$ may create a statistical association between $A$ and $Y$ that does not reflect a causal association.

For instance, in the context of green spaces, consider people choosing to live closer to green spaces (exposure $A$) and their experience of improved mental health (outcome $Y$).
A common cause could be socioeconomic status ($L$). Individuals with higher socioeconomic status may have the financial capacity to afford housing near green spaces and simultaneously afford better healthcare and lifestyle choices, contributing to improved mental health. Thus, while the data may show a statistical association between living closer to green spaces ($A$) and improved mental health ($Y$), this association may not reflect a direct causal relationship due to the confounding by socioeconomic status ($L$).

How might we obtain balance in this confounder for the treatments to be compared?  Addressing confounding by a common cause involves its adjustment. This adjustment effectively closes the backdoor path from the exposure to the outcome. Equivalently, conditioning on $L$ d-separates $A$ and $Y$.  Common adjustment methods include regression, matching, inverse probability of treatment weighting, and G-methods (covered in [@hernán2023]).  As indicated in @tbl-04 row one, any confounder that is a common cause of both $A$ and $Y$ must precede $A$ (and hence $Y$).  Often it will be possible to known that the a variable precedes the exposure. For example we may record a persons country of birth at any point in their life. In other cases, the timing may be unclear.  When we draw our causal diagram, we are asserting that this confounder really has occured before the exposure. This may often be a strong assertion. In this case, it may be useful to the scenario in which the confounder comes later.  We next turn to one such scenario. 


### 2. Mediator bias

Let us return to green spaces example, again we consider proximity to green spaces as the exposure ($A$), mental health as the outcome ($Y$), and physical activity as the mediator ($L$).

In this scenario, suppose that living close to green spaces ($A$) influences physical activity ($L$), which subsequently impacts mental health ($Y$). Notice that if we were to condition on physical activity ($L$), assuming it to be a confounder, we would then bias our estimates of the total effect of proximity to green spaces ($A$) on mental health ($Y$). Such a bias arises as a consequence of the chain rule. Conditioning on $L$ 'd-Separates' the total effect of $A$ on $Y$. This phenomenon is known as mediator bias.  Notable @montgomery2018 finds many dozens of examples of mediator bias in *experiments* in which control is made for variables that occur after the treatment.  For example, the practice of obtaining demographic and other information from participants *after* a study is an invitation to mediator bias. If the treatment affects these variables, and the variables affect the outcome (as we assume by controlling for them), then researchers may induce mediator bias. 


To avoid mediator bias we should, of course, avoid conditioning on a mediator.
Importantly, we avoid this problem by ensuring that $L$ occurs before the treatment $A$ and the outcome $Y$.  This solution is presented @tbl-04 row 2. 


### 3. Confounding by collider stratification (conditioning on a common effect)

Conditioning on a common effect, also known as collider stratification, occurs when a variable, denoted by $L$, is influenced by both the exposure, denoted by $A$, and the outcome, denoted by $Y$.

Imagine, in the context of green spaces, an individual's choice to live closer to green spaces (exposure $A$) and their improved mental health (outcome $Y$) are both influencing the individual's overall satisfaction with life (common effect $L$).
Initially, $A$ and $Y$ could be independent, represented as $A \coprod Y(a)$, suggesting that the decision to live near green spaces is not directly causing improved mental health.

However, when we condition on the life satisfaction $L$ (the common effect of $A$ and $Y$), a backdoor path between $A$ and $Y$ is opened. This could potentially induce a non-causal association between living closer to green spaces and improved mental health.
The reason behind this is that the overall life satisfaction $L$ can provide information about both the proximity to green spaces $A$ and the mental health status $Y$.
Hence, it may appear that there is an association between $A$ and $Y$ even when there may not be a direct causal relationship.

Causal diagrams point a way to respond to the problem of collider stratification bias: we should generally ensure that:

1.  all confounders $L$ that are common causes of the exposure $A$ and the outcome $Y$ are measured before $A$ has occurred, and
2.  $A$ is measured before $Y$ has occurred.

If such temporal order is preserved, $L$ cannot be an effect of $A$, and thus neither of $Y$.[^2]

#### 4. Confounding by conditioning on a descendant of a confounder  

The rules of d-separation also apply to conditioning on descendents of a confounder.  As show in @tbl-04 row five, we should ensure when conditioning on a measured descendant of an unmeasured collider because doing so evokes confounding by proxy.

### More complicated forms of confounding are combinations of elementary confounding

We next turn to more complicated scenarios that combine elements of the elementary structures of causality. 

### M-bias: conditioning on a collider that occurs before the exposure may introduce bias

@tbl-04 row 5 presents a form of pre-exposure over-conditioning bias known as "M-bias".  This bias combines the collider structure and the fork-structure revealing that it is possible to induce confounding even if we ensure that all variables have been measured before the treatment.  The collider structure is shown in the path $U_Y \to L_0$ and $U_A \to L_0$. We know from the collider rule that conditioning on $L_0$ opens a path between $U_Y$ and $U_A$. What is the result? We find that $U_Y$ is associated with the outcome $Y$ and $U_A$ is associated with treatment $A$.  Thus the association that is opened by conditioning on $L$ creates an open back-door path linking the treatment to the outcome. We have confounding.  How might such confounding play out in a real world setting? 

In the context of green spaces, consider the scenario where an individual's level of physical activity ($L$) is influenced by an unmeasured factor related to their propensity to live near green spaces ($A$) and another unmeasured factor linked to their mental health ($Y$). Here, physical activity $L$ does not directly affect the decision to live near green spaces $A$ or mental health status $Y$, but is a descendent of unmeasured variables that do. If we condition on physical activity $L$ in this scenario, we create the bias just described, known as "M-bias."  

How shall we respond? The solution is straightforward. If $L$ is neither a common cause of $A$ and $Y$ nor the effect of a shared common cause, then $L$ should not be included in a causal model.  In terms of the conditional exchangeability principle, we find $A \coprod Y(a)$ yet $A \cancel{\coprod} Y(a)| L$. So we must not condition on $L[@cole2010].[^3]

[^3]: Note, when we draw a chronologically ordered path from left to right the M shape for which "M-bias" takes its name changes to an E shape We shall avoid proliferating jargon and retain the term "M bias."

### 6. Conditioning on a descendent may reduce confounding

We can develop the rules of d-Separation to obtain unexpected strategies for confounding control.  Consider @tbl-04 row six. The causal diagram describes a setting in which there is an open back-door path linking the treatment and outcome. We have what appears to be intractable confounding.  Return to our green-space example, consider an unmeasured confounder $U$, perhaps a genetic factor, that affects one's desire to seek out isolation in green spaces $A$ and also independently affects one's mental health $Y$.  Were such an unmeasured confounder to exist we could not obtain an unbiased estimate for the causal effect of green-space on happiness. However, imagine a variable $L^\prime$, that is a trait that is expressed later in life, after decisions to seek out isolation have been made.  If this trait could be measured, even though it is expressed after the treatment and outcome has occurred, controlling for it would enable us to close the backdoor path between the treatment and the outcome. The reason this strategy work is that a measured effect is a proxy for its cause.  By conditioning on the late-adulthood trait, we partially condition on its cause, in this case a confounder that occurred before the treatment and leads to a confounding association between the treatment and outcome in the absence of causality.

#### 7. Confounding control with three waves of data

@tbl-04 row 7 presents another setting in which there is unmeasured confounding. However, we may use the rules of d-separation to develop strategies for data collection and data modelling that may greatly reduce the influence of unmeasured confounding on our causal inferences.  As shown the "response", but collecting data for the both the treatment and the outcome at baseline, and controlling for this information in our statistical models, any unmeasured association between the treatment $A_1$ and the outcome $Y_2$ would need to be *independent* of these baseline measurements.  Thus including the baseline exposure and outcome, along with other measured covariates that are measured descendents of unmeasured confounders, we exert considerable confounding control [@vanderweele2020]. 

A secondary advantage of such data collection is that it allows us to obtain an incident exposure effect, rather than merely a prevalence exposure effect.  Consider:

**The prevalence exposure effect** evaluates the association between the
exposure or treatment status at time $t1$ and the outcome observed at a
later time $t2$. It is defined by the pathway $A_{1} \to Y_{2}$. The
prevalent exposure effect does not consider the initial status of the
exposure. It is expressed:

$$
\text{prevalence exposure effect:} \quad A_{1} \to Y_{2}
$$

As such, the prevalence exposure effect describes the effect of current or
ongoing exposures on outcomes.  For example imagine that living far from green spaces makes people so depressed that they never respond to surveys. When we observe the association between green space and happiness in the data, we are left with only those people who are so incurably happy that even living far from green space cannot bring them down. As such , it may appear in our data that $A_{1} \to Y_{2}$ is helpful when,
in fact, the treatment is harmful; see: @hernán2016; @danaei2012;
@vanderweele2020; @bulbulia2022.

**Incident exposure effect**: evaluates the association between the
exposure or treatment status at time $t1$ and the outcome observed at a
later time $t2$ conditional on the baseline exposure:
$A_{0} \to A_{1} \to Y_{2}$. This model more closely emulates an
experiment because it considers the transition in treatment or exposure
status from $A_0$ to $A_1$. The initiation of a treatment provides a
clearer intervention from which to estimate a causal effect at $Y_2$. It
is expressed:

$$
\text{Incident exposure effect:} \quad \boxed{A_{0}} \to A_{1} \to Y_{2}
$$

Returning to our example, the any association between of $A_1$ and $Y_2$ would require that people initiate a change from the level of $A_0$ from baseline. Thus by controlling for the baseline value of the treatment we may learn about the effect of shifting one's access to green space status.

As in @tbl-04 row seven, we obtain further control by including the baseline outcome $Y_0$ as
well as the baseline exposure $A_0$ such that:

$$
\boxed{
\begin{aligned}
L_{0} \\
A_{0} \\
Y_{0}
\end{aligned}
}
\to A_{1} \to Y_{2}
$$

Thus the incident exposure effect better emulates a 'target trial' or a the
organisation of observational data into a hypothetical experiment in
which there is a 'time-zero' initiation of treatment in the data; see:
@hernán2016; @danaei2012; @vanderweele2020; @bulbulia2022.  

Causal diagrams reveal both the possibility and power of panel data collection, and equally importantly of how to appropriately model model panel data to obtain valid causal inference (note that a multi-level model or structural equation "growth" model would not obtain the incident exposure effect.) To obtain the incident exposure effect, we generally require that events in the data can be accurately classified into at least three relative time intervals and we must model the treatments and outcomes as separate elements in our statistical model. 


#### Summary and general rules for creating causal diagrams

In this chapter, we have reviewed the relevance of the potential outcomes framework to causal inference environmental psychology. In Section 1 we encountered three fundamental assumptions required for obtaining average treatment effects from data: 

1. **Causal consistency**: the outcome observed under the treatment condition for any unit matches the outcome that would have been observed had the unit received the treatment, and similarly for the control condition.

2. **Exchangeability**: the allocation of treatment is randomised and independent of the potential outcomes, conitional on measured covariates. 

3. **Positivity**: every unit has a non-zero probability of receiving the treatments to be compared.

These assumptions are typically satisfied in the setting of randomised experiments, which adeptly address the issue of missing potential outcomes between the groups under comparison. However, in observational studies, the non-random assignment of treatments complicates causal inference.  

We then turned to casual graphs and discussed their application for addressing the 'no unmeasured confounding' assumption, considering both elementary and complex confounding scenarios. 

We have seen that, at their essence, causal diagrams serve as a simplified visual language for articulating complex causal relationships within a given study. In causal diagrams, each symbol encodes specific assumptions about the causal structure of the variables it represents. For instance, when we draw an arrow from one variable to another, we assert that a causal relationship exists based on theoretical or empirical evidence. This is true for all relationships depicted in the graph, with the notable exception of the relationship between the treatment, $A$, and the outcome, $Y$.

Importantly, we have seen that when developing a causal diagram, the objective is to describe only those features of a complex causal setting that are relevant to assessing bias in the relationship of treatment $A$ and the outcome $Y$. Variables that are not relevant to evaluating this question should be omitted.

It is worth emphasising that wwe will often want to simplify our causal diagrams to the minimum level of complexity required to convey the problem at hand. For example the confounder symbol $L$ typically represents a vector of covariates. We may encapsulating numerous confounders within a single node where all arrows in and out of the node are functionally equivalent for the assessing bias in the association of $A$ and $Y$. This simplification is crucial as it allows researchers to focus on key variables and their interconnections without being overwhelmed by the full complexity of the underlying data.

Finally, the construction of a causal diagram is relative to a specific research interest and context. The graph is tailored to the particular causal questions at hand and the specific population under investigation -- the target population. For example, if the research interest were to shift to understanding the total effect of a set of covariates $L$ on the treatment $A$, the causal diagram would need be redrawn to reflect this new focus, altering the direction or presence of arrows to represent the hypothesised causal pathways accurately. 

Here we considered only seven applications of causal diagrams. Of course there are many more scenarios we would need to consider to evaluate how observational data might be leveraged to obtain scientific insights in environmental psychology. In each case, however, our causal diagrams will consist of combinations of four elementary relationships: the association of effect with its cause (elementary causality expressed between two variables), the fork (a common cause of two variables); the chain (a mediator between two variables); and the collider (the common effect of two variables). These rules by which variables becomes associated and disassociated by conditioning on nodes within these elementary relationships allows researchers to use causal diagrams to investigate innumerably many practical questions. Causal diagrams may also inform strategies for data collection, and may help to focus attention on which information should and should not be included in our statistical models. We hope the material we presented here will encourage environmental psychologist to learn more about causal inference and to integrate causal diagrams into their workflows. 


{{< pagebreak >}}

DON -- if we were to do Part 3 we could use this table

::: {#tbl-04}

```{=latex}
\terminologyelconfoundersexperiments
```
This describes elementary and complex confounding scenarios (table is adapted from [@bulbulia2023])

:::

{{< pagebreak >}}

## Funding

This work is supported by a grant from the Templeton Religion Trust (TRT0418).
JB received support from the Max Planck Institute for the Science of Human History.
The funders had no role in preparing the manuscript or the decision to publish it.

## Contributions

TBA

## References

::: {#refs}
:::



## Appendix: Causal Consistency in observational settings

Although we discussed the causal consistency assumption in Part 1, we did not examine how observational scientists may deal with it.

In observational research, there are typically multiple versions of treatment.
The theory of causal inference under multiple versions of treatment proves we can consistently estimate causal effects where the different versions of treatment are conditionally independent of the outcomes [@vanderweele2009, @vanderweele2009; @vanderweele2013; @vanderweele2018] 

Let $\coprod$ denote independence.
Where there are $K$ different versions of treatment $A$ and no confounding for $K$'s effect on $Y$ given measured confounders $L$ such that

$$
Y(k) \coprod K | L
$$

Then it can be proved that causal consistency follows. According to the theory of causal inference under multiple versions of treatment, the measured variable $A$ functions as a "coarsened indicator" for estimating the causal effect of the multiple versions of treatment $K$ on $Y(k)$ [@vanderweele2009; @vanderweele2013; @vanderweele2018].

In the context of green spaces, $A$ might represent the general action of moving closer to any green space and $K$ represents the different versions of this treatment.
For instance, $K$ could denote moving closer to different types of green spaces such as parks, forests, community gardens, or green spaces with varying amenities and features.

Here, the conditional independence implies that, given measured confounders $L$ (e.g. socioeconomic status, age, personal values), the type of green space one moves closer to ($K$) is independent of the outcomes $Y(k)$ (e.g. mental well-being under the $K$ conditions). In other words, the version of green space one chooses to live near does not affect the $K$ potential outcomes, provided the confounders $L$ are properly controlled for in our statistical models.