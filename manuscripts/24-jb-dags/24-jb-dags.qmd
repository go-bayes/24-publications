---
title: "Causal Directed Acyclic Graphs (DAGs): A Practical Guide"
abstract: |
  Causal inference requires contrasting counterfactual states of the world under pre-specified interventions. Obtaining counterfactual contrasts from data, in turn, relies on a framework of explicit assumptions and careful, multi-step workflows. Causal diagrams are powerful tools for clarifying whether and how the counterfactual contrasts we seek may be identified from data. Here, I explain how to use causal directed acyclic graphs (causal DAGs) to clarify whether and how causal effects may be identified from observational data. Along the way, I offer practical tips for reporting and suggestions for avoiding common pitfalls. 

   **KEYWORDS**: *Causal Inference*; *Culture*; *DAGs*;* *Evolution*; *Human Sciences*; *Longitudinal*.
author: 
  - name: Joseph A. Bulbulia
    affiliation: Victoria University of Wellington, New Zealand
    orcid: 0000-0002-5861-2056
    email: joseph.bulbulia@vuw.ac.nz
    corresponding: no
editor_options: 
  chunk_output_type: console
format:
  pdf:
    sanitise: true
    keep-tex: true
    link-citations: true
    colorlinks: true
    documentclass: article
    classoption: [single column]
    lof: false
    lot: false
    geometry:
      - top=30mm
      - left=25mm
      - heightrounded
      - headsep=22pt
      - headheight=11pt
      - footskip=33pt
      - ignorehead
      - ignorefoot
    template-partials: 
      - /Users/joseph/GIT/templates/quarto/title.tex
    header-includes:
      - \input{/Users/joseph/GIT/latex/latex-for-quarto.tex}
date: last-modified
execute:
  echo: false
  warning: false
  include: true
  eval: true
fontfamily: libertinus
bibliography: /Users/joseph/GIT/templates/bib/references.bib
csl: /Users/joseph/GIT/templates/csl/camb-a.csl
---


```{r}
#| label: load-libraries
#| echo: false
#| include: false
#| eval: true

## WARNING SET THIS PATH TO YOUR DATA ON YOUR SECURE MACHINE. 
# pull_path <-
#   fs::path_expand(
#     #"/Users/joseph/v-project\ Dropbox/Joseph\ Bulbulia/00Bulbulia\ Pubs/DATA/nzavs_refactor/nzavs_data_23"
#     "/Users/joseph/Library/CloudStorage/Dropbox-v-project/Joseph\ Bulbulia/00Bulbulia\ Pubs/DATA/nzavs-current/r-data/nzavs_data_qs"
#   )
# 


push_mods <-  fs::path_expand(
  "/Users/joseph/Library/CloudStorage/Dropbox-v-project/data/nzvs_mods/24/church-prosocial-v7"
)


#tinytext::tlmgr_update()

# WARNING:  COMMENT THIS OUT. JB DOES THIS FOR WORKING WITHOUT WIFI
#source("/Users/joseph/GIT/templates/functions/libs2.R")
# # WARNING:  COMMENT THIS OUT. JB DOES THIS FOR WORKING WITHOUT WIFI
# source("/Users/joseph/GIT/templates/functions/funs.R")

#ALERT: UNCOMMENT THIS AND DOWNLOAD THE FUNCTIONS FROM JB's GITHUB

# source(
#   "https://raw.githubusercontent.com/go-bayes/templates/main/functions/experimental_funs.R"
# )
# 
# source(
#   "https://raw.githubusercontent.com/go-bayes/templates/main/functions/funs.R"
# )

# check path:is this correct?  check so you know you are not overwriting other directors
#push_mods

# for latex graphs
# for making graphs
library("tinytex")
library("extrafont")
library("tidyverse")
library("kableExtra")
#devtools::install_github("go-bayes/margot")
library(margot)
loadfonts(device = "all")
```



## Introduction

Human research begins with two fundamental questions:

1. What do I want to know?
2. For which population does this knowledge generalise?

In the human sciences, our questions are typically causal. We aim to understand the effects of interventions on certain variables. However, many researchers report non-causal associations, collecting data, applying complex regressions, and reporting coefficients. We often speak of covariates as "predicting" outcomes. Yet, even when our models predict well, it remains unclear how these predictions relate to the scientific questions that sparked our interest. We fail to recognise that these "oracles" lack coherent interpretation.

Some say that association cannot imply causation. However, our experimental traditions reveal that when interventions are controlled and randomised, the coefficients we recover from statistical models can permit causal interpretations. 

Despite familiarity with experiments, many researchers struggle to emulate randomisation and control with non-experimental or "real-world" data. Though many use terms like "control" and employ sophisticated adjustment strategies—such as multilevel modelling and structural equation models—these practices are not systematic. We often overlook that what we take as control can undermine our ability to consistently estimate causal effects. Although the term "crisis" is overused, the state of causal inference across many human sciences, including experimental sciences, has much headroom for improvement. Moreover, poor experimental designs unintentially weaken causal claims; causal inferences in experiments is also deserving of greater attention. Fortuantely, recent decades have seen progress in the the health sciences, economics, and computer science have markedly improved our ability to obtain causal inferences in scientific research. This work demonstrates that obtaining causal inferences from data is feasible but requires careful, systematic workflows. 

Within the workflows of causal inference, causal diagrams—or causal graphs—are powerful tools for evaluating whether and how causal effects can be identified from data. My purpose here is to explain where these tools fit within causal inference workflows and to illustrate their practical applications. I focus on causal directed acyclic graphs (causal DAGs) because they are relatively easy to use and optimally clear for most applications. However, causal directed acyclic graphs can also be misused. I will also explain common pitfalls and how to avoid them.

In Part 1, we review the conceptual foundations of causal inference. The basis of all causal inference lies in counterfactual contrasts. Although there are different philosophical approaches to counterfactual reasoning, they are largely similar in practice.  The overview here builds on the Neyman-Rubin potential outcomes framework, extended for longitudinal data by epidemiologist James Robins.  Although this is not the framework within which causal directed acyclic graphs were developed, the potential outcomes framework is easier to interpret. (I discuss Pearl's non-parametric structural equation approach in Appendix E).

In Part 2, we describe how causal directed acyclic graphs help identify causal effects. We outline five elementary graphical structures that encode all causal relations, forming the building blocks of all causal directed acyclic graphs. We then examine five rules that clarify whether and how investigators can identify causal effects from data.

In Part 3, we apply causal directed acyclic graphs to practical problems, showing how repeated measures data collection can solve seven common identification issues. Timing is critical, but not sufficient alone. We also use causal diagrams to highlight the limitations of repeated-measures data collection for identifying causal effects, tempering enthusiasm for easy solutions. Indeed we will review how many statistical structural equation models and sophisticated multi-level models are not well-calibrated for identifying causal effects.

In Part 4, I offer practical suggestions for creating and reporting causal directed acyclic graphs in scientific research. These graphs represent investigator assumptions about causal (or structural) relationships in nature. These relationships cannot typically be derived from data alone and must be developed with scientific specialists. Where ambiguity or debate exists, investigators should report multiple causal diagrams and conduct distinct analyses. I explain how to do this and walk through the steps.

By understanding and applying causal directed acyclic graphs, researchers can more effectively identify and communicate causal relationships, in the service of the causal questions that animate our scientific interets. 

## Part 1: Causal Inference as Counterfactual Data Science.

The first step in answering a causal question is to ask it [@hernán2016]. 

1. What do causal quantity to I want to consistently estimate?
2. For which population does this knowledge generalise?

Causal diagrams come after we have stated a causal question and the population of interest, the 'target population'. We begin by considering what is required to state these questions precisely.

#### 1.1.1 The fundamental problem of causal inference

To ask a causal question, we must consider the concept of causality itself. Consider an intervention, $A$, and its effect, $Y$. We say that $A$ causes $Y$ if altering $A$ would lead to a change in $Y$ [@hume1902; @lewis1973]. If altering $A$ would not change $Y$, we say that $A$ has no causal effect on $Y$.

In causal inference, we aim to quantitatively contrast the potential outcomes of $Y$ in response to different levels of a well-defined intervention. Commonly, we refer to such interventions as 'exposures' or 'treatments;' we refer to the possible effects of interventions as 'potential outcomes.'

Consider a binary treatment variable $A \in \{0,1\}$. For each unit $i$ in the set $\{1, 2, \ldots, n\}$, when $A_i$ is set to 0, the potential outcome under this condition is denoted, $Y_i(0)$. Conversely, when $A_i$ is set to 1, the potential outcome is denoted, $Y_i(1)$. We refer to the terms $Y_i(1)$ and $Y_i(0)$ as 'potential outcomes' because until realised, the effects of interventions describe counterfactual states.

Suppose that each unit $i$ receives either $A_i = 1$ or $A_i = 0$. The corresponding outcomes are realised as $Y_i|A_i = 1$ or $Y_i|A_i = 0$. For now, let us assume that each realised outcome under that intervention is equivalent to one of the potential outcomes required for a quantitative causal contrast, such that $[(Y_i(a)|A_i = a)] = (Y_i|A_i = a)$. Thus when $A_i = 1$,  $Y_i(1)|A_i = 1$ is observed. However, when $A_i = 1$, it follows that $Y_i(0)|A_i = 1$ is not observed:

$$
Y_i|A_i = 1 \implies Y_i(0)|A_i = 1~ \text{is counterfactual}
$$

Conversely, if $A_i = 0$, we may assume the potential outcome $Y_i(0)|A_i = 0$) is observed as $Y_i|A_i = 0$. However, the potential outcome $Y_i(1)|A_i = 0$ is never realised and so not observed:

$$
Y_i|A_i = 0 \implies Y_i(1)|A_i = 0~ \text{is counterfactual}
$$

We define $\tau_i$ as the individual causal effect for unit $i$ and express the individual causal effect:

$$
\tau_i = Y_i(1) - Y_i(0)
$$

Notice that each unit can only be exposed to only one level of the exposure $A_i = a$ at a time. This implies that $\tau_i$, is not merely unobserved but inherently *unobservable*.

<!-- Although we cannot observe potential outcomes that do not occur, it is tempting to ask questions about them, 'What if Isaac Newton had not witnessed the falling apple?’ What if Leonardo da Vinci had never pursued art?' or 'What if Archduke Ferdinand had not been assassinated?' There are abundant examples from literature. Robert Frost contemplates, 'Two roads diverged in a yellow wood, and sorry I could not travel both, and be one traveller, long I stood...' (see: Robert Frost, 'The Road Not Taken':
https://www.poetryfoundation.org/poems/44272/the-road-not-taken). We have counterfactual questions for our personal experiences: 'What if I had had not interviewed for that job?' 'What if I had stayed in that relationship?' We may speculate, with reasons, but we cannot directly observe the potential outcomes we would need to verify our speculations. The physics of middle-sized dry goods prevents the joint realisation of the facts required for quantitative comparisons.  -->

That individual causal effects cannot be identified from observations is known as '*the fundamental problem of causal inference*' [@rubin1976; @holland1986].


#### 1.1.2 Causal effects from randomised experiments

Although it is not typically feasible to compute individual causal effects. under certain assumptions, it may be possible to estimate *average* treatment effects -- also called 'marginal effects.'  We define an average treatment effect ($ATE$) as the difference between the expected or average outcomes under treatment and contrast conditions for a pre-specified population, typically the population from which an observed sample is drawn.

Consider a binary treatment, $A \in \{0,1\}$

$$
\text{Average Treatment Effect}  = \mathbb{E}[Y(1)] - \mathbb{E}[Y(0)].
$$

This is our prespecified estimand for our target population. A challenge remains for computing these treatment-group averages, given that individual causal effects are unobservable. We can frame the problem framed by referring to the *full data* required to compute this estimand — that is in terms of the complete counterfactual dataset where the missing potential outcomes, inherent in observational data, were somehow available.  The text highlighted in red denotes inherently missing responses over the joint distribution of the full counterfactual dataset (also stated by underbraces). We find that for each treatment condition, half the observations over the joint distribution of the counterfactual data are inherently unobserved.

$$
\text{Average Treatment Effect} = \left(\underbrace{\underbrace{\mathbb{E}[Y(1)|A = 1]}_{\text{observed for } A = 1} + \underbrace{\textcolor{red}{\mathbb{E}[Y(1)|A = 0]}}_{\textcolor{red}{\text{unobserved for } A = 0}}}_{\text{effect among treated}}\right) - \left(\underbrace{\underbrace{\mathbb{E}[Y(0)|A = 0]}_{\text{observed for } A = 0} + \underbrace{\textcolor{red}{\mathbb{E}[Y(0)|A = 1]}}_{\textcolor{red}{\text{unobserved for } A = 1}}}_{\text{effect among untreated}}\right).
$$


Randomisation allows investigators to recover the treatment group averages even though treatment groups contain inherently missing observations. We do not require the joint distribution over the full-data (i.e. the counterfactual data) to obtain average treatment groups. This is because when investigators randomise units into treatment conditions, there is full adherence, and the sample is sufficiently large to rule out chance differences in the composition of the treatment groups to be compared, the distributions of confounders that could explain differences in the potential outcomes will be balanced across the conditions. Randomisation under such conditions rules out explanations for difference in treatment group average except the treatment. Put differently randomisation implies:

$$
\mathbb{E}[Y(0) | A = 1] = \mathbb{E}[Y(0) | A = 0]
$$

and

$$
\mathbb{E}[Y(1) | A = 1] = \mathbb{E}[Y(1) | A = 0]
$$

We assume, (by causal consistency, see: $\S 1.2.1$):

$$ \mathbb{E}[Y(1) | A = 1] = \mathbb{E}[Y| A = 1]$$

and

$$\mathbb{E}[Y(0) | A = 0] = \mathbb{E}[Y| A = 0]$$

It follows that the average treatment effect of the randomised experiment can be computed:

$$
\text{The Average Treatment Effect} = \widehat{\mathbb{E}}[Y | A = 1] - \widehat{\mathbb{E}}[Y | A = 0].
$$

There are four critical aspects for how ideally randomised experiments enable the estimation of average treatment effects worth highlighting.

First, the investigators must specify a population for whom they seek to generalise their results. We refer to this population as the *target population*. If the study population differs from the target population in the distribution of covariates that interact with the treatment, the investigators will have no guarantees their results will generalise (for discussions of sample/target population mismatch refer to: @imai2008misunderstandings; @westreich2019target; @westreich2017; @pearl2022; @bareinboim2013general; @stuart2018generalizability;
@webster2021directed.)

Second, because the units in the study sample at randomisation may differ may differ from the units in the study after randomisation, investigators must be careful to avoid biases that arise from sample/population mismatch over time.  If there is sample attrition or non-response, the treatment effect investigators obtain for the sample may differ from the treatment effect in the target population.  

Third, a randomised experiment recovers the causal effect of random treatment assignment, not of the treatment itself, which may differ if some participants do not adhere to their treatment (even if they remain in the study). The effect randomised assignment is called the *intent-to-treat effect*. The effect of perfect adherence is called the *per protocol effect* [@hernan2017per; @lash2020]. To obtain the per protocol effect for randomised experiments requires the application of methods for causal inference in observational settings.

Fourth, I have presented the average treatment effect on the difference scale, that is, as a difference in average potential outcomes for the target population under two distinct levels of treatment. However, depending on the scientific question at hand, investigators may wish to estimate causal effects on the risk-ratio scale, the rate-ratio scale the hazard-ratio scale, or another scale. Where there are interactions such that treatment effects vary across different strata of the population, an estimate of the causal effect on the risk difference
scale will differ in at least one stratum to be compared from the estimate on the risk ratio scale [@greenland2003quantifying]. The sensitivity of treatment effects in the presence of interactions to the scale of contrast underscores the importance of pre-specifying a scale for the causal contrast investigators hope to obtain.

Fifth, investigators may uninentionally spoil randomisation by adjusting for indicators that might be affected by the treatment, outcome, or both, by excluding participants using attention checks, by collecting covariate data that might be affected by the experimental conditions, by failing to account for non-response and loss-to-follow up, and by committing any number of other self-inflicted injuries. Unfortunately, such practices are widespread [@montgomery2018]. Notably causal graphical methods are useful for describing causal identification in experiments (refer to @hernan2017per), a topic we consider elsewhere [@cite..]

<!-- Fifth, in observational studies, investigators might wish to describe
the target population of interest as a restriction of the study sample
population. For example, investigators might wish to estimate the
average treatment effect only in the population that received the
treatment. This treatment effect is sometimes called the average treatment effect in the treated ($ATT$), and may be expressed:

$$\text{Average Treatment Effect in the Treated} = \mathbb{E}[Y(1) - Y(0) | A = 1]$$

Consider that if investigators are interested in the average treatment
effect in the treated, counterfactual comparisons are deliberately *restricted* to the sample population that was treated. That is, the investigators will seek to obtain the average of the missing counterfactual outcomes for *the treated population were they not treated*, without necessarily obtaining the counterfactual outcomes for the untreated population were they treated. This difference in focus may imply different assumptions and analytic workflows. **Appendix 1** describes an example for which the assumptions required to estimate the average treatment effect may be preferred. In what follows, we will use the term $ATE$ as a placeholder to mean the average treatment effect, or equivalently the 'marginal effect', for a target population on a pre-specified scale of causal contrast.

Setting aside the important detail that the 'average treatment effect' requires considerable care in its specification, it is worth pausing to marvel at how an ideally conducted randomised controlled experiment provides a means for identifying inherently unobservable counterfactuals. It does so by using a Sherlock-Holmes-method of inference by elimination of confounders, which randomisation balances across treatments. -->
<!-- 
 When experimenters observe a difference in average treatment effects, and all else goes right, they may infer that the distribution of potential outcomes differs by treatment because randomisation exhausts every other explanation except that of the treatment. They are entitled to this inference because randomisation balances the distribution of potential confounders across the treatment groups to be compared. -->

<!-- Outside of randomised experiments, however, we lack guarantees of balance in the confounders. For this reason, investigators should prefer developing sound randomised experiments for
addressing every causal question that experiments can address.
Unfortunately, randomised experiments cannot address many scientifically
important questions. This bitter constraint is familiar to evolutionary
human scientists. We typically confront 'What if?' questions that are
rooted in the unidirectional nature of human history. -->

Setting these considerations aside, understanding how randomisation obtains the missing counterfactual outcomes that we require to consistently estimate average treatment effects clarifies the tasks of causal inference in non-experimental settings [@hernán2008a; @hernán2006; @hernán2022]. 

<!-- We next examine these identification assumptions in greater detail because using causal diagrams without understanding these assumptions is unsafe. -->

### 1.2 Fundamental Identification Assumptions

There are three fundamental identification assumptions that must be satisfied to consistently estimate causal effects with data. These assumptions are typically satisfied in randomised controlled trials/experiments but not in real-world studies in which randomised treatment assignment is absent.

#### 1.2.1 Assumption 1: Causal Consistency

We satisfy the causal consistency assumption when, for each unit $i$ in the set $\{1, 2, \ldots, n\}$, the observed outcome corresponds to one
of the specific counterfactual outcomes to be compared such that:

$$
Y_i^{observed}|A_i = 
\begin{cases} 
Y_i(a^*) & \text{if } A_i = a^* \\
Y_i(a) & \text{if } A_i = a
\end{cases}
$$

The causal consistency assumption implies that the observed outcome at a specific exposure level equates to the counterfactual outcome for that individual at the observed exposure level. Although it seems straightforward to equate an individual's observed outcome with their counterfactual outcome, treatment conditions vary, and treatment heterogeneity poses considerable challenges for satisfying this assumption. See: **Appendix A** for further discussion of how investigators may obtain statisfy causal consistency in real-world settings. 



#### 1.2.2 Assumption 2: Positivity

We satisfy the positivity assumption if there is a non-zero probability
of receiving each treatment level for every combination of covariates
that occurs in the population. Where $A$ is the exposure and $L$ is a
vector of covariates, we say positivity is achieved if:

$$
0 < Pr(A = a | L = l) < 1, \quad \text{for all } a, l \text{ with } Pr(L = l) > 0
$$

There are two types of positivity violation:

1.  **Random non-positivity** occurs when an exposure is theoretically
    possible, but specific exposure levels are not represented in the
    data. Notably, random non-positivity is the only identifiability
    assumption verifiable with data.

2.  **Deterministic non-positivity** occurs when the exposure is
    implausible by nature. For instance, a hysterectomy in biological
    males would appear biologically implausible.

Satisfying the positivity assumption can present considerable data challenges [@westreich2010]. Suppose we had access to extensive panel data that has tracked 20,000 individuals randomly sampled from the target population over three years. Suppose further that we wanted to estimate a one-year causal effect of weekly religious service attendance on charitable donations. We control for baseline attendance to recover an incident exposure effect estimate. Assume that the natural transition rate from no religious service attendance to weekly service attendance is low, say one in a thousand annually. In that case, the effective sample for the treatment condition dwindles to 20. This example clarifies the problem. For rare exposures, the data required for valid causal contrasts may be sparse, even in large datasets. Where the positivity assumption is violated, causal diagrams will be of limited utility because observations in the data do not support valid causal inferences. (**Appendix B** presents a worked
example that illustrates the difficulty of satisfying this assumption in a setting of a cultural evolutionary questions.)


#### 1.2.3 Assumption 3: Conditional Exchangeability (or no unmeasured confounding or conditional ignorability)

We satisfy the conditional exchangeability assumption if the treatment groups are conditionally balanced in the variables that could affect the potential outcomes. In experimental designs, random assignment facilitates satisfaction of the conditional exchangeability assumption. In observational studies more effort is required. We must control for any covariate that could account for observed correlations between $A$ and $Y$ in the absence of a causal effect of $A$ on $Y$.

Let $\coprod$ again denote independence. Let $L$ denote the set of covariates necessary to ensure this conditional independence. We satisfy conditional exchangeability when:

$$
Y(a) \coprod A | L \quad \text{or equivalently} \quad A \coprod Y(a) | L
$$

Assuming conditional exchangeability is satisfied and the other assumptions required for consistent causal inference also hold, we may compute the average treatment effect (ATE) on the difference scale:

$$
\text{Average Treatment Effect} = \mathbb{E}[Y(1) | L] - \mathbb{E}[Y(0) | L]
$$


Note that in randomised controlled experiments, exchangeability is unconditional. Although adjustment by interacting the treatment with pre-treatment variables may improve efficiency and diminish threats to randomisation from chance inbalances in measured covariates, it is a confusion to think of such adjustment as "control." 

In "real-world" observational studies, where measured confounders are sufficient to ensure conditional exchangeability, we obtain obtain estimates for the average treatment effect by conditioning on the densities of measured confounders by treatment group. Where $A=a$ and $A = a^*$ are the treatment levels we seek to contrast:


$$
\widehat{\text{ATE}} =  \sum_l \big( \mathbb{E}[Y(a^*) \mid L] - \mathbb{E}[Y(a) \mid L] \big) \times Pr(L)
$$


Which gives by causal consistency gives us:
$$
\widehat{\text{ATE}} =  \sum_l \big( \mathbb{E}[Y \mid A = a^*, L] - \mathbb{E}[Y \mid A = a, L] \big) \times Pr(L)
$$ ^[note]


[note]: For continuous covariates $L$, we have: $$\widehat{\text{ATE}} =  \sum_l \big( \mathbb{E}[Y(a^*) \mid L] - \mathbb{E}[Y(a) \mid L] \big) \times Pr(L)$$ and $$\widehat{\text{ATE}} = \int \big( \mathbb{E}[Y\mid A = a^*, L] - \mathbb{E}[Y\mid A = a, L] \big) dP(L)$$



In the disciplines of cultural evolution, where experimental control is impractical and we are left with "real-world" data, obtaining consistent causal inferences hinge on the plausibility of satisfying this 'no unmeasured confounding' assumption. In **Appendix B**  I describe challenges for inferring causal effects from historical data. 

However these assumptions occur down stream from the primary functions of causal directed acyclic graphs, which we now ready to state: 

The primary function of a causal directed acyclic graph is to identify sources of bias that may lead to an association between an exposure and outcome in the absence of causation.  That is, causal directed acycic graphs visually encode features of a causal order necessary to evaluate the assumptions of conditional exchangeability -- also called 'no-unmeasured confounding', 'ignorability', and -- in the idiom of causal graphical models -- 'd-separation'.  Although directed acyclic graphs can also be useful for addressing broader threats and opportunities for causal inferences such as sample restriction, transportability, and measurement error bias, they are designed to evaluate the assumptions of conditional exchangeability, or equivalently of non-unmeasured confounding, ignorability, or of d-separation between the treatments and outcomes. 

Perhaps the most frequent error in the deployment of causal graphical models is using them to encode assumptions about the causal order pertaining to a study.  However, their function is merely to evaluate those features of the causal order relevant to evaluating whether balance in the variables that might affect both the treatment and the outcome can be ensured from measured covariates. 

<!-- Finally, it is important to underscore that without randomisation,
we cannot fully ensure the no-unmeasured confounding assumption that
enables us to recover the missing counterfactuals we require to
consistently estimate causal effects from data [@stuart2015;
@greifer2023]. Because we must nearly always assume unmeasured confounding, the workflows of causal data science must ultimately rely on sensitivity analyses to clarify how much unmeasured confounding would be required to compromise a study's findings [@vanderweele2019]. -->


### Summary of Part 1

Causal data science is not ordinary data science. In causal data science, the initial step involves formulating a precise causal question
that clearly identifies the exposure, outcome, and population of interest. 
We must then satisfy the three fundamental assumptions required for causal inference, which are implicit in the ideal of a randomised experiment: causal consistency: ensuring outcomes at a specific exposure level align with their counterfactual counterparts; positivity: the existence of a
non-zero probability for each exposure level across all covariate; conditional exchangeability: the absence of unmeasured confounding, or equivalently the 'ignorability' of treatment assignment, conditional on measured confounders. 


<!-- ## Preliminaries  -->
<!-- 
Causal diagrams, also called causal graph are graphical tools whose primary purpose is to enable investigators to evaluate identifications assumptions. Here, we will mostly focus on threats to identification arising from confounding biases.


**Confounding bias**: there is no confounding bias if:
  
  - the distribution of variables that might affect the outcome are imbalanced in the treatment groups to be compared
  
Causal graphs allow us to say:  
  - if there is no open back-door path between the treatment and the path between the treatment and outcome is unblocked, there is no confounding bais.

Before describing how causal diagrams work, we first define the meanings of their symbols. Note there is no single convention for creating causal graph, so it is important that we are clear when defining our meanings.


The concept of "confounding bias" helps to clarify what it is at stake when evaluating the *internal validity* of a study.  -->

{{< pagebreak >}}



## Part 2 Part 2: How Causal Directed Acyclic Graphs Clarify Identification

We introduce causal directed acyclic graphs by describing the meaning of our symbols.

### 2.1 Variable naming conventions
::: {#tbl-terminology}
```{=latex}
\terminologylocalconventionssimple
```
Variable naming conventions
:::

#### $X$ 

Denotes a random variable without reference to its role

#### $A$ 

Denotes the "treatment" or "exposure" - a random variable.

This is the variable for which we seek to understand the effect of intervening on it. It is the "cause." 

#### $A=a$ 

Denotes a fixed "treatment" or "exposure"

Random variable $A$ is set to level $A=a$.

#### $Y$ 

Denotes the outcome or response of an intervention.

It is the "effect."

#### $Y(a)$ 

Denotes the counterfactual or potential state of $Y$ in response to setting the level of the exposure to a specific level, $A=a$**

The outcome $Y$ as it would be observed when, perhaps contrary to fact, treatment $A$ is set to level $A=a$

There are different conventions for expressing a potential or counterfactual outcome, such as $Y^a$, $Y_a$. 

#### $L$ 

Denotes a measured confounder or set of confounders.

This set, if conditioned upon, insures that any differences between the potential outcomes under different levels of the treatment are the result of the treatment, and the not the result of a common cause of the treatement and the outcome.  Mathematically we write this independence: 

$Y(a)\coprod A \mid L$

<!-- 
Consider a scenario where happiness at time 0, $L$,  affects both the probability of getting married at time 1, $A$,  and one's happiness at time 2, $Y$. 

In this case, $L$ serves as a confounder because it influences both the treatment (marriage at time 1) and the outcome (happiness at time 2), potentially opening a back-door path that confounds the estimated effect of marriage on happiness.

To accurately estimate the causal effect of marriage on happiness, then, it is essential to control for $L$. With cross-sectional data, such control might be difficult.  -->

#### $U$ 

Denotes an unmeasured confounder, or confounders.

$U$ is variable or set of variables that may affect both the treatment and the outcome, that, even after conditioning on measured covariates, leads to association in the absence of causality: 

$Y(a) \cancel\coprod A \mid L \quad \text{[because of unmeasured } U]$

<!-- 
Suppose cultural upbringing affects both whether someone gets married and whether they are happy.  

If this variable is not measured, we cannot accurately estimate a causal effect of marriage on happiness. -->

#### $Z$ 

Denotes a modifier of the treatment effect

$Z$ that alters the magnitude or direction of the effect of a treatment $A$ on an outcome $Y$.

#### $M$ 

Denotes a mediator, a variable that transmits the effect of an exposure (or treatment) $A$ on an outcome $Y$.

#### $\bar{X}$ 


Denotes a sequence of variables, for example, a sequence of treatments. Example: 

Imagine we were interested in the causal effect of marriage and remarriage on well-being. In this case, there are two treatments $A_0$ and $A_1$ and four potential contrasts. For the scenario of marriage and remarriage affecting well-being, we denote the potential outcomes as $Y(a_0, a_1)$, where $a_0$ and $a_1$ represent the specific values taken by $A_0$ and $A_1$, respectively. Given two treatments, $A_0$ and $A_1$, there are four primary contrasts of interest correspond to the different combinations of these treatments. These contrasts allow us to compare the causal effects of being married versus not and of being remarried versus not on well-being. The potential outcomes under these conditions can be specified as follows:

1. $Y(0, 0)$: The potential outcome when there is no marriage.
2. $Y(0, 1)$: The potential outcome when there is marriage. 
2. $Y(1, 0)$: The potential outcome when there is divorce.
4. $Y(1, 1)$: The potential outcome from marriage prevalence.

Each of these outcomes allows for a specific contrast. Which do we want to contrast?  We can see the question about 'the causal effects of marriage on happiness' is ambiguous. We must stated the causal contrast we are interested in, given our substantive research interests.  This statment requires a sequences of exposures to be contrasted.

#### $\mathcal{R}$ 

Denotes a randomisation or a chance event.

As when treatment assignment is random.

#### $\mathcal{G}$ 

Denotes a graph, here, a causal directed acyclic graph.


Note that investigators use a variety of different symbols. As long as meanings are clear, the symbols we use are arbitrary.


### 2.2 Conventions for Causal Directed Acyclic Graphs

The conventions we use to describe components of our causal graphs are given in @tbl-general. 

::: {#tbl-general}
```{=latex}
\terminologygeneraldags
```
Nodes, Edges, Conditioning Conventions. 
:::


#### Node:

A node represents characteristics or features of units within a population on a causal diagram, termed a "variable." In causal graphs, we draw nodes with respect to the *target population*, which is the population for whom investigators seek causal inferences.

#### Edge without an Arrow:

An edge without an arrow indicates a path of association, without implying causality.

#### Arrow

An edge with an arrow represents a causal relationship from the node at the base of the arrow to the node at the tip of the arrow. For instance, in the absence of causation, we denote it as $A -- Y$, not $A \rightarrow Y$. We typically refrain from drawing an arrow from treatment to outcome in a causal directed acyclic graph (DAG) to avoid asserting a causal path from $A$ to $Y$. The main function of a causal DAG is to clarify causal identification.

Note that we should only draw a causal DAG for the sample population if it corresponds to the target population, as supported by Suzuki et al. (2020). As these causal paths are asserted, constructing causal diagrams requires expert judgement—bearing in mind that "with great power comes great responsibility."

#### Red Arrow

We denote a potential non-causal association between the treatment and outcome by colouring this arrow red. When applying d-separation rules to assess identification, we will disregard these arrows if confounding arises because the treatment and the outcome share a common cause, as in $\mathcal{G}$ where the confounding path runs from $A_1 \association L_0 \association Y_2$. The use of colour in arrows is specific to this review to highlight biases. Investigators may adopt other conventions, but the chosen convention should be clearly defined.

#### Dashed Arrow:

A dashed arrow denotes a true association between the treatment and outcome that becomes partially obscured when conditioning on a mediator. This specific convention, adopted here, highlights the causal direct effect, assuming its existence.

#### Dashed Red Arrow

A dashed red arrow represents over-conditioning bias from conditioning on a mediator.

#### Open Blue Arrow

We use an open blue arrow to indicate effect modification, which occurs when the levels of the effect of treatment vary within levels of a covariate. We do not assess the causal effect of the effect-modifier on the outcome, recognising that it may be incoherent to consider intervening on the effect-modifier.

#### Boxed Variable

Enclosing a variable in a box denotes a decision to control for, condition on, or adjust for that variable. This convention is widely used in causal data science.

#### Red-Boxed Variable

Colouring the box red when conditioning on a variable highlights the source of confounding bias.

#### Dashed Circle

A dashed circle explicitly denotes that no adjustment is made for a variable.

#### $\mathcal{R} \rightarrow A$:

This denotes randomisation into the treatment condition.

#### Sequential Order

Causal directed acyclic graphs are -- as the name implies -- acyclic.  No descendent node can cause an ancestor node. Directed edges or arrows define ancestral relations. Hence causal diagrams are, by default, sequentially ordered. 

Nevertheless, to make our causal graphs more readible we adopt the followign conventions:

1. The layout of a causal diagram is structured from left to right to reflect the assumed sequence of causality as it unfolds.
2. We often index our nodes using $X_t$ to indicate their relative timing and chronological order, where $t$ represents the time point or sequence in the timeline of events.
3. Where temporal order is uncertain or unknown, we use the notation $X_{\phi t}$ to propose a temporal order that is uncertain.

Typically, the timing of unmeasured confounders is not known, except that they occur before the treatments of interest; hence, we place confounders to the left of the treatments and outcomes they are assumed to affect, but without any time indexing.

Again, both spatial and temporal organisation in a causal directed acyclic graph are optional. Temporal order is implied the relationship of nodes and edges.  However, explicitly representing order in the layout of one's causal graph often makes it easier to evaluate, and the convention representing uncertainty is useful, particularly when the data do not ensure the relative timing of the occurance of the variable in a causal graph. 

### 2.3 Terminology for Statistical and Causal Independence

The bottom panel of @tbl-general illustrates the mathematical notationa we use to describe statistical and causal dependencies in the distributions of variables within a population.

#### Independence $\coprod$

We denote independence using the symbol $\coprod$. For example, $A \coprod Y(a)$ signifies that the treatment assignment $A$ is independent of the potential outcomes $Y(a)$. 

Note that it is the distributions of **counterfactual outcomes** that must be independent of the treatments. The distributions of observed outcomes will depend on treatments wherever the treatments are causally efficacious.

#### Dependence $\cancel{\coprod}$

We denote dependence using the symbol $\cancel{\coprod}$. For instance, $A \cancel{\coprod} Y(a)$ suggests that the treatment assignment $A$ is related to the potential outcomes $Y(a)$, which could introduce bias into causal estimates.

#### Conditioning $|$

The vertical line $|$ represents conditioning on a variable. We can express both conditional independence and conditional dependence:

- **Conditional Independence ($A \coprod Y(a) \mid L$):** indicates that once we account for a set of variables $L$, the treatment $A$ and the potential outcomes $Y(a)$ are independent.
- **Conditional Dependence ($A \cancel{\coprod} Y(a) \mid L$):** indicates that the treatment $A$ and the potential outcomes $Y(a)$ are not independent after conditioning on $L$, or perhaps because we have conditioned on $L$.


### 2.4 How Causal Directed Acyclic Graphs Relate Observations to Counterfactuals

#### Ancestral Relations in Directed Acyclic Graphs

We define the relation of "parent" and "child" on a directed acyclic graph as follows:

1. Node $A$ is a **parent** of node $B$ if there is a directed edge from $A$ to $B$, denoted $A \rightarrow B$.
2. Node $B$ is a **child** of node $A$ if there is a directed edge from $A$ to $B$, denoted $A \rightarrow B$.

It follows that a parent and child are **adjacent nodes** connected by a directed edge.

We denote the set of all parents of a node $B$ as $\text{Pa}(B)$.

In a directed acyclic graph, the directed edge $A \rightarrow B$ indicates a statistical dependency where $A$ may provide information about $B$. In a causal directed acyclic graph, the directed edge $A \rightarrow B$ is interpreted as a causal relationship such that $A$ is a direct cause of $B$.

We further define the relations of **ancestor** and **descendant** on a directed acyclic graph as follows:

1. Node $A$ is an **ancestor** of node $C$ if there exists a directed path from $A$ to $C$. Formally, $A$ is an ancestor of $C$ if there exists a sequence of adjacent nodes $(A, B_1, B_2, \ldots, B_k, C)$ such that $A \rightarrow B_1 \rightarrow B_2 \rightarrow \cdots \rightarrow B_k \rightarrow C$.
2. Node $C$ is a **descendant** of node $A$ if there exists a directed path from $A$ to $C$. Formally, $C$ is a descendant of $A$ if there exists a sequence of adjacent nodes $(A, B_1, B_2, \ldots, B_k, C)$ such that $A \rightarrow B_1 \rightarrow B_2 \rightarrow \cdots \rightarrow B_k \rightarrow C$.

It follows that a node can have multiple ancestors and descendants. 
<!-- 
We denote the set of all ancestors of a node $C$ as $\text{An}(C)$, and the set of all descendants of a node $A$ as $\text{De}(A)$.

 In a causal directed acyclic graph, we apply identifictation rules to evaluate the implications of structural assumptions encoded in a causal graph for consistently estimating the causal effects of pre-specified interventions on nodes.  -->

#### Markov Factorisation and The Local Markov Assumption

@pearl2009a p 52 asks us to imagine the following. Suppose we have a distribution $\mathcal{P}$ defined on n discrete variables, $X_1, X_2,  \dots, X_n$.  By the chain rule, the joint distribution for variables $X_1, X_2, \dots, X_n$ on graph can be decomposed into the product of $n$ conditional distributions such that we may obtain the following factorisation:

$$
\Pr(x_1, \dots, x_n) = \prod_{j=1}^n \Pr(x_j \mid x_1, \dots, x_{j-1})
$$


We translate nodes and edges on a graph into a set of conditional independences that a graph implies over statistical distributions. 
 
 
 According to **the local Markov assumption,** given its parents in a directed acyclic graph, a node is said to be independent of all its non-descendants. Under this assumption we obtain what Pearl calls Bayesian network factorisation, such that:

$$
\Pr(x_j \mid x_1, \dots, x_{j-1}) = \Pr(x_j \mid \text{pa}_j)
$$

This factorisation greatly simplifies the calculation of the joint distributions encoded in the directed acyclic graph (causal or non-causal) by reducing complex factorisations of the conditional distributions in $\mathcal{P}$ to simpler conditional distributions in the set $\text{PA}_j$, represented in the structural elements of a directed acyclic graph [@lauritzen1990; @pearl1988; @pearl1995; @pearl2009a].

#### Minimality Assumption

The Minimality assumption combines (a) the local Markov assumption with (b) the assumption that adjacent nodes on the graph are dependent. This is needed for causal directed acyclic graphs because the local Markov assumption permits that adjacent nodes may be independent [@neal2020introduction].

#### Causal Edges Assumption

The causal edges assumption states that every parent is a direct cause of its children. Given minimalism, the causal edges assumption allows us to read causal dependence in directed acyclic graphs. In Pearl's formalism, we use non-parametric structural equations to evaluate causal assumptions using statistical distributions (refer to Appendix E; @neal2020introduction).

#### Compatibility Assumption

The **compatibility assumption** ensures that the joint distribution of variables aligns with the conditional independencies implied by the causal graph. This assumption requires that the probabilistic model conforms to the graph's structural assumptions. Demonstrating compatibility directly from data is challenging, as it involves verifying that all conditional independencies specified by the causal directed acyclic graph (DAG) are present in the data. Therefore, we typically assume compatibility rather than empirically proving it [@pearl2009a].

#### Faithfulness

**Faithfulness** complements Markov Factorisation in causal diagrams. A causal diagram is considered faithful to a given set of data if all the conditional independencies present in the data are accurately depicted in the graph. Conversely, the graph is faithful if every dependency implied by the graph's structure can be observed in the data [@hernan2024WHATIF]. This concept ensures that the graphical representation of relationships between variables aligns with empirical evidence [@pearl2009a].

The distinction between **weak faithfulness** and **strong faithfulness** addresses the nature of observed independencies:

- **Weak faithfulness** allows for the possibility that some observed independencies might occur by coincidence, such as through a cancellation of effects among multiple causal paths.
- **Strong faithfulness**, on the other hand, assumes that all observed statistical relationships reflect the underlying causal structure directly, with no independencies arising purely by chance. This stronger assumption is often more pragmatic in real-world applications of causal inference, where the complexities of data can obscure underlying causal relationships.

The faithfulness assumption, whether weak or strong, is not directly testable from observed data. It is fundamentally a theoretical assumption about the relationship between the observed data and the underlying causal structure [@pearl2009a].

### 2.5 The d-Separation Criterion

**d-Separation**: In a causal diagram, a path is 'blocked' or 'd-separated' if a node along it interrupts causation. Two variables are d-separated if all paths connecting them are blocked, making them conditionally independent. Conversely, unblocked paths result in 'd-connected' variables, implying potential dependence [@pearl1995; @pearl2009a]. (Note that "d" stands for "directed".)

The rules of d-separation are as follows:

a. **Fork rule** ($A \leftarrow \boxed{B} \rightarrow C$): $A$ and $C$ are independent when conditioning on $B$ ($A \coprod C \mid B$) [@pearl1995].

b. **Chain rule** ($A \rightarrowNEW \boxed{B} \rightarrowNEW C$): Conditioning on $B$blocks the path between $A$and $C$ ($A \coprod C \mid B$) [@pearl1995].

c. **Collider rule** ($A \rightarrowred \boxed{B} \leftarrowred C$): $A$ and $C$ are independent until conditioning on $B$, which introduces dependence ($A \cancel{\coprod} C \mid B$) [@pearl1995].

According to these rules:

1. An open path (no variables conditioned on) is blocked only if two arrows point to the same node: $A \rightarrowred B \leftarrowred C$. The node of common effect is called a *collider*.
2. Conditioning on a collider does not block a path, such that $A \rightarrowred \boxed{B} \leftarrowred C$ may suggest an association of $A$ with $C$ in the absence of causation.
3. Conditioning on a descendant of a collider does not block a path, such that if $L \rightarrowred \boxed{B'}$, then $A \rightarrowred \boxed{B'} \leftarrowred C$ is open.
4. If a path does not contain a collider, any variable conditioned along the path is blocked, such that $A \rightarrowdotted \boxed{B} \rightarrowdotted C$ blocks the path from $A \rightarrowdotted C$ [@hernan2023, p. 78].

#### 2.6 The Backdoor Path Criterion and Backdoor Path Adjustment

To obtain an unbiased estimate for the causal effect of $A$ on $C$, we need to block all backdoor paths. We do this by conditioning on a set of covariates $B$that is sufficient to close all backdoor paths linking $A$and $C$. A path is effectively blocked by $B$if it includes at least one non-collider that is a member of $B$, or if it does not contain any collider or descendants of a collider.

@pearl2009a (p.173) defines this criterion more generally as follows: a set of variables $Z$ satisfies the backdoor criterion relative to variables $X_i$ and $X_j$ in a causal directed acyclic graph $\mathcal{G}$ if: 

1. No node in $Z$ is a descendant of $X_i$.
2. $Z$ blocks every path between $X_i$ and $X_j$ that includes an arrow pointing into $X_i$.

Furthermore, we say that if $X$ and $Y$ are two disjoint subsets of nodes in $\mathcal{G}$, then $Z$ meets the backdoor criterion relative to $(X, Y)$ if it meets the criteria for any pair $(X_i, X_j)$ where $X_i \in X$ and $X_j \in Y$ (@pear2009a, p 173).

The name "backdoor" refers to condition (2), which requires that only paths with arrows pointing at $X_i$ be blocked; these paths can be viewed as entering $X_i$ through the back door.

The backdoor criterion uses the rules of d-separation to identify and block all paths in a causal diagram that could introduce bias between a treatment (or exposure), $A$, and an outcome, $C$, in the absence of causation [@pearl2009a, p173]. 

The backdoor path criterion gives rise to the "backdoor path adjustment" theorem, such that if a set of variables $Z$ satisfies the backdoor criterion relative to $(X, Y)$ the causal effect of $X$ on $Y$ is identifiable given the formula:

$$ \Pr(y|\hat{x}) = \sum_z \Pr(y|x,z) \times \Pr(z))$$ 


Pearl's backdoor path adjustment theorem should look familiar.  Substituting our notation where $A$ is the treatment (a random variable) and $L$ denotes measured confounders sufficient to for ignorable treatment assigment (also random variables), we may compute causal contrasts using Pearl's backdoor path adjustment theorem using the standardisation formula:  

$$
\widehat{\text{ATE}} =  \sum_l \big( \mathbb{E}[Y \mid A = a^*, L] - \mathbb{E}[Y \mid A = a, L] \big) \times Pr(L)
$$ ^[note]



#### 2.7 Frontdoor Path Criterion

To obtain an unbiased estimate for the causal effect of $A$ on $C$ using the frontdoor criterion, we need to identify a set of variables $M$ that mediates the effect of $A$ on $C$. 

Pearl defines the frontdoor criterion more generally as follows: a set of variables $B$ satisfies the frontdoor criterion relative to variables $A$ and $C$ in a causal directed acyclic graph $\mathcal{G}$ if:

1. $B$ is affected by $A$.
2. $B$ affects $C$.
3. There are no backdoor paths from $A$ to $B$.
4. All backdoor paths from $B$ to $C$ are blocked by conditioning on $A$.

In other words, $B$ must be an intermediate variable that captures the entire causal effect of $A$ on $C$, with no confounding paths remaining between $A$ and $B$, and any confounding between $B$ and $C$ must be blocked by $A$.

The frontdoor criterion is less widely used compared to the backdoor criterion because it requires the identification of an appropriate mediator that fully captures the causal effect [@pearl2009a].  Here, we state the frontdoor path criterion for completeness. 


### 2.8 Comment on Pearl's Do-Calculus versus the Potential Outcomes Framework

 Here, we have developed counterfactual contrasts using the potential outcomes framework, Pearl develops counterfactual contrasts using operations on structural functionals, referred to as "do-calculus"  (Appendix E.) In practice, whether one uses the do-calculus (and the non-parametric structural equation models it relies on) or the potential outcomes framework to interpret causal inferences is typically irrelevant to identification results. However, there are theoretically interesting debates about edge cases.

In some cases, Pearl's non-parametric structural equation models permit the identification of contrasts that cannot be falsified under any experiment [@richardson2013]. Because advocates of non-parametric structual equation models treats causality as primitive, they are less concerned with the requirement for falsification [@pearl2009a, @diaz2021nonparametric, @Diaz2023; @rudolph2024mediation]. On the other hand, some advocates of the potential outcomes framework require falsifiability [@robins2010alternative; @richardson2023potential, @shpitser2016causal]. Conversely, there are edge cases where the potential outcomes framework achieves identification while the do-calculus does not [@richardson2013].

I have presented the potential outcomes framework because it is easier to interpret. Moreover, one does not need to be a verificationist to adopt it. For most practical purposes, the two frameworks are equivalent in terms of their utility for causal inference. Furthermore, readers should be aware that there are causal diagrams called "Single World Intervention Graphs" which enable investigators to represent conditional independences of potential outcomes on graphs [@richardson2013swigsprimer], which can be useful. 


 ### 2.9 The Five Elementary Structures of Causality Encoded In Causal Directed Acyclic Graphs


::: {#tbl-fiveelementary}

```{=latex}
\terminologydirectedgraph
```
Five elementary structures of causality used in causal directed acyclic graphs.
:::

@tbl-fiveelementary presents the five elementary structures of causality as represented in causal directed acyclic graphs:


Next consider that there are are five elemental structures from which all causal directed acyclic graphs may be composed. 


#### Causal Relations With Two Variables

1. **Causality Absent:** There is no causal effect between variables $A$ and $B$. They do not influence each other, denoted as $A \coprod B$, indicating they are statistically independent.
2. **Causality:** Variable $A$ causally affects variable $B$. This relationship suggests an association between them, denoted as $A \cancel{\coprod} B$, indicating they are statistically dependent.

#### Causal Relations with Three Variables

3. **Fork Relation:** Variable $A$ causally affects both $B$ and $C$. Variables $B$ and $C$ are conditionally independent given $A$, denoted as $B \coprod C \mid A$. This structure implies that knowing $A$ removes any association between $B$ and $C$ due to their common cause.
4. **Chain Relation:** A causal chain exists where $C$ is affected by $B$, which in turn is affected by $A$. Variables $A$ and $C$ are conditionally independent given $B$, denoted as $A \coprod C \mid B$. This indicates that $B$ mediates the effect of $A$ on $C$, and knowing $B$ breaks the association between $A$ and $C$.
5. **Collider Relation:** Variable $C$ is affected by both $A$ and $B$, which are independent. However, conditioning on $C$ induces an association between $A$ and $B$, denoted as $A \cancel{\coprod} B \mid C$. This structure is important because it suggests that $A$ and $B$, while initially independent, become associated when we account for their common effect $C$.

Understanding the basic relationships between two variables allows us to build upon these to create more complex relationships. These elementalary structures can be assembled in different combinations to clarify the causal relationships that are are presented in a causal directed acyclic graph. Such clarity is crucial for ensuring whether confounders may be balanced across treatment groups to be compared, conditional on measured co-variates, so that $Y(a) \coprod A \mid L$.
{{< pagebreak >}}


### 2.10 The Five Fundamental Rules of Confounding Control

@tbl-terminologyconfounders describe five elementary rules of confounding control:

::: {#tbl-terminologyconfounders}
```{=latex}
\terminologyelconfounders
```
Five elementary rules for confounding control.
:::

There are no shortcuts to reasoning about causality. Each causal question must be asked in the context of a specific scientific question, and each causal graph must be build under the best lights of domain expertise. However, the following five elementary rules for confounding control  are implied by the theorems that underpin casuald directed acyclic graphs. They may useful start for evaluating the prospects for causal identification across a broad range of settings.

1. **Ensure That Treatments Precedes Outcomes**: this rule is a logical consequence of our assumption that causality follows the arrow of time, and that a causal directed acyclic graph is faithful to this ordering. However, the assumption that treatments precede outcomes may be easily violated where investigators cannot ensure the relative timing of events from there data.  

Note that this assumption does raise concerns in settings where past outcomes may not affect future treatments.  Indeed, often an effective strategy for confounding control in such settings is to condition on past outcomes, and where relevant, one past treatments as well. For example, if we wish to identify the cuasal effect of $A_1$ on $Y_2$, if repeated-measures time series data are available, it may be useful to condition such that $\boxed{A_{-1}} \to \boxed{Y_0} \to A_1 ~~ Y_2$. Critically, the relations of variables must be arranged sequentially without cycles.  

Note further that to estimate a causal effect of $Y$ on $A$ we would focus on: $\boxed{Y_{-1}} \to \boxed{A_0} \to Y_1 ~~ A_2$. Departing from convention, here $Y$ denotes the treatment and $A$ denotes the outcome.  Graphs must be acyclic. Most processes in nature include feedback loops. There is no contradition as long as we represent these loops as sequential events.

2. **Condition on Common Causes or Their Proxies**: This rule applies to settings in which the treatment $A$ and the outcome $Y$ share common causes. By conditioning on these common causes, we block the open backdoor paths that could introduce bias into our causal estimates. Controlling for these common causes (or their proxies) helps to isolate the specific effect of $A$ on $Y$. Note that we do not draw a path from $A \to Y$ in this context because it represents an interventional distribution. In a causal directed acyclic graph, conditioning does not occur on interventional distributions. We do not box $A$ and $Y$. 

3. **Do Not Condition on a Mediator**:  this rule applies to settings in which the variable $L$ is a mediator of $A \to Y$. Recall Pearl's backdoor path criterion require that we do not condition on a decendant of the treatment. Here, conditioning on a $L$$ violates the backdoor path criterion, risking bias for a total causal effect estimate. If we are interested in total effect estimates, we must not condition on a mediator.  Note we draw the path from $A \to Y$ to underscore that this specific overconditioning threat occurs in in the presence of a true treatment effect. As we consider below, over-conditioning bias can operate in the absence of a true-treatment effect. This is important because condiitoning on a mediator might create associations in the absence of causation. In many settings, ensuring accuracy in the relative timing of events in our data will prevent the self-inflicted injury of conditioning on a common effect of the treatment. 

4. **Do Not Condition on a Collider**: this rule applies to settings in which we $L$ is a common effect of $A$ and $Y$. Conditioning on a collider may invoke a spurious association.  Again the backdoor path criterion require that we do not condition on a decendant of the treatment. We will not be tempted to condition on $L$ if we knew that it was an effect of $A$. In many settings, ensuring accuracy in the relative timing of events in our data will prevent the self-inflicted injury of conditioning on a common effect of the treatment and outcome.

5. **Proxy Rule: Conditioning on a Descendent Is Akin to Conditioning on Its Parent**: this rule applies to settings in which we $L’$ is an effect from another variable $L$.  The graph considers when $L’$ is downstream of a collider.  Here again, in many settings, ensuring accuracy in the relative timing of events in our data will prevent the self-inflicted injury of conditioning on a common effect of the treatment and outcome.

<!-- For example, suppose we condition on home ownership, which is an effect of wealth. Such conditioning will open up a non-causal path without causation because home ownership is a proxy for wealth.  Consider, if someone owns a house but is not married, they are more likely to be happy, for how else could they accumulate the wealth required for home ownership?  Likewise, if someone is unhappy and owns a house, we can infer that they are more likely to be married because how else would they be wealthy? Conditioning on a proxy for a collider here is akin to conditioning on the collider itself.   -->

<!-- However, we can also use the proxy rule to reduce bias. Return to the earlier example in which there is an unmeasured common cause of marriage and happiness, which we called "cultural upbringing"  Suppose we have not measured this variable but have measured proxies for this variable, such as country of birth, childhood religion, number of languages one speaks, and others.  By controlling for baseline values of these proxies, we can exert more control over unmeasured confounding. Even if bias is not eliminated, we should reduce bias wherever possible, which includes not introducing new biases, such as mediator bias, along the way.  Later in the course, we will teach you how to perform sensitivity analyses to verify the robustness of your results to unmeasured confounding.  Sensitivity analysis is critical because where the data are observational, we cannot entirely rule out unmeasured confounding.  -->

{{< pagebreak >}}


### Summary Part 2


We use causal directed acyclic graphs to represent and evalaute structural sources of bias. We do not use these causal graphs to represent the the entirity of the causal system in which we are interested, but rather only those features necessary to evaluate conditional exchangeabilty, or equivalently to evaluate d-separation.  Moreover, causal directed acyclic graphs should not be confused with the structural equation models employed in the statistical structural equation modelling traditions.  Although Pearl's formalism is built upon "Non-Parametric Structural Equation Models", the term "Structural Equation Model" is a false cognate. Causal graphs are structural models, not statistical models. We create causal graphs before we embark on statistical modelling. Their purpose is to clarify how to write statistical models by eludicating which variables we must include in our statistical models and equally important, which variables we must exclude from our statistical models to avoid invalidating our causal inferences.  All causal graph are grounded in our assumptions about the structures of causation. Although it is sometimes possible to use causal diagrams for causal discovery, their primary (and original) use is to evaluate the implications of assumptions. 

This distinction between structural and statistical models is fundamental because absent clearly defined causal contrasts and carefully evaluated assumptions about structural sources of bias, the statistical structural equation modelling tradition offers no guarantees that the coefficients investigators recover are interpretable. Misunderstanding this difference between structural and statistical models has led to considerable confusion across the human sciences [@vanderweele2015; @vanderweele2022; @vanderweele2022b; @bulbulia2022.]


## Part 3. How Causal Directed Acyclic Graphs Clarify The Importance of Timing of Events Recorded in Data


::: {#tbl-elementary-chronological-hyg}
```{=latex}
\terminologychronologicalhygeine
```
:::

As hinted at in the previous section, the five elementary rules of confounding control reveal the importance of ensuring accurate timing in the occurance of the variables whose structural features a causal directed acyclic graph encodes. We begin by consider seven examples of confounding problems resolved when such accuracy is ensured. 

The first seven case-studies illustrate the focus that causal directed acyclic graphs bring to fundmental imperative to ensure accurate timing in the chronology of events recorded in data.  These illustrations refer to causal graph in @tbl-elementary-chronological-hyg


### 3.1 Reverse Causation

### 3.2 Confounding by Common Cause

@tbl-elementary-chronological-hyg $\mathcal{G} 3.1$ illustrates confounding by common cause. Suppose there is a common cause, $L$, of the exposure, $A$, and outcome, $Y$. In this setting, $L$ may create a statistical association between $A$ and $Y$, implying causation in its absence. Most human scientists will be familiar of the threat to inference in this setting: a "third variable" leads to association without causation. 

Consider an example where smoking, $L$, is a common cause of both yellow fingers, $A$, and cancer, $Y$. Here, $A$ and $Y$ may show an association without causation. If investigators were to scrub the hands of smokers, this would not affect cancer rates.

@tbl-elementary-chronological-hyg $\mathcal{G} 3.2$ clarifies a response.  Condition on the common cause. Were investigators to condition on smoking, there would be no association between yellow fingers and cancer. 

### 3.3 Mediator Bias


@tbl-elementary-chronological-hyg $\mathcal{G} 3.1$ illustrates mediator bias. Conditioning on the effect of a treatment in this graph partially blocks the flow of information from treatment to outcome, biasing the total effect estimate.

Suppose investigators are interested in whether cultural 'beliefs in big Gods' $A$ affect social complexity $Y$. Suppose that 'economic trade', $L$ is both a common cause of the treatment and outcome. To address confounding by common cause we must condition on economic trade.  However, timing matters. If we condition on measurements that reflect economic trade after the emergence of beliefs in big Gods, we may bias our total effect estimate.


@tbl-elementary-chronological-hyg $\mathcal{G} 3.2$ clarifies a response. Ensure that measurements of economic trade are obtained for cultural histories before big-Gods arise.  Do not condition on post-treatment instances of economic trade.


### 3.4 Collider Bias

@tbl-elementary-chronological-hyg $\mathcal{G} 4.1$ illustrates collider bias. Imagine a randomised experiment investigating the effects of different settings on individuals' self-rated health. In this study, participants are assigned to either civic settings (e.g., community centres) or religious settings (e.g., places of worship). The exposure of interest, $A$, is the type of setting, and the outcome, $Y$, is self-rated health. Suppose there is no effect of setting on self-rated health. However, suppose both setting and rated health independently influence a third variable: cooperativeness. Specifically, imagine religious settings encourage cooperative behaviour, and at the same time, individuals with better self-rated health are more likely to engage cooperatively. Now suppose the investigators decide to condition on cooperativeness, which in reality is the common effect of an $A$, and the outcome $Y$. Their rational might be to study the effects of setting on health among those who are more cooperative, or perhaps to 'control for' cooperation in the health effects of religious setting. By introducing such 'control' the investigators would inadvertently introduce collider bias, because the control variable is a common effect of the exposure and the outcome.

@tbl-elementary-chronological-hyg $\mathcal{G} 4.2$ clarifies a response. If the worry is that cooperativeness is a confounder, ensure that cooperativeness is measured before the intiation of exposures to religious settings. Note that in experimental settings investigators do not have this worry, assuming randomisation succeeds and samples are large.  However, conditioning on a variable that is associated with the outcome may improve estimation efficiency, and safegaurds against random imbalance owing to sampling variability.



### 3.5 Collider Proxy Bias

@tbl-elementary-chronological-hyg $\mathcal{G} 5.1$ illustrates bias from conditioning on the proxy of a collider. Consider again the scenario described in $\sec 3.4$, however in place of controlling for cooperativeness investigators control for charitable donations, a proxy for cooperativeness. Here, because the control variable is a decendant of a collider, conditioning on the proxy is akin to conditioning on the the collider.


@tbl-elementary-chronological-hyg $\mathcal{G} 5.2$ clarifies a response. Do not condition on the effect of treatment. 

### 3.6 Post-Treatment Collider Stratification Bias

@tbl-elementary-chronological-hyg $\mathcal{G} 6.1$ illustrates post-treatment collider stratification bias. Consider a gain an experiment investigating the effect of religious service on self-rated health.  Suppose we measure "religiosity" after the experiment, although with other demographic data. Suppose further that religious setting affects religiosity, as does an unmeasured confounder, such as child-hood deprivation.  Suppose that childhood deprivation affects self-reported health.  Although our experiment ensured randomisation of the treatment, and therefore ensured there no-unmeasured common cause of the treatment and outcome, conditioning on the post-treatment variable "religiosity" opens a back-door path from the treatment to the outcome. This path is $A_0 \association L_1 \association U \association Y_2$. We introduced confounding into our randomised experiment.



@tbl-elementary-chronological-hyg $\mathcal{G} 6.2$ clarifies a response. Do not condition on a variable that the treatment may effect. (refer to @cole2010 for theoretical examples; refer to @montgomery2018 for evidence of widespread prevalence of post-treatment adjustment in political science experiments). 


### 3.7 Conditioning on Past Treatments and Past Outcomes to Control for Unmeasured Confounders


@tbl-elementary-chronological-hyg $\mathcal{G} 7.1$ illustrates the threat of unmeasured confounding. In "real world" studies this threat is ubiquitous. 

@tbl-elementary-chronological-hyg $\mathcal{G} 7.2$ clarifies a response. With at least three repeated measurements, investigators may greatly reduce unmeasured confoudning by controlling for past-measurements of the treatment as well as past-measurements of the outcome. With such control, any unmeasured confounder would need to be orthogonal to its effects at baseline (refer to @vanderweele2020). Moreover, controlling for past treatments allows investigators to estimate an **incident exposure**, effect over **a prevelence expsoure effect.**   The prevalence exposure effect describes the effect of current or ongoing exposures on outcomes. This effects risks pointing to erroneous conclusions. The incident exposure targets initiation into treatment, which is typically the effect we obtain from experiments to obtain
the incident exposure effect, we generally require that events in the data can be accurately classified into at least three relative time
intervals (refer to @hernán2016; @danaei2012; @vanderweele2020; @bulbulia2022)




## 3.8. Summary Part 3

## Part 4  How Causal Directed Acyclic Graphs Clarify The Insufficiency of the Timing of Events Recorded in Data

### 4.1. M-bias

### 4.2  M-bias with where the pre-treatment collider is a confounder

### 4.3  Opportunities for post-treatment conditioning for confounder control

### 4.4  Residual Confounding After Conditioning on Past Treatments and Past Outcomes

### 4.5  Intermediary Confounding in Causal Mediation

### 4.6. Treatment Confounder Feedback in Dynamic Treatment Regimes

### 4.7  Collider Stratification Bias in Dynamic Treatment Regimes. 

### 4.8  Summary Part 4.



::: {#tbl-chronology-notenough}
```{=latex}
\terminologychronologicalhygeineNOTENOUGH
```
Common confounding scenarios in which chronology is not enough.
:::







<!-- {{< pagebreak >}}
## Structures of Measurement Error Biases


::: {#tbl-measurement-error}
```{=latex}
\terminologymeasurementerror
```
Measurement-error bias
:::
{{< pagebreak >}}


## Structures of Selection-Restriction Biases from Attrition

::: {#tbl-censoring-bias}
```{=latex}
\terminologycensoring
```
Censoring (attrition) bias
:::

{{< pagebreak >}}
## Confounding in Randomised Controlled Experiments {#section-confounding-experiments}

::: {#tbl-experiments}
```{=latex}
\terminologyelconfoundersexperiments
```
Common confounding scenarios in experiments
:::

{{< pagebreak >}}

## Single World Intervention Graphs
::: {#tbl-experiments}
```{=latex}
\swigtable
```
Common confounding scenarios in experiments
:::


{{< pagebreak >}}
## The Clarity of Single World Intervention Graph: Case Study

::: {#tbl-experiments}
```{=latex}
\pearltable
```
Common confounding scenarios in experiments
:::

{{< pagebreak >}} -->



## Part 4. Practical Advice


### 4.1 How to Create Causal Diagrams to Address Causal Identification Problems {sec-how-to-create-causal-diagrams}


The **identification problem** centres on whether we can derive the true causal effect of a treatment ($A$) on an outcome ($Y$) from observed data.  Addressing the identification problem has two core components:

#### First, evaluate bias in the absence of a treatment effect

Before attributing any statistical association to causality, we must eliminate non-causal sources of correlation. We do this by:

* Identifying factors that influence both treatment ($A$) and outcome ($Y$).
* Developing adjustment strategies to control for confounders.
* Blocking backdoor paths that create indirect, non-causal links between $A$ and $Y$. By adjusting for confounders, we aim to achieve d-separation between $A$ and $Y$.

#### Second, evaluate bias in the presence of a treatment effect

After addressing potential confounders, we must ensure any remaining association between $A$ and $Y$ reflects a true causal relationship. We address **over-conditioning bias** by:

* Avoiding mediator bias 
* Avoiding collider bias
* Verifying that any association between $A$ and $Y$ after in unbiased after all adjustments.

Thus, causal inference demands a delicate balance: identify and control for confounders but avoid introducing new biases. Here is how investigators should construct their causal diagrams.

#### Step 1. Clarify the research question evaluated by the diagram

Before attempting to draw any causal diagram, state the problem your diagram addresses and the population to whom the problem applies.  Causal identification strategies may vary by question. For example, the confounding control strategy for evaluating the path $L\to Y$ will differ from that of assessing the path $A\to Y$.  For this reason, reporting coefficients other than the association between $A \to Y$ is typically ill-advised; see @westreich2013; @mcelreath2020; @bulbulia2023.

#### Step 2. Include all common causes of the exposure and outcome

Incorporate all common causes (confounders) of both the exposure and the outcome into your diagram. This includes both measured and unmeasured variables. Where possible, aggregate functionally similar common causes into a single variable notation (e.g., $L_0$ for demographic variables).


#### Step 3. Include all ancestors of measured confounders linked with the treatment, the outcome, or both

Include any ancestors (precursors) of measured confounders that are associated with either the treatment, the outcome, or both. This step is crucial for addressing hidden biases arising from unmeasured confounding. Simplify the diagram by grouping similar variables. 

#### Step 4. Explicitly state assumptions about relative timing

Explicitly annotate the temporal sequence of events using subscripts (e.g., $L_0$, $A_1$, $Y_2$). It is imperative that causal diagrams are acyclic.

#### Step 5. Arrange temporal order of causality visually

Arrange your diagram to reflect the temporal progression of causality, either left-to-right or top-to-bottom. This arrangement enhances the comprehensibility of causal relations and is vital for dissecting identification issues as discussed in [**Part 3**](#sec-part3), establishing temporal ordering is necessary for evaluating identification problems. 


#### Step 6. Box variables are those variables that we adjust for to control confounding 

Mark variables for adjustment (e.g., confounders) with boxes.

#### Step 7. Represent paths structurally, not parametrically

Focus on whether paths exist, not their functional form (linear, non-linear, etc.). Parametric descriptions are not relevant for bias evaluation in a causal diagram. (For an explanation of causal interaction and diagrams, see: @bulbulia2023.)

#### Step 8. Minimise paths to those necessary for the identification problem

Reduce clutter; only include paths critical for a specific question (e.g., backdoor paths, mediators).

#### Step 9. Consider Potential Unmeasured Confounders

Leverage domain expertise to clarify potential unmeasured confounders and represent them in your diagram. This proactive step aids in anticipating and addressing *all* possible sources of confounding bias.

#### **Step 10. State Graphical Conventions**

Establish and explain the graphical conventions used in your diagram (e.g., using red to highlight open backdoor paths). Consistency in symbol use enhances interpretability, while explicit descriptions improve accessibility and understanding.

 Practical Guide For Constructing Causal Diagrams and Reporting Results When Causal Structure is Unclear {#section-part4}

### 2.2. Reporting Causal Directed Acyclic Graphs in Cross-sectional designs

In environmental psychology, researchers often grapple with whether causal inferences can be drawn from cross-sectional data, especially when longitudinal data are unavailable. The challenge is common to cross-sectional designs.  However, it is important to appreciate that even longitudinal studies require careful assumption management. We next discuss how causal diagrams can guide inference in both data types, with examples relevant to environmental psychologists.

#### 1. Graphically encode causal assumptions

Causal inference turns on assumptions. Although cross-sectional analyses typically demand much stronger assumptions owing to the snapshot nature of data, these assumptions, when transparently articulated, do not permanently bar causal analysis. By stating different assumptions and modelling the data following these assumptions, we might find that certain causal conclusions are robust to these differences. Where the implications of different assumptions disagree, we can better determine the forms of data collection that would be required to settle such differences.  Below we consider an example where assumptions point to different conclusions, revealing the benefits of collecting time-series data to assess whether a variable is a confounder or a mediator. 


#### 2. Time-invariant confounders

In cross-sectional studies, some confounders are inherently stable over time, such as ethnicity, year and place of birth, and biological gender. For environmental psychologists examining the relationship between access to natural environments and psychological well-being, these stable confounders can be adjusted for without concern for introducing bias from mediators or colliders. For example, conditioning on one’s year of birth can help isolate recent urban development’s effect on mental health, independent of generational differences in attitudes toward green spaces.

#### 3. Stable confounders

While not immutable, other confounders are less likely to be influenced by the treatment. Variables such as sexual orientation, educational attainment, and often income level fall into this category. For instance, the effect of exposure to polluted environments on cognitive outcomes can be analysed by conditioning on education level, assuming that recent exposure to pollution is unlikely to change someone’s educational history retroactively.

#### 4. Timing and reverse causation

The sequence of treatment and outcome is crucial. Sometimes, the temporal order is clear, reducing concerns about reverse causation. Mortality is a definitive outcome where the timing issue is unambiguous. If researching the effects of air quality on mortality, the causal direction (poor air quality leading to higher mortality rates) is straightforward. However, consider the relationship between socio-economic status and health outcomes; the direction of causality is complex because socioeconomic factors can influence health (through access to resources), and poor health can affect socio-economic status (through reduced earning capacity).

#### 5. Create causal diagrams

Given the complexity of environmental influences on psychological outcomes, it’s prudent to construct multiple causal diagrams to cover various hypothetical scenarios. For example, when studying the effect of community green space on stress reduction, one diagram might assume the direct benefits of green space on stress. At the same time, another might include potential mediators like physical activity. By analysing and reporting findings based on multiple diagrams, researchers can examine the robustness of their conclusions across different theoretical frameworks and sets of assumptions.

@tbl-cs describes ambiguous confounding control arising from cross-sectional data. Suppose again we are interested in the causal effect of access to greenspace, denoted by $A$ on "happiness," denoted by $Y$.   We are uncertain whether exercise, denoted by $L$, is a common cause of $A$ and $Y$ and thus a confounder or whether exercise is a mediator along the path from $A$ to $Y$. That is: (1) those who exercise might seek access to green space, and (2) exercise might increase happiness. Alternatively, the availability of green space might encourage physical activity, which could subsequently affect happiness. Causal diagrams can disentangle these relationships by explicitly representing potential paths, thereby guiding appropriate strategies for confounding control selection. We recommend using multiple causal diagrams to investigate the consequences of different plausible structural assumptions. 

**Assumption 1: Exercise is a common cause of $A$ and $Y$**, this scenario is presented in @tbl-cs row 1. Here, our strategy for confounding control is to estimate the effect of $A$ on $Y$ conditioning on $L$. 

**Assumption 2: Exercise is a mediator of $A$ and $Y$**, this scenario is presented in @tbl-cs row 2. Here, our strategy for confounding control is simply estimating the effect of $A$ on $Y$ without including $L$ (assuming there are no other common causes of the treatment and outcome). 

::: {#tbl-cs}

```{=latex}
\examplecrosssection
```
Example of reporting multiple causal graphs in a cross-sectional design
:::


We can simulate data and run separate regressions to clarify how answers may differ, reflecting the different conditioning strategies embedded in the different assumptions. The following simulation generates data from a process in which exercise is a mediator (Scenario 2). (See Appendix C)



```{r}
#| label: simulation_cross_sectional-appendix
#| tbl-cap: "Code for a simulation of a data generating process in which the effect of exercise (L) fully mediates the effect of greenspace (A) on happiness (Y)."
#| out-width: 80%
#| echo: false
# load libraries
library(gtsummary) # gtsummary: nice tables
library(kableExtra) #  tables in latex/markdown
library(clarify) # simulate ATE

# simulation seed
set.seed(123) #  reproducibility

# define the parameters 
n = 1000 # Number of observations
p = 0.5  # Probability of A = 1 (access to greenspace)
alpha = 0 # Intercept for L (exercise)
beta = 2  # Effect of A on L 
gamma = 1 # Intercept for Y 
delta = 1.5 # Effect of L on Y
sigma_L = 1 # Standard deviation of L
sigma_Y = 1.5 # Standard deviation of Y

# simulate the data: fully mediated effect 
A = rbinom(n, 1, p) # binary exposure variable
L = alpha + beta*A + rnorm(n, 0, sigma_L) # continuous mediator
Y = gamma + delta*L + rnorm(n, 0, sigma_Y) # continuous outcome

# make the data frame
data = data.frame(A = A, L = L, Y = Y)

# fit regression in which L is assumed to be a mediator
fit_1 <- lm( Y ~ A + L, data = data)

# fit regression in which L is assumed to be a mediator
fit_2 <- lm( Y ~ A, data = data)

# create gtsummary tables for each regression model
table1 <- tbl_regression(fit_1)
table2 <- tbl_regression(fit_2)

# merge the tables for comparison
table_comparison <- tbl_merge(
  list(table1, table2),
  tab_spanner = c("Model: Exercise assumed confounder", 
                  "Model: Exercise assumed to be a mediator")
)
# make latex table
markdown_table_0 <- as_kable_extra(table_comparison, 
                                   format = "latex", 
                                   booktabs = TRUE)
# print                                   
markdown_table_0
```


This table presents the conditional treatment effect estimates.  We present code for obtaining marginal treatment effects in [Appendix C](#appendix-c) 

```{r}
#| label: ate_simulation_cross_sectional
#| fig-cap: ""
#| out-width: 100%
#| echo: false

# use `clarify` package to obtain ATE
library(clarify)
# simulate fit 1 ATE
set.seed(123)
sim_coefs_fit_1 <- sim(fit_1)
sim_coefs_fit_2 <- sim(fit_2)

# marginal risk difference ATE, simulation-based: model 1 (L is a confounder)
sim_est_fit_1 <-
  sim_ame(
    sim_coefs_fit_1,
    var = "A",
    subset = A == 1,
    contrast = "RD",
    verbose = FALSE
  )

# marginal risk difference ATE, simulation-based: model 2 (L is a mediator)
sim_est_fit_2 <-
  sim_ame(
    sim_coefs_fit_2,
    var = "A",
    subset = A == 1,
    contrast = "RD",
    verbose = FALSE
  )
# obtain summaries
summary_sim_est_fit_1 <- summary(sim_est_fit_1, null = c(`RD` = 0))
summary_sim_est_fit_2 <- summary(sim_est_fit_2, null = c(`RD` = 0))

# get coefficients for reporting
# ate for fit 1, with 95% CI
ATE_fit_1 <- glue::glue(
  "ATE =
                        {round(summary_sim_est_fit_1[3, 1], 2)},
                        CI = [{round(summary_sim_est_fit_1[3, 2], 2)},
                        {round(summary_sim_est_fit_1[3, 3], 2)}]"
)
# ate for fit 2, with 95% CI
ATE_fit_2 <-
  glue::glue(
    "ATE = {round(summary_sim_est_fit_2[3, 1], 2)},
                        CI = [{round(summary_sim_est_fit_2[3, 2], 2)},
                        {round(summary_sim_est_fit_2[3, 3], 2)}]"
  )
```



On the assumptions outlined in @tbl-cs row 1, in which we *assert* that exercise is a confounder, the average treatment effect of access to green space on happiness is `r ATE_fit_2`.

On the assumptions outlined in @tbl-cs row 2, in which we *assert* that exercise is a mediator, the average treatment effect of access to green space on happiness is `r ATE_fit_1`. 

Note that although the mediator $L$ is "highly statistically significant", including it in the model is a mistake. We obtain a negative effect estimate for the causal effect of green space access on happiness.

With only cross-sectional data, we must infer the results are inconclusive. Such understanding, although not the definitive answer we sought, is progress. The result tells us we should not be overly confident with our analysis (whatever p-values we recover!), and it clarifies that longitudinal data are needed. 

These findings illustrate the role that assumptions about the relative timing of exercise as a confounder or as a mediator play. 

### 4.1 Recommendations for Conducting and Reporting Causal Analyses with Cross-Sectional Data

When analysing and reporting analyses with cross-sectional data, researchers face the challenge of making causal inferences without the benefit of temporal information. 

The following recommendations aim to guide researchers in navigating these challenges effectively:

**Warning**: before proceeding with cross-sectional analysis, examine whether panel data are available. Longitudinal data can provide crucial temporal information that aids in establishing causality, offering a more robust framework for causal inference. If longitudinal data are unavailable, the recommendations above become even more critical for using cross-sectional data best.

#### 1. **Draw multiple causal diagrams**

Draw various causal diagrams to represent different theoretical assumptions about the relationships and timing of variables relevant to an identification problem. This approach comprehensively examines possible causal pathways, clarifying variables' roles as confounders, mediators, or colliders. For example, in studying the effect of urban green spaces on mental health, consider diagrams that account for both direct effects and pathways involving mediators like physical activity or social interaction.

#### 2. **Perform and report analyses for each assumption**

Conduct and transparently report separate analyses for each scenario your causal diagrams depict. This practice ensures that your study is theoretically grounded for each model. Presenting results from each analytical approach and the underlying assumptions and statistical methods promotes a balanced interpretation of findings. Although this practice may be unfamiliar to some editors and reviewers, it is crucial to address the inherent challenges of cross-sectional analysis by expanding the scope of investigation beyond a single hypothesis.

#### 3. **Interpret findings with attention to ambiguities**

Interpret results carefully, highlighting any ambiguities or inconsistencies across analyses. Discuss how varying assumptions about structural relationships and the timing of events can lead to divergent conclusions. For instance, exploring the theoretical and empirical implications of access to green spaces appears to positively affect mental health when considering exercise as a mediator but a negative effect when considered a confounder.

#### 4. **Report divergent findings**

Approach conclusions with caution, especially when findings suggest differing practical implications. Acknowledge the limitations of cross-sectional data in establishing causality and the potential for alternative explanations.

#### 5. **Identify avenues for future research**

Target future research that could clarify ambiguities. Consider the design of longitudinal studies or experiments capable of clarifying these ambiguities.

#### 6. **Supplement observational data with simulated data** 

Leverage data simulation to understand the complexities of causal inference. Simulating data based on various theoretical models allows researchers to examine the effect of different assumptions on their findings. This method tests analytical strategies under controlled conditions, assessing the robustness of conclusions against assumption violations or unobserved confounders.

#### 7. **Conduct sensitivity analyses to assess robustness**

implement sensitivity analyses to determine how dependent conclusions are on specific assumptions or parameters within your causal model. Use data simulation as a tool for these analyses, evaluating the sensitivity of results to various theoretical and methodological choices.


Cross-sectional data are limiting; however, by appropriately bounding uncertainties in your causal inferences, you may use them to advance understanding. May your clarity and caution serve as an example for others.


### 4.2 Recommendations for Reporting Causal Directed Acyclic Graphs in Longitudinal Designs

Causation occurs in time. Longitudinal designs offer a substantial advantage over cross-sectional designs for causal inference because sequential measurements allow us to capture causation and quantify its magnitude. We typically do not need to assert timing as in cross-sectional data settings. Because we know when variables have been measured, we can reduce ambiguity about the directionality of causal relationships. For instance, tracking changes in "happiness" following changes in access to green spaces over time can more definitively suggest causation than cross-sectional snapshots.


Despite this advantage, longitudinal researchers still face assumptions regarding the absence of unmeasured confounders or the stability of measured confounders over time. These assumptions must be explicitly stated.  As with cross-sectional designs, wherever assumptions differ, researchers should draw different causal diagrams that reflect these assumptions and subsequently conduct and report separate analyses. 


In this section, we simulate a dataset to demonstrate the benefits of incorporating both baseline exposure and baseline outcomes into analysing the effect of access to open green spaces on happiness. This approach allows us to control for initial levels of exposure and outcomes, offering a clearer understanding of the causal relationship. [Appendix D](#appendix-d-simulation-of-different-confounding-control-strategies) provides the code. [Appendix E](#appendix-e-non-parametric-estimation-of-average-treatment-effects-using-causal-forests) provides an example of a non-parametric estimator for the causal effect.  As mentioned before, by conditioning on baseline levels of access to green spaces and baseline mental health, researchers can more accurately estimate the *incident effect* of changes in green space access on changes in mental health. @tbl-lg offers an example of how we may use multiple causal diagrams to clarify the problem and our confounding control strategy. 


::: {#tbl-lg}

```{=latex}
\examplelongitudinal
```
This table is adapted from [@bulbulia2023]
:::


Our analysis assessed the average treatment effect (ATE) of access to green spaces on happiness across three distinct models: uncontrolled, standard controlled, and interaction controlled. These models were constructed using a hypothetical cohort of 10,000 individuals, incorporating baseline exposure to green spaces ($A_0$), baseline happiness ($Y_0$), baseline confounders ($L_0$), and an unmeasured confounder ($U$). The detailed simulation process and model construction are given in [Appendix D](#appendix-simulate-longitudinal-ate).



```{r}
#| label: codelg
#| echo: false
#| eval: true
# load libaries 
library(kableExtra)
if(!require(kableExtra)){install.packages("kableExtra")} # causal forest
if(!require(gtsummary)){install.packages("gtsummary")} # causal forest
if(!require(grf)){install.packages("grf")} # causal forest

# r_texmf()eproducibility
set.seed(123) 

# set number of observations
n <- 10000 

# baseline covariates
U <- rnorm(n) # Unmeasured confounder
A_0 <- rbinom(n, 1, prob = plogis(U)) # Baseline exposure
Y_0 <- rnorm(n, mean = U, sd = 1) # Baseline outcome
L_0 <- rnorm(n, mean = U, sd = 1) # Baseline confounders

# coefficients for treatment assignment
beta_A0 = 0.25
beta_Y0 = 0.3
beta_L0 = 0.2
beta_U = 0.1

# simulate treatment assignment
A_1 <- rbinom(n, 1, prob = plogis(-0.5 + 
                                    beta_A0 * A_0 +
                                    beta_Y0 * Y_0 + 
                                    beta_L0 * L_0 + 
                                    beta_U * U))

# coefficients for continuous outcome
delta_A1 = 0.3
delta_Y0 = 0.9
delta_A0 = 0.1
delta_L0 = 0.3
theta_A0Y0L0 = 0.5 # Interaction effect between A_1 and L_0
delta_U = 0.05

# simulate continuous outcome, including interaction
Y_2 <- rnorm(n,
             mean = 0 +
               delta_A1 * A_1 + 
               delta_Y0 * Y_0 + 
               delta_A0 * A_0 + 
               delta_L0 * L_0 + 
               theta_A0Y0L0 * Y_0 * 
               A_0 * L_0 + 
               delta_U * U,
             sd = .5)

# assemble data frame
data <- data.frame(Y_2, A_0, A_1, L_0, Y_0, U)

# model: no control
fit_no_control <- lm(Y_2 ~ A_1, data = data)

# model: standard covariate control
fit_standard <- lm(Y_2 ~ A_1 + L_0, data = data)

# model: interaction
fit_interaction  <- lm(Y_2 ~ A_1 + L_0 + A_0 + Y_0 + A_0:L_0:Y_0, data = data)

# create gtsummary tables for each regression model
tbl_fit_no_control<- tbl_regression(fit_no_control)  
tbl_fit_standard <- tbl_regression(fit_standard)
tbl_fit_interaction <- tbl_regression(fit_interaction)

# get only the treatment variable
tbl_list_modified <- lapply(list(
  tbl_fit_no_control,
  tbl_fit_standard,
  tbl_fit_interaction),
function(tbl) {
  tbl %>%
    modify_table_body(~ .x %>% dplyr::filter(variable == "A_1"))
})

# merge tables
table_comparison <- tbl_merge(
  tbls = tbl_list_modified,
  tab_spanner = c(
    "No Control",
    "Standard",
    "Interaction")
) |>
  modify_table_styling(
    column = c(p.value_1, p.value_2, p.value_3),
    hide = TRUE
  )

#create latex table for publication
markdown_table <-
  as_kable_extra(table_comparison, format = "latex", booktabs = TRUE) |>
  kable_styling(latex_options = "scale_down")
  
# print it
#markdown_table
```
```{r}
#| label: ate-sim-long
#| tbl-cap: "Code for calculating the average treatment effect."
#| echo: false
#| eval: true

# use `clarify` package to obtain ATE
if(!require(clarify)){install.packages("clarify")} # clarify package

# simulate fit 1 ATE
set.seed(123)
sim_coefs_fit_no_control<- sim(fit_no_control)  
sim_coefs_fit_std <- sim(fit_standard)
sim_coefs_fit_int <- sim(fit_interaction)

# marginal risk difference ATE, no controls
sim_est_fit_no_control <-
  sim_ame(
    sim_coefs_fit_no_control,
    var = "A_1",
    subset = A_1 == 1,
    contrast = "RD",
    verbose = FALSE
  )
# marginal risk difference ATE, simulation-based: model 1 (L is a confounder)
sim_est_fit_std <-
  sim_ame(
    sim_coefs_fit_std,
    var = "A_1",
    subset = A_1 == 1,
    contrast = "RD",
    verbose = FALSE
  )
# marginal risk difference ATE, simulation-based: model 2 (L is a mediator)
sim_est_fit_int <-
  sim_ame(
    sim_coefs_fit_int,
    var = "A_1",
    subset = A_1 == 1,
    contrast = "RD",
    verbose = FALSE
  )
# obtain summaries
summary_sim_coefs_fit_no_control <-
  summary(sim_est_fit_no_control, null = c(`RD` = 0))
summary_sim_est_fit_std <-
  summary(sim_est_fit_std, null = c(`RD` = 0))
summary_sim_est_fit_int <-
  summary(sim_est_fit_int, null = c(`RD` = 0))

# get coefficients for reporting
# ate for fit 1, with 95% CI
ATE_fit_no_control  <- glue::glue(
  "ATE = {round(summary_sim_coefs_fit_no_control[3, 1], 2)}, 
  CI = [{round(summary_sim_coefs_fit_no_control[3, 2], 2)},
  {round(summary_sim_coefs_fit_no_control[3, 3], 2)}]"
)
# ate for fit 2, with 95% CI
ATE_fit_std <- glue::glue(
  "ATE = {round(summary_sim_est_fit_std[3, 1], 2)}, 
  CI = [{round(summary_sim_est_fit_std[3, 2], 2)},
  {round(summary_sim_est_fit_std[3, 3], 2)}]"
)
# ate for fit 3, with 95% CI
ATE_fit_int <-
  glue::glue(
    "ATE = {round(summary_sim_est_fit_int[3, 1], 2)},
    CI = [{round(summary_sim_est_fit_int[3, 2], 2)},
    {round(summary_sim_est_fit_int[3, 3], 2)}]"
  )
# coefs
# ATE_fit_no_control
# ATE_fit_std
# ATE_fit_int
```

The ATE estimates from these models provide critical insights into the effects of green space exposure on individual happiness while accounting for various confounding factors. The model without control variables estimated `r ATE_fit_no_control`, significantly overestimating the treatment effect. Incorporating standard covariate control reduced this estimate to `r ATE_fit_std`, aligning more closely with the expected effect but still overestimating. Most notably, the model that included interactions among baseline exposure, outcome, and confounders yielded `r ATE_fit_int`, approximating the true effect of 0.3. This finding underscores the importance of including baseline values of the exposure and outcome wherever these data are available. 

### 4.3 Recommendations for Conducting and Reporting Causal Analyses with Longitudinal Data

Longitudinal data offer strong advantages for causal inference by enabling researchers to establish the relative timing of confounders, treatments, and outcomes. The temporal sequence of events is crucial for establishing causality because causality occurs in time. The following recommendations aim to guide researchers in leveraging longitudinal data effectively to conduct and report causal analyses:

#### 1. Draw multiple causal diagrams
   - **Identification problem diagram**: begin by constructing a causal diagram that outlines your initial assumptions about the relationships among variables, identifying potential confounders and mediators. This diagram should illustrate the complexity of the identification problem.
   - **Solution diagram**: next, create a separate causal diagram that proposes solutions to the identified problems. This may involve highlighting variables for conditioning to isolate the causal effect of interest or suggesting novel pathways for investigation. Having distinct diagrams for the problem and its proposed solutions clarifies your study's analytic strategy and theoretical underpinning.

@tbl-lg provides an example of a table with multiple causal diagrams clarifying potential sources of confounding threats and reports strategies for addressing them. 

#### 2. Attempt longitudinal designs with at least three waves of data

Incorporating data from at least three intervals considerably enhances your ability to infer causal relationships. This approach allows for the examination of temporal precedence and lagged effects. For example, by adjusting for physical activity measured before the treatment, we can ensure that physical activity does not result from a new initiation to green spaces, which we establish by measuring green space access at baseline. Establishing chronological order in the temporal sequence of events allows us to avoid confounding problems 1-4 in @tbl-04. 

#### 3. Calculate Average Treatment Effects for a clearly specified target population

Estimating the average treatment effect (ATE) across the entire study population provides a comprehensive measure of the intervention's effects. This step is crucial for understanding the treatment's overall effect and generalising findings to broader populations.

#### 4. Where causality is unclear, report results for multiple causal graphs

Given that the true causal structure may be complex and partially unknown, analysing and reporting results under each plausible causal diagram is prudent. This practice acknowledges the uncertainty inherent in causal modelling and demonstrates the robustness of findings across different theoretical frameworks.

#### 5. Conduct sensitivity analyses


Sensitivity analyses are essential for assessing the robustness of your findings to various assumptions within the causal model. These analyses can include simulations, as illustrated in Appendices C and D, to examine bias arising of unmeasured confounding, model misspecification, and alternative causal pathways on the study conclusions. Sensitivity analyses help to identify the conditions under which the findings hold, enhancing the credibility of the causal inferences. (For more about addressing missing data, see: [@bulbulia2024PRACTICAL].) 

#### 6. Address missing data at baseline and study attrition

Longitudinal studies often need help with missing data and attrition, which can introduce bias and affect the validity of causal inferences. Implement and report strategies for handling missing data, such as multiple imputation or sensitivity analyses that assess the bias arising from missing responses at the study's conclusion. (For more about addressing missing data, see: [@bulbulia2024PRACTICAL]). 


By following these recommendations, you will more effectively navigate the inherent limitations of observational longitudinal data, improving the quality of your causal inferences.



## 4 Summary

Although powerful aides, causal directed acyclic graphs may encourage false confidence wherever causal questions are ill-defined, the structures of the world are uncertain, data-quality are poor, statistical estimators are inadequate, or statistical models are misspecified.

### On the priority of assumptions. 

You might wonder, "If not from the data, where do our assumptions about causality come from?" This question will come up repeatedly throughout the course. The short answer is that our assumptions are based on existing knowledge. This reliance on current knowledge might seem counterintuitive for buiding scientific knowledge-— shouldn't we use data to build knowledge, not the other way around? Yes, but it is not that straightforward. Data often hold the answers we're looking for but can be ambiguous. When the causal structure is unclear, it is important to sketch out different causal diagrams, explore their implications, and, if necessary, conduct separate analyses based on these diagrams.

Otto Neurath, an Austrian philosopher and a member of the Vienna Circle, famously used the metaphor of a ship that must be rebuilt at sea to describe the process of scientific theory and knowledge development. 

> Duhem has shown ... that every statement about any happening is saturated with hypotheses of all sorts and that these in the end are derived from our whole world-view. We are like sailors who on the open sea must reconstruct their ship but are never able to start afresh from the bottom. Where a beam is taken away a new one must at once be put there, and for this the rest of the ship is used as support. In this way, by using the old beams and driftwood, the ship can be shaped entirely anew, but only by gradual reconstruction. [@neurath1973, p.199]

This quotation emphasises the iterative process that accumulates scientific knowledge;  new insights are cast from the foundation of existing knowledge. Causal diagrams are at home in Neurath's boat. The tradition of science that believes that knowledge develops from the results of statistical tests applied to data should be resisted. The data alone typically do not contain the answers we seek.





{{< pagebreak >}}



## Appendix A: Glossary


::: {#tbl-experiments}
```{=latex}
\glossaryTerms
```
Glossary
:::


## Appendix B: 

## Examples of common causal questions

::: {#tbl-common-interests}

```{=latex}
\terminologycommoncausalinterests
```
Common causal questions
:::



## Effect Modification 


::: {#tbl-common-interests}

```{=latex}
\terminologyeffectmodification
```
representing effect modification
:::



::: {#tbl-common-interests}

```{=latex}
\terminologyeffectmodificationtypes
```
Common causal questions
:::

{{< pagebreak >}}
## Appedix C:

## Time-varying Confounding:  Causal Mediation

::: {#tbl-mediation}
```{=latex}
\mediationfull
```
Anatomy of bias in mediation analysis: statistical SEM fails.
:::

{{< pagebreak >}}
## Appedix D

### Time-varying Confounding: Treatment Confounder Feedback

::: {#fig-timevarying-amplification}
```{=latex}
\feedbackA
```
Treatment-confounder feedback: statistical SEM fails.
:::

{{< pagebreak >}}
### Time-varying Confounding in the Absence of Treatment-confounder feedback 
::: {#fig-timevarying-nofeedback}
```{=latex}
\feedbackB
```
Anatomy of bias in treatment-confounder feedback
:::
{{< pagebreak >}}



## Appendix E {.appendixE}

Pearl's Do-Calculus and Structural Equation Models


{{< pagebreak >}}



<!-- **Internal Validity**: we say internal validity is compromised if the association between the treatment and outcome in a study does not consistently reflect causality in the sample population as defined at baseline. -->

<!-- **External validity**: we say external validity is compromised if the association between the treatment and outcome in a study does not consistently reflect causality in the target population as defined at baseline -->


<!-- The following failure modes threaten human science research: -->

<!--   - **Failures to Ask a Question**:  -->
<!--     - We have not stated the quantities we want to estimate. In causal inference, these are counter-factual quantities. -->
<!--     - We have not stated the population for whom knowledge is meant to generalise -->

<!--   - **Failures to Answer our Question**: -->
<!--     **Failures of Internal Validity**: the associations we obtain from the application of models to data do not reflect causality. -->
<!--           **Poor Estimators**:  -->
<!--           **Poor Data** -->
<!--           **Poor Models** -->

<!--     **Failures of External Validity**: our findings do not generalise from the *sample population* to the *target population*.  We fail when our results do not generalise as we think. -->

<!--   -  **Other Failures of Interpretation**: we made a paper airplane, and imagined we could fly it to the moon.  -->

