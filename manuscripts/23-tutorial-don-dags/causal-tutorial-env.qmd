---
title: "Causal Inference in Environmental Psychology: Through The Looking Glass of Counterfactual Data Science"
abstract: |
   Quantifying causal effects from observational data presents substantial challenges for environmental psychologists because the correlations obtained from observational settings are misleading causal guides. However, recent progress in methods for causal inference brings hope for transforming observational data into causal understanding. To clarify the assumptions under which we may obtain causal understanding from observational data, we provide an accessible introduction to the potential outcomes framework of causal inference. To help researchers better identify causation, we explain within this framework of assumptions how to use causal Directed Acyclic Graphs (DAGs). We conclude with practical guidelines for data collection and modelling, reviewing "tips" and "common pitfalls."  This chapter will interest environmental psychologists, and indeed any human scientist, who always wanted to learn about causal inference but never thought they could. 
authors: 
  - name: Donald W Hine
    orcid: 0000-0002-3905-7026
    email: donald.hine@canterbury.ac.nz
    affiliation: 
      name: University of Canterbury, School of Psychology, Speech and Hearing
      city: Wellington
      country: New Zealand
      url: www.wgtn.ac.nz/cacr
  - name: Joseph A. Bulbulia
    orcid: 0000-0002-5861-2056
    email: joseph.bulbulia@vuw.ac.nz
    affiliation: 
      name: Victoria University of Wellington, New Zealand, School of Psychology, Centre for Applied Cross-Cultural Research
      department: Psychology/Centre for Applied Cross-Cultural Research
      city: Wellington
      country: New Zealand
      url: www.wgtn.ac.nz/cacr
execute:
  warning: false
  eval: true
  echo: false
  include: true
keywords:
  - DAGS
  - Causal Inference
  - Confounding
  - History
  - Psychology
  - Panel
format:
  pdf:
    sanitize: true
    keep-tex: true
    link-citations: true
    colorlinks: true
    documentclass: article
    classoption: [singlecolumn]
    lof: false
    lot: false
    geometry: "top=30mm, left=20mm, heightrounded"
    include-in-header:
       - text: |
           \usepackage{cancel}
date: last-modified
csl: camb-a.csl
editor: 
  markdown: 
    wrap: sentence
---

```{r}
#| label: load-libraries
#| echo: false
#| include: false
#| eval: false

#   html:
#    html-math-method: katex

# Include in YAML for Latex
# sanitize: true
# keep-tex: true
# include-in-header:
#       - text: |
#           \usepackage{cancel}


# for making graphs
library("tinytex")
library(extrafont)
loadfonts(device = "all")


# libraries for jb (when internet is not accessible)
# read libraries
source("/Users/joseph/GIT/templates/functions/libs2.R")

# read functions
source("/Users/joseph/GIT/templates/functions/funs.R")

# read data/ set to path in your computer
pull_path <-
  fs::path_expand(
    "/Users/joseph/v-project\ Dropbox/data/current/nzavs_13_arrow"
  )

# for saving models. # set path fo your computer
push_mods <-
  fs::path_expand(
    "/Users/joseph/v-project\ Dropbox/data/nzvs_mods/00drafts/23-causal-dags"
  )


# keywords: potential outcomes, DAGs, causal inference, evolution, religion, measurement, tutorial
# 10.31234/osf.io/b23k7

#20012 words
# 75 refs
# 32 figs
```

## Introduction

Correlation does not imply causation.
However, quantifying a causal effect requires drawing inferences from correlations present in the data.
It can be no other way because statistical models produce measures of association, i.e., correlations.
And to quantify effects we must apply statistical models.

In experimental settings, quantitative causal inferences from associational models is made possible through randomisation, controlled interventions, and chronologically ordered data collection.

By contrast, in observational settings, the data researchers collect are not the products of randomisation and control.
Moreover, observational researchers are often limited to cross-sectional data, which lack temporal order.
In observational settings, then, credible causal inference from correlations can be, at best, indirect.
The problems of inferring causation in observational settings are widely known.
However, many observational researchers persist in using hedging causal language to reporting their results, offering cautious policy recommendations couched in words such as "might" and "suggests" [@bulbulia2022] However, because causal association can easily run in the opposite direction of manifest associations, every intention to report a observational correlations as suggesting causation is more safely presumed guilty unless demonstrated otherwise.

Are there any settings in which we might we obtain valid causal inferences from observational data?
If so, by which pathways might we pursue such inferences?

Environmental psychologists have strong motivations to ask these questions.
Among the most compelling is an interest in understanding the probable effects of interventions on environmental attitudes and behaviours, which is essential for offering effective policy advice.
Moreover, although randomised experiments can sometimes afford causal understanding, experimental studies are typically too expensive to perform at scale.
Designing and implementing interventions and then tracking their effects over long periods are typically infeasible, and often decisions cannot wait.
Experimental control often comes at the price of ecological validity, limiting applications.
For these reasons, the bandwidth of psychologically interesting and practically relevant questions that experiments may address is limited.
By contrast, observational data is abundant.
If we could obtain causal insights from observational data, the state of knowledge in environmental psychology would be greatly accelerated and the field would be better equipped to offer advice.

Considerable progress in the methods for estimating causation from observational data during the past twenty years offer reasons for hope [@vanderweele2015].
Although most of this progress has occurred outside of psychology, psychological scientists are growing increasingly interested in such methods [@rohrer2018; @mcelreath2020].
Efforts to unite methods for causal inference with environmental psychology, then, would appear both promising and timely.

We believe that rate of progress in the uptake of causal inference in environmental psychology (and in psychological science more broadly) will turn on how rapidly researchers understand the theoretical basis on which methods for causal inference rely.
The theoretical basis of causal effect estimation is vital because to obtain causal effect estimates from observational data relies on assumptions.
Put differently, before researchers apply statistical methods to data, they must devote considerable attention to planning and design than is their custom.
Whether the assumptions required for causal inference are credibly met will typically require the input of experts familiar with the questions at hand.
In some cases the data allow reasoned guesses about whether assumptions have been satisfied.
However such cases are the exception are rare.
That causal inference relies on untestable assumptions invites many risks.
On the one hand, if we persist in demanding verification from data researchers obtained from conventional thresholds they risk missing causal inferences that are supported by reasonable assumptions.
On the other hand, researchers might be tempted to admit implausible assumptions without warrant.
This latter risk is particularly worrying in those social sciences where publication biases favour results.
Too few journals are interested in studies that report uncertainty, or that describe why results cannot be obtained from the data to hand.
We believe that to accelerate progress in environmental psychology, and in the human sciences more generally, the theoretical foundations of causal inference must be accessible.
The assumptions that underpin causal inference must be explained in a way that builds clear intuitions about the underlying problem each assumption addresses.
Researchers must understand how these intuitions flow on to the tasks of data collection and data organisation.
For causal inference to bring about the scientific transformation, each practical step from assumption to data collection must also be grounded in solid intuitions.
Because the theoretical foundations of causal inference remain unfamilar and opaque to most psychological scientists, including those with strong backgrounds in statistical modelling, we believe that a gentle introduction focussed on developing the necessary intuitions and rigour habits may be useful.

Here, then, we present an introduction of the "potential outcomes" framework of causal inference, also known as the "counterfactual" framework for causal inference [@hernan2023]).

**Part 1** introduces the *the three fundamental assumptions* of causal inference.
Here develop causal intuitions by anchoring them in the framework of randomised experiments, which will be familiar to most readers of this chapter.
We shall discover that the building intuitions for causal inference from an understanding of experiments greatly demystify the assumptions on which causal inference relies and offers clear directives for data collection.
Surprisingly, perhaps, this discussion will bring new theoretical understand of experiments that may be helpful to improving their designs.

**Part 2** introduces Directed Acyclic Graphs (DAGs) -- or causal diagrams -- as powerful tools for investigating causal assumptions in observational settings, or in randomised experimental setting with repeated measures.
We shall discover that the functionality of causal diagrams rests on three rules, and that these three rules in turn rest on a robust system of mathematical proofs.
The basis of causal diagrams in mathematical proves should inspire confidence.
The relative ease with which causal diagrams can be constructed in the absence of mathematical formalism makes causal diagrams accessible.
Yet we will learn that causal diagrams only make sense in relation to the framework of assumptions described in Part 1.
Lacking a firm grasp of these assumptions, the application of causal diagrams to causal questions is perilous.
We offer strategies for constructing causal diagrams that will help reserach use them safely and effectively, both in research design and modelling.

**Part 3** Briefly reviews actionable directives for data collection and modelling within environmental psychology and related fields.
This advice will be useful human scientists who seek quantitative understanding about the probable effects of interventions on environmental attitudes and behaviours.

## Part 1. An introduction to the potential outcomes framework of causal inference.

The content provided is both accurate and well-articulated, with a few minor exceptions.
The below edits clarify some of the language and fix minor errors:

## Part 1: An Introduction to the Potential Outcomes Framework of Causal Inference

### Step 1: Understand that we assess the relationships of cause and effect by contrasting the outcomes of interventions

Suppose we are interested in well-being.
Given this interest, we might be curious about understanding the factors that influence it.
One such factor could be the accessibility of urban green spaces \[CITE\].
That is, living closer to abundant green spaces may enhance well-being compared to living in an environment devoid of such abundance.
How might we obtain a quantitative causal understanding?

First, we define a 'cause'.
In a setting of causal inference, a cause is equivalent to an "intervention" or "exposure" or "treatment."

Second, we define an "effect." In a setting of causal inference, an effect is the outcome of the intervention.
To quantitatively understand the effect of a cause or intervention, we state a contrast of outcomes under two interventions at some scale of measurement.
For example, the effect of abundant green spaces for individual $i$ may be expressed as:

$$
\text{Well-being effect of abundant greenspace}_{i} = \text{Well-being of abundant green space}_i - \text{Well-being of no abundant green space}_i
$$

We could also express this contrast on a ratio scale:

$$
\text{Well-being effect of abundant greenspace}_i = \frac{\text{Well-being of abundant green space}_i}{\text{Well-being of no abundant green space}_i}
$$

#### **Step 1.1: Quantifying a causal effect demands a contrast of interventions**

The concept of causal effect requires at least two potential interventions.
Comparing outcomes under these different interventions provides insight into causation.
For instance, contrasting an intervention against its absence always yields zero contrast in effects, represented mathematically as:

$$
\text{Effect of abundant greenspace} = \text{Effect of abundant green space} - \text{Effect of abundant green space} = 0
$$

The contrast of potential outcomes under different interventions offers evidence for causation when the contrast does not equal zero:

$$
\text{Causal effect of Do X} = \text{Causal effect of Do  X} - \text{Causal Effect of Do Not X} \neq 0
$$

Depending on the residual quantity, we can determine whether the effect is positive, negative, or null.

### Step 2: Understand that causation occurs in time

Causes are related to effects by time.
We may say, "each effect is borne of a cause." That is, at the scale of the the universe that people occupy, effects cannot precede their.
Although intuitive, this concept is often overlooked in psychological science.
For instance, causation requires that we assume:

$$
\text{cause} \to \text{effect}
$$

However, when we work with cross-sectional data, we cannot generally rule out:

$$
\text{labelled effect} \to \text{labelled cause}
$$

Therefore, we should be wary of cross-sectional data for causal inference.

### Step 3: Understand that causal inference is requires contrasting "potential" (or "counterfactual") outcomes that are never fully observed.

#### **Proof by contradiction that individual causal effects are not observed.**

Assume that individual causal effects can be fully observed.
This implies that for an individual, we can observe both the effect of taking a decision (exposure to abundant green space) and not taking the decision (no living with abundant green space) simultaneously.
However, by definition, an individual can only exist in one state at a time --- either one has green space access or they do not -- we cannot have both.

This contradicts our initial assumption, leading us to conclude that individual causal effects can never be fully observed.

Another proof is as follows:

#### **Direct proof individual causal effects are not observed.**

Consider a scenario where the causal effect for an individual is understood as the difference in outcomes under two distinct scenarios --- one where a decision is enacted (for example, enjoying green space) and one where it is not (devoid of green space).

However, for any individual in a particular moment, only one of these circumstances can occur ---- the individual either embraces green space or they do not.
We can only observe the outcome corresponding to the actual decision made, but we cannot observe the outcome for the counterfactual scenario (the decision not enacted).

In the language of causal inference, individual causal effects encounter the "fundamental problem of causal inference" [@rubin1976; @holland1986].
That is, individual causal effects are necessarily only partially observed.

#### **Step 3.1: The implication: causal inference requires solving a special and peculiar missing data problem**

If we are to quantify causal effects, our challenge is to bridge the gap between the observable world and the inherently unobservable world.
Causal inference is a missing data problem (@westreich2015; @edwards2015)

### Step 4: Understand that the problem of missing counterfactual is easier to solve when the target of inference is the average (or marginal) causal effect obtained from an experiment

The average causal effect represents the difference in average outcomes of a population under two distinct treatment conditions.
In formal terms, the average causal effect of a treatment (A) on an outcome (Y) can be defined as follows:

$$
\text{Average Causal Effect} = \mathbb{E}[Y(A = 1)] - \mathbb{E}[Y(A = 0)]
$$

where $\mathbb{E}[Y(A = a)]$ denotes the expected value of the outcome under treatment condition $A = a$.
Here, $A = 1$ indicates the presence of the treatment, while $A = 0$ represents its absence.

#### Step 4.1 Understand that the problem of missing counterfactual outcomes remains for average treatment effects.

Thatt the problem of missing counterfactuals remains for average treatment effects becomes evident when we consider a scenario involving two contrasting states: the presence of treatment ($A = 1$) and the absence of treatment ($A = 0$).

If an individual is subjected to treatment, we observe the outcome $Y_{\text{observed}_i}|A_i = 1$, but miss the counterfactual outcome $Y(A_i = 0)$, i.e., the outcome if the individual had not received the treatment.

Similarly, if the treatment is not administered, we observe $Y_{\text{observed}_i}|A_i = 0$, but miss the counterfactual outcome $Y(A_i = 1)$.

Thus, for each individual, only one of the potential outcomes is observed, and the other one is missing.
The challenge is that we cannot compute the individual causal effect, $Y(A_i = 1) - Y(A_i = 0)$, because we only observe one of the two terms for each individual.

To compute the average causal effect over a population, we should ideally sum these individual causal effects and divide by the population size:

$$
\text{Average Causal Effect} = \frac{1}{N}\sum_{i=1}^{N} [Y(A_i = 1) - Y(A_i = 0)]
$$

However, as we only observe one term of the subtracted pair for each individual, we cannot calculate this sum directly.
We again encounter the fundamental problem in causal inference when calculate average causal effects: the problem of individual-level missing counterfactuals persists "under the hood" of each intervention that we seek to compare.

#### Step 4.2 Understand how experiments satisfy assumptions need to impute missing responses that are needed to obtain average treatment effects

Randomised experiments are designed to impute missing counterfactuals that are necessary to calculate average treatment effects by random treatment assignment.
To build intuitions that enable understanding for how experiments recover missing counterfactuls, consider their defining features.

#### **Step 4.3. Understand the principle of randomisation**

In a random experiment, each participant has an equal chance of being in either the treatment or the control group.
This concept can be formulated:

$$
\text{Randomisation} = \text{Equal Probability of Assignment to Groups}
$$

##### **Step 4.5. Define experiment with reference measurement of interventions**

An experiment is an event in which an experimenter (or group of experimenters) applies different treatments to another group by virtue of randomised assignment.
Thus, an experiment consists is constituted as follows:

$$
\text{Experiment} = \text{Controlled Application of Interventions + Measurement of Outcome}
$$

##### **Step 4.6. Understand that experiments align potential outcomes with observed outcomes by satisfying a causal consistency assumption**

Causal consistency is the assumption that the potential outcome under an intervention level is equivalent to the observed outcome when that intervention is applied.

In the context of an experiment, observed outcomes may be credibly mapped one-to-one with potential outcomes.

Recall that $Y(a)$ denotes the potential outcome under treatment level $A = a$.
Let $\delta_{ate}$} denote the average causal effect on the difference scale.
The target of causal inference for the average causal effect of a treatment is thus

$$
\delta_{ate} = E[Y(1)] - E[Y(0)]
$$

This represents the contrast in average outcome under treatment and average outcome without treatment.

##### **Step 4.7. Understand that experiments satisfy a causal consistency assumption, thus recovering half of the potential outcomes needed for estimating the ATE**

In an experiment, when a treatment ($A = 1$) is applied, we assume the observed outcome aligns with the potential outcome under treatment:

$$
(Y_{\text{observed}_i}|A_i = 1) = Y_i(1)
$$

And similarly, when no treatment ($A = 0$) is administered, the observed outcome corresponds to the potential outcome without treatment:

$$
(Y_{\text{observed}_i}|A_i = 0) = Y_i(0)
$$

In other words, if an individual is assigned to the treatment group ($A_i = 1$), then the observed outcome corresponds to the potential outcome under treatment ($Y_i(a_i = 1)$).
Similarly, for those in the control group ($A_i = 0$), the observed outcomes correspond to the potential outcomes under no treatment ($Y_i(a_i = 0)$).

Thus, causal consistency allows us to infer the potential outcomes corresponding to the actual treatment each individual receives.

#### **Step 4.8. Understand that experiments satisfy a causal consistency assumption, thus recovering half of the potential outcomes needed for estimating the ATE**\*

Exchangeability is a statistical concept implying that, in expectation, the distribution of potential outcomes is the same under different treatment conditions, regardless of the treatment assignment.
This is to say, if we were able to have God-like access to all the potential outcomes, we would not expect systematic differences between those assigned to treatment and those assigned to control.

This assumption is typically ensured in experimental designs by **by randomisation**.
Random assignment to treatment ensures that all observed and unobserved characteristics are, on average, evenly distributed between treatment and control groups.
Thus, any difference in the observed outcomes between these groups can be attributed to the treatment effect, rather than to systematic differences in the groups.

Thus, under the assumption of exchangeability, the observed outcomes in the control group can be used to estimate the unobserved potential outcomes in the treatment group and vice versa.
This allows us to impute the missing half of the potential outcomes required for ATE estimation.

#### **Step 4.8. Understand that experiments satisfy a positivity assumption, thus ensuring that the causal contrasts that we compute are not impossible.**

Positivity is a vital assumption for causal inference in observational settings.
However in experiments it is so obviously satisfied that the assumption is easly overlooked.
The assumption states that every unit has a non-zero probability of receiving each level of treatment.
Because experimentalists actively intervene in the world, positivity is ensured.
Positivity esures we always have data to estimate the potential outcomes.
As such, we can compute the average causal effect.
However in observational settings, it is precisely control that is lacking.
The data that we need to assess causal effects may fail if (1) **weak positivity** is violated: the relevant observations have not been collected.
For example if moving either to abundant greenspace or away from abundant greenspace is exceedly rare, the data might be insufficient to compute average causal contrast; (2) **strong positivity** is violated: the intervention is impossible: e.g. data are collected in country were all green spaces have been destroyed.

### Step 5: Build intuitions for how causal assumptions may be translated from experimental to observational settings

The leap from controlled experiments to observational research typically poses challenges because outside of experiments the conditions that satisfy consistency, exchangeability, and positivity are not automatically satisfied.

In experiments, we have:

$$
\delta_{ate} = E[Y | do(A = 1)] - E[Y | do(A = 0)]
$$

Where $$E[Y | do(A = 1)]$$ is the expected outcome if we apply the treatment to the entire population, and $$E[Y | do(A = 0)]$$ is the expected outcome if we withhold the treatment from the entire population.
However, in observational settings, we simply have the observed association:

$$
OA = E[Y | A = 1] - E[Y | A = 0]
$$

Next we consider the problems in equating $0A$ with $\delta_{ate}$ by offering any interpretation of raw correlations in observational data (including cautious suggestions of causality.)

#### **Step 5.1. Understand how causal consistency may fail in observational settings**

The application of treatments in an observational setting may not be consistent.
Consider the case of moving closer to abundant green spaces.
Here, what constitutes "moving closer to abundant green spaces" is rarely the same intervention across the population that moves.
The intervention might differ in the following ways:

a.  Diversity in green spaces: green spaces may vary widely in terms of biodiversity and aesthetic appeal, hence leading to varying treatments, which may affect outcomes.

b.  Availability of amenities: green spaces equipped with walking paths, benches, recreational, and especially cafes, create differences in the treatments, which may affect outcomes.

c.  Variation in proximity: How close an individual is to a green space can vary, influencing access and frequency of use, again the treatments compared do not resemble those of controlled experiments.

d.  Size and type of green space: the outcomes can vary depending on whether the green space is a park, forest, or community garden.

When we try to define the "treatment" of abundant access to green spaces it is clear that nearly every attempt to do so will fall short of a hypothetical experiment in which a researchers intervene to consistently control the experience.

#### **Step 5.2. Understand how exchangeability may fail in observational settings**

In an observational study, it is easy to imagine that individuals in different treatment groups are systematically different.
For example, people moving closer to green spaces may differ on several attributes, including:

a.  Socioeconomic status: Individuals from different economic strata may perceive and utilise green spaces differently.

b.  Age: Younger individuals may have different outcomes from green spaces compared to older individuals.

c.  Mental health status: Individuals with better mental health might be more inclined to move closer to green spaces.

d.  Lifestyle preferences: Those with outdoor lifestyle preferences might choose to live near green spaces, but their lifestyle, not the green space, could be causing the effect.

e.  Personal values: People who value sustainability may perceive greater benefits from living near green spaces, rendering the comparison groups different in ways that affect outcomes.

f.  Social connections: Individuals with strong community ties may perceive more benefits from green spaces, and the distribution of such people might vary between conditions.

Again there are many ways in which groups that are not randomly assigned to the conditions we wish to compare might differ in ways that affect observed well-being independly of access to abundant urban greenspaces.

#### **Step 5.3. Understand how positivity may fail in observational settings**

Positivity could fail for the reasons we discussed, either because the exposure is too rare (violating weak positivity in the data) or because the exposure could not occur (violating strong positivity).
Again, lacking experimental control, positivity cannot be assumed without considering contextual details.
Notably, satisfaction of (weak) positivity is the one fundamental assumption of causal inference that can be assessed by observing the distribution of interventions in the data.
For example, if the intervention is too rare to evaluate, or is missing within certain strata of our comparison groups, we say that weak positivity is violated.
And we should stop our study there.

### Step 6. Build intution for how the fundamental assumptions for causal inference may be satisfied in observational settings

#### **Step 6.1 Understand how we may use the theory of causal inference under multiple versions of treatment to address causal consistency in observational settings**.

In observational research, there are typically multiple versions of treatment.
The theory of causal inference under multiple versions of treatment proves we can consistently estimate causal effects where the different versions of treatment are conditionally independent of the outcomes [@vanderweele2009, @vanderweele2009; @vanderweele2013; @vanderweele2018] (See Appendix 1 for additional discussion of the proof and its implications.)

Let $\coprod$ denote independence.
Where there are $K$ different versions of treatment $A$ and no confounding for $K$'s effect on $Y$ given measured confounders $L$ such that

$$
Y(k) \coprod K | L
$$

Then it can be proved that causal consistency follows.According to the theory of causal inference under multiple versions of treatment, the measured variable $A$ functions as a "coarsened indicator" for estimating the causal effect of the multiple versions of treatment $K$ on $Y(k)$ [@vanderweele2009; @vanderweele2013; @vanderweele2018].

In the context of green spaces, $A$ might represent the general action of moving closer to any green space and $K$ represents the different versions of this treatment.
For instance, $K$ could denote moving closer to different types of green spaces such as parks, forests, community gardens, or green spaces with varying amenities and features.

Here, the conditional independence implies that, given measured confounders $L$ (e.g. socioeconomic status, age, personal values), the type of green space one moves closer to ($K$) is independent of the outcomes $Y(k)$ (e.g. mental well-being under the $K$ conditions).
In other words, the version of green space one chooses to live near does not affect the $K$ potential outcomes, provided the confounders $L$ are properly controlled for in our statistical models.

Here, the measured variable $A$ (moving closer to green spaces) functions as a simplified or "coarsened indicator" for the causal effect of multiple versions of the treatment $K$ (moving closer to different types of green spaces) on $Y(k)$ (health outcomes).
This suggests we can estimate the causal effect of moving closer to green spaces in general, even though this treatment consists of multiple versions, as long as the conditions of conditional independence are satisfied.

### **Step 6.2 Understand how we may condition on confounders to address the assumption of (conditional) exchangeability**

We say that conditional exchangeability is satisfied if, after controlling for observed covariates, the assignment of treatment is independent of the potential outcomes under treatment.
Conceptually, assuming both causal consistency (including no interference) and positivity are satisfied, satisfaction of the conditional exchangeability assumption implies that if units were swapped between treatment conditions, the distribution of potential outcomes under different exposures would remain unchanged.
This assumption is needed to ensure a balance between exposures in confounders that might affect the outcome.
With this assumption satisfied, the counterfactual observations derived from the consistency and positivity assumptions can be viewed as randomly assigned to the exposure conditions under which they were observed.
In effect (so to speak), satisfying the conditional exchangeability assumption is an attempt to simulate experimentally controlled randomisation with observational data.

Let $L$ to be the set of measured covariates required to ensure conditional independence.
Let $\coprod$ denote independence.
We express the exchangeability of counterfactual outcomes conditional on measured covariates $L$ as

$$
Y(a) \coprod  A|L
$$

Where the exchangeability assumption holds, along with the consistency and positivity assumptions, we may express the average treatment effect (ATE) on the difference scale

$$
\delta_{ATE}  = \mathbb{E}[Y(a^*)|L = l] - \mathbb{E}[Y(a)|L = l]
$$

Satisfaction of this assumption allows us to approximate a randomised experiment with observational data.

Considering once again causal inference for the effect on metnal well-being of moving closer to green spaces, $A$ represents this move (the treatment), while $Y(a)$ denotes the potential outcomes of move on well-being.

$L$ denotes the set of measured confounders that might influence both the decision to move closer to a green space and the potential outcomes (or that may be an effect of such factors).
Again, these confounders might include factors such as socioeconomic status, age, mental health status, lifestyle preferences, personal values, social connections, occupational demands, family circumstances, economic factors.

If the set of observed covariates is sufficient to measure all the factors that could explain away the association of treatment and outcome, we may say that the conditional exchangeability assumption is satisfied.
Typically this assumption cannot be tested by observational data.
For this reason, it is important to perform sensitivity analyses to evaluate how much unmeasured confounding would be required to explain away any observed associations.

#### **Step 6.3 Understand how we may assess the positivity assumption in observational settings**

We say that the positivity assumption is satisfied if there is a non-zero probability of receiving or not receiving the exposure within each level of all measured covariates.
In other words, within every stratum of every covariate, the probability of each exposure value must be greater than zero.
Mathematically the positivity assumption is expressed:

$$
0 < \Pr(A=a|L)<1, ~ \forall a \in A, ~ \forall l \in L
$$

Causal inference encounters challenges without the satisfaction of the positivity assumption, which again in its weak form can be tested by the data [@westreich2010].

## Part 2. An Introduction to Causal Diagrams

Alice had learned that causal diagrams were developed to assist researchers in identifying the conditions under which causal effects can be discerned from data [@pearl1995; @pearl2009; @greenland1999].
This suits her well.
All those proofs were tiring her.

To use causal diagrams, we must understand the following terminology and conventions:

1.  **Nodes and edges**: nodes represent variables or events within a causal system, while edges signify relationships or interactions between these variables.

2.  **Directed and undirected edges**: directed edges, depicted as arrows, signify an assumed causal link from one variable to another.
    In contrast, undirected edges, which lack arrows, signify an assumed association exists but no direct causal link is implied.
    These edges in a causal diagram indicate potential avenues of influence between nodes.

3.  **Ancestors and descendants**: we call a variable an "ancestor" if it directly or indirectly influences another variable.
    Conversely, we call a variable a "descendant" if it is influenced, directly or indirectly, by another variable.

4.  **D-separation**: we call a path "blocked," or "d-separated," if a node along it prevents the transmission of influence.
    Two variables are considered d-separated if all paths between them are blocked; otherwise, they are d-connected[@pearl1995].

5.  **Acyclic**: Causal diagrams must be acyclic -- they cannot contain feedback loops.
    More precisely: no variable can be an ancestor or descendant of itself.
    *Therefore, in cases where repeated measurements are taken, nodes must be indexed by time.* As mentioned, repeated measures time series data are almost always required to estimate causal effects quantitatively.
    In Part 3 we consider how adding baseline measures of the outcome and exposure in a three-wave repeated measures design greatly enhances causal estimation [@pearl2009].
    To represent the nodes of this design on a graph we must index them by time because the nodes are repeated.

Pearl showed that the principles of d-separation enable us to evaluate relationships between nodes in a causal diagram [@pearl1995].

The rules of d-separation:

a.  **Chain rule**: in a chain structure, where three variables are connected sequentially (represented as $A \rightarrow B \rightarrow C$,) conditioning on $B$ d-separates $A$ and $C$.

b.  **Fork rule**: in a fork structure, where $B$ is a common cause of both $A$ and $C$ (represented as $A \leftarrow B \rightarrow C$,) conditioning on $B$ d-separates $A$ and $C$.

c.  **Collider rule**: in a collider structure, where $B$ is a common effect of both $A$ and $C$ (represented as $A \rightarrow B \leftarrow C$,) $B$ d-separates $A$ and $C$ only if neither $B$ nor any of $B$'s descendants are conditioned upon.

In each case, if $B$ does not d-separate $A$ and $C$, $A$ and $C$ are considered to be d-connected given $B$.
This suggests an open path between $A$ and $C$.
If all paths between $A$ and $C$ are blocked, or equivalently, if no path remains open, then $A$ and $C$ are d-separated given a set of conditioning variables [@pearl2009].

The rules of d-separation clarify which variables to adjust for when estimating causal effects: we seek a set of variables that d-separates the exposure from the outcome.
By conditioning on such an adjustment set, we block all confounding paths, leaving only the causal effect [@pearl2009].

6.  **Adjustment set**: a collection of variables that we either condition upon or deliberately avoid conditioning upon to block all backdoor paths between the exposure and the outcome in the causal diagram [@pearl2009].

7.  **Confounders**: a member of an adjustment set.
    Importantly, *we call a variable as a "confounder" in relation to a specific adjustment set.*[^1]

[^1]: VanderWeele's Modified Disjunctive Cause Criterion provides practical guidance for controlling for confounding [@vanderweele2019]: According to this criterion, a member of any set of variables that can reduce or remove the bias caused by confounding is deemed a member of this confounder set.
    VanderWeele's strategy for defining a confounder set is as follows: a.
    Control for any variable that causes the exposure, the outcome, or both.
    b.
    Control for any proxy for an unmeasured variable that is a shared cause of both exposure and outcome.
    c.
    Define an instrumental variable as a variable associated with the exposure but does not influence the outcome independently, except through the exposure.
    Exclude any instrumental variable that is not a proxy for an unmeasured confounder from the confounder set.

    Note that the concept of a "confounder set" is broader than the concept of an "adjustment set." Every adjustment set is a member of a confounder set.
    So the Modified Disjunctive Cause Criterion will eliminate confounding when the data permit.
    However a confounder set includes variables that will reduce confounding in cases where confounding cannot be eliminated.
    Confounding can almost never be elimiated with certainty.
    For this reason we perform sensitivity analyses.
    However confounding should be reduced wherever possible.
    Hence we opt for "confounder sets."

<!-- -->

8.  **Backdoor criterion**: a set of conditions under which the effect of a treatment on an outcome can be obtained by controlling for a specific set of variables.
    The backdoor criterion guides the selection of **adjustment sets** [@pearl1995].

9.  **Identification problem**: the challenge of estimating the causal effect of a variable using observed data.
    Causal diagrams were developed to address the identification problem.

Alice now turns to the description of the elemental confounds

### 1. The problem of confounding by a common cause

The problem of confounding by common cause arises when there is a variable, denoted by $L$, that influences both the exposure, denoted by $A$, and the outcome, denoted by $Y.$ Because $L$ is a common cause of both $A$ and $Y$, $L$ may create a statistical association between $A$ and $Y$ that does not reflect a causal association.

For instance, in the context of green spaces, consider people choosing to live closer to green spaces (exposure $A$) and their experience of improved mental health (outcome $Y$).
A common cause could be socioeconomic status ($L$).
Individuals with higher socioeconomic status may have the financial capacity to afford housing near green spaces and simultaneously afford better healthcare and lifestyle choices, contributing to improved mental health.
Thus, while the data may show a statistical association between living closer to green spaces ($A$) and improved mental health ($Y$), this association may not reflect a direct causal relationship due to the confounding by socioeconomic status ($L$).

The figure referenced as @fig-dag-common-cause represents such a scenario.
The association of $A$ and $Y$ in the data is confounded by the common cause $L$.
The dashed red arrow in the graph signifies the bias introduced by the open backdoor path from $A$ to $Y$ that arises due to their common cause $L$.

```{tikz}
#| label: fig-dag-common-cause
#| fig-cap: "Counfounding by a common cause. The dashed path indicates bias arising from the open backdoor path from A to Y."
#| out-width: 80%
#| echo: false

\usetikzlibrary{positioning}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations}
\tikzstyle{Arrow} = [->, thin, preaction = {decorate}]
\tikzset{>=latex}
% Define a simple decoration
\tikzstyle{cor} = [-, dashed, preaction = {decorate}]


\begin{tikzpicture}[{every node/.append style}=draw]
\node [ellipse, draw=white] (L) at (0, 0) {L$_{t0}$};
\node [rectangle, draw=white] (A) at (4, 0) {A$_{t1}$};
\node [rectangle, draw=white] (Y) at (8, 0) {Y$_{t2}$};
\draw [cor, draw=red] (A) to (Y);
\draw [-latex, bend left, draw =black] (L) to (Y);
\draw [-latex, black] (L) to (A);
\end{tikzpicture}
```

### Advice: attend to the temporal order of all measured variables

Addressing confounding by a common cause involves its adjustment.
This adjustment effectively closes the backdoor path from the exposure to the outcome.
Equivalently, conditioning on $L$ d-separates $A$ and $Y$.
Common adjustment methods include regression, matching, inverse probability of treatment weighting, and G-methods (covered in [@hernán2023]).
@fig-dag-common-cause-solution clarifies that any confounder that is a common cause of both $A$ and $Y$ must precede $A$ (and hence $Y$), since effects follow their causes chronologically.

After we have time-indexing the nodes on the graph it becomes evident that **control of confounding generally necessitates time-series data.**

```{tikz}
#| label: fig-dag-common-cause-solution
#| fig-cap: "Solution: adjust for pre-exposure confounder. The implication: obtain time series data to ensure the confounder occurs before the exposure."
#| out-width: 80%
#| echo: false

\usetikzlibrary{positioning}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations}
\tikzstyle{Arrow} = [->, thin, preaction = {decorate}]
\tikzset{>=latex}

% Define a simple decoration
\tikzstyle{cor} = [-, dashed, preaction = {decorate}]

\begin{tikzpicture}[{every node/.append style}=draw]
\node [rectangle, draw=black] (L) at (0, 0) {L$_{t0}$};
\node [ellipse, draw=white] (A) at (4, 0) {A$_{t1}$};
\node [rectangle, draw=white] (Y) at (8, 0) {Y$_{t2}$};
\draw [-latex, draw=white] (A) to (Y);
\draw [-latex, bend left, draw =black] (L) to (Y);
\draw [-latex, black] (L) to (A);
\end{tikzpicture}

```

### 2. Confounding by collider stratification (conditioning on a common effect)

Conditioning on a common effect, also known as collider stratification, occurs when a variable, denoted by $L$, is influenced by both the exposure, denoted by $A$, and the outcome, denoted by $Y$.

Imagine, in the context of green spaces, an individual's choice to live closer to green spaces (exposure $A$) and their improved mental health (outcome $Y$) are both influencing the individual's overall satisfaction with life (common effect $L$).
Initially, $A$ and $Y$ could be independent, represented as $A \coprod Y(a)$, suggesting that the decision to live near green spaces is not directly causing improved mental health.

However, when we condition on the life satisfaction $L$ (the common effect of $A$ and $Y$), a backdoor path between $A$ and $Y$ is opened.
This could potentially induce a non-causal association between living closer to green spaces and improved mental health.
The reason behind this is that the overall life satisfaction $L$ can provide information about both the proximity to green spaces $A$ and the mental health status $Y$.
Hence, it may appear that there is an association between $A$ and $Y$ even when there may not be a direct causal relationship.

```{tikz}
#| label: fig-dag-common-effect
#| fig-cap: "Confounding by conditioning on a collider. The dashed red path indicates bias from the open backdoor path from A to Y."
#| out-width: 80%
#| echo: false

\usetikzlibrary{positioning}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations}
\tikzstyle{Arrow} = [->, thin, preaction = {decorate}]
\tikzset{>=latex}

% Define a simple decoration
\tikzstyle{cor} = [-, dashed, preaction = {decorate}]

\begin{tikzpicture}[{every node/.append style}=draw]
\node [rectangle, draw=white] (A) at (0, 0) {A$_{t0}$};
\node [ellipse, draw=white] (Y) at (4, 0) {Y$_{t1}$};
\node [rectangle, draw=black] (L) at (8, 0) {L$_{t2}$};
\draw [-latex, draw=black, bend right] (A) to (L);
\draw [-latex, draw=black] (Y) to (L);
\draw [cor, draw=red] (A) to (Y);

\end{tikzpicture}

```

### Advice: attend to the temporal order of all measured variables

To address the problem of conditioning on a common effect, we should *generally* ensure that:

1.  all confounders $L$ that are common causes of the exposure $A$ and the outcome $Y$ are measured before $A$ has occurred, and
2.  $A$ is measured before $Y$ has occurred.

If such temporal order is preserved, $L$ cannot be an effect of $A$, and thus neither of $Y$.[^2]

[^2]: This rule is not absolute.
    As indicated in @fig-dag-descendent-solution, it may be helpful in certain circumstances to condition on a confounder that occurs after the outcome has occurred.

```{tikz}
#| label: fig-dag-common-effect-solution
#| fig-cap: "Solution: time idexing of confounders helps to avoid collider bias and maintain d-separation. The graph makes the imperative clear: we must collect time series data with confounders measured before the exposure, and that we must likewise measure the exposure before the outcome, with data collected repeatitively on the same units."
#| out-width: 80%
#| echo: false

\usetikzlibrary{positioning}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations}
\tikzstyle{Arrow} = [->, thin, preaction = {decorate}]
\tikzset{>=latex}
% Define a simple decoration
\tikzstyle{cor} = [-, dashed, preaction = {decorate}]


\begin{tikzpicture}[{every node/.append style}=draw]

\node [rectangle, draw=black] (L) at (0, 0) {L$_{t0}$};
\node [ellipse, draw=white] (A) at (4, 0) {A$_{t1}$};
\node [rectangle, draw=white] (Y) at (8, 0) {Y$_{t2}$};
\draw [-latex, draw=white] (A) to (Y);
\draw [-latex, bend left, draw =black] (L) to (Y);
\draw [-latex, black] (L) to (A);
\end{tikzpicture}

```

### M-bias: conditioning on a collider that occurs before the exposure may introduce bias

Typically, indicators for confounders should be included only if they are known to be measured before their exposures - with notable exceptions described below in fig-dag-descendent-solution-2.

In the context of green spaces, consider the scenario where an individual's level of physical activity ($L$) is influenced by an unmeasured factor related to their propensity to live near green spaces ($A$) and another unmeasured factor linked to their mental health ($Y$).
Here, physical activity $L$ does not directly affect the decision to live near green spaces $A$ or mental health status $Y$, but is a descendent of unmeasured variables that do.

If we condition on physical activity $L$ in this scenario, we evoke what is known as "M-bias".
If $L$ is neither a common cause of $A$ and $Y$ nor the effect of a shared common cause, then $L$ should not be included in a causal model.
@fig-m-bias represents a case where $A \coprod Y(a)$ but $A \cancel{\coprod} Y(a)| L$.

M-bias is another example of collider stratification bias, a phenomenon where conditioning on a common effect or a descendent of a common effect induces an association between variables that were previously independent [@cole2010].[^3]

[^3]: Note, when we draw a chronologically ordered path from left to right the M shape for which "M-bias" takes its name changes to an E shape We shall avoid proliferating jargon and retain the term "M bias."

```{tikz}
#| label: fig-m-bias
#| fig-cap: "M-bias: confounding control by including previous outcome measures. The dashed red path indicates bias from the open backdoor path from A to Y by conditioning on pre-exposure variable L. The solution: do not condition on L.  The graph makes it evident that conditioning on variables measured before the exposure is not sufficient to prevent confounding."
#| out-width: 80%
#| echo: false

\usetikzlibrary{positioning}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations}
\tikzstyle{Arrow} = [->, thin, preaction = {decorate}]
\tikzstyle{DoubleArrow} = [-, preaction = {decorate}]
\tikzset{>=latex}

% Define a simple decoration
\tikzstyle{cor} = [-, dashed, preaction = {decorate}]


\begin{tikzpicture}[{every node/.append style}=draw]

\node [rectangle, draw=white] (U1) at (0, 2) {U1};
\node [rectangle, draw=white] (U2) at (0, -2) {U2};
\node [rectangle, draw=black, align=left] (L) at (4, 0) {L$_{t0}$};
\node [rectangle, draw=white] (A) at (8, 0) {A$_{t1}$};
\node [rectangle, draw=white] (Y) at (12, 0) {Y$_{t2}$};

\draw [-latex, draw=black] (U1) to (L);
\draw [-latex, draw =black] (U2) to (L);
\draw [-latex, draw=black, bend left] (U1) to (Y);
\draw [-latex, draw =black, bend right] (U2) to (A);
\draw [cor,  draw=red] (A) to (Y);


\end{tikzpicture}
```

### Advice: adopt the modified disjunctive cause criterion for confounding control

Again, the modified disjunctive cause criterion will satisfy the backdoor criterion in all cases and reduce bias where this criterion cannot be fully satisfied:

a.  Control for any variable that causes the exposure, the outcome, or both.
b.  Control for any proxy for an unmeasured variable that is a shared cause of both exposure and outcome.
c.  Define an instrumental variable as a variable associated with the exposure but does not influence the outcome independently, except through the exposure. Exclude any instrumental variable that is not a proxy for an unmeasured confounder from the confounder set (see: @vanderweele2020 page 441, [@vanderweele2019])

Of course, the difficulty is in determining which variables belong to a confounder set.
Specialist knowledge can facilitate this task.
However, the data alone typically do not settle this question.
(For exceptions see: bulbulia2021).

### 3. Mediator bias

Applying this to our green spaces example, again we consider proximity to green spaces as the exposure ($A$), mental health as the outcome ($Y$), and physical activity as the mediator ($L$).

In this scenario, living close to green spaces ($A$) influences physical activity ($L$), which subsequently impacts mental health ($Y$).
If we condition on physical activity ($L$), we may bias our estimates of the total effect of proximity to green spaces ($A$) on mental health ($Y$).
This bias arises because conditioning on $L$ can obscure the direct effect of $A$ on $Y$, as it blocks the indirect path through $L$.
This phenomenon, known as mediator bias, is depicted in @fig-dag-mediator.

One might assume that conditioning on a mediator does not introduce bias when there is no causal relationship between $A$ and $Y$.
However, this is not always the case.
Consider a situation where $L$ is a common effect of the exposure $A$ and an unmeasured variable $U$ linked to the outcome $Y$.
Here, including $L$ may inflate the association between $A$ and $Y$, even if $A$ is not related to $Y$ and $U$ does not cause $A$.
This case is depicted in @fig-dag-descendent.

Hence, unless one is specifically investigating mediation analysis, it is generally inadvisable to condition on a post-treatment variable.
Being aware of the timeline in the spatial organisation of the graph underlines a critical principle for data collection: if we cannot guarantee that $L$ is measured before $A$, and if $A$ may affect $L$, including $L$ in our model could lead to mediator bias.
This scenario is illustrated in @fig-dag-descendent.

```{tikz}
#| label: fig-dag-mediator
#| fig-cap: "Confounding by conditioning on a mediator. The dashed black arrow indicates bias arising from partially blocking the path between A and Y."
#| out-width: 80%
#| echo: false

\usetikzlibrary{positioning}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations}
\tikzstyle{Arrow} = [->, thin, preaction = {decorate}]
\tikzset{>=latex}

% Define a simple decoration
\tikzstyle{cor} = [-, dashed, preaction = {decorate}]

\begin{tikzpicture}[{every node/.append style}=draw]
\node [ellipse, draw=white] (A) at (0, 0) {A$_{t0}$};
\node [rectangle, draw=black] (L) at (4, 0) {L$_{t1}$};
\node [rectangle, draw=white] (Y) at (8, 0) {Y$_{t2}$};
\draw [-latex, bend left, draw=black, dotted] (A) to (Y);
\draw [-latex, draw =black] (L) to (Y);
\draw [-latex, black] (A) to (L);
\end{tikzpicture}
```

### Advice: attend to the temporal order of all measured variables

To mitigate the issue of mediator bias, particularly when focusing on total effects, we should generally avoid conditioning on a mediator.
We avoid this problem by ensuring that $L$ occurs before the treatment $A$ and the outcome $Y$ (Note: a counter-example is presented in @fig-dag-descendent-solution-2).
Again, we discover the importance of explicitly stating and measuring the temporal order of our variables.[^4]

[^4]: Note that if $L$ were associated with $Y$ and could not be caused by $A$, conditioning on $L$ would typically enhance the precision of the causal effect estimate of $A \to Y$.
    This precision enhancement holds even if $L$ occurs after $A$.
    However, the onus is on the researcher to show that the post-treatment factor cannot be a consequence of the exposure.

```{tikz}
#| label: fig-dag-mediator-solution
#| fig-cap: "Solution: do not condition on a mediator. The implication: by ensuring temporal order in data collection we diminish the probabilty of  mistaking an effect of an exposure for its confounder."
#| out-width: 80%
#| echo: false

\usetikzlibrary{positioning}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations}
\tikzstyle{Arrow} = [->, thin, preaction = {decorate}]
\tikzset{>=latex}
% Define a simple decoration
\tikzstyle{cor} = [-, dashed, preaction = {decorate}]

\begin{tikzpicture}[{every node/.append style}=draw]
\node [rectangle, draw=black] (L) at (0, 0) {L$_{t0}$};
\node [ellipse, draw=white] (A) at (4, 0) {A$_{t1}$};
\node [rectangle, draw=white] (Y) at (8, 0) {Y$_{t2}$};
\draw [-latex, draw=blue] (A) to (Y);
\draw [-latex, bend left, draw =black] (L) to (Y);
\draw [-latex, black] (L) to (A);
\end{tikzpicture}
```

### 4. Conditioning on a descendant may induce confounding

Let us consider this principle in the context of our green spaces example.
Again denote "proximity to green spaces" by $A$, "mental health" by $Y$.
Denote "physical activity" by $L$, and "sun exposure" by $L^\prime$.

In this scenario, assume $L^\prime$, sun exposure, is caused by an unobserved variable $U$, and is influenced by $A$, the proximity to green spaces.
Further, assume $U$ affects the outcome $Y$, mental health.

Conditioning on $L^\prime$, which is a descendant of $A$ and $U$, can lead to a spurious association between $A$ and $Y$ through the path $A \to L^\prime \to U \to Y$.
This situation, shown in @fig-dag-descendent, illustrates how conditioning on a descendant can introduce confounding, resulting in a distorted causal estimation.

```{tikz}
#| label: fig-dag-descendent
#| fig-cap: "Confounding by descent: the red dashed path illustrates the introduction of bias by conditioning on the descendant of a confounder that is affected by the exposure, thus opening of a backdoor path between the exposure, A, and the outcome, Y."
#| out-width: 80%
#| echo: false

\usetikzlibrary{positioning}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations}
\tikzstyle{Arrow} = [->, thin, preaction = {decorate}]
\tikzset{>=latex}
% Define a simple decoration
\tikzstyle{cor} = [-, dashed, preaction = {decorate}]


\begin{tikzpicture}[{every node/.append style}=draw]

\node [rectangle, draw=white] (U) at (0, 0) {U};
\node [ellipse, draw=white] (A) at (2, 0) {$A_{t0}$};
\node [rectangle, draw=black](L) at (4, 0) {$L^\prime_{t1}$};
\node [ellipse, draw=white] (Y) at (6, 0) {$Y_{t2}$};

\draw [-latex, bend right=50] (U) to (L);
\draw [-latex, bend left, draw=red] (U) to (Y);
\draw [-latex,draw=black] (A) to (L);
\draw [cor, draw=red] (A) to (U);

\end{tikzpicture}
```

Again, the advice is evident from the chronology of the graph: we should measure the ($L^\prime$) before the exposure ($A$).
This solution is presented in @fig-dag-descendent-solution.

```{tikz}
#| label: fig-dag-descendent-solution
#| fig-cap: "Solution: again the graph makes it clear that our data must ensure temporal order of the measurements. By ensuring that L occurs before A confounding is controlled. The figure also makes it evident that L need not affect Y to be a confounder (i.e. a member of a confounder set)."
#| out-width: 80%
#| echo: false

\usetikzlibrary{positioning}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations}
\tikzstyle{Arrow} = [->, thin, preaction = {decorate}]
\tikzset{>=latex}

\begin{tikzpicture}[{every node/.append style}=draw]

\node [rectangle, draw=white] (U) at (0, 0) {U};
\node [rectangle, draw=black] (L) at (2, 0) {$L^{\prime}_{t0}$};
\node [rectangle, draw=white](A) at (4, 0) {$A_{t1}$};
\node [ellipse, draw=white] (Y) at (6, 0) {$Y_{t2}$};

\draw [-latex, draw=black] (U) to (L);
\draw [-latex, draw=black] (L) to (A);
\draw [-latex, bend right=50, draw =black] (U) to (Y);


\end{tikzpicture}

```

### 5. Conditioning on a descendent may reduce confounding

In our greenspace example, consider an unmeasured confounder $U$, perhaps a genetic factor, that impacts both proximity to green spaces $A$, mental health $Y$, and a variable $L^\prime$, such as a behavioural trait that only manifests later in life.

As shown in @fig-dag-descendent-solution-2, if we adjust for $L^\prime$, we might be able to reduce the confounding caused by the unmeasured $U$.
Even though $L^\prime$ may occur after the exposure and even after the outcome, conditioning on it can help control for the confounding because it acts as a proxy for an unmeasured common cause of the exposure and the outcome.

This scenario reveals that adhering strictly to a rule that only allows us to condition on pre-exposure and pre-outcome variables may not always be optimal.
It highlights the need for careful contemplation of data collection strategies.
We cannot resort solely to algorithmic rules for confounding control.
Every case necessitates its unique approach.

```{tikz}
#| label: fig-dag-descendent-solution-2
#| fig-cap: "Solution: conditioning on a confounder that occurs after the exposure and the outcome might address a problem of unmeasured confounding if the confounder is a descendent of a prior common cause of the exposure and outcome. The dotted paths denote that the effect of U on A and Y is partially adjusted by conditioning on L', even though L' occurs after the outcome. The paths are dotted to represent a reduction of bias by conditioning on the post-outcome descendent of an unmeasured common cause of the exposure and outcome.  How might this work? Consider a genetic factor that affects the exposure and the outcome early in life might be measured by an indicator late that is expressed (and may be measured) later in life. Adjusting for such an indicator would constitute an example of post-outcome confounding control."
#| out-width: 80%
#| echo: false

\usetikzlibrary{positioning}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations}
\tikzstyle{Arrow} = [->, thin, preaction = {decorate}]
\tikzset{>=latex}


\begin{tikzpicture}[{every node/.append style}=draw]

\node [rectangle, draw=white] (U) at (0, 0) {U};
\node [rectangle, draw=black] (L) at (6, -1) {$L^{\prime}_{t3}$};
\node [rectangle, draw=white](A) at (2, 0) {A$_{t1}$};
\node [ellipse, draw=white] (Y) at (4, 0) {Y$_{t2}$};

\draw [-latex, bend right = 10, draw=black] (U) to (L);
\draw [-latex, draw=black, dotted] (A) to (Y);
\draw [-latex, bend right=20, draw =black, dotted] (U) to (Y);
\draw [-latex, draw =black, dotted] (U) to (A);

\end{tikzpicture}

```

### 6. Using causal diagrams to inform data collection: the three-wave panel design

In a three-wave panel design, we collect data across three intervals to facilitate observational causal inference.
The causal diagram in @fig-dag-6 clarifies the virtues of panel data collection

1.  **Baseline Data Collection**:

    -   **Confounding Data**: At baseline, we collect data on confounders, along with data on the exposure and the outcome.
        This initial data collection phase is crucial for measuring common causes of the treatment and outcome, or the descendents of such common causes.

    -   **Exposure and Outcome Data**: Baseline measurements of exposure and outcome allow our data collection to more effectively mimic an experiment.

        -   **Incidence Effect Evaluation**: The baseline exposure allows us to interpret the post-baseline exposure effect as an incidence effect rather than a prevalence effect.
            This interpretation means we can assess the change due to a new occurrence (incidence) of the exposure rather than its overall presence (prevalence).

        -   **Sample Adequacy for Rare Exposures**: Especially when the exposure is uncommon, measuring the baseline exposure and outcome can help assess the adequacy of the sample size.

        -   **Temporal Ordering and Confounding Control**: Incorporating the outcome at baseline helps confirm the temporal order of the cause-effect relationship, thereby guarding against reverse causation.
            Moreover, when we also control for the exposure at baseline, an unmeasured confounder would have to negate the association between the exposure at one wave post-baseline and the outcome at two waves post-baseline, independent of the baseline effect.

2.  **First Follow-Up Data Collection (Baseline +1)**:

    -   At this stage, we measure the exposure. This follow-up allows us to capture the causal effect of changes in the exposure since baseline.

3.  **Second Follow-Up Data Collection (Baseline +2)**:

    -   We measure the outcome at this stage. Similar to the first follow-up, this measurement allows us to capture a controlled effect for the outcome since the baseline measurement.

Through this design, any unmeasured confounder affecting both the exposure and the outcome would need to do so independently of these baseline measurements of exposure and outcome.
Causal diagrams effectively illustrate this methodology, as shown in @fig-dag-6, clarifying the paths of causation, potential sources of confounding, and the methods used for controlling these confounders.
As a result, these diagrams are powerful tools for observational causal inference in a three-wave panel design.

```{tikz}
#| label: fig-dag-6
#| fig-cap: "Causal diagram adapted from Vanderweele et al.'s three-wave panel design. The dotted line indicates a reduction in bias arising from including baseline measures for the exposure and outcome. For an unmeasured confounder U to bias the exposure-outcome association, it would need to do so independently of these outcome and exposure baseline measures. The graph clarifies that by measuring confounders before the exposure and the exposure before the outcome, we reduce the potential for reverse causation, collider stratification, and mediator biases."
#| out-width: 80%
#| echo: false

\usetikzlibrary{positioning}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations}
\tikzstyle{Arrow} = [->, thin, preaction = {decorate}]
\tikzset{>=latex}
\tikzstyle{cor} = [-, dashed, preaction = {decorate}]


\begin{tikzpicture}[{every node/.append style}=draw]
\node [rectangle, draw=white] (U) at (0, 0) {U};
\node [rectangle, draw=black, align=left] (L) at (2, 0) {L$_{t0}$ \\A$_{t0}$ \\Y$_{t0}$};
\node [rectangle, draw=white] (A) at (4, 0) {A$_{t1}$};
\node [ellipse, draw=white] (Y) at (6, 0) {Y$_{t2}$};
\draw [-latex, draw=black] (U) to (L);
\draw [-latex, draw=black] (L) to (A);
\draw [cor, draw = black, dotted] (A) to (Y);
\draw [-latex, bend left=50, draw =black] (L) to (Y);
\draw [-latex, bend right=50, draw =black, dotted] (U) to (Y);
\draw [-latex, bend left=50, draw =black, dotted] (U) to (A);




\end{tikzpicture}
```

### 7. The Inevitability of Unmeasured Confounding

In observational research, there is an unavoidable issue known as unmeasured confounding.
These are confounders that are not included in the data set, thus not accounted for during analysis, which can introduce bias into the study.
This may occur due to constraints such as data availability, financial or ethical reasons.
Despite all efforts, it is virtually impossible to measure and control for every possible confounder.
Therefore, it is necessary to estimate the potential impact of these unmeasured confounders and consider their influence on the study's findings.

One of the methods to assess the potential impact of unmeasured confounding is through sensitivity analysis, which quantifies how strong an unmeasured confounder would need to be to fully explain away an observed association.
A commonly used metric for this purpose is the E-Value.

The E-Value quantifies the minimum strength of association that an unmeasured confounder would require with both the exposure and outcome, over and above the measured confounders, to explain away an observed exposure-outcome association.
An E-Value close to 1 suggests that the observed association is vulnerable to unmeasured confounding, whereas a high E-Value suggests that a remarkably strong unmeasured confounder would be necessary to explain the observed association, thus providing evidence towards the robustness of the observed association.

E-Values can be computed through the `EValue` package in R [@mathur2018].
This package provides an intuitive, user-friendly interface for researchers to compute E-Values.
Observational researchers have no excuses not to conduct sensitivity analyses.

### Part 3. Summary, Pitfalls and Tips.

### Summary

We introduced the potential outcomes framework of causal inference, focussing on three fundamental assumptions:

1.  **Causal Consistency**: This assumption posits that for every unit, the outcome under treatment is equal to the observed outcome if the unit was treated, and vice versa for non-treatment.
    In essence, it says that 'the treatment' is consistently defined.

2.  **Exchangeability**: The assignment of treatment is random and independent of the potential outcomes.
    This is a essential virtue of experimental designs The process of randomisation ensures that every participant has an equal chance of being assigned to the treatment or control group, creating balance in the factors that might affect the outcomes under the different treatments.
    Randomisation is powerful because it removes any systematic bias in the treatment assignment.

3.  **Positivity**: Each unit under study has a non-zero probability of receiving the treatment.
    This ensures that comparisons between treated and untreated units are meaningful and well-defined.

We observed that in the context of randomised experiments, these assumptions are generally met, and in being met, experimentalists address the problem of missing potential outcomes in the treatment groups that researchers compare.


Having built core intuitions for how experiments recover average causal effect estimates, we considered how the three fundamental assumptions required for causal inferences may be easily violated in observational studies.
Where treatment assignment is not random and can be influenced by observed or unobserved variables, correlation is not equivalent to causation.

Next, we clarified the conditions where, assuming the three fundamental assumptions of causal inference have been satisified, we may to recover causal effect estimates from observational data.

To obtain such causal effect estimates we must:

-   Precisely defining the interventions that need to be compared.
-   Conditioning on confounders, variables associated with both the treatment and the outcome, or that are descendents of such common causes.
-   Ensuring positivity, that is, each individual has some probability of receiving each level of treatment. This ensures the comparability of treatment groups.

The second part of the article discusses the use of causal diagrams for dealing with identification problems in observational settings.
The key points take home messages were:

-   For confounding control to work, researchers generally require time-series data that determine the temporal order of events.
-   Causal diagrams reveal that the three-wave panel design is a powerful tool for addressing causal questions with observational data. 
-   Nevertheless, sensitivity analysis should always be performed, and can be performed relatively easily. 

We conclude by summarising our advice through "Tips" and "Pitfalls" 

### Data Collection Tips

1.  Use time series data.
2.  Ensure significant change from baseline in treatment (positivity).
3.  Clearly define measurements for treatment, outcome, and baseline confounders.
4.  Include baseline treatment measures.
5.  Include baseline outcome measures.
6.  Strive for high sample retention.

### Graph Drawing Tips

1.  Define all nodes unambiguously.
2.  Keep the graph simple and focused.
3.  Explicitly state any novel conventions.
4.  Maintain acyclicity in the graph.
5.  Arrange nodes in chronological order.
6.  Time-stamp nodes to reflect the temporal sequence of causation.
7.  Apply a modified disjunctive cause criterion pragmatically.
8.  Add nodes for unmeasured confounding where helpful.
9.  Illustrate nodes for post-treatment selection.
10. Remember, causal diagrams are qualitative tools, not detailed maps.

### Pitfalls to Avoid

1.  Avoid using cross-sectional data.
2.  Don't misuse causal diagrams without understanding counter-factual data science.
3.  Don't create diagrams without time indices.
4.  Avoid excessive nodes in the graph.
5.  Don't draw arrows into the manipulation in experimental studies.
6.  Don't inaccurately describe bias when exposure and outcome are d-separated.
7.  Don't ignore causal diagrams during research design.
8.  Avoid representing interactions and non-linear dynamics in causal diagrams.
9.  Remember that structural equation models are not true structural models: do not mistake structural equation models for causal diagrams (NOTE Don, we haven't yet said this, but we)

{{< pagebreak >}}

## Funding

This work is supported by a grant from the Templeton Religion Trust (TRT0418).
JB received support from the Max Planck Institute for the Science of Human History.
The funders had no role in preparing the manuscript or the decision to publish it.

## References

::: {#refs}
:::
