% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames,x11names}{xcolor}
%
\documentclass[
  singlecolumn]{article}

\usepackage{amsmath,amssymb}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage[]{libertinus}
\ifPDFTeX\else  
    % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\usepackage[top=30mm,left=20mm,heightrounded]{geometry}
\setlength{\emergencystretch}{3em} % prevent overfull lines
\setcounter{secnumdepth}{-\maxdimen} % remove section numbering
% Make \paragraph and \subparagraph free-standing
\ifx\paragraph\undefined\else
  \let\oldparagraph\paragraph
  \renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
  \let\oldsubparagraph\subparagraph
  \renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi

\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{241,243,245}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.40,0.45,0.13}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\BuiltInTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\ExtensionTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.28,0.35,0.67}{#1}}
\newcommand{\ImportTok}[1]{\textcolor[rgb]{0.00,0.46,0.62}{#1}}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\NormalTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\RegionMarkerTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.07,0.07,0.07}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}

\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
% definitions for citeproc citations
\NewDocumentCommand\citeproctext{}{}
\NewDocumentCommand\citeproc{mm}{%
  \begingroup\def\citeproctext{#2}\cite{#1}\endgroup}
\makeatletter
 % allow citations to break across lines
 \let\@cite@ofmt\@firstofone
 % avoid brackets around text for \cite:
 \def\@biblabel#1{}
 \def\@cite#1#2{{#1\if@tempswa , #2\fi}}
\makeatother
\newlength{\cslhangindent}
\setlength{\cslhangindent}{1.5em}
\newlength{\csllabelwidth}
\setlength{\csllabelwidth}{3em}
\newenvironment{CSLReferences}[2] % #1 hanging-indent, #2 entry-spacing
 {\begin{list}{}{%
  \setlength{\itemindent}{0pt}
  \setlength{\leftmargin}{0pt}
  \setlength{\parsep}{0pt}
  % turn on hanging indent if param 1 is 1
  \ifodd #1
   \setlength{\leftmargin}{\cslhangindent}
   \setlength{\itemindent}{-1\cslhangindent}
  \fi
  % set entry spacing
  \setlength{\itemsep}{#2\baselineskip}}}
 {\end{list}}
\usepackage{calc}
\newcommand{\CSLBlock}[1]{\hfill\break\parbox[t]{\linewidth}{\strut\ignorespaces#1\strut}}
\newcommand{\CSLLeftMargin}[1]{\parbox[t]{\csllabelwidth}{\strut#1\strut}}
\newcommand{\CSLRightInline}[1]{\parbox[t]{\linewidth - \csllabelwidth}{\strut#1\strut}}
\newcommand{\CSLIndent}[1]{\hspace{\cslhangindent}#1}

\input{/Users/joseph/GIT/templates/latex/custom-commands.tex}
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\AtBeginDocument{%
\ifdefined\contentsname
  \renewcommand*\contentsname{Table of contents}
\else
  \newcommand\contentsname{Table of contents}
\fi
\ifdefined\listfigurename
  \renewcommand*\listfigurename{List of Figures}
\else
  \newcommand\listfigurename{List of Figures}
\fi
\ifdefined\listtablename
  \renewcommand*\listtablename{List of Tables}
\else
  \newcommand\listtablename{List of Tables}
\fi
\ifdefined\figurename
  \renewcommand*\figurename{Figure}
\else
  \newcommand\figurename{Figure}
\fi
\ifdefined\tablename
  \renewcommand*\tablename{Table}
\else
  \newcommand\tablename{Table}
\fi
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Listing}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}
\makeatother
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\@ifpackageloaded{subcaption}{}{\usepackage{subcaption}}
\makeatother
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\usepackage{bookmark}

\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same} % disable monospaced font for URLs
\hypersetup{
  pdftitle={Causal Diagrams: A Practical Guide},
  pdfauthor={Joseph A. Bulbulia},
  pdfkeywords={Directed Acyclic Graph, Causal
Inference, Confounding, Feedback, Interaction, Internal
validity, External validity, Mediation},
  colorlinks=true,
  linkcolor={blue},
  filecolor={Maroon},
  citecolor={Blue},
  urlcolor={Blue},
  pdfcreator={LaTeX via pandoc}}

\title{Causal Diagrams: A Practical Guide}
\author{Joseph A. Bulbulia}
\date{2024-02-01}

\begin{document}
\maketitle
\begin{abstract}
In causal inference, accurately quantifying a causal effect requires
contrasting hypothetical counterfactual states simulated from data. This
complex task relies on a framework of explicit assumptions and
systematic, multi-step workflows. Causal diagrams (directed acyclic
graphs, or DAGs) are powerful tools for evaluating assumptions and
designing research. However, when misused, causal diagrams encourage
false confidence. This guide offers practical advice for creating
sequentially ordered causal diagrams that are effective and safe.
Focussing on the time structure of causation reveals the benefits of
sequential order in the spatial organisation of causal diagrams, both
for data analysis and collection. After reviewing fundamentals, we
employ \emph{sequentially ordered causal diagrams} to elucidate often
misunderstood concepts of causal interaction (moderation), mediation,
and dynamic longitudinal feedback, as well as measurement-error bias and
target validity. Overall, sequentially ordered causal diagrams
underscore causal inference's mission-critical demand for accuracy in
the relative timing of confounders, exposures, and outcomes recorded in
one's data.
\end{abstract}

\subsection{Introduction}\label{introduction}

Here, I offer readers of \emph{Evolutionary Human Science} practical
guidance for creating causal diagrams.

\textbf{Part 1} introduces core concepts and theories in causal data
science, emphasising the fundamental assumptions and the demands they
impose on causal inferential workflows. We review the motivation in
causal data science for adopting an `estimands first' approach ---
pre-specifying counterfactual contrasts to address a well-defined causal
question within a clearly defined target population.

\textbf{Part 2} introduces essential terminology and explores elementary
use cases, focussing on the benefits of sequentially order in one's
graph for evaluating confounding biases.

\textbf{Part 3} uses sequentially ordered causal diagrams to clarify
concepts of interaction (moderation) and effect modification.

\textbf{Part 4} clarifies biases arising from measurement error bias.

\textbf{Part 5} clarifies restriction biases arising from (a) attrition/
censoring; (b) mismatch between a study sample population and a target
sample population. We also consider heterogeniety biases arising from
failures to sufficiently restrict one's sample.

\textbf{Part 6} uses causal diagrams to clarify causal estimation in
which there are multiple exposures, focussing on causal mediation, and
time-varying treatments.

There are many good resources available for learning causal diagrams
(\citeproc{ref-barrett2021}{Barrett 2021};
\citeproc{ref-cinelli2022}{Cinelli \emph{et al.} 2022};
\citeproc{ref-greenland1999}{Greenland \emph{et al.} 1999a};
\citeproc{ref-hernan2023}{Hernan and Robins 2023};
\citeproc{ref-mcelreath2020}{McElreath 2020};
\citeproc{ref-pearl2009}{Pearl 2009a}; \citeproc{ref-rohrer2018}{Rohrer
2018}; \citeproc{ref-suzuki2020}{Suzuki \emph{et al.} 2020}). This work
hopes to contribute to these resources, first by providing additional
conceptual orientation to the frameworks and workflows of causal data
science, outside of which the application of causal diagrams is risky;
second, by underscoring the benefits of sequential order in one's causal
diagrams to clarify demands for data collection and analysis; third by
using causal diagrams to clarify threats to target validity arising from
sample/target population mismatch, fourth, by using causal diagrams to
clarify questions of causal interaction, mediation, and longitudinal
feedback, about which there remains considerable confusions among many
human scientists.

\subsection{Part 1. Overview of Causal Data
Science}\label{part-1.-overview-of-causal-data-science}

The first step in answering a causal question is to ask it
(\citeproc{ref-hernuxe1n2016}{Hernán \emph{et al.} 2016a}). Causal
diagrams come later, when we consider which forms of data might enable
us to address our pre-specified causal questions.

\paragraph{1.1.1 The fundamental problem of causal
inference}\label{the-fundamental-problem-of-causal-inference}

To ask a causal question, we must consider the concept of causality
itself. Consider an intervention, \(A\), and its effect, \(Y\). We say
that \(A\) causes \(Y\) if altering \(A\) would lead to a change in
\(Y\) (\citeproc{ref-hume1902}{Hume 1902};
\citeproc{ref-lewis1973}{Lewis 1973}). If altering \(A\) would not
change \(Y\), we say that \(A\) has no causal effect on \(Y\).

In causal inference, we aim to quantitatively contrast the potential
outcomes of \(Y\) in response to different levels of a well-defined
intervention. Commonly, we refer to such interventions as `exposures' or
`treatments;' we refer to the possible effects of interventions as
`potential outcomes.'

Consider a binary treatment variable \(A \in \{0,1\}\). For each unit
\(i\) in the set \(\{1, 2, \ldots, n\}\), when \(A_i\) is set to 0, the
potential outcome under this condition is denoted, \(Y_i(0)\).
Conversely, when \(A_i\) is set to 1, the potential outcome is denoted,
\(Y_i(1)\). We refer to the terms \(Y_i(1)\) and \(Y_i(0)\) as
`potential outcomes' because until realised, the effects of
interventions describe counterfactual states.

Suppose that each unit \(i\) receives either \(A_i = 1\) or \(A_i = 0\).
The corresponding outcomes are realised as \(Y_i|A_i = 1\) or
\(Y_i|A_i = 0\). For now, let us assume that each realised outcome under
that intervention is equivalent to one of the potential outcomes
required for a quantitative causal contrast, such that
\([(Y_i(a)|A_i = a)] = (Y_i|A_i = a)\). Thus when \(A_i = 1\),
\(Y_i(1)|A_i = 1\) is observed. However, if \(A_i = 1\), it follows that
\(Y_i(0)|A_i = 1\) is not observed:

\[
Y_i|A_i = 1 \implies Y_i(0)|A_i = 1~ \text{is counterfactual}
\]

Conversely, if \(A_i = 0\), we may assume the potential outcome
\(Y_i(0)|A_i = 0\)) is observed as \(Y_i|A_i = 0\). However, the
potential outcome \(Y_i(1)|A_i = 0\) is never realised and so not
observed:

\[
Y_i|A_i = 0 \implies Y_i(1)|A_i = 0~ \text{is counterfactual}
\]

We define \(\tau_i\) as the individual causal effect for unit \(i\) and
express the individual causal effect:

\[
\tau_i = Y_i(1) - Y_i(0)
\]

Notice that each unit can only be exposed to only one level of the
exposure \(A_i = a\) at a time. This implies that \(\tau_i\), is not
merely unobserved but inherently \emph{unobservable} (see discussion in
Appendix 3).

Although we cannot observe potential outcomes that do not occur, it is
tempting to ask questions about them, `What if Isaac Newton had not
witnessed the falling apple?' What if Leonardo da Vinci had never
pursued art?' or `What if Archduke Ferdinand had not been assassinated?'
There are abundant examples from literature. Robert Frost contemplates,
`Two roads diverged in a yellow wood, and sorry I could not travel both,
and be one traveller, long I stood\ldots{}' (see: Robert Frost, `The
Road Not Taken':
https://www.poetryfoundation.org/poems/44272/the-road-not-taken). We
have counterfactual questions for our personal experiences: `What if I
had had not interviewed for that job?' `What if I had stayed in that
relationship?' We may speculate, with reasons, but we cannot directly
observe the potential outcomes we would need to verify our speculations.
The physics of middle-sized dry goods prevents the joint realisation of
the facts required for quantitative comparisons. That individual causal
effects cannot be identified from observations is known as `\emph{the
fundamental problem of causal inference}'
(\citeproc{ref-holland1986}{Holland 1986};
\citeproc{ref-rubin1976}{Rubin 1976}).

\paragraph{1.1.2 Causal effects from randomised
experiments}\label{causal-effects-from-randomised-experiments}

It is not feasible to compute individual causal effects. However, under
certain assumptions, we can estimate \emph{average} treatment effects --
also called `marginal effects,' -- by comparing groups that have
received different levels of treatment. The average treatment effect,
\(ATE\), is defined as the difference between the expected outcomes
under treatment and contrast conditions. Suppose the treatment, \(A\),
is a binary variable \(A \in \{0,1\}\):

\[
\text{Average Treatment Effect}  = \mathbb{E}[Y(1)] - \mathbb{E}[Y(0)].
\] A challenge remains in computing these treatment-group averages when
individual causal effects are unobservable. Consider the problem framed
in terms of \emph{full data} that would be required to compute these
averages --- that is in terms of the complete counterfactual dataset
where the missing potential outcomes, inherent in observational data,
were somehow available:

\[
\text{Average Treatment Effect} = \left(\underbrace{\underbrace{\mathbb{E}[Y(1)|A = 1]}_{\text{observed for } A = 1} + \underbrace{\mathbb{E}[Y(1)|A = 0]}_{\text{unobserved for } A = 0}}_{\text{effect among treated, by the law of total expectation}}\right) - \left(\underbrace{\underbrace{\mathbb{E}[Y(0)|A = 0]}_{\text{observed for } A = 0} + \underbrace{\mathbb{E}[Y(0)|A = 1]}_{\text{unobserved for } A = 1}}_{\text{effect among untreated, by the law of total expectation}}\right).
\]

Consider that for each treatment condition, half the observations needed
to compute a treatment-group average are (inherently) unobserved.

Randomisation allows investigators to recover the treatment group
averages even though treatment groups contain inherently missing
observations. When investigators effectively randomise units into
treatment conditions, and there is full adherence, the distributions of
confounding factors that could explain differences in the potential
outcomes are balanced across the conditions. Randomisation (in a perfect
experiment) ensures that nothing can explain a difference in treatment
group average except the treatment. This implies (by the law of iterated
expectations):

\[
\widehat{\mathbb{E}}[Y(0) | A = 1] = \widehat{\mathbb{E}}[Y(0) | A = 0]
\]

and

\[
\widehat{\mathbb{E}}[Y(1) | A = 1] = \widehat{\mathbb{E}}[Y(1) | A = 0]
\]

We assume, (by causal consistency, see: \(\S 1.2.1\)):

\[\widehat{\mathbb{E}}[Y(1) | A = 1] = \widehat{\mathbb{E}}[Y| A = 1]\]

and

\[\widehat{\mathbb{E}}[Y(0) | A = 0] = \widehat{\mathbb{E}}[Y| A = 0]\]

It follows that the average treatment effect of the randomised
experiment can be computed (by the law of iterated expectations):

\[
\text{The Estimated Average Treatment Effect} = \widehat{\mathbb{E}}[Y | A = 1] - \widehat{\mathbb{E}}[Y | A = 0].
\]

Understanding how randomisation obtains the missing counterfactual
outcomes that we require to consistently estimate average treatment
effects clarifies the tasks of causal inference in non-experimental
settings (\citeproc{ref-hernuxe1n2008a}{Hernán \emph{et al.} 2008};
\citeproc{ref-hernuxe1n2022}{Hernán \emph{et al.} 2022};
\citeproc{ref-hernuxe1n2006}{Hernán and Robins 2006}).

\subsubsection{1.2 Fundamental Identification
Assumptions}\label{fundamental-identification-assumptions}

There are three fundamental identification assumptions that must be
satisfied to consistently estimate causal effects with data.

\paragraph{1.2.1 Assumption 1: Causal
Consistency}\label{assumption-1-causal-consistency}

We satisfy the causal consistency assumption if, for each unit \(i\) in
the set \(\{1, 2, \ldots, n\}\), the observed outcome corresponds to one
of the specific counterfactual outcomes to be compared such that:

\[
Y_i^{observed}|A_i = 
\begin{cases} 
Y_i(a^*) & \text{if } A_i = a^* \\
Y_i(a) & \text{if } A_i = a
\end{cases}
\]

The causal consistency assumption implies that the observed outcome at a
specific exposure level equates to the counterfactual outcome for that
individual at the observed exposure level. Although it seems
straightforward to equate an individual's observed outcome with their
counterfactual outcome, treatment conditions vary, and treatment
heterogeneity poses considerable challenges for satisfying this
assumption. See: \textbf{Appendix A}

\paragraph{1.2.2 Assumption 2: Conditional Exchangeability (no
unmeasured
confounding)}\label{assumption-2-conditional-exchangeability-no-unmeasured-confounding}

We satisfy the conditional exchangeability assumption if the treatment
groups are conditionally balanced in the variables that could affect the
potential outcomes. In experimental designs, random assignment
facilitates satisfaction of the conditional exchangeability assumption.
In observational studies more effort is required. We must control for
any covariate that could account for observed correlations between \(A\)
and \(Y\) in the absence of a causal effect of \(A\) on \(Y\).

Let \(\coprod\) again denote independence. Let \(L\) denote the set of
covariates necessary to ensure this conditional independence. We satisfy
conditional exchangeability when:

\[
Y(a) \coprod A | L \quad \text{or equivalently} \quad A \coprod Y(a) | L
\]

Assuming conditional exchangeability is satisfied and the other
assumptions required for consistent causal inference also hold, we may
compute the average treatment effect (ATE) on the difference scale:

\[
ATE = \mathbb{E}[Y(1) | L] - \mathbb{E}[Y(0) | L]
\]

In the disciplines of cultural evolution, where experimental control is
impractical, causal inferences hinge on the plausibility of satisfying
this `no unmeasured confounding' assumption (see: \textbf{Appendix 1})

Importantly, causal diagrams primarily function to identify sources of
bias that may influence the association between an exposure and outcome.
They highlight those aspects of the assumed causal order relevant to the
assessment of `no-unmeasured confounding.' Although causal diagrams can
also be useful for examining structural features of measurement-error
bias and threats to target validity from sample/population mismatch,
certain of these threats are not amenable to Markov factorisation (see
\(\S 2.3\)). For instance, in \(\S 2.8\), we consider the limitations of
causal diagrams in addressing uncorrelated measurement error bias, which
does not manifest on causal diagrams. In \(\S 3.1.6\), we employ causal
diagrams to clarify threats to external validity from sample restriction
(effect-modifier-restriction bias). In these instances, we must
introduce colouring conventions that repurpose causal diagrams for
somewhat different problems than those they were originally designed to
address, namely, problems of confounding bias.

Finally, it is important to underscore that without randomisation, we
cannot fully ensure the no-unmeasured confounding assumption that
enables us to recover the missing counterfactuals we require to
consistently estimate causal effects from data
(\citeproc{ref-greifer2023}{Greifer \emph{et al.} 2023};
\citeproc{ref-stuart2015}{Stuart \emph{et al.} 2015}). Because we must
nearly always assume unmeasured confounding, the workflows of causal
data science must ultimately rely on sensitivity analyses to clarify how
much unmeasured confounding would be required to compromise a study's
findings (\citeproc{ref-vanderweele2019}{VanderWeele 2019}).

\paragraph{1.2.3 Assumption 3:
Positivity}\label{assumption-3-positivity}

We satisfy the positivity assumption if there is a non-zero probability
of receiving each treatment level for every combination of covariates
that occurs in the population. Where \(A\) is the exposure and \(L\) is
a vector of covariates, we say positivity is achieved if:

\[
0 < Pr(A = a | L = l) < 1, \quad \text{for all } a, l \text{ with } Pr(L = l) > 0
\]

There are two types of positivity violation:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Random non-positivity} occurs when an exposure is
  theoretically possible, but specific exposure levels are not
  represented in the data. Notably, random non-positivity is the only
  identifiability assumption verifiable with data.
\item
  \textbf{Deterministic non-positivity} occurs when the exposure is
  implausible by nature. For instance, a hysterectomy in biological
  males would appear biologically implausible.
\end{enumerate}

Satisfying the positivity assumption can present considerable data
challenges (\citeproc{ref-westreich2010}{Westreich and Cole 2010}).
Suppose we had access to extensive panel data that has tracked 20,000
individuals randomly sampled from the target population over three
years. Suppose further that we wanted to estimate a one-year causal
effect of weekly religious service attendance on charitable donations.
We control for baseline attendance to recover an incident exposure
effect estimate (see: \(\S 2.3\) and \textbf{Appendix 2}). Assume that
the natural transition rate from no religious service attendance to
weekly service attendance is low, say one in a thousand annually. In
that case, the effective sample for the treatment condition dwindles to
20. This example clarifies the problem. For rare exposures, the data
required for valid causal contrasts may be sparse, even in large
datasets. Where the positivity assumption is violated, causal diagrams
will be of limited utility because observations in the data do not
support valid causal inferences. (\textbf{Appendix 1} develops a worked
example that illustrates the difficulty of satisfying this assumption in
a setting of a cultural evolutionary question.)

\subsubsection{1.3 Conceptual, Data, and Modelling
Assumptions}\label{conceptual-data-and-modelling-assumptions}

We have reviewed the three fundamental assumptions of causal inference.
However, we must also consider further conceptual, data, and modelling
assumptions that, in addition to the foundational assumptions we just
reviewed, must also be satisfied to obtain valid causal inferences. We
next consider a subset of these assumptions. \textbf{Appendix B}
describes these further assumptions

\subsubsection{Summary of Part 1}\label{summary-of-part-1}

Causal data science is not ordinary data science. In causal data
science, the initial step involves formulating a precise causal question
that clearly identifies the exposure, outcome, and population of
interest. We must then satisfy the three fundamental assumptions
required for causal inference, which are implicit in the ideal of a
randomised experiment: causal consistency: ensuring outcomes at a
specific exposure level align with their counterfactual counterparts;
conditional exchangeability: the absence of unmeasured confounding;
positivity: the existence of a non-zero probability for each exposure
level across all covariate

\subsection{Part 2. Applications of Sequentially Ordered Causal Diagrams
for Understanding Structural Sources of
Bias}\label{part-2.-applications-of-sequentially-ordered-causal-diagrams-for-understanding-structural-sources-of-bias}

Having outlined basic features of the causal inference framework, we are
now in a position to use causal diagrams to elucidate elementary
structural sources of bias (\citeproc{ref-greenland1999}{Greenland
\emph{et al.} 1999a}; \citeproc{ref-pearl1995}{Pearl 1995},
\citeproc{ref-pearl2009}{2009a}). We begin by defining our terminology.

\subsubsection{2.1 Variable naming
conventions}\label{variable-naming-conventions}

@\#tbl-conventionsnaming presents our variable naming conventions.

\begin{table}

\caption{\label{tbl-conventionsnaming}Variable naming conventions in
this article.}

\centering{

\terminologylocalconventions 

}

\end{table}%

\subsubsection{2.2 Graphical conventions}\label{graphical-conventions}

\begin{table}

\caption{\label{tbl-conventionsgraph}Graphical conventions.}

\centering{

\terminologylocalconventions 

}

\end{table}%

\subsubsection{2.3 Terminology}\label{terminology}

\paragraph{2.3.1 Terminology pertaining to units, samples and
populations}\label{terminology-pertaining-to-units-samples-and-populations}

\textbf{Unit}: an entity, such as an object or person or culture, that
may be considered an individual member of a population of similar such
entities.

\textbf{Sample}: The collection of units observed and recorded in a
study. The concept of causation, which unfolds over time, necessitates
further distinctions:

\begin{itemize}
\item
  \textbf{Baseline sample}: the specific units present in a study at
  random-treatment assignment or pseudo-randomisation (in observational
  studies.)
\item
  \textbf{Censored sample}: the set of units remaining in a study after
  (pseudo)random-treatment assignment. This material collection of
  entities may over time as units drop out, and possibly re-enter the
  study. Although there can be multiple censored samples reflecting
  different points or conditions in the study. For example, if attrition
  is censoring.
\end{itemize}

\textbf{Population}: an abstract concept denoting a collection of units
characterised by certain features and events. Because causation unfolds
over time, it is helpful to distinguish between different types of
populations.

\begin{itemize}
\item
  \textbf{Restricted (selected) population}: a population is considered
  `restricted' relative to another population if its units share some
  but not all features of the larger group.
\item
  \textbf{Target population}: the population for which a study aims to
  generalise its findings, defined by a set of features and events,
  specified in advance of the study. For example, if a study is
  investigating the effects of religion on charity, its target
  population might be all people who identify as Christian in New
  Zealand (see; \textbf{Appendix 1}).
\item
  \textbf{Source population}: the actual population from which the
  study's sample is drawn. It could be a subset of the target
  population. For example, all individuals living in New Zealand who may
  be sampled from the New Zealand electoral roll. Whether the source
  population is a restriction of the target population depends on the
  details of a study. For example, if a study pertains to religious
  service, the source population might be broader if it included people
  who did not attend religious services. The source and target
  populations may be equivalent.
\item
  \textbf{Baseline sample population}: the population from which the
  units assigned to treatment are drawn. For example, those New
  Zealanders (from the source population) who, after being invited
  randomly from the New Zealand Electoral Roll to participate, enrol in
  the study are sampled from this population. Depending on the
  scientific question and context, investigators may use eligibility
  criteria to restrict the baseline sample population so that it better
  aligns with the target population. Investigators may also apply
  sampling weights to the baseline population to better align it with
  the target population (\citeproc{ref-cole2010generalizing}{Cole and
  Stuart 2010}). Because weights carry modelling assumptions, weighting
  must be done with care.
\item
  \textbf{Censored sample population}: over time, the units that
  comprise the baseline sample may change, for example, from attrition
  (right censoring). Importantly, population from which the censored
  units are drawn may no longer resemble the study population at
  baseline. Censoring is \emph{uninformative} if the change in units
  does not affect the outcome, or if there is no effect of treatment
  (the sharp causal null hypothesis). Censoring is \emph{informative} if
  the baseline population differs in the distribution of those features
  that modify the effect of the treatment, and no correction is applied.
  Unbiased effect estimates for the baseline population will
  nevertheless be biased for the target population in at least one
  measure of effect (\citeproc{ref-greenland2009commentary}{Greenland
  2009}; \citeproc{ref-lash2020}{Lash \emph{et al.} 2020}). This is why
  it is important for investigators to state a causal effect of interest
  with respect to \emph{the full data} that includes the counterfactual
  quantities for the treatments to be compared in a clearly defined
  target population and with a specific causal contrast
  (\citeproc{ref-westreich2017}{Westreich \emph{et al.} 2017a}).
\end{itemize}

\paragraph{2.3.2 General terminology pertaining to causal
inference}\label{general-terminology-pertaining-to-causal-inference}

\textbf{Randomised controlled experiment}: a study in which the
investigators assign units to treatments by chance. Random assignment
ensures an even distribution of both known and unknown confounders
across treatment groups, thus addressing \emph{the identification
problem.}

\textbf{Observational study}: a study in which treatment assignment is
not controlled by the investigators. In such studies, treatments
(equivalently `exposures') occur naturally without investigator
intervention. Workflows in causal inference aim to ensure balance in the
distribution of confounders across treatments. We cannot assume that
such methods can fully account for all confounding (\emph{the assumption
of unmeasured confounding}). No statistical test can verify the
assumption of no unmeasured confounding (\emph{the underdetermination of
causality by data}).

\textbf{Natural experiment}: an observational study in which treatment
assignment, although not controlled by the investigators, occurs in a
manner that approximates randomness, allow for stronger confidence of
unbiased causal effect estimates than is typical observational studies.
However, because nature might not flip coins as we think, assumptions of
random treatment assignment must be scrutinised.

\textbf{Average treatment effect (ATE)}: quantifies the contrast in the
average of the potential outcomes for the \emph{entire} population under
two treatment levels, at some scale of contrast, such as the difference
scale.

\textbf{Marginal effect}: synonym for the average treatment effect.

\textbf{Intent-to-treat effect}: the causal effect of random treatment
assignment. Ideally randomised experiments consistently estimate the
effect of randomisation to a condition, not adherence. If experimenters
simply select the sample that adheres to treatment (removing other cases
as irrelevant) they may introduce confounding. It is a mistake to select
participants into a study after randomisation on the basis of adherence
(Hernán \emph{et al.} (\citeproc{ref-hernan2017per}{2017})).

\textbf{Per-protocol effect}: the effect of adherence under
randomisation (\citeproc{ref-hernan2017per}{Hernán \emph{et al.} 2017}).
A safe assumption is that:
\(\widehat{ATE}_{\text{target}}^{\text{per-protocol}} \ne \widehat{ATE}_{\text{target}}^{\text{intent-to-treat}}\)
(\citeproc{ref-hernuxe1n2004}{Hernán 2004};
\citeproc{ref-tripepi2007}{Tripepi \emph{et al.} 2007}).

\textbf{Confounding bias}: the association between the exposure and the
outcome does not reflect a true causal association for the study
population because the potential outcomes are not independent of the
exposure, conditional on measure covariates: \(Y(a)\cancel\coprod A|L\).

\textbf{Measurement error bias}: a discrepancy between a variable's true
value and its observed or recorded value.

\textbf{Selection bias}: this term has different meanings in different
areas of causal inference. Our interest is in settings where the
association between the exposure and the outcome in a study population
does not reflect the causal association in the target population
(\citeproc{ref-hernuxe1n2017}{Hernán 2017}).

\textbf{Informative censoring}: occurs when the units in one's study
differ from the target population in ways that are related to the
underlying causal question, potentially introducing bias into causal
inferences (see: \(\S 3.1.5\)).

\textbf{Non-informative censoring:} the censoring mechanism is
independent of the outcome in the uncensored population and does not
itself introduce bias.

\textbf{Target validity}: we say that a study achieves target validity
if its results generalise to the target population. This concept is also
known as `external validity.'

\paragraph{2.3.2 Terminology pertaining to causal
diagrams}\label{terminology-pertaining-to-causal-diagrams}

\paragraph{2.3.2.1 Elements of causal
diagrams}\label{elements-of-causal-diagrams}

\textbf{Node}: characteristic or features of units in a population
(`variable') represented on a causal diagram. In a causal diagram, nodes
are drawn with reference to variables defomed for the target population.

\textbf{Arrow}: denotes a causal relationship linking nodes. Unless the
pathway pertains to the treatment/outcome path, all arrows are
\emph{assumed}. For example, \(L\to A\) denotes an assumed causal
relationship between \(A\) and \(Y\). A causal diagram evaluates whether
the causal relationship between exposure and outcome can be identified
according to the assumptions that are \emph{asserted} by a causal
diagram. Again, we do not \emph{assert} the \(A\to Y\) path, nor do we
\emph{assert} its absence. All other paths on the diagram must be
asserted. This should be done without regard to data collection. The
diagram should refer to causal pathways in the target population -- not
the sample population, unless the sample population is the target
population. Because these causal paths are asserted, causal diagrams
should be informed by expert judgement. \emph{With great power comes
great responsibility}.

Note that when estimating total effects, we should generally regard
nodes other than the exposure and outcome as nuisance parameters that
are of no intrinsic interest. Again when our interest is in estimating
average treatment effects for a target population, it is advisable to
report coefficients in statistical without interpretation because we
have no assurance that these estimates accurately reflect causation
(\citeproc{ref-westreich2013}{Westreich and Greenland 2013}). Given the
present state of the causal crisis, unless one has built a clearly
defined causal model for investigating effect-modification
(\citeproc{ref-athey2019}{Athey \emph{et al.} 2019};
\citeproc{ref-athey2021}{Athey and Wager 2021}), it is arguably better
not to report these nuisance regression coefficients at all.

\textbf{Ancestor (parent)}: a node with a direct or indirect influence
on others, positioned upstream in the causal chain.

\textbf{Descendant (child)}: a node influenced, directly or indirectly,
by upstream nodes (parents).

\textbf{Acyclic}: a causal diagram cannot contain feedback loops. More
precisely, no variable can be an ancestor or descendant of itself. If
variables are repeatedly measured here, it is especially important to
index nodes by the relative timing of the nodes.

\textbf{The identification problem}: the challenge of estimating the
causal effect of a variable using by adjusting for measured variables on
units in a study. Causal diagrams were developed to address the
identification problem by application of the rules of d-separation to a
causal diagram.

\textbf{Conditioning}: the process of explicitly accounting for a
variable in our statistical analysis to address the identification
problem. In causal diagrams, we usually represent conditioning by
drawing a box around a node of the conditioned variable, for example,
\(\boxed{L_{0}}\to A_{1} \to L_{2}\). We do not box exposures and
outcomes, because we assume they are included in a model by default.
Depending on the setting, we may condition by regression stratification,
inverse-probability of treatment weighting, g-methods, doubly robust
machine learning algorithms, or other methods.

\textbf{Adjustment set}: a collection of variables we must either
condition upon or deliberately avoid conditioning upon to obtain a
consistent causal estimate for the effect of interest
(\citeproc{ref-pearl2009}{Pearl 2009a}).

\textbf{Confounder}: a member of an adjustment set. Notice a variable is
a `confounder' in relation to a specific adjustment set. `Confounder' is
a relative concept (\citeproc{ref-lash2020}{Lash \emph{et al.} 2020}).

\textbf{Collider}: a variable in a causal diagram at which two incoming
paths meet head-to-head. For example if
\(A \rightarrowred \boxed{L} \leftarrowred Y\), then \(L\) is a
collider. If we do not condition on a collider (or its descendants), the
path between \(A\) and \(Y\) remains closed. Conditioning on a collider
(or its descendants) will induce an association between \(A\) and \(Y\)

\textbf{Effect-modifier}: a variable is an effect-modifier, or
`effect-measure modifier' if its presence changes the magnitude or
direction of the effect of an exposure or treatment on an outcome across
the levels or values of this variable. In other words, the effect of the
exposure is different at different levels of the effect-modifier.

\textbf{Instrumental variable}: an ancestor of the exposure but not of
the outcome. An instrumental variable affects the outcome only through
its effect on the exposure and not otherwise. Whereas conditioning on a
variable causally associated with the outcome but not with the exposure
will generally increase modelling precision, we should avoid
conditioning on instrumental variables
(\citeproc{ref-cinelli2022}{Cinelli \emph{et al.} 2022}). There are two
exceptions to this rule. First, we may be interested in instrumental
variable analysis (see \textbf{Lonati et al,} this issue). Second, when
an instrumental variable is the descendant of an unmeasured confounder,
we should generally condition the instrumental variable to provide a
partial adjustment for a confounder.

\paragraph{2.3.2.2 Markov factorisation and related
concepts}\label{markov-factorisation-and-related-concepts}

\textbf{Markov factorisation} represents the joint probability
distribution of all variables in a causal diagram. It asserts that this
joint distribution can be decomposed into a product of conditional
distributions, each corresponding to a variable conditioned on its
parents in the DAG. Formally, if we have variables
\(X_1, X_2, \dots, X_n\) and a causal diagram that encodes causal
assumptions about these variables, then the joint probability
distribution of these variables can be written as:

\[
P(X_1, X_2, \dots, X_n) = \prod_{i=1}^{n} P(X_i | \text{Parents}(X_i))
\]

Again, the structure of a causal diagram encodes assumptions about
causal relationships. Markov factorisation uses this structure to
decompose the joint probability distribution of the relationships
defined by a causal daiagram.

It does this by factoring the conditional independencies implied by the
causal diagram. If there is no direct path between two variables in the
causal diagram, and they do not share a common ancestor, the variables
are conditionally independent given each variables' parents. This is a
fundamental feature because it allows for the simplification of complex
joint distributions into more manageable parts. By breaking down the
joint probability into parts, Markov factorisation greatly simplifies
understanding of complex dependencies so they can be read from graphs
(\citeproc{ref-lauritzen1990}{Lauritzen \emph{et al.} 1990};
\citeproc{ref-pearl1988}{Pearl 1988}, \citeproc{ref-pearl2009a}{2009b}).

\textbf{Causal Markov assumption:} states that when conditioned on its
direct antecedents, any given variable is rendered independent from all
other variables that it does not cause (\citeproc{ref-hernan2023}{Hernan
and Robins 2023}). Thus, once we account for a variable's immediate
causes, it ceases to provide additional causal information about any
other variables in the system, except for those it directly causes. This
assumption allows for inferring the causal effects of interventions from
causal diagrams (\citeproc{ref-pearl2009a}{Pearl 2009b}).

\textbf{Compatibility}: the joint distribution of the variables is said
to be compatible with the graph if it upholds the conditional
independencies the graph implies (\citeproc{ref-pearl2009a}{Pearl
2009b}).

\textbf{Faithfulness}: a causal diagram is considered faithful to a
given set of data if all the conditional independencies present in the
data are accurately depicted in the graph. Conversely, the graph is
faithful if every dependency implied by the graph's structure can be
observed in the data. This concept ensures that the graphical
representation of relationships between variables aligns with the
empirical evidence (\citeproc{ref-pearl1995}{Pearl 1995}).

Although the assumption of faithfulness or `weak faithfulness' allows
for the possibility that some of the independencies in the data might
occur by coincidence (i.e., because of a cancellation of different
effects), the assumption of strong faithfulness does not. The strong
faithfulness condition assumes that the observed data's statistical
relationships directly reflect the underlying causal structure, with no
independence relationships arising purely by coincidental cancellations.
This is a stronger assumption than (weak) faithfulness and is often more
practical in real-world applications of causal inference. Note that the
faithfulness assumption (whether weak or strong) is not testable by
observed data -- it is an assumption about the relationship between the
observed data and the underlying causal structure.

\subsubsection{2.4 Rules of d-separation}\label{rules-of-d-separation}

\textbf{D-separation}: in a causal diagram, a path is `blocked' or
`d-separated' if a node along it interrupts causation. Two variables are
d-separated if all paths connecting them are blocked, making them
conditionally independent. Conversely, unblocked paths result in
`d-connected' variables, implying potential dependence
(\citeproc{ref-pearl1995}{Pearl 1995}).

The rules of d-separation are as follows:

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\item
  \textbf{Fork rule} (\(A \leftarrowred \boxed{L} \rightarrowred Y\)):
  \(A\) and \(Y\) are independent when conditioning on \(L\)
  (\(A \coprod Y | L\)) (\citeproc{ref-pearl1995}{Pearl 1995}).
\item
  \textbf{Chain rule} (\(A \to \boxed{L} \rightarrowdotted Y\)):
  Conditioning on \(L\) blocks the path between \(A\) and \(Y\)
  (\(A \coprod Y | L\)) (\citeproc{ref-pearl1995}{Pearl 1995}). (This
  will introduce bias if in reality, \(A \to Y\))
\item
  \textbf{Collider rule}
  (\(A \rightarrowred \boxed{L} \leftarrowred Y\)): \(A\) and \(Y\) are
  independent until conditioning on \(L\), which introduces dependence
  (\(A \cancel{\coprod} Y | L\)) (\citeproc{ref-pearl1995}{Pearl 1995}).
\end{enumerate}

According to the rules of d-separation:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  An open path (no variables conditioned on) is blocked only if two
  arrows point to the same node: \(A \rightarrowred L \leftarrowred Y\)
  is blocked. The node of common effect is called a \emph{collider}.
\item
  Conditioning on a collider does not block a path, such that
  \(A \rightarrowred \boxed{L} \leftarrowred Y\) may suggest an
  association of A with Y in the absence of causation.
\item
  Conditioning on a descendant of a collider does not block a path, such
  that if \(L \to \boxed{L'}\), then
  \(A \rightarrowred \boxed{L'} \leftarrowred Y\) is open. (Note: Unless
  our interest is causal mediation it is important to avoid conditioning
  on any mediator \(L\) in which
  (\(A \rightarrow {L} \rightarrowdotted Y\)). This may bias a true
  effect of \(A \to Y\).)
\item
  If a path does not contain a collider, any variable conditioned along
  the path is blocked, such that
  \(A \rightarrow \boxed{L} \rightarrowdotted Y\) blocks the path from
  \(A\to Y\); see Hernan and Robins (\citeproc{ref-hernan2023}{2023}), p
  78.
\end{enumerate}

To obtain an unbiased estimate for the causal effect of \(A\) on \(Y\),
we must block all backdoor paths. We do this by conditioning on a set of
covariates \(L\) that is sufficient to close all backdoor paths linking
\(A\) and \(Y\). A path is effectively blocked by \(L\) if it includes
at least one non-collider that is a member of \(L\), or if it does not
contain any collider or descendants of a collider. The `backdoor
criterion' uses the rules of d-separation to evaluate structural sources
of bias, and close all paths in a causal diagram that connect a
treatment (or exposure), \(A\), and an outcome, \(Y\), in the absence of
causation (\citeproc{ref-pearl2009a}{Pearl 2009b}).

\textbf{Modified Disjunctive Cause Criterion}: VanderWeele
(\citeproc{ref-vanderweele2019}{2019}) recommends obtaining a maximally
efficient adjustment which he calls a `confounder set' A member of this
set is any set of variables that can reduce or remove a structural
sources of bias. The strategy is as follows:

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\tightlist
\item
  Control for any variable that causes the exposure, the outcome, or
  both.
\item
  Control for any proxy for an unmeasured variable that is a shared
  cause of both the exposure and outcome.
\item
  Define an instrumental variable as a variable associated with the
  exposure but does not influence the outcome independently, except
  through the exposure. Exclude any instrumental variable that is not a
  proxy for an unmeasured confounder from the confounder set
  (\citeproc{ref-vanderweele2019}{VanderWeele 2019}).
\end{enumerate}

Note that the concept of a `confounder set' is broader than that of an
`adjustment set.' Every adjustment set is a member of a confounder set.
Hence, the Modified Disjunctive Cause Criterion will eliminate bias when
the data permit. However, a confounder set includes variables that will
reduce bias in cases where confounding cannot be eliminated.

This is a useful strategy because confounding can rarely be eliminated
with certainty (recall our \emph{the assumption of universal unmeasured
confounding}.) The \emph{Modified Disjunctive Cause Criterion} allows us
to mitigate bias as best we can in advance of sensitivity analyses that
evaluate the robustness of our results to unmeasured confounding.

Importantly, software tools such as the present versions of
\texttt{Dagitty} and \texttt{ggdag} (\citeproc{ref-barrett2021}{Barrett
2021}; \citeproc{ref-textor2011}{Textor \emph{et al.} 2011}), although
beneficial for identifying adjustment sets conditional on a supplied
model, may nevertheless overlook pragmatically optimal strategies for
confounding control. The software will not select the best confounder
set where unmeasured confounding persists. Therefore, reliance on these
tools should be balanced with independent causal diagram interpretation
skills. For this reason, I recommend learning to visually inspect graphs
to identify sources of bias and strategies for bias reduction, even when
bias cannot be fully eliminated. We shall discover that sequentially
ordered graphs greatly benefit the demands of such inspection.

\textbf{Prevalent exposure effect}: evaluates the association between
the exposure or treatment status at time \(t1\) and the outcome observed
at a later time \(t2\). It is defined by the pathway
\(A_{1} \to Y_{2}\). The prevalent exposure effect does not consider the
initial status of the exposure. It is expressed:

\[
\text{Prevalent exposure effect:} \quad A_{1} \to Y_{2}
\]

The prevalence exposure effect describes the effect of current or
ongoing exposures on outcomes. However, such and effect estimate is
rarely of interest, and risks pointing to erroneous conclusions. For
example where the effects of an exposure are deadly, the exposure will
select out those units who are susceptible from a population, leaving
only robust units. It may appear that \(A_{1} \to Y_{2}\) is helpful
when, in fact, the treatment is harmful; see: Hernán \emph{et al.}
(\citeproc{ref-hernuxe1n2016}{2016a}); Danaei \emph{et al.}
(\citeproc{ref-danaei2012}{2012}); VanderWeele \emph{et al.}
(\citeproc{ref-vanderweele2020}{2020}); Bulbulia
(\citeproc{ref-bulbulia2022}{2022}).

\textbf{Incident exposure effect}: evaluates the association between the
exposure or treatment status at time \(t1\) and the outcome observed at
a later time \(t2\) conditional on the baseline exposure:
\(A_{0} \to A_{1} \to Y_{2}\). This model more closely emulates an
experiment because it considers the transition in treatment or exposure
status from \(A_0\) to \(A_1\). The initiation of a treatment provides a
clearer intervention from which to estimate a causal effect at \(Y_2\).
It is expressed:

\[
\text{Incident exposure effect:} \quad \boxed{A_{0}} \to A_{1} \to Y_{2}
\]

Further control is obtained by including the baseline outcome \(Y_0\) as
well as the baseline exposure \(A_0\) such that:

\[
\boxed{
\begin{aligned}
L_{0} \\
A_{0} \\
Y_{0}
\end{aligned}
}
\to A_{1} \to Y_{2}
\]

The incident exposure effect better emulates a `target trial' or a the
organisation of observational data into a hypothetical experiment in
which there is a `time-zero' initiation of treatment in the data; see:
Hernán \emph{et al.} (\citeproc{ref-hernuxe1n2016}{2016a}); Danaei
\emph{et al.} (\citeproc{ref-danaei2012}{2012}); VanderWeele \emph{et
al.} (\citeproc{ref-vanderweele2020}{2020}); Bulbulia
(\citeproc{ref-bulbulia2022}{2022}). \emph{To obtain the incident
exposure effect, we generally require that events in the data can be
accurately classified into at least three relative time intervals.}

\textbf{Time-varying confounding:} occurs when a confounder that changes
over time also acts as a mediator or collider in the causal pathway
between exposure and outcome. Controlling for such a confounder can
introduce bias. Not controlling for it can retain bias. We discuss
time-varying confounders in \textbf{Part 3}.

\textbf{Statistical model:} a mathematical representation of the
relationships between variables in which we quantify covariances and
their corresponding uncertainties in the data. Statistical models
typically correspond to multiple causal structures
(\citeproc{ref-hernan2023}{Hernan and Robins 2023};
\citeproc{ref-pearl2018}{Pearl and Mackenzie 2018};
\citeproc{ref-vanderweele2022b}{VanderWeele and Vansteelandt 2022}).
That is, the causes of such covariances cannot be identified without
assumptions.

\textbf{Structural model:} defines assumptions about causal
relationships. Causal diagrams graphically encode these assumptions
(\citeproc{ref-hernan2023}{Hernan and Robins 2023}), leaving out the
assumption about whether the exposure and outcome are causally
associated. Outside of randomised experiments, we cannot compute causal
effects in the absence of structural models. A structural model is
needed to interpret the statistical findings in causal terms. Structural
assumptions should be developed in consultation with experts. The role
of structural assumptions when interpreting statistical results remains
poorly understood across many human sciences and forms the motivation
for my work here.

\subsubsection{2.5 Further terminology (my
conventions)}\label{further-terminology-my-conventions}

\textbf{Causal incoherence}: the relative temporal order of events
necessary to evaluate causation is not maintained. For example, if
investigators stratify on post-treatment mediators or colliders, or if
they attempt to estimate \(A_2 \rightarrowred Y_0\), their study will be
causally incoherent. Sequentially ordered causal diagrams assist
investigators in avoiding \emph{causal incoherence}. Note that the
`relative timing of events' includes the restriction (or `selection') of
units in one's study. For example, if the study at baseline is
restricted, yet the restriction is not considered, the relative temporal
order of the events necessary to evaluate causation is not maintained.

\textbf{Estimands first}: before answering a causal question, ask it by
stating a clearly defined causal contrast and scale of contrast, for a
well-defined target population. That is, put estimands first.

\textbf{Under-determination of causality by the data}: when constructing
causal diagrams, investigators must assume all causal paths depicted in
the diagram, except the direct path from exposure to outcome, which must
remain open in the presence of a true treatment effect and closed in its
absence. Additionally, investigators must rigorously consider the
implications of measurement error, interactions between exposure and
outcome, and mismatches between relevant characteristics of the units in
the sample and those of the target population. The data do not determine
most assumptions required for causal inference. A causal diagram
qualitatively encodes these investigator-determined assumptions.

\textbf{The assumption of ubiquitous effect-modifiers}: assume the
causal effect of the exposure on the outcome is modified by other
variables. This `assumption' cannot generally be verified with data. It
is motivated by caution.

\textbf{The assumption of informative censoring}: assume
effect-modifier-restriction bias from an unit attrition in one's sample
(censoring). Note, this assumption applies whether attrition induces
confounding bias, linking exposure to outcome via a back-door path. It
also applies to potential mismatch between the baseline population and
the target population in the distributions of treatment effect
modifiers. This assumption invokes \emph{the assumption of ubiquitous
effect-modifiers}.

\textbf{The assumption of universal measurement error:} nearly every
instrument measures with error. Assess the structural role that
measurement error plays in biasing effect estimates. This `assumption'
cannot generally be verified with data. It is motivated by caution.

\textbf{Selection bias Rorschach test:} the term `selection bias' is
like a Rorschach test everyone sees something different. Causal diagrams
function as interpreters that clarifying the exact problem an
investigator has in mind.

\textbf{The assumption of universal unmeasured confounding}: even if the
if treatment-assignment is not random, if you seek inference for a
per-protocol effect, assume unmeasured confounding.

\textbf{Sequentially ordered causal diagram}: A causal diagram is deemed
`sequential' when its nodes and arrows are arranged to consistently
reflect the progression of time. In such diagrams, the spatial
representation follows the temporal sequence. As such, the graphs are
`ordered'. Conversely, if the arrangement does not adhere to the
temporal sequence, the diagram is described as `disordered'. For
instance, \(L_0 \to A_1 \to Y_2\) presents an ordered structure, while
\(A_1 \leftarrow L_0 \to Y_2\) presents a disordered structure. Despite
their mathematical equivalence, sequentially structured causal diagrams
provide clearer insights into the data collection requirements due to
their temporal coherence.

\textbf{Your DAG is always wrong (advice)}: your causal diagram (DAG)
should only include as much information as required to address your
causal question. By design your causal diagram (DAG) is `wrong'
representation of causality because it is only meant to show those
features of a causal system that must be understood to obtain valid
causal effect estimates. If your causal diagram is not wrong, you should
consider making it so.

\textbf{Make your DAG good (responsibility)}: \emph{the assumption of
universal unmeasured confounding} implies that your causal diagrams
should present unmeasured sources of confounding.

\textbf{With great power comes great responsibility:} follows from the
\emph{under-determination of causality by the data.} The data do not
inherently reveal causal relationships; instead, causal relationships
are discerned through the frameworks of theory and assumptions of
science. With the power to shape interpretations and draw conclusions
from \emph{assumptions} comes the significant responsibility to act
openly, judiciously, and thoroughly.

\textbf{With great responsibility comes great baggage:} causal data
science is hard. It demands rigour, both in data analysis and
collection.

\subsubsection{2.6 Advice for drawing a sequentially ordered causal
diagram}\label{advice-for-drawing-a-sequentially-ordered-causal-diagram}

A causal diagram is intended to succinctly depict structural sources of
bias, rather than to represent data statistically. This distinction is
fundamental because the structure suggested by a causal diagram is often
not verifiable by data, making it `structural' in nature, as distinct
from the graphs used in structural equation modelling
(\citeproc{ref-bulbulia2022}{Bulbulia 2022};
\citeproc{ref-greenland1999c}{Greenland \emph{et al.} 1999b};
\citeproc{ref-hernan2023}{Hernan and Robins 2023};
\citeproc{ref-pearl2009a}{Pearl 2009b}). Misunderstanding this
difference between structural and statistical models has led to
considerable confusion across the human sciences
(\citeproc{ref-vanderweele2015}{VanderWeele 2015};
\citeproc{ref-vanderweele2022}{VanderWeele 2022};
\citeproc{ref-vanderweele2022b}{VanderWeele and Vansteelandt 2022}).

Although a sequentially ordered causal diagram is mathematically
identical to one without such order, the following examples reveal that
`sequential hygiene' in a diagram's layout can considerably enhance the
understanding of causal relationships. A sequentially hygienic graph
aligns the arrangement of nodes and arrows to reflect the assumed
temporal sequence of events. The conventions I adopt for maintaining
sequential hygiene are:

\textbf{Clearly define all nodes on the graph}: ambiguity leads to
confusion.

\textbf{Simplify the graph by combining nodes where this is possible:}
Keep only those nodes and edges essential for clarifying the
identification problem at hand. Make your DAG wrong (i.e.~simplify.)

\textbf{Present unmeasured confounders}: they cannot be ruled out. Make
your DAG good.

\textbf{Apply a multi-step graphing strategy}: we must address
confounding bias, measurement-error bias, and threats to target
validity. Addressing all-threats in one graph stands in tension with the
demand for simplicity. Hernan and Robins
(\citeproc{ref-hernan2023}{2023}) suggests that we initially, isolate
confounding bias and `selection bias' (threats to target validity, which
may include confounding bias), then contemplate measurement bias using a
secondary graph. Their argument is that we may require multiple graphs
to retain focus when addressing structural sources of bias, see Hernan
and Robins (\citeproc{ref-hernan2023}{2023}) p.125.

In short, to \emph{make your DAG both wrong and good} may require making
multiple causal diagrams.

\textbf{Define the graph for the target population}: As we shall see in
\(\S 3.1.6\) causal effects may be unbiased in a restricted population
yet fail to generalise to the target population. For this reason, it is
important to represent graph causal assumptions for the target
population. We denote censoring events using the convention
\$Y\^{}\{\text{C}\}\leftarrowblue U\_\{C=1\}\rightarrowblue \$ which
denotes the distribution of the covariates \(L\) may differ in the
restricted popoulation \(C=1\) compared with the unrestricted population
\(C = 1\), where the counterfactual outcomes are stated for the
uncensored sample population \(Y^{\text{C}}\), the potential outcomes
for the uncensored (target) population.

\textbf{Maintain sequential order in the spatial organisation of the
graph:} Generally arrange nodes in \emph{relative} temporal sequence,
usually from left to right or top to bottom. Although drawing the
sequence to scale is unnecessary, the order of events should be clear
from the layout. This provides an intuitive visual representation of how
one event is assumed to precede another in time.

\textbf{Time-index nodes}: in addition to spatially ordering one's graph
to match the flow of time, it is often helpful to time-index nodes by
the relative appearance of the event in time. This explicit indexing
helps in demarcating the temporal relationship between variables, adding
precision to the diagram with the organisation:

\[\boxed{L_{0}} \to A_{1} \to Y_{2}\]

This arrangement clearly illustrates the temporal sequence of these
variables. When measures are repeated, it is essential to time-index
nodes. For example, a powerful confounding control strategy is to
condition on baseline exposure and outcome. Because causal diagrams must
be acyclic, indexing the nodes allows us to keep track of the repeated
measurements.

\[
\boxed{
\begin{aligned}
A_{0} \\
Y_{0}
\end{aligned}
}
\to A_{1} \to Y_{2}
\]

\textbf{Define any novel convention in a causal diagram explicitly}: do
not assume familiarity.

\textbf{Ensure acyclicity in the graph}: This guarantees that a node
cannot be its own ancestor, thereby eliminating circular paths.

\textbf{Draw nodes for unmeasured confounders}: assume unmeasured
confounding always exists, whether depicted on the graph or not. This
assumption reveals the importance of sensitivity analyses when
estimating causal effects.

\textbf{Illustrate nodes for informative censoring.} This facilitates
understanding of potential sources of selection bias (see:
\(\S 3.1.6\)).

\textbf{Causal diagrams are not designed to present non-linear effects}
Do not draw arrows into arrows. Causal diagrams are qualitative tools
that encode assumptions about causal relationships and dependencies.
When we extend these diagrams to clarify structural threats to validity
from measurement error bias and sample-restriction bias, some of our
conventions will be `off-label.' Strictly speaking, causal diagrams
facilitate a spatial Markov factorisation of causal networks to identify
structural sources of bias within these networks.

\subsubsection{2.7 The four elemental structural sources of confounding
bias}\label{the-four-elemental-structural-sources-of-confounding-bias}

We have reviewed key terminology, conventions, and rules. It is time to
put causal diagrams into action, focusing on what Richard McElreath
calls the `four fundamental confounders' see: McElreath
(\citeproc{ref-mcelreath2020}{2020}) p.185. Note that because we
distinguish between the concepts of `confounders' and `confounding', we
will examine the four elemental structural sources of confounding bias.

\paragraph{2.7.1. The elemental structural source of confounding bias
from an unadjusted common
cause}\label{the-elemental-structural-source-of-confounding-bias-from-an-unadjusted-common-cause}

The first elemental structural source of bias arises when there is a
common cause, \(L\), of the exposure, \(A\), and outcome, \(Y\). In this
setting, \(L\) may create a statistical association between \(A\) and
\(Y\), implying causation in its absence.

Consider an example where smoking, \(L\), is a common cause of both
yellow fingers, \(A\), and cancer, \(Y\). Here, \(A\) and \(Y\) may show
an association without causation. If we were to intervene to scrub the
hands of smokers, this would not affect their cancer rates.
Figure~\ref{fig-dag-common-cause} represents this elemental bias, where
the red arrow signifies the bias from the open path connecting \(A\) and
\(Y\), caused by their common cause \(L\).

\begin{figure}

\centering{

\includegraphics[width=0.8\textwidth,height=\textheight]{short-causal-diagrams_files/figure-pdf/fig-dag-common-cause-1.pdf}

}

\caption{\label{fig-dag-common-cause}Confounding by a common cause. The
red path indicates bias from the open backdoor path from A to Y.}

\end{figure}%

\paragraph{\texorpdfstring{2.7.1.2 Advice: avoid causal incoherence by
ensuring \(L\) occurs before
\(A\)}{2.7.1.2 Advice: avoid causal incoherence by ensuring L occurs before A}}\label{advice-avoid-causal-incoherence-by-ensuring-l-occurs-before-a}

To address confounding by a common cause, we should adjust for it by
blocking the backdoor path from the exposure to the outcome. Such
adjustment will restore balance across the levels of \(A\) in the
distribution of confounders that might affect the potential outcomes to
be contrasted, \(Y(a*), Y(a)\). Again, standard methods for adjustment
include regression, matching, inverse probability of treatment
weighting, classical G-methods (\citeproc{ref-hernan2023}{Hernan and
Robins 2023}), and more recently, targeted learning frameworks
(\citeproc{ref-hoffman2023}{Hoffman \emph{et al.} 2023}).

Figure~\ref{fig-dag-common-cause-solution} quickly reveals what is
needed:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Ensure \(A_1\) occurs before \(Y_2\).
\item
  Only condition on \(L_0\) if \(L_0\) occurs before \(A_1\).
\end{enumerate}

\begin{figure}

\centering{

\includegraphics[width=0.8\textwidth,height=\textheight]{short-causal-diagrams_files/figure-pdf/fig-dag-common-cause-solution-1.pdf}

}

\caption{\label{fig-dag-common-cause-solution}Solution: adjust for
pre-exposure confounder. The implication: obtain time series data to
ensure that confounders occur before the exposure.}

\end{figure}%

After time-indexing the nodes on the graph, it becomes evident that
\emph{control of confounding generally requires being able to accurately
identify the relative timing of events in our data.} Timing is
everything, as they say. For instance, if the data contain measurements
of \(A\) and \(Y\), we must ensure that the data support \(A_1\to Y_2\)
and not \(A_2 \to Y_1\). Consider if an experiment reported that they
were unsure whether outcomes were recorded prior to or following the
experiment, or if it was deemed sufficient to report correlations
without precise timing of the experiment. `Might have been before
treatment, might have been after.' We would be confused. Yet, this is
the setting we often face with many, albeit not all, cross-sectional
data sets.

To clarify, cross-sectional data can be relevant for causal inference,
provided that the data align with the assumptions outlined in a causal
diagram. For example, in cases where exposures are infrequent, we may
opt for retrospective data collection, mindful of how measurement error
in recollected data might pave pathways for bias, as illustrated in
examples from Barrett (\citeproc{ref-barrett2021}{2021}). Importantly, a
causal diagram should ideally be conceptualised prior to data
collection, ensuring that causal demands are not merely shaped by the
available data. Once established, a sequentially ordered causal diagram
becomes instrumental in illuminating the risks associated with relying
on data where the sequence of events encoded in the graph cannot be
justifiably assumed for the measured variables. Notably, concerns
persist even with precise time-series data, particularly in scenarios
where clear baseline values for treatment initiation are undefined or
undetermined (\citeproc{ref-hernuxe1n2016}{Hernán \emph{et al.} 2016a}).
In numerous cross-sectional datasets, we risk modelling relationships
incorrectly, such as \(Y_2 \to A_1\) or
\(Y_1 \rightarrowred \boxed{L_3} \leftarrowred A_1\). These challenges,
elucidated by sequentially ordered causal diagrams, highlight the
importance of meticulous planning in data collection or comprehensive
validating assumptions for existing data if investigators are to
circumvent \emph{causally incoherent} models.

\textbf{Appendix 2} clarifies the advantages of time-series models that
include baseline values for both the outcome and the exposure in units
tracked over time. Such models facilitate determining the relative
timing of not merely of relevant events but of their initiation, such
that:

\[
U \to
\boxed{
\begin{aligned}
L_{0} \\
A_{0} \\
Y_{0}
\end{aligned}
}
\to A_{1} \to Y_{2}
\]

When a sequence of events that includes baseline values of the exposure
and outcome can be discerned from the data, and provided that other
critical conditions are met --- including those related to measurement
error (\citeproc{ref-blackwell2017}{Blackwell \emph{et al.} 2017};
\citeproc{ref-bulbulia2023e}{Bulbulia 2023a};
\citeproc{ref-hernuxe1n2009}{Hernán and Cole 2009};
\citeproc{ref-vanderweele2012a}{VanderWeele and Hernán 2012a}),
selection and sampling biases (\citeproc{ref-hernuxe1n2004a}{Hernán
\emph{et al.} 2004}; \citeproc{ref-hernuxe1n2017}{Hernán 2017};
\citeproc{ref-lu2022}{Lu \emph{et al.} 2022a};
\citeproc{ref-westreich2012berkson}{Westreich 2012}), and the correct
specification of all models --- our confidence in causal inferences is
bolstered. Although sequential order avoids \emph{causal incoherence},
few observational datasets can guarantee complete protection against
bias even with appropriately sequenced variables. Consequently, no
causal inference workflow is complete without the inclusion of
sensitivity analyses (\citeproc{ref-vanderweele2017}{VanderWeele and
Ding 2017}), a recurring mantra in this guide

\paragraph{2.7.2. The elemental structural bias from conditioning on a
mediator}\label{the-elemental-structural-bias-from-conditioning-on-a-mediator}

If we condition on \(L\) and it forms part of the causal pathway linking
the treatment and the outcome, conditioning on \(L\) may bias the effect
of \(A\) on \(Y\). Here, we focus on \emph{mediator bias}.

Take `beliefs in big Gods' to be the treatment \(A_{0}\), `social
complexity' to be the outcome \(Y_{2}\), and `economic trade' to be the
stratified mediator \(L_{1}\).

In this example, beliefs in big Gods \(A_{0}\) directly influence
economic trade \(L_{1}\), which then affects social complexity
\(Y_{2}\). Conditioning on economic trade, \(L_{1}\), will downwardly
bias estimates of the total effect of beliefs in Big Gods \(A\) on
social complexity \(Y_{2}\). Figure~\ref{fig-dag-mediator} presents this
problem.

\begin{figure}

\centering{

\includegraphics[width=0.8\textwidth,height=\textheight]{short-causal-diagrams_files/figure-pdf/fig-dag-mediator-1.pdf}

}

\caption{\label{fig-dag-mediator}Bias from conditioning on a mediator.
The dashed red arrow indicates bias arising from partially blocking the
path between A and Y. Here, a true effect of A on Y is attenuated.}

\end{figure}%

\paragraph{\texorpdfstring{2.7.2.2 Advice: avoid causal incoherence by
ensuring \(L\) occurs before
\(A\)}{2.7.2.2 Advice: avoid causal incoherence by ensuring L occurs before A}}\label{advice-avoid-causal-incoherence-by-ensuring-l-occurs-before-a-1}

Figure~\ref{fig-dag-common-effect-solution-2} presents the solution. We
have encountered the solution before. To avoid mediator bias:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Ensure \(A_1\) occurs before \(Y_2\).
\item
  Only condition on \(L_0\) if \(L_0\) occurs before \(A_1\).
\end{enumerate}

Our sequentially ordered causal diagram shows demands on data collection
and for data integrity. If we are interested in estimating the total
effect of \(A\to Y\), we must ensure we have measured the relative
timing in the occurrences of \(L\), \(A\), and \(Y\).

Note that mediator bias can and does occur in randomised experiments,
wherever the measured variables, such as demographic variables, are
taken \emph{after} random assignment. Do not `control' for
post-treatment variables in an experiment, and collect demographic
information prior to administering the treatment! See: Montgomery
\emph{et al.} (\citeproc{ref-montgomery2018}{2018}).

\begin{figure}

\centering{

\includegraphics[width=0.8\textwidth,height=\textheight]{short-causal-diagrams_files/figure-pdf/fig-dag-common-effect-solution-2-1.pdf}

}

\caption{\label{fig-dag-common-effect-solution-2}Solution: we avoid
mediator bias by ensuring the correct temporal measurement of the
confounder. Here, we assume confounding control conditional on measured
co-variates L. We draw the black path between A and Y because this path
will not be biased when there is a true causal effect of A on Y.}

\end{figure}%

\paragraph{2.7.3 The elemental structural bias from conditioning on a
common
effect.}\label{the-elemental-structural-bias-from-conditioning-on-a-common-effect.}

\paragraph{2.7.3.1.1 Case when the collider is a common effect of the
exposure and
outcome}\label{case-when-the-collider-is-a-common-effect-of-the-exposure-and-outcome}

Consider a scenario in which a variable \(L\) is affected by the
treatment \(A\) and outcome \(Y\) (\citeproc{ref-cole2010}{Cole \emph{et
al.} 2010}). According to the rules of d-separation, conditioning on a
common effect, \(L\), will open a non-causal association between \(A\)
and \(Y\). In mathematical terms, when \(A\) and \(Y\) are independent,
their joint probability should equal the product of their individual
probabilities: \(P(A, Y) = P(A)P(Y)\). However, conditioning on \(L\)
alters this relationship. The joint probability of \(A\) and \(Y\) given
\(L\), \(P(A, Y | L)\), does not equal the product of \(P(A | L)\) and
\(P(Y | L)\). Thus, the common effect \(L\) creates an association
between \(A\) and \(Y\) that is not causal.

Imagine a randomised experiment investigating the effects of different
settings on individuals' self-rated health. In this study, participants
are assigned to either civic settings (e.g., community centres) or
religious settings (e.g., places of worship). The exposure of interest,
\(A\), is the type of setting, and the outcome, \(Y\), is self-rated
health. Suppose there is no effect of setting on self-rated health.
However, suppose both setting and rated health independently influence a
third variable: cooperativeness. Specifically, imagine religious
settings encourage cooperative behaviour, and at the same time,
individuals with better self-rated health are more likely to engage
cooperatively.

Suppose the investigators decide to condition on cooperativeness, which
is the common effect of an \(A\), and the outcome \(Y\). Their rational
might be to study the effects of setting on health among those who are
more cooperative, or perhaps to `control for' cooperation in the health
effects of religious setting. By introducing such `control' the
investigators would inadvertently introduce collider bias, because the
control variable is a common effect of the exposure and the outcome.

\begin{figure}

\centering{

\includegraphics[width=0.8\textwidth,height=\textheight]{short-causal-diagrams_files/figure-pdf/fig-dag-common-effect-1.pdf}

}

\caption{\label{fig-dag-common-effect}Bias from conditioning on a
collider. The red path indicates bias from the open backdoor path from A
to Y.}

\end{figure}%

\paragraph{\texorpdfstring{2.7.3.1.2 Advice: avoid causal incoherence by
ensuring \(L\) occurs before
\(A\)}{2.7.3.1.2 Advice: avoid causal incoherence by ensuring L occurs before A}}\label{advice-avoid-causal-incoherence-by-ensuring-l-occurs-before-a-2}

We have encountered the solution to this problem before. To avoid
collider bias:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Ensure \(A_1\) occurs before \(Y_2\).
\item
  Only condition on \(L_0\) if \(L_0\) occurs before \(A_1\).
\end{enumerate}

Figure~\ref{fig-dag-common-effect-solution-3} repeats the previous
solutions given in Figure~\ref{fig-dag-common-cause-solution},
Figure~\ref{fig-dag-common-effect}, and
Figure~\ref{fig-dag-common-effect-solution-2}. We are again directed to
demands for ensuring the relative timing in the occurrence of the
variables we need to model. To quantitatively model causality, we must
accurately locate the relative occurrence of \(L\), \(A\), and \(Y\) in
time. This strategy helps to avoid the self-inflicted injury of
conditioning on a collider of the exposure and outcome. Note that with
much cross-sectional data, we cannot ensure that \(L\) occurs before
\(A\) and \(Y\). Unless the relative timing of events can be extracted,
cross-sectional data tempts \emph{causally incoherent} confounding
control strategies.

\begin{figure}

\centering{

\includegraphics[width=0.8\textwidth,height=\textheight]{short-causal-diagrams_files/figure-pdf/fig-dag-common-effect-solution-3-1.pdf}

}

\caption{\label{fig-dag-common-effect-solution-3}Solution: we ensure
that A and Y are d-separated by ensuring L occurs before A occurs.}

\end{figure}%

\paragraph{2.7.3.2.1 Case when the collider is the effect of exposure
but not of the
outcome}\label{case-when-the-collider-is-the-effect-of-exposure-but-not-of-the-outcome}

We have considered how mediator bias may attenuate the total effect
estimate of \(A\) on \(Y\). However, we should not imagine that
conditioning on the effect of an exposure will always bias effect
estimates downward. Consider a scenario in which \(L\) is affected by
both the exposure \(A\) and an unmeasured variable \(U\) related to the
outcome \(Y\) but not to \(A\). Assume that there is no causal effect of
\(A\) on \(Y\). In this scenario, conditioning on \(L\) introduces bias
by opening a backdoor path between \(A\) and \(Y\).

For example, consider a randomised experiment designed to assess the
effect of setting on self-rated health, comparing civic settings with
religious settings. Assume that the setting, \(A\), does not actually
affect health, \(Y\). However, suppose that religious settings
inadvertently increase people's cooperativeness, \(L\). Also, imagine
there are prior health conditions, an unmeasured variable \(U\), that
influence both cooperativeness and self-rated health. If the researchers
condition their analysis on cooperativeness, a side effect of the
religious setting, they might inadvertently create a spurious link from
the setting to self-rated health. This could suggest a causal effect
where none exists. In contrast, had the researchers not conditioned on
this post-treatment variable, cooperativeness, the influence of the
unmeasured confounder, \(U\), on health would have remained balanced
across settings, avoiding the introduction of collider bias.
Figure~\ref{fig-dag-descendant} presents these biasing paths in red.

\begin{figure}

\centering{

\includegraphics[width=0.8\textwidth,height=\textheight]{short-causal-diagrams_files/figure-pdf/fig-dag-descendant-1.pdf}

}

\caption{\label{fig-dag-descendant}Confounding by descent: the red path
illustrates the biasing path introduced by conditioning on a variable L
that is affected by the treatment in a setting where an unmeasured
confounder U affects both L and the outcome. Although L is not a
mediator, conditioning on post-treatment variable L opens a backdoor
path between A and Y.}

\end{figure}%

Figure~\ref{fig-dag-descendant} shows the setting of post-exposure
\emph{collider bias}. Conditioning on the collider \(L_{1}\) in the
analysis induces a non-causal association between \(A_{0}\) and
\(Y_{2}\).

\paragraph{\texorpdfstring{2.7.3.2.2 Advice: avoid causal incoherence by
ensuring \(L\) occurs before
\(A\)}{2.7.3.2.2 Advice: avoid causal incoherence by ensuring L occurs before A}}\label{advice-avoid-causal-incoherence-by-ensuring-l-occurs-before-a-3}

The strategy builds on the strategy presented in
Figure~\ref{fig-dag-common-cause-solution},
Figure~\ref{fig-dag-common-effect}, and
Figure~\ref{fig-dag-common-effect-solution-2} and
Figure~\ref{fig-dag-common-effect-solution-3}.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Ensure \(A_1\) occurs before \(Y_2\).
\item
  Only condition on \(L_0\) if \(L_0\) occurs before \(A_1\).
\end{enumerate}

\begin{figure}

\centering{

\includegraphics[width=0.8\textwidth,height=\textheight]{short-causal-diagrams_files/figure-pdf/fig-dag-descendant-solution-1.pdf}

}

\caption{\label{fig-dag-descendant-solution}This problem of confounding
by descent is solved by conditioning on the pre-exposure confounder L,
that is, by ensuring it is not measured post-exposure.}

\end{figure}%

\paragraph{2.7.3.3.1 Case of conditioning on a pre-exposure collider
(M-bias)}\label{case-of-conditioning-on-a-pre-exposure-collider-m-bias}

\begin{figure}

\centering{

\includegraphics[width=0.8\textwidth,height=\textheight]{short-causal-diagrams_files/figure-pdf/fig-m-bias-1.pdf}

}

\caption{\label{fig-m-bias}M-bias: Control by including previous outcome
measures. The problem arises because L is a collider of unmeasured
confounders U1 and U2, and conditioning on L opens a path from U2 to U1
to A and Y, d-connecting A and Y. This path is shown in red.}

\end{figure}%

One must be cautious not to over-condition on pre-exposure variables. In
settings where we condition on a variable that is itself not associated
with the exposure or outcome but is the descendant of an unmeasured
instrumental variable as well as of an unmeasured cause of the outcome,
we may inadvertently induce biasing path known as `M-bias', illustrated
in Figure~\ref{fig-m-bias},

M-bias can arise even though a variable \(L\) that induces it occurs
before the treatment \(A\). Conditioning on \(L\) creates a spurious
association between \(A\) and \(Y\) by opening the path between the
unmeasured confounders. We assume that \(A\) and \(Y\) d-separated, and
as such, \(A \coprod Y(a)\). However, when stratified by \(L\), this
independence is violated: \(A \cancel{\coprod} Y(a)| L\). This form of
bias is another manifestation of collider stratification bias.

Note: when the path is ordered sequentially from left to right, the `M'
shape, giving M-bias its name, changes to an `E' shape. However, we
retain the term `M-bias' to spare terminological confusion.

\paragraph{\texorpdfstring{2.7.3.3.2 Advice: causal coherence is not
sufficient: do not condition \emph{indiscriminately} on pre-exposure
variables}{2.7.3.3.2 Advice: causal coherence is not sufficient: do not condition indiscriminately on pre-exposure variables}}\label{advice-causal-coherence-is-not-sufficient-do-not-condition-indiscriminately-on-pre-exposure-variables}

\begin{figure}

\centering{

\includegraphics[width=0.8\textwidth,height=\textheight]{short-causal-diagrams_files/figure-pdf/fig-m-bias-solution-1.pdf}

}

\caption{\label{fig-m-bias-solution}M-bias is avoided by not
conditioning on pre-exposure variable L. Advice: be weary of McElreath's
`Causal Salad', and instead condition indiscriminately on pre-exposure
variables in L.}

\end{figure}%

Figure~\ref{fig-m-bias-solution} illustrates the solution to the issue
of conditioning on a pre-exposure collider, which can lead to M-bias.
The mere occurrence of a confounder prior to exposure does not justify
indiscriminate conditioning.

In theory, the guidance is clear: steer clear of an arbitrary approach
to confounding control. Richard McElreath aptly calls the arbitrary
method, `the causal salad' (\citeproc{ref-mcelreath2020}{McElreath
2020}).

In practice, however, making decisions about conditioning demands
careful consideration of the specific causal question being addressed.
It is often challenging to discern whether a variable is a collider, a
confounder, or a proxy for either. It is crucial to remember that a
causal diagram \emph{stipulates} all relevant causal paths, except for
the one between exposure and outcome. These stipulations are a great
power that carries a correspondingly great responsibility. Across causal
data science, there is a consensus that the insights of subject-matter
experts are vital in developing a causal diagram. These assumed
structures, once defined, guide the investigators' decisions in data
modelling. The example of M-bias highlights a critical point: accurate
temporal measurement of variables does not negate the potential for bias
when conditioning on a pre-exposure variable.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Ensure \(A_1\) occurs before \(Y_2\).
\item
  Ensure \(L_0\) occurs before \(A_1\).
\item
  If \(L_0\) is not a common cause of \(A_1\) and \(Y_2\), ensure it is
  the descendant (proxy) of an unmeasured common cause.
\end{enumerate}

\paragraph{2.7.4 The elemental structural bias of conditioning on a
descendant of a
collider}\label{the-elemental-structural-bias-of-conditioning-on-a-descendant-of-a-collider}

\paragraph{2.7.4.1.1 Case when conditioning on a descendant of a
collider creates
bias}\label{case-when-conditioning-on-a-descendant-of-a-collider-creates-bias}

Consider a randomised experiment designed to assess the effect of
setting on self-rated health, comparing civic settings with religious
settings. As discussed in previous examples, conditioning on
cooperativeness opens a non-causal path between the exposure (setting)
and the outcome (self-rated health). Now, suppose the investigators do
not directly condition on cooperativeness but instead on a proxy for
cooperativeness, such as `community involvement,' defined as the extent
of individuals' engagement in community activities or initiatives. This
proxy is likely influenced by the type of setting, as religious settings
might encourage more community engagement, and at the same time,
individuals with better self-rated health might be more active in their
communities. By conditioning on `community involvement,' a proxy for
cooperation, the investigators might inadvertently introduce bias by
inducing a spurious link between the setting and self-rated health,
similar to the bias introduced by directly conditioning on
cooperativeness. The variable \(L'\) in
Figure~\ref{fig-dag-descendant-proxy} denotes the proxy; the red paths
reveal the open back-door path that arises from conditioning on a proxy
of a collider.

\begin{figure}

\centering{

\includegraphics[width=0.8\textwidth,height=\textheight]{short-causal-diagrams_files/figure-pdf/fig-dag-descendant-proxy-1.pdf}

}

\caption{\label{fig-dag-descendant-proxy}Confounding by descent: the red
path illustrates the biasing path introduced by conditioning on a
variable L'. Here L' is a descendant of a variable, L, that is affected
both by the treatment and an unmeasured confounder U. Because U affects
the outcome, conditioning on the descendant of L, L' induces collider
bias.}

\end{figure}%

\paragraph{\texorpdfstring{2.7.4.1.2 Advice: avoid causal incoherence by
ensuring \(L_0\) occurs before \(A\) and it's proxy is not a mediator
our common effect of the \(A_1\) and
\(Y_2\)}{2.7.4.1.2 Advice: avoid causal incoherence by ensuring L\_0 occurs before A and it's proxy is not a mediator our common effect of the A\_1 and Y\_2}}\label{advice-avoid-causal-incoherence-by-ensuring-l_0-occurs-before-a-and-its-proxy-is-not-a-mediator-our-common-effect-of-the-a_1-and-y_2}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Ensure \(A_1\) occurs before \(Y_2\).
\item
  Ensure \(L_0\) occurs before \(A_1\).
\item
  If \(L_0\) is not measured, ensure \(L'\), a proxy of \(L_0\), is not
  a common effect of \(A_1\) or \(Y_2\) or a mediator along their path.
\end{enumerate}

\paragraph{2.7.4.2.1. Case when conditioning on a descendant of a
confounder reduces
bias}\label{case-when-conditioning-on-a-descendant-of-a-confounder-reduces-bias}

\begin{figure}

\centering{

\includegraphics[width=0.8\textwidth,height=\textheight]{short-causal-diagrams_files/figure-pdf/fig-dag-descendant-solution-2-1.pdf}

}

\caption{\label{fig-dag-descendant-solution-2}Conditioning on a proxy of
a confounder that occurs after both the exposure and the outcome can
reduce unmeasured confounding. The red dotted paths underscore that the
confounding influence of U on A and Y is diminished by conditioning on
L'. Even though L' occurs after the outcome. The paths are dotted to
represent the bias reduction achieved by conditioning on the
post-outcome descendant of an unmeasured common cause of the exposure
and outcome. An example is a genetic factor affecting the exposure and
outcome early in life, which can be measured later in life. Adjusting
for such an indicator is an example of post-outcome confounding
control.}

\end{figure}%

Consider a scenario in which adjusting for a post-treatment descendant
variable, \(L^\prime\), can make strategic sense. Imagine an unmeasured
confounder \(U\) that influences \(A\), \(Y\), and \(L^\prime\), with
the effect on \(L^\prime\) occurring after both the exposure, \(A\), and
the outcome, \(Y\) have occurred. Because \(L^\prime\) is a descendant
of \(U\), adjusting for \(L^\prime\) will reduce the confounding caused
by the unmeasured confounder \(U\).

Is this scenario plausible? Consider an unmeasured genetic factor \(U\)
that influences both the exposure \(A\) and the outcome \(Y\) early in
life, but is expressed later in life through a developmental marker
\(L^\prime\). Assume that \(L^\prime\) is not affected by \(A\) or
\(Y\). Even though \(L'\) occurs after the outcome, conditioning on
\(L'\) is useful for confounding control because \(L'\) provides
information about \(U\). This example is presented in
Figure~\ref{fig-dag-descendant-solution}, and illustrates the prospect
for post-outcome conditioning for confounding control.

\paragraph{2.7.4.2.2 Advice: it need not be causally incoherent to
condition on a post-exposure proxy of a pre-outcome
confounder}\label{advice-it-need-not-be-causally-incoherent-to-condition-on-a-post-exposure-proxy-of-a-pre-outcome-confounder}

The strategy for confounding control given in
Figure~\ref{fig-dag-descendant-solution} follows the modified
disjunctive cause criterion, which suggests including as a covariate any
proxy for an unmeasured variable that is a common cause of both the
exposure and the outcome (\citeproc{ref-vanderweele2019}{VanderWeele
2019}). The prospect that we may use descendants for confounding control
is however consistent with general advice given throughout this section:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Ensure \(A_1\) occurs before \(Y_2\).
\item
  Ensure \(L_0\) occurs before \(A_1\).
\item
  If \(L_0\) is not measured, ensure \(L'\), a proxy of \(L_0\), is not
  a common effect of \(A_1\) or \(Y_2\) or a mediator along their path.
\end{enumerate}

Practically speaking, determining which variables belong in the
confounder set can be challenging. We take instruction from the best
lights of experts. However, science is the practice of revising expert
opinion. We assume experts may be wrong. For this reason, we should
perform sensitivity analyses. It should go without saying that we will
not let experts tell us whether the \(A\to Y\) path exists or does not
exist. Estimating the magnitude of the association between the exposure
and outcome is the scientific task at hand.

In \textbf{Appendix 2}, I consider how causal data scientists might
apply a sequential causal diagram to data collection in a three-wave
panel design.

\subsubsection{2.8 Application of causal diagrams to measurement
bias}\label{application-of-causal-diagrams-to-measurement-bias}

\begin{figure}

\centering{

\includegraphics[width=0.5\textwidth,height=\textheight]{short-causal-diagrams_files/figure-pdf/fig-measure-error-1.pdf}

}

\caption{\label{fig-measure-error}Graph of uncorrelated non-differential
measurement error. A true effect may be attenuated if either the
exposure or outcome (or both) are measured with error.}

\end{figure}%

\paragraph{2.8.1 Structural approaches to measurement error
bias}\label{structural-approaches-to-measurement-error-bias}

The primary purpose of a causal diagram is to evaluate
\emph{identification problem} that may lead to biased estimation of the
\(A \to Y\) path (\citeproc{ref-deffner2022}{Deffner \emph{et al.}
2022}; \citeproc{ref-greenland1999}{Greenland \emph{et al.} 1999a};
\citeproc{ref-hernan2023}{Hernan and Robins 2023};
\citeproc{ref-mcelreath2020}{McElreath 2020};
\citeproc{ref-pearl2009a}{Pearl 2009b}; \citeproc{ref-rohrer2018}{Rohrer
2018}; \citeproc{ref-suzuki2020}{Suzuki \emph{et al.} 2020}).

However, recall, \emph{the assumption of universal measurement error}:
measurement error is ubiquitous. Certain problems of measurement error
bias can be represented as structural problems of confounding
(\citeproc{ref-hernuxe1n2009}{Hernán and Cole 2009};
\citeproc{ref-vanderweele2012a}{VanderWeele and Hernán 2012a}).
Researchers do this by decompose a variable \(X\) into its measurement
\(X'\) and the unmeasured source of error in its measurement, such that
\(X \to X' \leftarrow U_{\text{X}}\) For example, if the true exposure
can effect the error in which the true outcome is measured, an open path
can link the true exposure the measured outcome in the absence of
causation, \(X\rightarrowred U_{\text{Y}} \rightarrowred Y'\)
(\citeproc{ref-bulbulia2023e}{Bulbulia 2023a}).

\paragraph{2.8.2 Causal diagrams are poorly suited for diagnosing
non-structural measurement error
bias}\label{causal-diagrams-are-poorly-suited-for-diagnosing-non-structural-measurement-error-bias}

Not all measurement error bias can be fully encapsulated in a causal
diagram. For example, when there is a true causal effect of the exposure
on the outcome, but when the errors of the measured exposure and of the
measured outcome are uncorrelated, measurement error will
\emph{attenuate} a true effect. Figure~\ref{fig-measure-error} presents
the attenuation of a true effect using a dotted arrow, however this
convention does not reflect a confounding bias. According to the rules
of d-separation the outcome and exposure remain d-connected in the
presence of such errors. A standard causal diagram would draw a line
connecting \(A\) and \(Y\). However, the bias here does not arise from
confounding but rather from the obscuring of true measurement states.

Although not all measurement biases can be conveyed by by causal
diagrams, VanderWeele and Hernán
(\citeproc{ref-vanderweele2012a}{2012a}) show that under certain
conditions, we can infer the direction of a causal effect from observed
associations. Specifically, if (1) the association between the measured
variables \(A^{\prime}_{1}\) and \(Y^{\prime}_{2}\) is positive, (2) the
measurement errors for these variables are not correlated, and (3) we
assume distributional monotonicity for the effect of \(A\) on \(Y\)
(applicable, for example, when both are binary), then a positive
observed association implies a positive causal effect from \(A\) to
\(Y\). Conversely, a negative observed association implies a negative
causal effect, especially if the error terms are correlated positively
or independently of the true exposure and outcome. This conclusion
relies on the assumption of distributional monotonicity for the effect
of \(A\) on \(Y\). Moreover, if the error terms are positively
correlated, than a negative association in the measure measured
variables indicates a negative effectt of \(A\) on \(Y\). (See Bulbulia
(\citeproc{ref-bulbulia2023e}{2023a}) for more discussion.)

\subsubsection{2.9 Sequential causal diagrams reveal give the lie to
causally incoherence
science}\label{sequential-causal-diagrams-reveal-give-the-lie-to-causally-incoherence-science}

In \emph{Part 2} we learned that we may avoid \emph{causal incoherence}
by ensuring accuracy in the timing of all measured variables. We also
learned that such accuracy is not sufficient. We must also opporate on
the correct representation of reality. Yet in our model of reality, all
but the exposure/outcome path must be assumed. \emph{With great power
comes great responsibility.}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Ensure \(A_1\) occurs before \(Y_2\).
\item
  Ensure \(L_0\) occurs before \(A_1\).
\item
  If \(L_0\) is not a common cause of \(A_1\) and \(Y_2\), ensure it is
  the descendant (proxy) of an unmeasured common cause.
\item
  If \(L_0\) is not measured, ensure \(L'\), a proxy of \(L_0\), is not
  a common effect of \(A_1\) or \(Y_2\) or a mediator along their path.
\end{enumerate}

\subsection{Part 3. Interaction (Moderation), Mediation, and
Longitudinal
Feedback}\label{part-3.-interaction-moderation-mediation-and-longitudinal-feedback}

\subsubsection{3.1.1 Causation in interaction and
effect-modification?}\label{causation-in-interaction-and-effect-modification}

In causal data science, we may think of interaction or moderation in one
of two ways, either as (1) joint intervention or (2)
effect-modification. Before considering these distinct concepts, a word
of warning.

\paragraph{3.1.2 Warning: a causal diagram is a non-parametric encoding
of causal
assumptions}\label{warning-a-causal-diagram-is-a-non-parametric-encoding-of-causal-assumptions}

Causal diagrams were developed to investigate confounding, not
interactions. It is generally ill-advised to draw arrows into arrows. In
\(\S 3.1.6\) we modify standard conventions to represent effect
modification. However, these graphs are not, strictly speaking, causal
diagrams. For this reason, we must clearly state our conventions and
purposes there.

\paragraph{3.1.3 Interaction as a joint
intervention}\label{interaction-as-a-joint-intervention}

This form of interaction occurs when the combined effect of two
interventions (or treatments) is different from what would be expected
based on their individual effects.

Consider two treatments, denoted as \(A\) and \(B\), and their outcome
as \(Y\). A joint intervention causal interaction implies that the
effect of \(A\) and \(B\) together on \(Y\) (denoted as \(Y(A,B)\)) is
not merely the sum of their individual effects.

For instance, consider the effect of beliefs in Big Gods (exposure
\(A\)) on social complexity (outcome \(Y\)), potentially influenced by a
culture's monumental architecture (exposure \(B\)). To assess the
individual and combined effects of \(A\) and \(B\), we look for evidence
of causal interaction on the difference scale. Evidence for interaction
would be present if the following inequality were to hold:

\[\bigg(\underbrace{\mathbb{E}[Y(1,1)]}_{\text{joint exposure}} - \underbrace{\mathbb{E}[Y(0,0)]}_{\text{neither exposed}}\bigg) - \bigg[ \bigg(\underbrace{\mathbb{E}[Y(1,0)]}_{\text{only A exposed}} - \underbrace{\mathbb{E}[Y(0,0)]}_{\text{neither exposed}}\bigg) + \bigg(\underbrace{\mathbb{E}[Y(0,1)]}_{\text{only B exposed}} - \underbrace{\mathbb{E}[Y(0,0)]}_{\text{neither exposed}} \bigg)\bigg] \neq 0 \]

This equation simplifies to

\[ \underbrace{\mathbb{E}[Y(1,1)]}_{\text{joint exposure}} - \underbrace{\mathbb{E}[Y(1,0)]}_{\text{only A exposed}} - \underbrace{\mathbb{E}[Y(0,1)]}_{\text{only B exposed}} + \underbrace{\mathbb{E}[Y(0,0)]}_{\text{neither exposed}} \neq 0 \]

A positive value would indicate evidence for additive interaction. A
negative value would indicate evidence for sub-additive interaction. A
value near zero would imply no reliable evidence for interaction.

\begin{figure}

\centering{

\includegraphics[width=0.8\textwidth,height=\textheight]{short-causal-diagrams_files/figure-pdf/fig-dag-interaction-1.pdf}

}

\caption{\label{fig-dag-interaction}This diagram presents the individual
and joint effects of two exposures, A and B, on outcome Y. We assume
that A and B are causally independent. If either exposure affects the
other, then we may conduct effect-modification analysis or mediation
analysis, but we should avoid causal interaction analysis. The diagram
includes confounders L and W. Control for these confounders is necessary
to close the backdoor paths that relate each exposure, A and B, to the
outcome. Each exposure has equal status in our model: Y(a,b) = Y(b,a).
The red path denotes paths of confounding.}

\end{figure}%

Figure~\ref{fig-dag-interaction} clarifies the need to evaluate two
sources of counfounding, one for each intervention (\(A\) and \(B\)).
The graph resembles others we have considered. By adjusting for
\(L_{0}\) we obtain an unbiased estimate of the \(A\to Y\) path. By
adjusting for \(W_{0}\) we obtain an unbiased estimate of the \(B\to Y\)
path. As indicated in Figure~\ref{fig-dag-interaction-solved}, we must
condition on both \(L_{0}\) and \(W_{0}\) to identify causal interaction
conceived as a joint interaction.

An important complication is that evidence for causal interaction may
differ depending on the measurement scale one chooses to assess it
VanderWeele (\citeproc{ref-vanderweele2012}{2012}). Evidence for the
strength of a causal effect estimate for interaction in the presence of
effect-modification will differ depending on whether the effect is
measured on the ratio scale as opposed to the difference scale (see:
VanderWeele and Knol (\citeproc{ref-vanderweele2014}{2014}), who
recommends using the causal difference scale for most policy settings.)

\begin{figure}

\centering{

\includegraphics[width=0.8\textwidth,height=\textheight]{short-causal-diagrams_files/figure-pdf/fig-dag-interaction-solved-1.pdf}

}

\caption{\label{fig-dag-interaction-solved}We adjust for confounding in
causal interaction analysis by adjusting for all confounders of the A to
Y path as well as all for the B to Y path. The box over the confounders
indicates the biasing paths are closed.}

\end{figure}%

Again we arrange Figure~\ref{fig-dag-interaction-solved} to follow the
assumed sequence of causation. Doing so better clarifies demands for
data quality -- the timing of the events must be ensured. Data
collection should also draw on expert knowledge about how \(A\) and
\(B\) may be related over time; measurements of \(A\) and \(B\) should
be taken within intervals in which \(A\) and \(B\) are unlikely to
affect each other.

\paragraph{3.1.4 Interaction as effect-modification of a single
intervention}\label{interaction-as-effect-modification-of-a-single-intervention}

\begin{figure}

\centering{

\includegraphics[width=0.8\textwidth,height=\textheight]{short-causal-diagrams_files/figure-pdf/fig-dag-effect-modification-1.pdf}

}

\caption{\label{fig-dag-effect-modification}A simple sequentially
ordered graph depicting effect-modification. We are interested in G as
an effect-modifier of A on Y. A must not affect G (although G may affect
A, we do not suppose such an effect). The blue path indicates
effect-modification of A by G. The red path indicates an open backdoor
path. We draw a box around G to indicate that we are conditioning on G
in our model. We are interested in estimating the effect of A on Y. We
must close all backdoor paths between A and Y, in this case, by
conditioning on L. We are not interested in the effect of double
intervention of G and A. Hence, we do not need to condition on Z to
close the backdoor path from G to Y.}

\end{figure}%

It is often scientifically interesting to consider whether treatment
effects vary over levels of other variable without imagining a double
intervention. We call a variable over which the treatment effect varies,
an `effect-modifier' or an `effect-measure modifier.' We call the
phenomenon of variation in the effect of the exposure over levels of a
covariate, `effect-modification,' or `effect-measure modification' or
'treatment effect heterogeneity. The counterfactual contrasts required
to estimate effect-modification differ from those of a joint
intervention. Suppose \(A\) is the treatment, \(G\) is the modifier, and
\(Y\) is the outcome. effect-modification assesses whether the effect of
\(A\) on \(Y\) is different across levels of \(G\) (i.e., whether the
effect of \(A\) on \(Y\) is different when \(G = g1\) compared to when
\(G = g2\)).

Figure~\ref{fig-dag-effect-modification} consider whether
effect-modification of \(A\) on \(Y\) across levels of \(G\). Because we
are not interested in the causal effect of \(G\) as such, but rather,
how the effect of \(A\) varies across \(G\), we would not need to adjust
\(G\) by \(Z\). However, as we shall consider in the next section, the
presence and absence of effect-modification may depend on other
variables in a causal network, as well as on which other variables
investigators condition on in their models. To foreshadow, we suppose
that \(Z\), a parent of \(G\), is an effect-modifier of \(A\) on \(Y\).
Were we to include \(Z\) in the model, the effect estimate for \(G\) on
\(Y\) may be attenuated or erased. There is here no clear fact of the
matter about whether and how much \(G\) is an effect-modifier outside of
researcher modelling decisions. I remind readers: \emph{with absolute
power comes absolute responsibility.}

To better understand the interest of effect-modification, again consider
a study investigating whether beliefs in big Gods affect social
complexity. Suppose we compare two distinct geographical groups: North
American societies (\(G=1\)) and Continental societies (\(G=2\)).
Suppose we want to examine the causal effect of changing the exposure
from \(A = 0\) to \(A = 1\) within each group and then compare these
effects across the groups. The relevant causal contrasts are given as
follows:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Causal effect within North American societies (}\(G=1\)):
  \[\hat{\tau}_{g1} = \hat{\mathbb{E}}[Y(1)|G=1] - \hat{\mathbb{E}}[Y(0)|G=1]\]
\end{enumerate}

Here, \(\hat{\tau}_{g1}\) represents the estimated causal effect of
changing the exposure from \(A = 0\) to \(A = 1\) within the North
American societies.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\item
  \textbf{Causal effect within Continental societies (}\(G=2\)):

  \[\hat{\tau}_{g2} = \hat{\mathbb{E}}[Y(1)|G=2] - \hat{\mathbb{E}}[Y(0)|G=2]\]

  Similarly, \(\hat{\tau}_{g2}\) denotes the estimated causal effect for
  the Continental societies.
\item
  \textbf{Comparing causal effects across groups}:

  \[\hat{\gamma} = \hat{\tau}_{g1} - \hat{\tau}_{g2}\]
\end{enumerate}

The estimated quantity \(\hat{\gamma}\) computes the difference in the
causal estimands between the two groups. A nonzero \(\hat{\gamma}\)
indicates effect-modification, suggesting that the effect of changing
the exposure differs between the two groups. If we were to observe that
\(\hat{\gamma} \neq 0\), this would provide evidence for variability in
the effect of the exposure on the outcome in different groups. Note that
the causal effect for one group might be indistinguishable from zero,
and yet we might nevertheless find evidence for effect-modification if
the comparison group exhibits reliably different responses from the
contrast group that is indistinguishable from zero.

\paragraph{3.1.5 Evidence for effect-modification is relative to
inclusion of other variables in the
model}\label{evidence-for-effect-modification-is-relative-to-inclusion-of-other-variables-in-the-model}

The `sharp-null hypothesis' states there is no effect of the exposure on
the outcome for any unit in the target population. Unless the
`sharp-null hypothesis' is false, there may be effect-modification. For
any study worth conducting, we cannot evaluate whether the sharp-null
hypothesis is false. If we could the experiment would be otiose.
Therefore, we must assume the possibility of effect-modification.
Notably, whether a variable is an effect-modifier also depends on which
other variables are included in the model. That is, just as for the
concept of a `confounder', where a variable is an `effect-modifier'
cannot be stated without reference to an assumed causal order and an
explicit statement about which other variables will be included in the
model (\citeproc{ref-vanderweele2012}{VanderWeele 2012}).

Figure~\ref{fig-eff-mod-rel} presents a scenario in which the marginal
association between \(A\) and \(Y\) is unbiased. The exposure \(A\) is
unconditionally associated with \(Y\). Recall our convention
\(\boxedblue{G}\) denotes effect-modification with conditioning and
\(\circledotted{Z}\) indicates effect-modification without conditioning.
This graph states that the conditional association of \(A\) on \(Y\)
varies within levels of \(\boxedblue{G}\) (which is conditioned on), and
furthermore that \(G\) is an \emph{effect-modifier by proxy}
(\citeproc{ref-vanderweele2009distinction}{VanderWeele 2009c}). Here,
\(\circledotted{Z}\) fully mediates the association of \(G\) and \(Y\).
That is \(\circledotted{Z}\) causes both \(G\) and \(Y\), and \(G\) has
no causal effect on \(Y\).

\begin{figure}

\centering{

\includegraphics[width=0.8\textwidth,height=\textheight]{short-causal-diagrams_files/figure-pdf/fig-eff-mod-rel-1.pdf}

}

\caption{\label{fig-eff-mod-rel}Consider a randomised experiment. There
is no confounding. Here, the marginal association between A and Y
provides an unbiased estimate for the causal effect of A on Y. Does the
conditional association of A on Y vary within levels of G? The causal
diagram allows for a classification of G as an effect modifier of A on Y
by proxy. G modifies A's effect on Y in virtue of G's relationship to Z,
which, according to this graph, is a direct effect modifier for the
effect of A on Y.}

\end{figure}%

Figure~\ref{fig-dag-effect-modification-4} presents the same a
randomised experiment as in the previous causal diagram. We again assume
that there is no confounding of the marginal association between the
exposure, \(A\), and the outcome, \(Y\). However, suppose we were to
adjust for \(Z\) and ask, does the conditional association of \(A\) on
\(Y\) vary within levels of \(G\), after adjusting for \(Z\)? That is,
does \(G\) remain an effect-modifier of the exposure on the outcome?
VanderWeele and Robins (\citeproc{ref-vanderweele2007}{2007}) proved
that for effect-modification to occur, at least one other arrow besides
the treatment must enter into the outcome. According to
Figure~\ref{fig-dag-effect-modification-4}, the only arrow into \(Y\)
other than \(A\) arrives from \(Z\). Because \(Y\) is independent of
\(G\) conditional on \(Z\) we may infer that \(G\) is no longer an
effect modifier for the effect of \(A\) on \(Y\). Viewed another way,
\(G\) no longer co-varies with \(Y\) conditional on \(Z\) and so cannot
act as an effect-modifier.

\begin{figure}

\centering{

\includegraphics[width=0.8\textwidth,height=\textheight]{short-causal-diagrams_files/figure-pdf/fig-dag-effect-modification-4-1.pdf}

}

\caption{\label{fig-dag-effect-modification-4}Conditioning on Z renders
G independent of Y. G is no longer an effect modifier after conditioning
on Z because G is independent of Y. Although Z is an unconditional
effect modifier, G is not.}

\end{figure}%

Figure~\ref{fig-dag-effect-modification-5} presents the same a
randomised experiment as in the previous graph. If we do not condition
on \(B\), then \(G\) will not modify the effect of \(A\) on \(Y\)
because \(G\) will not be associated with \(Y\). However, if we were to
condition on \(B\), then both \(B\) (an effect modifier by proxy) and
\(G\) may become effect-modifiers for the causal effect of \(A\) on
\(Y\). In this setting, both \(B\) and \(G\) are conditional
effect-modifiers.

Note that casual graphs help us to evaluate classifications of
conditional and unconditional effect modifiers. They may also help to
clarify conditions in which conditioning on unconditional
effect-modifiers may remove conditional effect-modification. However we
cannot not tell from a causal diagram whether the ancestors of an
unconditional effect-modifier will be conditional effect-modifiers for
the effect of the exposure on the outcome; see: VanderWeele and Robins
(\citeproc{ref-vanderweele2007}{2007}), also Suzuki \emph{et al.}
(\citeproc{ref-suzuki2013counterfactual}{2013}). Causal diagrams express
non-parametric relations. I have adopted an off-label colouring
convention to denote instances of effect-modification to highlight
possible pathways for effect-modification, which may be relative to
other variables in a model.

\begin{figure}

\centering{

\includegraphics[width=0.8\textwidth,height=\textheight]{short-causal-diagrams_files/figure-pdf/fig-dag-effect-modification-5-1.pdf}

}

\caption{\label{fig-dag-effect-modification-5}Blue path denotes
effect-modification for G by conditioning on B. Both B and G are
conditional effect modifiers.}

\end{figure}%

Figure~\ref{fig-dag-effect-modification-5b} reveals the relativity of
effect-modification. If investigators do not condition on \(B\), then
\(G\) cannot be a conditional effect-modifier because \(G\) would then
be independent of \(Z\) because \(B\) is a collider. However, as we
observed in Figure~\ref{fig-dag-effect-modification-5}, conditioning on
\(B\), a collider, may open a path for effect-modification of \(G\) by
\(Z\). Both \(B\) and \(G\) are conditional effect modifiers.

\begin{figure}

\centering{

\includegraphics[width=0.8\textwidth,height=\textheight]{short-causal-diagrams_files/figure-pdf/fig-dag-effect-modification-5b-1.pdf}

}

\caption{\label{fig-dag-effect-modification-5b}Blue path denotes
effect-modification. Here G is not an effect modifier because B, a
common effect (collider) of G and Z, is not conditioned on. Any
conditional effect modification for G would require conditioning on B,
and not-conditioning on G. Otherwise G will be d-separated from Y.}

\end{figure}%

\begin{figure}

\centering{

\includegraphics[width=0.8\textwidth,height=\textheight]{short-causal-diagrams_files/figure-pdf/fig-dag-effect-modification-5c-1.pdf}

}

\caption{\label{fig-dag-effect-modification-5c}Blue path denotes
effect-modification. Neither, G nor B are unconditional effect-modifiers
for the effect of A on Y after Z is conditioned upon. If investigators
condition on Z, the causal diagram implies they will not find evidence
for effect-modification by B or G, which are conditionally independent
of Y once Z is conditioned upon.}

\end{figure}%

Figure~\ref{fig-dag-effect-modification-5c} considers the implications
of conditioning on \(Z\), which is the only unconditional
effect-modifier on the graph. If \(Z\) is measured, conditioning on
\(Z\) will remove effect-modification for \(B\) and \(G\) because
\(B,G\coprod Y |Z\). This examples again reveals the context dependency
of effect-modification. Here, causal diagrams are useful for clarifying
features of dependent and independent effect modification. For further
discussion, see: Suzuki \emph{et al.}
(\citeproc{ref-suzuki2013counterfactual}{2013}); VanderWeele
(\citeproc{ref-vanderweele2009distinction}{2009c}).

\paragraph{3.1.6 Effect-modification and selection-restriction
bias}\label{effect-modification-and-selection-restriction-bias}

\paragraph{3.1.6.1 Selection-restriction arising from
confounding}\label{selection-restriction-arising-from-confounding}

Suppose it is the 1950s. The investigators wish to examine whether a
hand-grip-release-response varies with the voltage of electricity
running through a grip. They devise an experiment consistent with ethics
of that era.

\begin{itemize}
\tightlist
\item
  \(A\): the intervention: electric shock at 250v or electric shock at
  30v administered with following responses to a 1950s statistics quiz
\item
  \(Y\): the outcome: speed of release reflex.
\item
  \(Z\): potential effect modifiers: features on which the effect of
  voltage, \(A\), on grip release speed, \(Y\), may vary, such as shock
  sensitivity.
\item
  \(\Pr(\text{strong shock})\): the probability of administering the
  shock, set at \(\Pr(\text{strong shock})= 0.33\) for each condition,
  no matter how a subject responds
\item
  Participants are told that if they achieve a score of 70\% on their
  test, and remain in the experiment to completion, they will be
  rewarded with a zoot suit (male) or high-waisted trousers (females).
\end{itemize}

\begin{figure}

\centering{

\includegraphics[width=0.8\textwidth,height=\textheight]{short-causal-diagrams_files/figure-pdf/fig-experiment-selection-collider-adjustment-1.pdf}

}

\caption{\label{fig-experiment-selection-collider-adjustment}In some
settings, bias from loss-to-follow up may arise from confounding.
Estimates are biased because the exposure affects attrition, as does
another variable, U, that also affects the outcome. Note that the causal
diagram presents the outcome Y\_2 for the uncensored population (C=1).
This is the population for which investigators seek to compute causal
contrasts.}

\end{figure}%

The randomisation of the treatment assignment (low voltage vs hi
voltage) ensures that the potential outcomes under treatment assignment
are independent of all confounders. However, the treatment involves
administering a sequence of strong shocks with 1/3 probability. Suppose
participants are allowed to drop out. Suppose there is attrition. The
investigators surmise their intervention has something to do with
attrition. In this setting, the effects estimated for the censored
sample population (the sample after attrition) may differ from those
that would have been estimated for the baseline population had everyone
remained in the study. A bias emerges because attrition, which is
affected by the treatment, is also the effect of an unmeasured shock
sensitivity that affects both attrition and the outcome (reflex speed).
The resulting bias from the confounding bias from the censoring is
presented in Figure~\ref{fig-experiment-selection-collider-adjustment}.

\paragraph{3.1.6.2 Selection-restriction in the absence of confounding
(effect-modifier distribution
bias)}\label{selection-restriction-in-the-absence-of-confounding-effect-modifier-distribution-bias}

Figure~\ref{fig-experiment-selection-restriction-adjustment-2} presents
a general problem arising from restriction of the target sample at every
phase of the study, even if the exposure does not cause the restriction,
and indeed even if restriction occurs before treatment assignment.
Assume the baseline population aligns with the target population in all
features relevant for causal effect estimation. Suppose further that
censoring is unrelated to the exposure. Imagine Australians are fonder
of zoot suits. They are more inclined to remain in the study. In that
case, the marginal effect of the exposure on the outcome would differ
for the target population (Australians and New Zealanders) because the
distribution of effect-modifiers would change \emph{on the units}
measured after censoring. Notice there is no backdoor path linking the
exposure to the outcome. As such, there is no confounding bias. Here,
the threat of \emph{effect-modifier distribution bias} applies to
generalisation or \emph{target validity}, not to confounding. This
threat arises wherever there is a difference in the baseline population
and the censored population in the distribution of variables \(L\) that
modify the effects of the exposure on the outcome. Effect-modifiers
should be assumed to be commonplace. As such, wherever there is
restriction such that the units in a study differ from the target
population, the results investigators obtain at the end of data
collection may not be those that they seek. (A derivation is presented
in \textbf{Appendix 4.}).

Figure~\ref{fig-experiment-selection-restriction-adjustment-2} presents
the scenario just described. Note \(Y_2\) defines the outcome for the
target population. Effect modification is denoted by
\(\boxedblue{Z^{C=1}}\). An unmeasured cause of censoring event
\(U_{C=1}\) relates the censoring event to the potential outcome,
indicated by the blue path. The relationship presented here is not one
of confounding. Nevertheless, the marginal effect estimated for the
censored population will not necessarily reflect the marginal effect in
the uncensored population, which cannot be directly observed from the
censored data. Note that such \emph{informative censoring} requires a
model for recovering the missing outcomes that would have been observed
had censoring not occurred. This typically requires the assumption that
pre-censoring data together with a correctly specified missingness model
are sufficient to recover marginal effects for the uncensored; see:
Malinsky \emph{et al.}
(\citeproc{ref-malinsky2022semiparametric}{2022}); Li \emph{et al.}
(\citeproc{ref-li2023non}{2023}).

\begin{figure}

\centering{

\includegraphics[width=0.8\textwidth,height=\textheight]{short-causal-diagrams_files/figure-pdf/fig-experiment-selection-restriction-adjustment-2-1.pdf}

}

\caption{\label{fig-experiment-selection-restriction-adjustment-2}In the
presence of effect-modification for any variables in Z, the effect
estimated in the end-of-study (censored) population will not necessarily
be the same as that estimated in the uncensored (baseline) population,
except under the sharp null hypothesis or in exceptional circumstances
(see Appendix 4). The sharp null hypothesis cannot be assumed, nor can
the exceptional circumstances be assessed from data because the
treatment effects for the missing censored population are not observed.
Again, note the causal diagram presents the outcome Y\_2 for the
uncensored population (C=1).}

\end{figure}%

Consider further that the baseline population is a \emph{restriction} of
the target population. As such, the same considerations that apply to
restriction of the sample after treatment assignment apply to the sample
before treatment assignment with respect to the target population. That
is, we may think of the sample population as a censoring of the target
population. Consistently estimated effects for the sample population
will need not generalise if there are effect modifiers for the treatment
effect. Therefore, where population distributions can be obtained for
known effect-modifiers, investigators should weight individuals in the
study to these target population distributions. There is a long
tradition in survey research for computing such weights; see: Horvitz
and Thompson (\citeproc{ref-horvitz1952generalization}{1952}); Cole and
Stuart (\citeproc{ref-cole2010generalizing}{2010}); Lumley
(\citeproc{ref-lumley2004analysis}{2004}); Imai \emph{et al.}
(\citeproc{ref-imai2008misunderstandings}{2008}), however any model
brings model mis-specification risks; see: Dahabreh and Hernán
(\citeproc{ref-dahabreh2019}{2019}); Dahabreh \emph{et al.}
(\citeproc{ref-dahabreh2021study}{2021}).

\begin{figure}

\centering{

\includegraphics[width=0.8\textwidth,height=\textheight]{short-causal-diagrams_files/figure-pdf/fig-experiment-selection-restriction-adjustment-4-1.pdf}

}

\caption{\label{fig-experiment-selection-restriction-adjustment-4}If
conditioning on Q removes a conditional effect-modification of A on Y by
Z, we may seek to condition on Q, an unconditional effect modifier.
However, where censoring is informative, adjustment by Q will not ensure
that the distribution of Q remains constant in the censored population.
Our graph brings into focus the problem of informative censoring such
that Pr(Q=q\textbar C=1) ≠ Pr(Q=q\textbar C). Again, note the causal
diagram presents the outcome Y\_2 for the uncensored population (C=1).}

\end{figure}%

It might be tempting to try to address the problem of censoring bias
using regression adjustment. For example Lu \emph{et al.}
(\citeproc{ref-lu2022}{2022a}) correctly note that selection raises
challenges for causal inference not from over-adjustment bias, but
rather from selection of a stratum of the population. The authors
further correctly note that the problem for inference in after stratum
specific selection is a problem of \emph{target validity} such that one
may consistently compute causal effects within the censored population
without bias yet still fail to recover the desired causal effect
estimates for the target population. Lu \emph{et al.}
(\citeproc{ref-lu2022}{2022a}) propose a solution in which a
conditioning on the single common cause of selection and the exposure
restores target validity see: Lu \emph{et al.}
(\citeproc{ref-lu2022}{2022a}), p704, Figure 4. However, as noted above,
the variables in \(Z\) over which the exposure \(A\) modifies \(Y\) do
not necessarily (and perhaps not generally) cause censoring. To
conditionally remove the effect-modification of \(Z\) by conditioning on
\(Q\) would require that:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \(Q\) is an unconditional effect-modifier of \(A\) on \(Y\) and all
  variables in \(Z\) are conditional-effect modifiers.
\item
  \(Q\) captures all effect-modification of \(A\) on \(Y\).
\item
  Conditioning on \(Q\) does not induce new conditional
  effect-modification.
\item
  The distribution of \(Q\) is known to be the same in the restricted
  sample as in the unrestricted sample.
\end{enumerate}

These conditions impose strong, unverifiable demands on \(Q\).

Finally, return to the example of censoring with confounding bias
discussed in \(\S 3.1.6.1\), depicted in
Figure~\ref{fig-experiment-selection-collider-adjustment}. Suppose the
investigators believe that they can measure the confounder denoted
\(V\). By conditioning on the measured confounder \(V\), there is no
longer a backdoor path linking the exposure to the outcome. Conditioning
on \(V\) removes confounding. However, if \(V\) were an unconditional
effect-modifier, even a consistently estimated effect of \(A\) on \(Y\)
would produce a marginal effect estimate for the censored sample that
may differ from the marginal effect estimate of the target population.
Additionally, there may be other unconditional effect modifiers other
than \(V\) for whom censoring is informative.
Figure~\ref{fig-experiment-selection-collider-adjustment-measured}
presents the inadequacy of conditioning on \(V\).

\begin{figure}

\centering{

\includegraphics[width=0.8\textwidth,height=\textheight]{short-causal-diagrams_files/figure-pdf/fig-experiment-selection-collider-adjustment-measured-1.pdf}

}

\caption{\label{fig-experiment-selection-collider-adjustment-measured}Target-validity
may be compromised even if causal effect estimates in the censored
sample are unbiased for the censored sample population. The red path
linking exposure to outcome is blocked by conditioning on V. However, if
censoring is informative, even if the confounder of the censoring event
is blocked, sample/population mismatch in the distribution of
effect-modifiers will threaten target (or external) validity.}

\end{figure}%

\paragraph{3.1.7 Causal diagrams and
effect-modification}\label{causal-diagrams-and-effect-modification}

Causal diagrams primarily address confounding but can also clarify
structural biases, particularly when sample units differ from the target
population in ways that affect treatment outcomes.

Even without confounding, \emph{effect-modifier distribution bias}
arising from sample restriction can persist. Adjustment is needed at two
stages: before treatment assignment, using weights for any known
effect-modifiers in the target population, and after treatment,
adjusting for variables that modify the direct effect of treatment.
Failure to correct for skewed distributions of effect modifiers can
compromise the generalisability of findings.

Recovering missing counterfactuals after restriction requires combining
sampling weights with models accounting for censoring events. While
multiple imputation can estimate missing outcomes, it must be applied
cautiously to avoid model mis-specification and preserve causal order.
Incorrect borrowing of information post-treatment can introduce
confounding bias (\citeproc{ref-bulbulia2023a}{Bulbulia \emph{et al.}
2023}; \citeproc{ref-westreich2015}{Westreich \emph{et al.} 2015}).

Despite the best formal implementation, model mis-specification is an
inherent risk, especially in censoring and survey models. Developing
models for consistent, valid causal contrasts in the target population
is a current challenge at the intersection of machine learning and
causal inference (\citeproc{ref-bareinboim2022}{Bareinboim \emph{et al.}
2022}; \citeproc{ref-cui2020}{Cui \emph{et al.} 2020};
\citeproc{ref-kuxfcnzel2019}{Künzel \emph{et al.} 2019}). Causal
diagrams are part of complex workflows, and their use comes with
significant responsibility and considerations
(\citeproc{ref-bareinboim2013general}{Bareinboim and Pearl 2013};
\citeproc{ref-bulbulia2023a}{Bulbulia \emph{et al.} 2023};
\citeproc{ref-cole2010generalizing}{Cole and Stuart 2010};
\citeproc{ref-dahabreh2021study}{Dahabreh \emph{et al.} 2021};
\citeproc{ref-hernuxe1n2023}{Hernán and Monge 2023};
\citeproc{ref-kern2016assessing}{Kern \emph{et al.} 2016};
\citeproc{ref-suzuki2016}{Suzuki \emph{et al.} 2016};
\citeproc{ref-westreich2017transportability}{Westreich \emph{et al.}
2017b}).

\subsubsection{3.2. Causation in mediation
analysis}\label{causation-in-mediation-analysis}

\paragraph{3.2.1 Structural equation models lack
structure}\label{structural-equation-models-lack-structure}

In the human sciences, mediation analysis is often mired in confusion, a
situation exacerbated by the complex nature of causal relationships it
aims to reveal. However, confusion dissipates when we define our causal
question in relation to the counterfactuals we hope to estimate. Beyond
the intrinsic challenges of mediation analysis, much of the prevailing
confusion stems from the prevalent use of structural equation models
(SEMs). These models generally lack a systematic way of modelling the
complex counterfactual contrasts that are relevant to evaluating
causality. The widespread disconnect between the dominant modelling
traditions and the demands of causal data science is a particularly
worrying feature of the causal crisis that pervades many human sciences
presently. We have no guarantees they that such models are
interpretable. However, we can do better by clearly defining our
estimands with respect to a clearly defined target population. Causal
diagrams are powerful compasses by which to clarify the conditions under
which these estimands may be identified from data.

\paragraph{3.2.2 Structure comes from clearly defining
estimands}\label{structure-comes-from-clearly-defining-estimands}

To gain a clearer understanding of what causal mediation entails, it is
helpful to deconstruct the total effect into the natural direct and
indirect effects.

The total effect of treatment \(A\) on outcome \(Y\) is defined as the
aggregate difference between the potential outcomes when the treatment
is applied versus when it is not. The estimand for the total effect (TE)
can be expressed as follows:

\[
TE = \mathbb{E}[Y(1)] - \mathbb{E}[Y(0)]
\]

The total effect can be further decomposed into direct and indirect
effects, which allow us to address questions of mediation. The potential
outcome \(Y(1)\) taking into account the mediator can be expanded:

\[ 
\mathbb{E}[Y(1)] = \mathbb{E}[Y(1, M(1))]
\]

Here, the effect of the exposure, set to \(A = 1\), is considered along
with the effect of the mediator at its natural value when \(A = 1\).

Similarly, the potential outcome \(\mathbb{E}[Y(0)]\) taking into
account the mediator can be expanded:

\[ 
\mathbb{E}[Y(0)] = \mathbb{E}[Y(0, M(0))]
\]

Here, we focus on the effect of the exposure, set to \(A = 0\), along
with the effect of the mediator at its natural value when \(A = 0\).

Next consider these quantities of interest as they relate to causal
mediation analysis. We can clarify our estimand by decomposing the Total
Effect (TE, which is equivalent to the average treatment effect, or
marginal effect) into the Natural Direct Effect (NDE) and the Natural
Indirect Effect (NIE).

\textbf{Natural Direct Effect (NDE)} is the effect of the treatment on
the outcome while maintaining the mediator at the level it would have
been if the treatment had \emph{not} been applied. The Natural Direct
Effect (NDE) is given:

\[
 NDE = \textcolor{blue}{\mathbb{E}[Y(1, M(0))]} - \mathbb{E}[Y(0, M(0))]
 \]

Here, the counterfactual quantities that are not directly realised in
the data are highlighted in blue:
\(\textcolor{blue}{\mathbb{E}[Y(1, M(0))]}\). Noticethat we add this
term to the potential outcomes when \(A=0\), namely,
\(\mathbb{E}[Y(0)]\), recalling: \(\mathbb{E}[Y(0, M(0))] = Y(0)\)

\textbf{Natural Indirect Effect (NIE):} is the effect of the exposure on
the outcome that is mediated. To obtain these quantities we must compare
the potential outcome \(Y\) under treatment, where the mediator assumes
its natural level under treatment with the potential outcome when the
mediator assumes its natural value under no treatment is given:

\[
 NIE = \mathbb{E}[Y(1, M(1))] - \textcolor{blue}{\mathbb{E}[Y(1, M(0))]}
\]

Here, the counterfactual quantities that are not directly realised in
the data are again highlighted in blue:
\(\textcolor{blue}{\mathbb{E}[Y(1, M(0))]}\). Notice that we subtract
the term from the potential outcomes when \(A=1\), namely,
\(\mathbb{E}[Y(1)]\), recalling:
\(\mathbb{E}[Y(1, M(1))] = \mathbb{E}[Y(1)]\).

Then, by rearranging this decomposition, we can demonstrate that the
total effect (TE) is the sum of the NDE and NIE. We do this by adding
and subtracting the term \(\textcolor{blue}{\mathbb{E}[Y(1, M(0))]}\),
highlighted in blue to our equation is given:

\[
\text{Total Effect (TE)} = \underbrace{\bigg\{\mathbb{E}[Y(1, M(1))] - \textcolor{blue}{\mathbb{E}[Y(1, M(0))]}\bigg\}}_{\text{Natural Indirect Effect (NIE)}} + \underbrace{\bigg\{\textcolor{blue}{\mathbb{E}[Y(1, M(0))]} - \mathbb{E}[Y(0, M(0))]\bigg\}}_{\text{Natural Direct Effect (NDE)}}
\]

The decomposition of the total effect into natural direct and indirect
effects greatly clarifies the targets of interest in causal mediation
analysis where the interest is in recovering natural indirect and direct
effects, see VanderWeele (\citeproc{ref-vanderweele2015}{2015}). These
are the quantities that causal mediation analysis often seeks
(\citeproc{ref-shi2021}{Shi \emph{et al.} 2021};
\citeproc{ref-steen2017}{Steen \emph{et al.} 2017};
\citeproc{ref-valeri2014}{Valeri \emph{et al.} 2014};
\citeproc{ref-vanderweele2014a}{VanderWeele and Vansteelandt 2014};
\citeproc{ref-vansteelandt2012}{Vansteelandt \emph{et al.} 2012}).
However, to express these quantities requires conceptualising them in
relation to counterfactuals. Lacking a counterfactual framework, it is
unclear what our statistical analysis would be estimating. Note that
VanderWeele (\citeproc{ref-vanderweele2015}{2015}) provides a full
decomposition that includes causal interaction in settings of causal
mediation.

We next use sequentially ordered causal diagrams assess the stringent
demands for satisfying the assumptions of `no unmeasured confounding' in
causal mediation analysis.

\paragraph{3.2.3 Sequential causal diagrams in causal mediation
analysis}\label{sequential-causal-diagrams-in-causal-mediation-analysis}

\begin{figure}

\centering{

\includegraphics[width=0.8\textwidth,height=\textheight]{short-causal-diagrams_files/figure-pdf/fig-dag-mediation-assumptions-1.pdf}

}

\caption{\label{fig-dag-mediation-assumptions}This causal diagram
illustrates the four fundamental assumptions needed for causal mediation
analysis. The first assumption pertains to the brown paths. It requires
the absence of an unmeasured exposure-outcome confounder, and assumes
that conditioning on L is sufficient for such confounding control. The
second assumption pertains to the blue paths. It requires the absence of
an unmeasured mediator-outcome confounder and assumes that conditioning
on V is sufficient for such confounding control. The third assumption
pertains to the green paths. It requires the absence of an unmeasured
exposure-mediator confounder and assumes that conditioning on Q is
sufficient for such confounding control. The fourth and final assumption
pertains to the red paths. It requires the absence of a mediator-outcome
confounder that is affected by the exposure and assumes that there is no
path from the exposure to V to M. If the exposure were to affect V, then
conditioning on V would block the exposure's effect on the mediator, as
indicated by dashed red path.}

\end{figure}%

Recall the advice for confounding control that we encountered repeatedly
in Part 2:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Ensure \(A_1\) occurs before \(Y_2\).
\item
  Ensure \(L_0\) occurs before \(A_1\).
\end{enumerate}

Although this advice is useful when estimating the causal effects of
single exposures, matters are more complicated when estimating the
causal effects of multiple exposures.

Consider again the hypothesis that cultural beliefs in `big Gods'
influence social complexity, with political authority serving as a
mediator. We assume for present purposes we have well-defined
interventions and outcomes. What requirements are necessary to answer
our causal mediation question?

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{No unmeasured exposure-outcome confounder}
\end{enumerate}

This requirement is expressed: \(Y(a,m) \coprod A | L\). After
accounting for the covariates in set \(L\), there must be no unmeasured
confounders influencing cultural beliefs in Big Gods, \(A\), and social
complexity \(Y\). For example, if our study examines the causal effect
of cultural beliefs in Big Gods (the exposure) on social complexity (the
outcome), and the covariates in \(L\) include factors such as geographic
location and historical context, we need to ensure that these covariates
effectively block any confounding paths between \(A\) and \(Y\).
Figure~\ref{fig-dag-mediation-assumptions} shows this confounding path
in brown.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  \textbf{No unmeasured mediator-outcome confounder}
\end{enumerate}

This requirement is expressed: \(Y(a,m) \coprod M | V\). After
controlling for the covariate set \(V\), we must ensure that no other
unmeasured confounders affect the political authority \(M\) and social
complexity \(Y\). For instance, if trade networks affect political
authority and social complexity, to obstruct the unblocked path linking
our mediator and outcome we must account for trade networks.
Furthermore, we must be entitled to assume the absence of any other
confounders for the mediator-outcome path.
Figure~\ref{fig-dag-mediation-assumptions} shows this confounding path
in blue.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  \textbf{No unmeasured exposure-mediator confounder}
\end{enumerate}

This requirement is expressed: \(M(a) \coprod A | Q\). After controlling
for the covariate set \(Q\), we must ensure that no additional
unmeasured confounders affect cultural beliefs in big Gods \(A\) and
political authority \(M\). For example, the capability to construct
large ritual theatres may influence the belief in big Gods and the level
of political authority. If we have indicators for this technology
measured prior to the emergence of big Gods (these indicators being
\(Q\)), we must assume that accounting for \(Q\) closes the backdoor
path between the exposure and the mediator.
Figure~\ref{fig-dag-mediation-assumptions} shows this confounding path
in green.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{3}
\tightlist
\item
  \textbf{No mediator-outcome confounder affected by the exposure}
\end{enumerate}

This requirement is expressed: \(Y(a,m) \coprod M(a^*) | V\). We must
ensure that no variables confounding the relationship between political
authority and social complexity in \(V\) are themselves influenced by
the cultural beliefs in big Gods (\(A\)). For example, when studying the
effect of cultural beliefs in big Gods (\(A\), the exposure) on social
complexity (\(Y\), the outcome) as mediated by political authority
(mediator), there can be no un-modelled factors, such as trade networks
(\(V\)), that influence both political authority and social complexity
and are themselves affected by the belief in big Gods.
Figure~\ref{fig-dag-mediation-assumptions} shows this confounding path,
\(A\to \boxed{V}\rightarrowdotted M\).

Assumption 4, that there is no exposure-induced confounding in the
mediator-outcome relationship, is often a considerable obstacle for
causal mediation analysis. Where the exposure influences a confounder of
the mediator and outcome, we face a dilemma. Without adjusting for this
confounder, a backdoor path between the mediator and the outcome would
remain open. However, by adjusting for it, we partially obstruct the
path between the exposure and the mediator, leading to bias. In this
setting, we cannot recover the natural direct and indirect effects
directly from any observational data and may need to settle for
investigating controlled direct effects, which stipulate fixed values
for the mediator; see: VanderWeele
(\citeproc{ref-vanderweele2015}{2015}); Robins and Greenland
(\citeproc{ref-robins1992}{1992}).

Notice again that the requirements for counterfactual data analysis are
considerably stricter than has been appreciated in the structural
equation modelling traditions. Natural direct effect estimates and
natural indirect effects estimates require conceptualising a
counterfactual that is never directly observed from the data, namely:
\(\textcolor{blue}{Y(1, M(0))}\) see: VanderWeele
(\citeproc{ref-vanderweele2015}{2015}).

Unfortunately, a generation of researchers must unlearn the habit of
leaping from a description of a statistical process as embodied in a
structural equation diagram to the analysis of the data. It has been
over three decades since Robins and Greenland demonstrated that we
cannot understand the quantities we are estimating in mediation analysis
without first specifying the estimands of interest in terms of the
targeted counterfactuals of interest (\citeproc{ref-robins1992}{Robins
and Greenland 1992}).

\paragraph{3.2.4 Controlled direct effects (and other estimands for
mediation)}\label{controlled-direct-effects-and-other-estimands-for-mediation}

In the previous section, we focused on the assumptions necessary for
decomposing natural direct and indirect effects in causal mediation
analysis. It is crucial to note that consistent estimates for natural
direct and indirect effects are compromised if there exists a confounder
affected by the exposure, which also influences the mediator-outcome
relationship. Nonetheless, if all other assumptions hold, we can fix
this mediator at a specific level to estimate a `controlled direct
effect' of the exposure at different mediator levels.

Consider a scenario where estimating a controlled direct effect is of
interest. Suppose we aim to understand the effect of a stringent
pandemic lockdown, \(A\), on psychological distress, \(Y\), focusing on
trust in government, \(M\), as a mediator. Further, suppose that
pandemic lockdowns may plausibly influence attitudes towards the
government through pathways that also affect psychological distress. For
instance, people might trust the government more when it provides income
relief payments, which may also reduce psychological distress. Under the
rules of d-separation, conditioning on income relief payments, denoted
as \(V\), would attenuate the natural value of the mediator, trust in
the government, under exposure to the lockdowns. This blocking of the
exposure's effect is represented by the causal path
\(A \to \boxed{V} \rightarrowdotted Y\). Additionally, the exposure's
effect on the mediator is partially blocked by the causal path
\(A \to \boxed{V} \rightarrowdotted M\). However, if we do not condition
on \(V\), the path from trust in government, \(M\), to psychological
distress, \(Y\), would be confounded by the common cause \(V\), hence:
\(Y \leftarrowred V \rightarrowred M\).

In such a scenario, it would not be feasible to consistently decompose
the total effect of the exposure (pandemic lockdowns) on the outcome
(psychological distress) into natural indirect and direct effects.
Nevertheless, if all other assumptions hold, we could ascertain from
data the controlled direct effect of pandemic lockdowns on psychological
distress under fixed levels of trust in government.

For example, we could examine the effect of the pandemic lockdown if we
were able to intervene and set everyone's trust in government to, say,
one standard deviation above the baseline, compared with fixing trust in
government to the average level at baseline. We might use `shift
functions' that specify interventions as functions of the data. For
instance, we might investigate interventions that `shift only those
whose mistrust of government was below the mean level of trust at
baseline and compare these potential outcomes with those observed.'
Asking and answering precisely formulated causal questions such as this
might lead to clearer policy advice, especially in situations where
policymakers can influence public attitudes towards the government; see:
Williams and Díaz (\citeproc{ref-williams2021}{2021}); Díaz \emph{et
al.} (\citeproc{ref-duxedaz2021}{2021}); Hoffman \emph{et al.}
(\citeproc{ref-hoffman2022}{2022}); Hoffman \emph{et al.}
(\citeproc{ref-hoffman2023}{2023}).

In any case, I hope this brief discussion of causal mediation analysis
clarifies that it would be unwise to simply examine the coefficients
obtained from structural equation models and interpret them as
meaningful as in statistical mediation analysis. We have no guarantees
that these coefficients are interpretable. Rather, to answer any causal
question, we must first state it, with respect to clearly defined
counterfactual contrasts and a target population.

For those interested in estimands for causal mediation analysis, I
recommend visiting the CMAverse website
(\url{https://bs1125.github.io/CMAverse/articles/overview.html},
accessed 12 December 2023). This excellent resource provides
comprehensive documentation, software, and practical examples, including
sensitivity analyses. Next, we will consider more complex scenarios that
involve feedback between treatments and confounders across multiple time
points, settings in which traditional statistical methods also fail
provide valid causal inferences.

\subsubsection{3.3 Causation in repeated measures time-series
data}\label{causation-in-repeated-measures-time-series-data}

\paragraph{3.3.1 Multiple sequential exposures generate counterfactuals
for each exposure that is modelled in
time}\label{multiple-sequential-exposures-generate-counterfactuals-for-each-exposure-that-is-modelled-in-time}

Our discussion of causal mediation analysis focused on how effects from
two sequential exposures may combine to influence an outcome. This
concept can be expanded to investigate the causal effects of multiple
sequential exposures. In such cases, researchers often gravitate to
longitudinal growth models and multi-level models. Where do
counterfactuals fit within these modelling traditions? Without
incorporating counterfactuals, the conclusions we derive lack clear
causal interpretations. Sequentially ordered causal diagrams help us to
understand the challenges, and opportunities, when attempting to address
causal questions in these scenarios using repeated measures data.

Consider a study in which we hope to estimate the effects of multiple
sequential exposures on multiple sequential outcomes over time. Imagine
there are two treatment intervals and two outcomes at each interval,
each corresponding to one of the following fixed treatment regimens:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Always Treat (Y(1,1))}
\item
  \textbf{Never Treat (Y(0,0))}
\item
  \textbf{Treat Once First (Y(1,0))}
\item
  \textbf{Treat Once Second (Y(0,1))}
\end{enumerate}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.1351}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.5405}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3243}}@{}}
\caption{Table outlines four fixed treatment regimens and six causal
contrasts in time-series data where exposure
varies.}\label{tbl-regimens}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Type
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Description
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Counterfactual Outcome
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Type
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Description
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Counterfactual Outcome
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Regime & Always treat & Y(1,1) \\
Regime & Never treat & Y(0,0) \\
Regime & Treat once first & Y(1,0) \\
Regime & Treat once second & Y(0,1) \\
Contrast & Always treat vs.~Never treat & E{[}Y(1,1) - Y(0,0){]} \\
Contrast & Always treat vs.~Treat once first & E{[}Y(1,1) - Y(1,0){]} \\
Contrast & Always treat vs.~Treat once second & E{[}Y(1,1) -
Y(0,1){]} \\
Contrast & Never treat vs.~Treat once first & E{[}Y(0,0) - Y(1,0){]} \\
Contrast & Never treat vs.~Treat once second & E{[}Y(0,0) - Y(0,1){]} \\
Contrast & Treat once first vs.~Treat once second & E{[}Y(1,0) -
Y(0,1){]} \\
\end{longtable}

We can compute six causal contrasts for these four fixed regimens, as
shown in Table~\ref{tbl-regimens}.

Treatment assignments may be modelled as conditional shift functions of
previous outcomes, a concept known as `time-varying treatment regimens'
(\citeproc{ref-duxedaz2021}{Díaz \emph{et al.} 2021};
\citeproc{ref-robins2008estimation}{Robins and Hernan 2008}) Comparisons
between relevant counterfactual quantities are necessary to estimate the
causal effects of time-varying treatment regimens. In causal mediation
analysis time-varying confounding was a concern (recall causal mediation
Condition 4: the exposure must not affect the confounders of the
mediator/outcome path). The same principle of confounding control
applies to sequential time-varying treatments. Unlike traditional causal
mediation analysis, however, we might be interested in treatment
sequences extended to more than two intervention intervals. Which
estimand we wish to recover must be explicitly stated, for only then can
we understand what it is that our statistical model aims to recover; see
Richardson and Robins (\citeproc{ref-richardson2013}{2013}).

\paragraph{3.3.2 Sequential causal diagrams clarify treatment confounder
feedback}\label{sequential-causal-diagrams-clarify-treatment-confounder-feedback}

Sequential causal diagrams help us to reveal limitations with
traditional multi-level regression analysis and structural equation
modelling analysis. Consider again a study that aims to assess certain
causal effects of beliefs in Big Gods on social complexity. We start by
estimating a fixed treatment regime. Assume we have well-defined
concepts of big Gods and social complexity, and assume we have accurate
measurements over time. Suppose we assess the effects of beliefs in big
Gods two centuries after a shift to big-Gods has occurred.

Fixed treatment strategies require comparing `always believe in big
Gods' versus `never believe in big Gods' and their effects on social
complexity conceived as a counterfactual contrast across conditionally
exchangeable groups of `treated' and `untreated' societies. Refer to
Figure~\ref{fig-dag-9}. Here, \(A_{t}\) represents the belief in big
Gods at time \(t\), and \(Y_{t}\) denotes the outcome, social
complexity, at time \(t\). Imagine economic trade, represented as
\(L_{t}\), is a time-varying confounder that changes in response to the
exposure, and influences economic trade. An unmeasured confounder,
\(U\), such as oral traditions, might also influence belief in big Gods
and social complexity.

In a scenario where we can reasonably infer that the level of economic
trade at time \(0\) (\(L_{0}\)) affects beliefs in big Gods at time
\(1\) (\(A_{1}\)), we draw an arrow from \(L_{0}\) to \(A_{1}\).
Conversely, if belief in big Gods at time \(1\) (\(A_{1}\)) affects
future levels of economic trade (\(L_{2}\)), an arrow from \(A_{1}\) to
\(L_{2}\) is warranted. This causal diagram demonstrates a feedback
process between the time-varying exposure \(A\) and the time-varying
confounder \(L\). Figure~\ref{fig-dag-9} dispLs this exposure-confounder
feedback dynamic. In practical scenarios, the diagram might include more
arrows, but our goal here is to illustrate the issue of
exposure-confounder feedback with the minimal necessary arrows.

What would happen if we were to condition on the time-varying confounder
\(L_{2}\)? We would block all backdoor paths between the exposure
\(A_{1}\) and the outcome \(Y_{4}\), which is crucial for eliminating
confounding. This result of our confounding strategy is positive: we
exert confounding control. However, this conditioning also closes
previously open paths, introducing structural sources of bias. For
example, the path
\(A_{1} \rightarrowred \boxed{ L_{2}} \leftarrowred U \rightarrowred Y_{4}\),
previously closed, would now be activated as the time-varying confounder
\(\boxed{ L_{2}}\) is a common effect of \(A_{1}\) and \(U\). This
result of our confounding strategy is negative: we lose confounding
control. Conditioning on a time-varying confounder is a double-edged
sword: it is essential for blocking backdoor paths but it potentially
opens other problematic pathways of bias. This conundrum when
conditioning on a confounder affected by prior exposure -- being damned
if we do, damned if we don't -- is a critical consideration in
longitudinal analysis. Knowing nothing else, we may presume it to be the
rule, rather than the exception.

\begin{figure}

\centering{

\includegraphics[width=1\textwidth,height=\textheight]{short-causal-diagrams_files/figure-pdf/fig-dag-9-1.pdf}

}

\caption{\label{fig-dag-9}Exposure confounder feedback is a problem for
time-series models. If we do not condition on L\(_2\), a backdoor path
is open from A\(_3\) to Y\(_4\) (we remove this path to simplify the
graph). However, if conditioning on L\(_2\) introduces collider bias,
opening a path, coloured in red, between A\(_1\) and Y\(_4\). Here, we
cannot use conventional methods to estimate the effects of multiple
exposures. Instead we must use special methods, such G-methods, TMLE,
SDR and others. Structural equation models (SEM) or multi-level models
will not eliminate bias here.}

\end{figure}%

\paragraph{3.3.3 Sequential causal diagrams clarify time-varying
confounding in the absence of treatment-confounder
feedback}\label{sequential-causal-diagrams-clarify-time-varying-confounding-in-the-absence-of-treatment-confounder-feedback}

We find scope for time-varying confounding, even in the absence of
treatment-confounder feedback (\citeproc{ref-hernan2023}{Hernan and
Robins 2023}). When a time-varying exposure and a time-varying
confounder share a common cause, even in cases where the exposure does
not directly influence the confounder, a backdoor path is opened because
the time-varying confounder is a common effect of two unmeasured
confounders. Figure~\ref{fig-dag-time-vary-common-cause-A1-l1} presents
this scenario, with the red paths revealing the bias. Again, standard
methods such as regression and structural equation modelling (SEM)
cannot recover unbiased causal effect estimates.

\begin{figure}

\centering{

\includegraphics[width=1\textwidth,height=\textheight]{short-causal-diagrams_files/figure-pdf/fig-dag-time-vary-common-cause-A1-l1-1.pdf}

}

\caption{\label{fig-dag-time-vary-common-cause-A1-l1}Exposure confounder
feedback is a problem for time-series models. Here, we do not assume
that A\_1 affects L\_2. However, if an unmeasured variable, U2, affects
both the exposure A\_1 and the confounder L\_2. Conditioning on L\_2
opens a path, outlined in red, linking the exposure to outcome. We
cannot use standard methods such as structural equation modelling (SEM)
or multi-level models to recover our desired causal effect estimates.}

\end{figure}%

\paragraph{3.3.4 Standard methods fail to address time-varying
confounding}\label{standard-methods-fail-to-address-time-varying-confounding}

Time varying confounding poses significant challenges in many human
sciences, and the problems are pronounced in the evolutionary human
science. Causal diagrams reveal these problem are not adequately
addressed by conventional regression-based methods, including
multi-level models and SEM because we cannot simultaneously control
confounding, avoid mediator bias, and avoid collider stratification bias
(\citeproc{ref-hernuxe1n2006}{Hernán and Robins 2006};
\citeproc{ref-robins1986}{Robins 1986}; \citeproc{ref-robins1999}{Robins
\emph{et al.} 1999}). The first wave of solutions involved methods in
which inverse treatment probability weighting (marginal structural
models) and regression simulation-prediction methods (g-computation) and
combinations thereof enabled robust estimation of the desired
quantities. Recent advances combining targeted learning and
semi-parametric estimation methods using machine learning further reduce
the demands on investigators to correctly specify the functional forms
of their models (\citeproc{ref-duxedaz2021}{Díaz \emph{et al.} 2021},
\citeproc{ref-duxedaz2021}{2021}; \citeproc{ref-hoffman2022}{Hoffman
\emph{et al.} 2022}, \citeproc{ref-hoffman2023}{2023};
\citeproc{ref-vanderlaan2018}{Van Der Laan and Rose 2018};
\citeproc{ref-wager2018}{Wager and Athey 2018}). These methods have yet
gained widespread acceptance across the human sciences, including the
evolutionary sciences. I hope this article encourages the readers of
this special issue of \emph{Evolutionary Human Sciences} to learn more
about these causal inference methods. Useful overviews include: Hernan
and Robins (\citeproc{ref-hernan2023}{2023}); Díaz \emph{et al.}
(\citeproc{ref-duxedaz2021}{2021}); VanderWeele
(\citeproc{ref-vanderweele2015}{2015}); Hoffman \emph{et al.}
(\citeproc{ref-hoffman2022}{2022}); Hoffman \emph{et al.}
(\citeproc{ref-hoffman2023}{2023}); Chatton \emph{et al.}
(\citeproc{ref-chatton2020}{2020}); Shiba and Kawahara
(\citeproc{ref-shiba2021}{2021}); Sjölander
(\citeproc{ref-sjuxf6lander2016}{2016}); Breskin \emph{et al.}
(\citeproc{ref-breskin2020}{2020}); VanderWeele
(\citeproc{ref-vanderweele2009a}{2009b}); Vansteelandt \emph{et al.}
(\citeproc{ref-vansteelandt2012}{2012}); Shi \emph{et al.}
(\citeproc{ref-shi2021}{2021}).

Finally, `Single World Intervention Graphs' (SWIGs) explicitly represent
counterfactual outcomes on a graph. These graphs are powerful tools for
developing analytic strategies when there is time-varying confounding.
As their developers emphasises, SWIGs should be viewed as templates
rather than causal diagrams because they do not adhere to the property
of Markov factorisation. The use of SWIGs goes beyond the scope of this
guide, however, for those interested, more information can be found in
Richardson and Robins (\citeproc{ref-richardson2013}{2013}) and Hernan
and Robins (\citeproc{ref-hernan2023}{2023}) ,pp 95-95.

\subsection{4 Conclusions}\label{conclusions}

\subsubsection{4.1 Summary}\label{summary}

Causation inherently occures in time, with causes preceding effects.
However, quantifying causal effects requires contrasting counterfactual
states -- events that could have occurred under different conditions but
which did not occur. To estimate average causal effects, which are
summaries of contrasts between these counterfactual states, causal data
science relies on a framework of explicit assumptions and clear,
multi-stepped workflows.

\textbf{Part 1} provided an overview of the basic assumptions for causal
inference. We began by specifying an explicit causal question pertaining
to clearly stated outcomes for a well-defined intervention on a target
population. From there, we must ensure that the exposures of interest
correspond to well-defined interventions (causal consistency), that
measured covariates control for confounding (exchangeability), and that
exposures to be compared have a non-zero probability of occurring at all
levels of these measured covariates (positivity) in the exposures we
seek to compare. We noted that there are further assumptions pertaining
to measurement and the retention of alignment of one's sample with the
target population, both at baseline and thereafter. There are,
additionally, ever-present threats to valid inference from model
misspecification. We learned that causal diagrams function primarily to
assist researchers in evaluating the exchangeability assumption. This
material is important because causal diagrams serve their purposes only
within the framework of assumptions and workflows of causal data
science. Outside this framework, causal diagrams can tempt false
confidence.

\textbf{Part 2} outlined key concepts and fundamental techniques for
utilising causal diagrams to address basic challenges in controlling
confounding. We highlighted the importance of preserving a chronological
sequence in causal diagrams to accurately reflect the presumed order of
causation. For example, by assessing confounders before exposures and
exposures before outcomes, we can prevent typical hazards to causal
inference, like mediator bias and post-stratification bias, as well as
the \textbf{causal incoherence} that arises from estimating outcomes
based on exposures. Causal diagrams are not just useful for assessing
control of confounding; they also clarify the essential requirements for
data collection.

\textbf{Part 3:} combined attention to the counterfactual framework with
sequentially ordered causal diagrams to clarify the concepts of causal
interaction (moderation), causal mediation, and dynamic longitudinal
feedback. We discovered that the concept of causal interaction can
either mean a combined effect of two joint interventions or the
modification of a single effect across sub-populations. To properly
evaluate `interaction', we must specify our causal questions in advance.
We found that causal mediation involves a dual exposure and that the
identification problems of causal mediation are subject to stringent
assumptions. We described special use cases for causal diagrams in
helping clarify features of treatment effect heterogeneity
(effect-modifier distribution bias), which may arise without
confounding-bias. We considered confounder-treatment feedback, in
settings where we are interested in the causal effects of multiple
interventions over time, and again encountered considerable
identification problems that elude standard methods for complex time
series data. Throughout, sequential causal diagrams have enriched our
understanding of the problems and opportunities for deriving valid
causal inferences from events encoded in data.

\subsubsection{4.2 The Causal Revolution
Ahead}\label{the-causal-revolution-ahead}

Although the causal revolution is progressing and gaining momentum,
supporting institutions must evolve to facilitate it. The necessity for
researchers to acquire new skills, coupled with the intensive
requirement for precise time-series data collection, has significant
implications for research design, funding, and the accepted speed of
scientific advancement. To foster essential changes in causal inference
education and practice, the human sciences need a shift from being
predominantly output-focused to creating supportive environments that
promote retraining and accurate time-series data collection. Such
investments are worthwhile. They will transition the human sciences from
butterfly collections of correlations to a deeper causal understanding
of the complex evolutionary, cultural, and psychological dynamics that
drive our curiosities.

\newpage{}

\subsection{Funding}\label{funding}

This work is supported by a grant from the Templeton Religion Trust
(TRT0418) and RSNZ Marsden 3721245, 20-UOA-123; RSNZ 19-UOO-090. I also
received support from the Max Planck Institute for the Science of Human
History. The Funders had no role in preparing the manuscript or the
decision to publish it.

\subsection{Acknowledgements}\label{acknowledgements}

I am grateful to Dr Inkuk Kim for checking my manuscript and offering
feedback.

I am also grateful to two anonymous reviewers and the editor, Charles
Efferson, of \emph{Evolutionary Human Sciences} for their constructive
feedback that improved this manuscript.

Any remaining errors are my own.

\subsection{References}\label{references}

\phantomsection\label{refs}
\begin{CSLReferences}{1}{0}
\bibitem[\citeproctext]{ref-angrist2009mostly}
Angrist, JD, and Pischke, J-S (2009) \emph{Mostly harmless econometrics:
An empiricist's companion}, Princeton university press.

\bibitem[\citeproctext]{ref-athey2019}
Athey, S, Tibshirani, J, and Wager, S (2019) Generalized random forests.
\emph{The Annals of Statistics}, \textbf{47}(2), 1148--1178.
doi:\href{https://doi.org/10.1214/18-AOS1709}{10.1214/18-AOS1709}.

\bibitem[\citeproctext]{ref-athey2021}
Athey, S, and Wager, S (2021) Policy Learning With Observational Data.
\emph{Econometrica}, \textbf{89}(1), 133--161.
doi:\href{https://doi.org/10.3982/ECTA15732}{10.3982/ECTA15732}.

\bibitem[\citeproctext]{ref-auspurg2021has}
Auspurg, K, and Brüderl, J (2021) Has the credibility of the social
sciences been credibly destroyed? Reanalyzing the {``many analysts, one
data set''} project. \emph{Socius}, \textbf{7}, 23780231211024421.

\bibitem[\citeproctext]{ref-bareinboim2013general}
Bareinboim, E, and Pearl, J (2013) A general algorithm for deciding
transportability of experimental results. \emph{Journal of Causal
Inference}, \textbf{1}(1), 107--134.

\bibitem[\citeproctext]{ref-bareinboim2022}
Bareinboim, E, Tian, J, and Pearl, J (2022) Recovering from selection
bias in causal and statistical inference. In, 1st edn, Vol. 36, New
York, NY, USA: Association for Computing Machinery, 433450. Retrieved
from \url{https://doi.org/10.1145/3501714.3501740}

\bibitem[\citeproctext]{ref-barrett2021}
Barrett, M (2021) \emph{Ggdag: Analyze and create elegant directed
acyclic graphs}. Retrieved from
\url{https://CRAN.R-project.org/package=ggdag}

\bibitem[\citeproctext]{ref-basten2013}
Basten, C, and Betz, F (2013) Beyond work ethic: Religion, individual,
and political preferences. \emph{American Economic Journal: Economic
Policy}, \textbf{5}(3), 67--91.
doi:\href{https://doi.org/10.1257/pol.5.3.67}{10.1257/pol.5.3.67}.

\bibitem[\citeproctext]{ref-becker2016}
Becker, SO, Pfaff, S, and Rubin, J (2016) Causes and consequences of the
protestant reformation. \emph{Explorations in Economic History},
\textbf{62}, 125.

\bibitem[\citeproctext]{ref-beheim2021}
Beheim, B, Atkinson, QD, Bulbulia, J, \ldots{} Willard, AK (2021)
Treatment of missing data determined conclusions regarding moralizing
gods. \emph{Nature}, \textbf{595}(7866), E29--E34.
doi:\href{https://doi.org/10.1038/s41586-021-03655-4}{10.1038/s41586-021-03655-4}.

\bibitem[\citeproctext]{ref-blackwell2017}
Blackwell, M, Honaker, J, and King, G (2017) A unified approach to
measurement error and missing data: Overview and applications.
\emph{Sociological Methods and Research}, \textbf{46}(3), 303--341.
Retrieved from
\url{http://journals.sagepub.com/doi/full/10.1177/0049124115585360}

\bibitem[\citeproctext]{ref-breskin2020}
Breskin, A, Edmonds, A, Cole, SR, \ldots{} others (2020) G-computation
for policy-relevant effects of interventions on time-to-event outcomes.
\emph{International Journal of Epidemiology}, \textbf{49}(6), 20212029.

\bibitem[\citeproctext]{ref-bulbulia2022}
Bulbulia, JA (2022) A workflow for causal inference in cross-cultural
psychology. \emph{Religion, Brain \& Behavior}, \textbf{0}(0), 1--16.
doi:\href{https://doi.org/10.1080/2153599X.2022.2070245}{10.1080/2153599X.2022.2070245}.

\bibitem[\citeproctext]{ref-bulbulia2023e}
Bulbulia, JA (2023a) Measurement error bias explained with sequential
causal diagrams. Retrieved from \url{https://osf.io/jq78d}

\bibitem[\citeproctext]{ref-bulbulia2023c}
Bulbulia, JA (2023b) Selection bias (with and without confounding)
explained with sequential causal diagrams. Retrieved from
\url{https://osf.io/cjgey}

\bibitem[\citeproctext]{ref-bulbulia2023a}
Bulbulia, JA, Afzali, MU, Yogeeswaran, K, and Sibley, CG (2023)
Long-term causal effects of far-right terrorism in {N}ew {Z}ealand.
\emph{PNAS Nexus}, \textbf{2}(8), pgad242.

\bibitem[\citeproctext]{ref-bulbuliaj.2013}
Bulbulia, J., Geertz, AW, Atkinson, QD, \ldots{} Wilson, DS (2013) The
cultural evolution of religion. In P. J. Richerson and M. Christiansen,
eds., Cambridge, MA: MIT press, 381404.

\bibitem[\citeproctext]{ref-calonico2022}
Calonico, S, Cattaneo, MD, Farrell, MH, and Titiunik, R (2022)
\emph{Rdrobust: Robust data-driven statistical inference in
regression-discontinuity designs}. Retrieved from
\url{https://CRAN.R-project.org/package=rdrobust}

\bibitem[\citeproctext]{ref-chatton2020}
Chatton, A, Le Borgne, F, Leyrat, C, \ldots{} Foucher, Y (2020)
G-computation, propensity score-based methods, and targeted maximum
likelihood estimator for causal inference with different covariates
sets: a comparative simulation study. \emph{Scientific Reports},
\textbf{10}(1), 9219.
doi:\href{https://doi.org/10.1038/s41598-020-65917-x}{10.1038/s41598-020-65917-x}.

\bibitem[\citeproctext]{ref-cinelli2022}
Cinelli, C, Forney, A, and Pearl, J (2022) A Crash Course in Good and
Bad Controls. \emph{Sociological Methods \& Research},
00491241221099552.
doi:\href{https://doi.org/10.1177/00491241221099552}{10.1177/00491241221099552}.

\bibitem[\citeproctext]{ref-cole2010}
Cole, SR, Platt, RW, Schisterman, EF, \ldots{} Poole, C (2010)
Illustrating bias due to conditioning on a collider. \emph{International
Journal of Epidemiology}, \textbf{39}(2), 417--420.
doi:\href{https://doi.org/10.1093/ije/dyp334}{10.1093/ije/dyp334}.

\bibitem[\citeproctext]{ref-cole2010generalizing}
Cole, SR, and Stuart, EA (2010) Generalizing evidence from randomized
clinical trials to target populations: The ACTG 320 trial.
\emph{American Journal of Epidemiology}, \textbf{172}(1), 107--115.

\bibitem[\citeproctext]{ref-collinson2003}
Collinson, P (2003) \emph{The reformation: A history}, Weidenfeld;
Nicholson; London, England.

\bibitem[\citeproctext]{ref-cui2020}
Cui, Y, Kosorok, MR, Sverdrup, E, Wager, S, and Zhu, R (2020) Estimating
heterogeneous treatment effects with right-censored data via causal
survival forests. Retrieved from
\url{https://arxiv.org/abs/2001.09887v5}

\bibitem[\citeproctext]{ref-dahabreh2021study}
Dahabreh, IJ, Haneuse, SJA, Robins, JM, \ldots{} Hernán, MA (2021) Study
designs for extending causal inferences from a randomized trial to a
target population. \emph{American Journal of Epidemiology},
\textbf{190}(8), 1632--1642.

\bibitem[\citeproctext]{ref-dahabreh2019}
Dahabreh, IJ, and Hernán, MA (2019) Extending inferences from a
randomized trial to a target population. \emph{European Journal of
Epidemiology}, \textbf{34}(8), 719--722.
doi:\href{https://doi.org/10.1007/s10654-019-00533-2}{10.1007/s10654-019-00533-2}.

\bibitem[\citeproctext]{ref-danaei2012}
Danaei, G, Tavakkoli, M, and Hernán, MA (2012) Bias in observational
studies of prevalent users: lessons for comparative effectiveness
research from a meta-analysis of statins. \emph{American Journal of
Epidemiology}, \textbf{175}(4), 250--262.
doi:\href{https://doi.org/10.1093/aje/kwr301}{10.1093/aje/kwr301}.

\bibitem[\citeproctext]{ref-decoulanges1903}
De Coulanges, F (1903) \emph{La cité antique: Étude sur le culte, le
droit, les institutions de la grèce et de rome}, Hachette.

\bibitem[\citeproctext]{ref-deffner2022}
Deffner, D, Rohrer, JM, and McElreath, R (2022) A Causal Framework for
Cross-Cultural Generalizability. \emph{Advances in Methods and Practices
in Psychological Science}, \textbf{5}(3), 25152459221106366.
doi:\href{https://doi.org/10.1177/25152459221106366}{10.1177/25152459221106366}.

\bibitem[\citeproctext]{ref-duxedaz2021}
Díaz, I, Williams, N, Hoffman, KL, and Schenck, EJ (2021) Non-parametric
causal effects based on longitudinal modified treatment policies.
\emph{Journal of the American Statistical Association}.
doi:\href{https://doi.org/10.1080/01621459.2021.1955691}{10.1080/01621459.2021.1955691}.

\bibitem[\citeproctext]{ref-edwards2015}
Edwards, JK, Cole, SR, and Westreich, D (2015) All your data are always
missing: Incorporating bias due to measurement error into the potential
outcomes framework. \emph{International Journal of Epidemiology},
\textbf{44}(4), 14521459.

\bibitem[\citeproctext]{ref-foster2023}
Foster, DJ, and Syrgkanis, V (2023) Orthogonal statistical learning.
\emph{The Annals of Statistics}, \textbf{51}(3), 879--908.
doi:\href{https://doi.org/10.1214/23-AOS2258}{10.1214/23-AOS2258}.

\bibitem[\citeproctext]{ref-gawthrop1984}
Gawthrop, R, and Strauss, G (1984) Protestantism and literacy in early
modern germany. \emph{Past \& Present}, (104), 3155.

\bibitem[\citeproctext]{ref-geertz2013}
Geertz, AW, Atkinson, QD, Cohen, E, \ldots{} Wilson, DS (2013) The
cultural evolution of religion. In P. J. Richerson and M. Christiansen,
eds., Cambridge, MA: MIT press, 381404.

\bibitem[\citeproctext]{ref-greenland2003quantifying}
Greenland, S (2003) Quantifying biases in causal models: Classical
confounding vs collider-stratification bias. \emph{Epidemiology},
300--306.

\bibitem[\citeproctext]{ref-greenland2009commentary}
Greenland, S (2009) Commentary: Interactions in epidemiology: Relevance,
identification, and estimation. \emph{Epidemiology}, \textbf{20}(1),
14--17.

\bibitem[\citeproctext]{ref-greenland1999}
Greenland, S, Pearl, J, and Robins, JM (1999a) Causal diagrams for
epidemiologic research. \emph{Epidemiology (Cambridge, Mass.)},
\textbf{10}(1), 37--48.

\bibitem[\citeproctext]{ref-greenland1999c}
Greenland, S, Pearl, J, and Robins, JM (1999b) Causal diagrams for
epidemiologic research. \emph{Epidemiology (Cambridge, Mass.)},
\textbf{10}(1), 37--48.

\bibitem[\citeproctext]{ref-greifer2023}
Greifer, N, Worthington, S, Iacus, S, and King, G (2023) \emph{Clarify:
Simulation-based inference for regression models}. Retrieved from
\url{https://iqss.github.io/clarify/}

\bibitem[\citeproctext]{ref-hahn2020}
Hahn, PR, Murray, JS, and Carvalho, CM (2020) Bayesian regression tree
models for causal inference: Regularization, confounding, and
heterogeneous effects (with discussion). \emph{Bayesian Analysis},
\textbf{15}(3), 965--1056.
doi:\href{https://doi.org/10.1214/19-BA1195}{10.1214/19-BA1195}.

\bibitem[\citeproctext]{ref-hernan2023}
Hernan, MA, and Robins, JM (2023) \emph{Causal inference}, Taylor \&
Francis. Retrieved from
\url{https://books.google.co.nz/books?id=/_KnHIAAACAAJ}

\bibitem[\citeproctext]{ref-hernuxe1n2004}
Hernán, MA (2004) A definition of causal effect for epidemiological
research. \emph{Journal of Epidemiology \& Community Health},
\textbf{58}(4), 265--271.
doi:\href{https://doi.org/10.1136/jech.2002.006361}{10.1136/jech.2002.006361}.

\bibitem[\citeproctext]{ref-hernuxe1n2017}
Hernán, MA (2017) Invited commentary: Selection bias without colliders
\textbar{} american journal of epidemiology \textbar{} oxford academic.
\emph{American Journal of Epidemiology}, \textbf{185}(11), 10481050.
Retrieved from \url{https://doi.org/10.1093/aje/kwx077}

\bibitem[\citeproctext]{ref-hernuxe1n2008a}
Hernán, MA, Alonso, A, Logan, R, \ldots{} Robins, JM (2008)
Observational studies analyzed like randomized experiments: An
application to postmenopausal hormone therapy and coronary heart
disease. \emph{Epidemiology}, \textbf{19}(6), 766.
doi:\href{https://doi.org/10.1097/EDE.0b013e3181875e61}{10.1097/EDE.0b013e3181875e61}.

\bibitem[\citeproctext]{ref-hernuxe1n2009}
Hernán, MA, and Cole, SR (2009) Invited commentary: Causal diagrams and
measurement bias. \emph{American Journal of Epidemiology},
\textbf{170}(8), 959--962.
doi:\href{https://doi.org/10.1093/aje/kwp293}{10.1093/aje/kwp293}.

\bibitem[\citeproctext]{ref-hernuxe1n2004a}
Hernán, MA, Hernández-Díaz, S, and Robins, JM (2004) A structural
approach to selection bias. \emph{Epidemiology}, \textbf{15}(5),
615--625. Retrieved from \url{https://www.jstor.org/stable/20485961}

\bibitem[\citeproctext]{ref-hernuxe1n2023}
Hernán, MA, and Monge, S (2023) Selection bias due to conditioning on a
collider. \emph{BMJ}, \textbf{381}, p1135.
doi:\href{https://doi.org/10.1136/bmj.p1135}{10.1136/bmj.p1135}.

\bibitem[\citeproctext]{ref-hernuxe1n2006}
Hernán, MA, and Robins, JM (2006) Estimating causal effects from
epidemiological data. \emph{Journal of Epidemiology \& Community
Health}, \textbf{60}(7), 578--586.
doi:\href{https://doi.org/10.1136/jech.2004.029496}{10.1136/jech.2004.029496}.

\bibitem[\citeproctext]{ref-hernan2017per}
Hernán, MA, Robins, JM, et al. (2017) Per-protocol analyses of pragmatic
trials. \emph{N Engl J Med}, \textbf{377}(14), 1391--1398.

\bibitem[\citeproctext]{ref-hernuxe1n2016}
Hernán, MA, Sauer, BC, Hernández-Díaz, S, Platt, R, and Shrier, I
(2016a) Specifying a target trial prevents immortal time bias and other
self-inflicted injuries in observational analyses. \emph{Journal of
Clinical Epidemiology}, \textbf{79}, 7075.

\bibitem[\citeproctext]{ref-hernuxe1n2016b}
Hernán, MA, Sauer, BC, Hernández-Díaz, S, Platt, R, and Shrier, I
(2016b) Specifying a target trial prevents immortal time bias and other
self-inflicted injuries in observational analyses. \emph{Journal of
Clinical Epidemiology}, \textbf{79}, 7075.

\bibitem[\citeproctext]{ref-hernuxe1n2008}
Hernán, MA, and Taubman, SL (2008) Does obesity shorten life? The
importance of well-defined interventions to answer causal questions.
\emph{International Journal of Obesity (2005)}, \textbf{32 Suppl 3},
S8--14.
doi:\href{https://doi.org/10.1038/ijo.2008.82}{10.1038/ijo.2008.82}.

\bibitem[\citeproctext]{ref-hernuxe1n2022}
Hernán, MA, Wang, W, and Leaf, DE (2022) Target trial emulation: A
framework for causal inference from observational data. \emph{JAMA},
\textbf{328}(24), 2446--2447.
doi:\href{https://doi.org/10.1001/jama.2022.21383}{10.1001/jama.2022.21383}.

\bibitem[\citeproctext]{ref-hoffman2023}
Hoffman, KL, Salazar-Barreto, D, Rudolph, KE, and Díaz, I (2023)
Introducing longitudinal modified treatment policies: A unified
framework for studying complex exposures.
doi:\href{https://doi.org/10.48550/arXiv.2304.09460}{10.48550/arXiv.2304.09460}.

\bibitem[\citeproctext]{ref-hoffman2022}
Hoffman, KL, Schenck, EJ, Satlin, MJ, \ldots{} Díaz, I (2022) Comparison
of a target trial emulation framework vs cox regression to estimate the
association of corticosteroids with COVID-19 mortality. \emph{JAMA
Network Open}, \textbf{5}(10), e2234425.
doi:\href{https://doi.org/10.1001/jamanetworkopen.2022.34425}{10.1001/jamanetworkopen.2022.34425}.

\bibitem[\citeproctext]{ref-holland1986}
Holland, PW (1986) Statistics and causal inference. \emph{Journal of the
American Statistical Association}, \textbf{81}(396), 945960.

\bibitem[\citeproctext]{ref-horvitz1952generalization}
Horvitz, DG, and Thompson, DJ (1952) A generalization of sampling
without replacement from a finite universe. \emph{Journal of the
American Statistical Association}, \textbf{47}(260), 663--685.

\bibitem[\citeproctext]{ref-hume1902}
Hume, D (1902) \emph{Enquiries Concerning the Human Understanding: And
Concerning the Principles of Morals}, Clarendon Press.

\bibitem[\citeproctext]{ref-imai2008misunderstandings}
Imai, K, King, G, and Stuart, EA (2008) Misunderstandings between
experimentalists and observationalists about causal inference.
\emph{Journal of the Royal Statistical Society Series A: Statistics in
Society}, \textbf{171}(2), 481--502.

\bibitem[\citeproctext]{ref-johnson2015}
Johnson, DD (2015) Big gods, small wonder: Supernatural punishment
strikes back. \emph{Religion, Brain \& Behavior}, \textbf{5}(4), 290298.

\bibitem[\citeproctext]{ref-kennedy2023}
Kennedy, EH (2023) Towards optimal doubly robust estimation of
heterogeneous causal effects. \emph{Electronic Journal of Statistics},
\textbf{17}(2), 3008--3049.
doi:\href{https://doi.org/10.1214/23-EJS2157}{10.1214/23-EJS2157}.

\bibitem[\citeproctext]{ref-kern2016assessing}
Kern, HL, Stuart, EA, Hill, J, and Green, DP (2016) Assessing methods
for generalizing experimental impact estimates to target populations.
\emph{Journal of Research on Educational Effectiveness}, \textbf{9}(1),
103--127.

\bibitem[\citeproctext]{ref-kitagawa2018}
Kitagawa, T, and Tetenov, A (2018) Who should be treated? Empirical
welfare maximization methods for treatment choice. \emph{Econometrica},
\textbf{86}(2), 591--616. Retrieved from
\url{https://www.jstor.org/stable/44955978}

\bibitem[\citeproctext]{ref-kuxfcnzel2019}
Künzel, SR, Sekhon, JS, Bickel, PJ, and Yu, B (2019) Metalearners for
estimating heterogeneous treatment effects using machine learning.
\emph{Proceedings of the National Academy of Sciences},
\textbf{116}(10), 4156--4165.
doi:\href{https://doi.org/10.1073/pnas.1804597116}{10.1073/pnas.1804597116}.

\bibitem[\citeproctext]{ref-lash2020}
Lash, TL, Rothman, KJ, VanderWeele, TJ, and Haneuse, S (2020)
\emph{Modern epidemiology}, Wolters Kluwer. Retrieved from
\url{https://books.google.co.nz/books?id=SiTSnQEACAAJ}

\bibitem[\citeproctext]{ref-lauritzen1990}
Lauritzen, SL, Dawid, AP, Larsen, BN, and Leimer, H-G (1990)
Independence properties of directed markov fields. \emph{Networks},
\textbf{20}(5), 491505.

\bibitem[\citeproctext]{ref-lewis1973}
Lewis, D (1973) Causation. \emph{The Journal of Philosophy},
\textbf{70}(17), 556--567.
doi:\href{https://doi.org/10.2307/2025310}{10.2307/2025310}.

\bibitem[\citeproctext]{ref-li2023non}
Li, W, Miao, W, and Tchetgen Tchetgen, E (2023) Non-parametric inference
about mean functionals of non-ignorable non-response data without
identifying the joint distribution. \emph{Journal of the Royal
Statistical Society Series B: Statistical Methodology}, \textbf{85}(3),
913--935.

\bibitem[\citeproctext]{ref-lu2022}
Lu, H, Cole, SR, Howe, CJ, and Westreich, D (2022a) Toward a Clearer
Definition of Selection Bias When Estimating Causal Effects.
\emph{Epidemiology (Cambridge, Mass.)}, \textbf{33}(5), 699--706.
doi:\href{https://doi.org/10.1097/EDE.0000000000001516}{10.1097/EDE.0000000000001516}.

\bibitem[\citeproctext]{ref-lu2022toward}
Lu, H, Cole, SR, Howe, CJ, and Westreich, D (2022b) Toward a clearer
definition of selection bias when estimating causal effects.
\emph{Epidemiology}, \textbf{33}(5), 699--706.

\bibitem[\citeproctext]{ref-lu2021revisiting}
Lu, H, Cole, SR, Platt, RW, and Schisterman, EF (2021) Revisiting
overadjustment bias. \emph{Epidemiology}, \textbf{32}(5), e22--e23.

\bibitem[\citeproctext]{ref-lu2023selection}
Lu, H, Gonsalves, GS, and Westreich, D (2023) Selection bias requires
selection: The case of collider stratification bias. \emph{American
Journal of Epidemiology}, kwad213.

\bibitem[\citeproctext]{ref-lumley2004analysis}
Lumley, T (2004) Analysis of complex survey samples. \emph{Journal of
Statistical Software}, \textbf{9}, 1--19.

\bibitem[\citeproctext]{ref-major2023exploring}
Major-Smith, D (2023) Exploring causality from observational data: An
example assessing whether religiosity promotes cooperation.
\emph{Evolutionary Human Sciences}, \textbf{5}, e22.

\bibitem[\citeproctext]{ref-malinsky2022semiparametric}
Malinsky, D, Shpitser, I, and Tchetgen Tchetgen, EJ (2022)
Semiparametric inference for nonmonotone missing-not-at-random data: The
no self-censoring model. \emph{Journal of the American Statistical
Association}, \textbf{117}(539), 1415--1423.

\bibitem[\citeproctext]{ref-mcelreath2020}
McElreath, R (2020) \emph{Statistical rethinking: A {B}ayesian course
with examples in r and stan}, CRC press.

\bibitem[\citeproctext]{ref-montgomery2018}
Montgomery, JM, Nyhan, B, and Torres, M (2018) How conditioning on
posttreatment variables can ruin your experiment and what to do about
It. \emph{American Journal of Political Science}, \textbf{62}(3),
760--775.
doi:\href{https://doi.org/10.1111/ajps.12357}{10.1111/ajps.12357}.

\bibitem[\citeproctext]{ref-muuxf1oz2012}
Muñoz, ID, and Laan, M van der (2012) Population intervention causal
effects based on stochastic interventions. \emph{Biometrics},
\textbf{68}(2), 541--549.
doi:\href{https://doi.org/10.1111/j.1541-0420.2011.01685.x}{10.1111/j.1541-0420.2011.01685.x}.

\bibitem[\citeproctext]{ref-murray2021a}
Murray, EJ, Marshall, BDL, and Buchanan, AL (2021) Emulating target
trials to improve causal inference from agent-based models.
\emph{American Journal of Epidemiology}, \textbf{190}(8), 1652--1658.
doi:\href{https://doi.org/10.1093/aje/kwab040}{10.1093/aje/kwab040}.

\bibitem[\citeproctext]{ref-nalle1987}
Nalle, ST (1987) Inquisitors, priests, and the people during the
catholic reformation in spain. \emph{The Sixteenth Century Journal},
557587.

\bibitem[\citeproctext]{ref-nie2021}
Nie, X, and Wager, S (2021) Quasi-oracle estimation of heterogeneous
treatment effects. \emph{Biometrika}, \textbf{108}(2), 299--319.
doi:\href{https://doi.org/10.1093/biomet/asaa076}{10.1093/biomet/asaa076}.

\bibitem[\citeproctext]{ref-norenzayan2016}
Norenzayan, A, Shariff, AF, Gervais, WM, \ldots{} Henrich, J (2016) The
cultural evolution of prosocial religions. \emph{Behavioral and Brain
Sciences}, \textbf{39}, e1.
doi:\href{https://doi.org/10.1017/S0140525X14001356}{10.1017/S0140525X14001356}.

\bibitem[\citeproctext]{ref-ogburn2021}
Ogburn, EL, and Shpitser, I (2021) Causal modelling: The two cultures.
\emph{Observational Studies}, \textbf{7}(1), 179--183.
doi:\href{https://doi.org/10.1353/obs.2021.0006}{10.1353/obs.2021.0006}.

\bibitem[\citeproctext]{ref-ogburn2022}
Ogburn, EL, Sofrygin, O, Díaz, I, and Laan, MJ van der (2022) Causal
inference for social network data. \emph{Journal of the American
Statistical Association}, \textbf{0}(0), 1--15.
doi:\href{https://doi.org/10.1080/01621459.2022.2131557}{10.1080/01621459.2022.2131557}.

\bibitem[\citeproctext]{ref-pearl1988}
Pearl, J (1988) \emph{Probabilistic reasoning in intelligent systems:
Networks of plausible inference}, Morgan kaufmann.

\bibitem[\citeproctext]{ref-pearl1995}
Pearl, J (1995) Causal diagrams for empirical research.
\emph{Biometrika}, \textbf{82}(4), 669--688.

\bibitem[\citeproctext]{ref-pearl2009}
Pearl, J (2009a) \emph{\href{https://doi.org/10.1214/09-SS057}{Causal
inference in statistics: An overview}}.

\bibitem[\citeproctext]{ref-pearl2009a}
Pearl, J (2009b) \emph{Causality}, Cambridge University Press.

\bibitem[\citeproctext]{ref-pearl2018}
Pearl, J, and Mackenzie, D (2018) \emph{The book of why: The new science
of cause and effect}, Basic books.

\bibitem[\citeproctext]{ref-richardson2013}
Richardson, TS, and Robins, JM (2013) Single world intervention graphs:
A primer. In, Citeseer.

\bibitem[\citeproctext]{ref-robins1986}
Robins, J (1986) A new approach to causal inference in mortality studies
with a sustained exposure period---application to control of the healthy
worker survivor effect. \emph{Mathematical Modelling}, \textbf{7}(9-12),
1393--1512.

\bibitem[\citeproctext]{ref-robins2008estimation}
Robins, J, and Hernan, M (2008) Estimation of the causal effects of
time-varying exposures. \emph{Chapman \& Hall/CRC Handbooks of Modern
Statistical Methods}, 553--599.

\bibitem[\citeproctext]{ref-robins1992}
Robins, JM, and Greenland, S (1992) Identifiability and exchangeability
for direct and indirect effects. \emph{Epidemiology}, \textbf{3}(2),
143155.

\bibitem[\citeproctext]{ref-robins1999}
Robins, JM, Greenland, S, and Hu, F-C (1999) Estimation of the causal
effect of a time-varying exposure on the marginal mean of a repeated
binary outcome. \emph{Journal of the American Statistical Association},
\textbf{94}(447), 687--700.
doi:\href{https://doi.org/10.1080/01621459.1999.10474168}{10.1080/01621459.1999.10474168}.

\bibitem[\citeproctext]{ref-rohrer2018}
Rohrer, JM (2018) Thinking clearly about correlations and causation:
Graphical causal models for observational data. \emph{Advances in
Methods and Practices in Psychological Science}, \textbf{1}(1), 2742.

\bibitem[\citeproctext]{ref-rubin1976}
Rubin, DB (1976) Inference and missing data. \emph{Biometrika},
\textbf{63}(3), 581--592.
doi:\href{https://doi.org/10.1093/biomet/63.3.581}{10.1093/biomet/63.3.581}.

\bibitem[\citeproctext]{ref-sheehan2022}
Sheehan, O, Watts, J, Gray, RD, \ldots{} Atkinson, QD (2022) Coevolution
of religious and political authority in austronesian societies.
\emph{Nature Human Behaviour}.
doi:\href{https://doi.org/10.1038/s41562-022-01471-y}{10.1038/s41562-022-01471-y}.

\bibitem[\citeproctext]{ref-shi2021}
Shi, B, Choirat, C, Coull, BA, VanderWeele, TJ, and Valeri, L (2021)
CMAverse: A suite of functions for reproducible causal mediation
analyses. \emph{Epidemiology}, \textbf{32}(5), e20e22.

\bibitem[\citeproctext]{ref-shiba2023uncovering}
Shiba, K, Daoud, A, Hikichi, H, \ldots{} Kawachi, I (2023) Uncovering
heterogeneous associations between disaster-related trauma and
subsequent functional limitations: A machine-learning approach.
\emph{American Journal of Epidemiology}, \textbf{192}(2), 217--229.

\bibitem[\citeproctext]{ref-shiba2021}
Shiba, K, and Kawahara, T (2021) Using propensity scores for causal
inference: Pitfalls and tips. \emph{Journal of Epidemiology},
\textbf{31}(8), 457463.

\bibitem[\citeproctext]{ref-sjuxf6lander2016}
Sjölander, A (2016) Regression standardization with the R package
stdReg. \emph{European Journal of Epidemiology}, \textbf{31}(6),
563--574.
doi:\href{https://doi.org/10.1007/s10654-016-0157-3}{10.1007/s10654-016-0157-3}.

\bibitem[\citeproctext]{ref-slingerland2020coding}
Slingerland, E, Atkinson, QD, Ember, CR, \ldots{} Gray, RD (2020) Coding
culture: Challenges and recommendations for comparative cultural
databases. \emph{Evolutionary Human Sciences}, \textbf{2}, e29.

\bibitem[\citeproctext]{ref-steen2017}
Steen, J, Loeys, T, Moerkerke, B, and Vansteelandt, S (2017) Medflex: An
{R} package for flexible mediation analysis using natural effect models.
\emph{Journal of Statistical Software}, \textbf{76}, 146.

\bibitem[\citeproctext]{ref-stuart2018generalizability}
Stuart, EA, Ackerman, B, and Westreich, D (2018) Generalizability of
randomized trial results to target populations: Design and analysis
possibilities. \emph{Research on Social Work Practice}, \textbf{28}(5),
532--537.

\bibitem[\citeproctext]{ref-stuart2015}
Stuart, EA, Bradshaw, CP, and Leaf, PJ (2015) Assessing the
Generalizability of Randomized Trial Results to Target Populations.
\emph{Prevention Science}, \textbf{16}(3), 475--485.
doi:\href{https://doi.org/10.1007/s11121-014-0513-z}{10.1007/s11121-014-0513-z}.

\bibitem[\citeproctext]{ref-suzuki2013counterfactual}
Suzuki, E, Mitsuhashi, T, Tsuda, T, and Yamamoto, E (2013) A
counterfactual approach to bias and effect modification in terms of
response types. \emph{BMC Medical Research Methodology}, \textbf{13}(1),
1--17.

\bibitem[\citeproctext]{ref-suzuki2016}
Suzuki, E, Mitsuhashi, T, Tsuda, T, and Yamamoto, E (2016) A typology of
four notions of confounding in epidemiology. \emph{Journal of
Epidemiology}, \textbf{27}(2), 49--55.
doi:\href{https://doi.org/10.1016/j.je.2016.09.003}{10.1016/j.je.2016.09.003}.

\bibitem[\citeproctext]{ref-suzuki2020}
Suzuki, E, Shinozaki, T, and Yamamoto, E (2020) Causal Diagrams:
Pitfalls and Tips. \emph{Journal of Epidemiology}, \textbf{30}(4),
153--162.
doi:\href{https://doi.org/10.2188/jea.JE20190192}{10.2188/jea.JE20190192}.

\bibitem[\citeproctext]{ref-swanson1967}
Swanson, GE (1967) Religion and regime: A sociological account of the
{R}eformation.

\bibitem[\citeproctext]{ref-swanson1971}
Swanson, GE (1971) Interpreting the reformation. \emph{The Journal of
Interdisciplinary History}, \textbf{1}(3), 419446. Retrieved from
\url{http://www.jstor.org/stable/202620}

\bibitem[\citeproctext]{ref-tchetgen2012}
Tchetgen, EJT, and VanderWeele, TJ (2012) On causal inference in the
presence of interference. \emph{Statistical Methods in Medical
Research}, \textbf{21}(1), 5575.

\bibitem[\citeproctext]{ref-textor2011}
Textor, J, Hardt, J, and Knüppel, S (2011) DAGitty: A graphical tool for
analyzing causal diagrams. \emph{Epidemiology}, \textbf{22}(5), 745.

\bibitem[\citeproctext]{ref-tripepi2007}
Tripepi, G, Jager, KJ, Dekker, FW, Wanner, C, and Zoccali, C (2007)
Measures of effect: Relative risks, odds ratios, risk difference, and
{`}number needed to treat{'}. \emph{Kidney International},
\textbf{72}(7), 789--791.
doi:\href{https://doi.org/10.1038/sj.ki.5002432}{10.1038/sj.ki.5002432}.

\bibitem[\citeproctext]{ref-valeri2014}
Valeri, L, Lin, X, and VanderWeele, TJ (2014) Mediation analysis when a
continuous mediator is measured with error and the outcome follows a
generalized linear model. \emph{Statistics in Medicine},
\textbf{33}(28), 48754890.

\bibitem[\citeproctext]{ref-vanderlaan2011}
Van Der Laan, MJ, and Rose, S (2011) \emph{Targeted Learning: Causal
Inference for Observational and Experimental Data}, New York, NY:
Springer. Retrieved from
\url{https://link.springer.com/10.1007/978-1-4419-9782-1}

\bibitem[\citeproctext]{ref-vanderlaan2018}
Van Der Laan, MJ, and Rose, S (2018) \emph{Targeted Learning in Data
Science: Causal Inference for Complex Longitudinal Studies}, Cham:
Springer International Publishing. Retrieved from
\url{http://link.springer.com/10.1007/978-3-319-65304-4}

\bibitem[\citeproctext]{ref-vanderweele2009}
VanderWeele, TJ (2009a) Concerning the consistency assumption in causal
inference. \emph{Epidemiology}, \textbf{20}(6), 880.
doi:\href{https://doi.org/10.1097/EDE.0b013e3181bd5638}{10.1097/EDE.0b013e3181bd5638}.

\bibitem[\citeproctext]{ref-vanderweele2009a}
VanderWeele, TJ (2009b) Marginal structural models for the estimation of
direct and indirect effects. \emph{Epidemiology}, 1826.

\bibitem[\citeproctext]{ref-vanderweele2009distinction}
VanderWeele, TJ (2009c) On the distinction between interaction and
effect modification. \emph{Epidemiology}, 863--871.

\bibitem[\citeproctext]{ref-vanderweele2012}
VanderWeele, TJ (2012) Confounding and Effect Modification: Distribution
and Measure. \emph{Epidemiologic Methods}, \textbf{1}(1), 55--82.
doi:\href{https://doi.org/10.1515/2161-962X.1004}{10.1515/2161-962X.1004}.

\bibitem[\citeproctext]{ref-vanderweele2015}
VanderWeele, TJ (2015) \emph{Explanation in causal inference: Methods
for mediation and interaction}, Oxford University Press.

\bibitem[\citeproctext]{ref-vanderweele2018}
VanderWeele, TJ (2018) On well-defined hypothetical interventions in the
potential outcomes framework. \emph{Epidemiology}, \textbf{29}(4), e24.
doi:\href{https://doi.org/10.1097/EDE.0000000000000823}{10.1097/EDE.0000000000000823}.

\bibitem[\citeproctext]{ref-vanderweele2019}
VanderWeele, TJ (2019) Principles of confounder selection.
\emph{European Journal of Epidemiology}, \textbf{34}(3), 211219.

\bibitem[\citeproctext]{ref-vanderweele2022}
VanderWeele, TJ (2022) Constructed measures and causal inference:
Towards a new model of measurement for psychosocial constructs.
\emph{Epidemiology}, \textbf{33}(1), 141.
doi:\href{https://doi.org/10.1097/EDE.0000000000001434}{10.1097/EDE.0000000000001434}.

\bibitem[\citeproctext]{ref-vanderweele2017}
VanderWeele, TJ, and Ding, P (2017) Sensitivity analysis in
observational research: Introducing the e-value. \emph{Annals of
Internal Medicine}, \textbf{167}(4), 268--274.
doi:\href{https://doi.org/10.7326/M16-2607}{10.7326/M16-2607}.

\bibitem[\citeproctext]{ref-vanderweele2013}
VanderWeele, TJ, and Hernan, MA (2013) Causal inference under multiple
versions of treatment. \emph{Journal of Causal Inference},
\textbf{1}(1), 120.

\bibitem[\citeproctext]{ref-vanderweele2012a}
VanderWeele, TJ, and Hernán, MA (2012a) Results on differential and
dependent measurement error of the exposure and the outcome using signed
directed acyclic graphs. \emph{American Journal of Epidemiology},
\textbf{175}(12), 1303--1310.
doi:\href{https://doi.org/10.1093/aje/kwr458}{10.1093/aje/kwr458}.

\bibitem[\citeproctext]{ref-vanderweele2012b}
VanderWeele, TJ, and Hernán, MA (2012b) Results on differential and
dependent measurement error of the exposure and the outcome using signed
directed acyclic graphs. \emph{American Journal of Epidemiology},
\textbf{175}(12), 1303--1310.
doi:\href{https://doi.org/10.1093/aje/kwr458}{10.1093/aje/kwr458}.

\bibitem[\citeproctext]{ref-vanderweele2014}
VanderWeele, TJ, and Knol, MJ (2014) A tutorial on interaction.
\emph{Epidemiologic Methods}, \textbf{3}(1), 3372.

\bibitem[\citeproctext]{ref-vanderweele2020}
VanderWeele, TJ, Mathur, MB, and Chen, Y (2020) Outcome-wide
longitudinal designs for causal inference: A new template for empirical
studies. \emph{Statistical Science}, \textbf{35}(3), 437466.

\bibitem[\citeproctext]{ref-vanderweele2007}
VanderWeele, TJ, and Robins, JM (2007) Four types of effect
modification: a classification based on directed acyclic graphs.
\emph{Epidemiology (Cambridge, Mass.)}, \textbf{18}(5), 561--568.
doi:\href{https://doi.org/10.1097/EDE.0b013e318127181b}{10.1097/EDE.0b013e318127181b}.

\bibitem[\citeproctext]{ref-vanderweele2022b}
VanderWeele, TJ, and Vansteelandt, S (2022) A statistical test to reject
the structural interpretation of a latent factor model. \emph{Journal of
the Royal Statistical Society Series B: Statistical Methodology},
\textbf{84}(5), 20322054.

\bibitem[\citeproctext]{ref-vanderweele2014a}
VanderWeele, T, and Vansteelandt, S (2014) Mediation analysis with
multiple mediators. \emph{Epidemiologic Methods}, \textbf{2}(1), 95115.

\bibitem[\citeproctext]{ref-vansteelandt2012}
Vansteelandt, S, Bekaert, M, and Lange, T (2012) Imputation strategies
for the estimation of natural direct and indirect effects.
\emph{Epidemiologic Methods}, \textbf{1}(1), 131158.

\bibitem[\citeproctext]{ref-vansteelandt2022}
Vansteelandt, S, and Dukes, O (2022) Assumption-lean inference for
generalised linear model parameters. \emph{Journal of the Royal
Statistical Society Series B: Statistical Methodology}, \textbf{84}(3),
657685.

\bibitem[\citeproctext]{ref-wager2018}
Wager, S, and Athey, S (2018) Estimation and inference of heterogeneous
treatment effects using random forests. \emph{Journal of the American
Statistical Association}, \textbf{113}(523), 1228--1242.
doi:\href{https://doi.org/10.1080/01621459.2017.1319839}{10.1080/01621459.2017.1319839}.

\bibitem[\citeproctext]{ref-watts2015}
Watts, J, and Gray, R (2015) \emph{Broad supernatural punishment but not
moralising high gods precede the evolution of political complexity in
austronesia}, Victoria University Empirical Philosophy Workshop.
Wellington, New Zealand.

\bibitem[\citeproctext]{ref-watts2018}
Watts, J, Sheehan, O, Bulbulia, Joseph A, Gray, RD, and Atkinson, QD
(2018) Christianity spread faster in small, politically structured
societies. \emph{Nature Human Behaviour}, \textbf{2}(8), 559564.
doi:\href{https://doi.org/gdvnjn}{gdvnjn}.

\bibitem[\citeproctext]{ref-weber1905}
Weber, M (1905) \emph{The protestant ethic and the spirit of capitalism:
And other writings}, Penguin.

\bibitem[\citeproctext]{ref-weber1993}
Weber, M (1993) \emph{The sociology of religion}, Beacon Press.

\bibitem[\citeproctext]{ref-westreich2012berkson}
Westreich, D (2012) Berkson's bias, selection bias, and missing data.
\emph{Epidemiology (Cambridge, Mass.)}, \textbf{23}(1), 159.

\bibitem[\citeproctext]{ref-westreich2010}
Westreich, D, and Cole, SR (2010) Invited commentary: positivity in
practice. \emph{American Journal of Epidemiology}, \textbf{171}(6).
doi:\href{https://doi.org/10.1093/aje/kwp436}{10.1093/aje/kwp436}.

\bibitem[\citeproctext]{ref-westreich2015}
Westreich, D, Edwards, JK, Cole, SR, Platt, RW, Mumford, SL, and
Schisterman, EF (2015) Imputation approaches for potential outcomes in
causal inference. \emph{International Journal of Epidemiology},
\textbf{44}(5), 17311737.

\bibitem[\citeproctext]{ref-westreich2017}
Westreich, D, Edwards, JK, Lesko, CR, Stuart, E, and Cole, SR (2017a)
Transportability of trial results using inverse odds of sampling
weights. \emph{American Journal of Epidemiology}, \textbf{186}(8),
1010--1014.
doi:\href{https://doi.org/10.1093/aje/kwx164}{10.1093/aje/kwx164}.

\bibitem[\citeproctext]{ref-westreich2017transportability}
Westreich, D, Edwards, JK, Lesko, CR, Stuart, E, and Cole, SR (2017b)
Transportability of trial results using inverse odds of sampling
weights. \emph{American Journal of Epidemiology}, \textbf{186}(8),
1010--1014.

\bibitem[\citeproctext]{ref-westreich2013}
Westreich, D, and Greenland, S (2013) The table 2 fallacy: Presenting
and interpreting confounder and modifier coefficients. \emph{American
Journal of Epidemiology}, \textbf{177}(4), 292298.

\bibitem[\citeproctext]{ref-wheatley1971}
Wheatley, P (1971) \emph{The pivot of the four quarters : A preliminary
enquiry into the origins and character of the ancient chinese city},
Edinburgh University Press. Retrieved from
\url{https://cir.nii.ac.jp/crid/1130000795717727104}

\bibitem[\citeproctext]{ref-whitehouse2023}
Whitehouse, H, François, P, Savage, PE, \ldots{} Haar, B ter (2023)
Testing the big gods hypothesis with global historical data: A review
and {``}retake{''}. \emph{Religion, Brain \& Behavior}, \textbf{13}(2),
124166.

\bibitem[\citeproctext]{ref-williams2021}
Williams, NT, and Díaz, I (2021) \emph{Lmtp: Non-parametric causal
effects of feasible interventions based on modified treatment policies}.
doi:\href{https://doi.org/10.5281/zenodo.3874931}{10.5281/zenodo.3874931}.

\end{CSLReferences}

\newpage{}

\subsubsection{Appendix A: Causal Consistency Under Multiple Versions of
Treatment}\label{appendix-a-causal-consistency-under-multiple-versions-of-treatment}

To better understand how the causal consistency assumption might fail,
consider a question that has been discussed in the evolutionary human
science literature about whether a society's beliefs in big Gods affects
its development of social complexity (\citeproc{ref-beheim2021}{Beheim
\emph{et al.} 2021}; \citeproc{ref-johnson2015}{Johnson 2015};
\citeproc{ref-norenzayan2016}{Norenzayan \emph{et al.} 2016};
\citeproc{ref-sheehan2022}{Sheehan \emph{et al.} 2022};
\citeproc{ref-slingerland2020coding}{Slingerland \emph{et al.} 2020};
\citeproc{ref-watts2015}{Watts and Gray 2015};
\citeproc{ref-whitehouse2023}{Whitehouse \emph{et al.} 2023}).
Historians and anthropologists report that such beliefs vary over time
and across cultures in intensity, interpretations, institutional
management, and rituals (\citeproc{ref-bulbuliaj.2013}{Bulbulia, J.
\emph{et al.} 2013}; \citeproc{ref-decoulanges1903}{De Coulanges 1903};
\citeproc{ref-geertz2013}{Geertz \emph{et al.} 2013};
\citeproc{ref-wheatley1971}{Wheatley 1971}). Knowing nothing else, we
might expect that variation in content and settings could influence
social complexity. Moreover, the treatments realised in one society
might affect the treatments realised in other societies, that is, there
might be \emph{spill-over} effects in the exposures (`treatments') to be
compared (\citeproc{ref-murray2021a}{Murray \emph{et al.} 2021};
\citeproc{ref-shiba2023uncovering}{Shiba \emph{et al.} 2023}).

The theory of causal inference under multiple versions of treatment,
developed by VanderWeele and Hernán, formally addresses this challenge
of treatment-effect heterogeneity
(\citeproc{ref-vanderweele2009}{VanderWeele 2009a},
\citeproc{ref-vanderweele2018}{2018};
\citeproc{ref-vanderweele2013}{VanderWeele and Hernan 2013}). The
authors proved that if the treatment variations, \(K\), are
conditionally independent of the potential outcomes, \(Y(k)\), given
covariates \(L\), then conditioning on \(L\) allows us to consistently
estimate causal effects over the heterogeneous treatments
(\citeproc{ref-vanderweele2009}{VanderWeele 2009a}).

Where \(\coprod\) denotes independence, we may assume causal consistency
where the interventions to be compared are independent of their
potential outcomes, conditional on covariates, \(L\):

\[
K \coprod Y(k) | L
\]

That is, according to the theory of causal inference under multiple
versions of treatment, we may think of \(K\) as a `coarsened indicator'
for \(A\).

Notice that the theory allows us to clarify what we require to estimate
causal effects in the presence of interference or `spill-over', which we
may consider to be a special case of treatment-effect heterogeneity. To
handle interference, we say the potential outcome of each unit
\(i \neq j\) must be independent of its own treatment received as well
as of the treatment that all other units received on \(j \neq i\),
conditional on measured covariates \(L\):

\[
    Y_i(k) \coprod K_i, K_j | L, \quad \forall i, \forall j \neq i
\]

Although the theory of causal inference under multiple versions of
treatment provides a formal solution to the problem of treatment-effect
heterogeneity, computing and interpreting causal effect estimates under
this theory can be challenging.

Consider the question of whether a reduction in Body Mass Index (BMI)
affects health (\citeproc{ref-hernuxe1n2008}{Hernán and Taubman 2008}).
Weight loss can occur through various methods, each with different
health implications. Specific methods, such as regular exercise or a
calorie-reduced diet, benefit health. However, weight loss might result
from adverse conditions such as infectious diseases, cancers,
depression, famine, or accidental amputations, which we may suppose are
generally not beneficial to health, at least not in the same way as,
say, reducing weight by increasing physical activity. Hence, even if
causal effects of `weight loss' could be consistently estimated when
adjusting for covariates \(L\) in these settings, we might be uncertain
about how to interpret the effect we are consistently estimating. This
uncertainty highlights the need for precise and well-defined causal
questions. For example, rather than stating the intervention vaguely as
`weight loss', we could state the intervention clearly and specifically,
say, `weight loss achieved through aerobic exercise over at least five
years, compared with no weight loss.' This specificity in the definition
of the exposure, along with comparable specificity in the statement of
the outcomes helps to ensure that the causal estimates we obtain are not
merely unbiased but also interpretable; for discussion see: Hernán
\emph{et al.} (\citeproc{ref-hernuxe1n2022}{2022}); Murray \emph{et al.}
(\citeproc{ref-murray2021a}{2021}); Hernán and Taubman
(\citeproc{ref-hernuxe1n2008}{2008}).

Beyond uncertainties for the interpretation of heterogeneous treatment
effect estimates, there is, as just mentioned, the additional
consideration that we cannot fully verify from data whether the measured
covariates \(L\) suffice to render the multiple versions of treatment
independent of the counterfactual outcomes. This problem is acute when
there is \emph{interference}, which occurs when treatment effects are
relative to the density and distribution of treatment effects in a
population. Scope for interference will often make it difficult to
warrant the assumption that the potential outcomes are independent of
the many versions of treatment that have been realised, dependently, on
the administration of previous versions of treatments across the
population (\citeproc{ref-bulbulia2023a}{Bulbulia \emph{et al.} 2023};
\citeproc{ref-ogburn2022}{Ogburn \emph{et al.} 2022};
\citeproc{ref-vanderweele2013}{VanderWeele and Hernan 2013}).

In short, although the theory of causal inference under multiple
versions of treatment provides a formal solution for consistent causal
effect estimation in observational settings, \emph{treatment
heterogeneity} remains a practical threat. Generally, we should assume
that causal consistency is unrealistic unless proven innocent.

For now, we note that the causal consistency assumption provides a
theoretical starting point for recovering the missing counterfactuals
required for computing causal contrasts. It identifies half of these
missing counterfactuals directly from observed data. The concept of
conditional exchangeability, which we explore next, offers a means for
recovering the remaining half.

\newpage{}

\subsection{Appendix B: Additional Assumptions Beyond the Three
Fundamental Assumptions of Causal
Inference}\label{appendix-b-additional-assumptions-beyond-the-three-fundamental-assumptions-of-causal-inference}

\paragraph{1.3.1 Overly ambitious
estimands}\label{overly-ambitious-estimands}

In causal inference, the Average Treatment Effect (ATE) conceived as
comparison between population-wide simulations at two levels of
exposure, \(E[Y(1)] - E[Y(0)]\), is often artificial. Artificiality is
evident for continuous exposures, where such comparisons simplify the
complexity of real-world phenomena into a low dimensional summary, such
as a contrast of a one-standard-deviation difference in the mean, or a
comparison of one quartile of exposure to another quartile of exposure.
In practice, the requirements for targeting such contrasts impose a
strong reliance on statistical models, which introduce further
opportunities for bias. Such comparisons might also strain the
positivity assumption because the relevant events occur infrequently or
are absent within the strata of covariates required to satisfy
conditional exchangeability. Moreover, because treatment effects
arebrarely linear and may not be monotonic. For this reason,
comparingbarbitrary points on a continuous scale, while relying on
correctbmodelling specifications, risks drawing erroneous
conclusionsb(\citeproc{ref-calonico2022}{Calonico \emph{et al.} 2022};
\citeproc{ref-ogburn2021}{Ogburn and Shpitser 2021}). In short, the
simplifications and modelsbrequired for obtaining standard causal
estimands often lack realism. The practical inferences that we draw from
them may be misleading (\citeproc{ref-vansteelandt2022}{Vansteelandt and
Dukes 2022}).

Furthermore, the `average treatment effect' itself might not be our
primary scientific interest. In many setting we may want to understand
heterogeneity in treatment effects without a clear understanding in
advance of modelling where such heterogeneity may be found
(\citeproc{ref-wager2018}{Wager and Athey 2018}). Presently, methods for
valid causal inference in settings of heterogeneous treatment effects
remain inchoate see: Tchetgen and VanderWeele
(\citeproc{ref-tchetgen2012}{2012}); Wager and Athey
(\citeproc{ref-wager2018}{2018}); Cui \emph{et al.}
(\citeproc{ref-cui2020}{2020}); Foster and Syrgkanis
(\citeproc{ref-foster2023}{2023}); Foster and Syrgkanis
(\citeproc{ref-foster2023}{2023}); Kennedy
(\citeproc{ref-kennedy2023}{2023}); Nie and Wager
(\citeproc{ref-nie2021}{2021}).

Recently, causal data scientists have explored new classes of estimands
and estimators, such as modified treatment policies or `shift
interventions' (\citeproc{ref-duxedaz2021}{Díaz \emph{et al.} 2021};
\citeproc{ref-hoffman2023}{Hoffman \emph{et al.} 2023};
\citeproc{ref-vanderweele2018}{VanderWeele 2018};
\citeproc{ref-williams2021}{Williams and Díaz 2021}) and optimal
treatment policies (\citeproc{ref-athey2021}{Athey and Wager 2021};
\citeproc{ref-kitagawa2018}{Kitagawa and Tetenov 2018}). Such estimands
allow researchers to specify and examine a broader range of causal
contrasts, such as treating those as treating only those likely to
respond, or those who meet certain ethical criteria not determined by
statisticians, or those who optimise a pre-specified
(\citeproc{ref-cui2020}{Cui \emph{et al.} 2020};
\citeproc{ref-duxedaz2021}{Díaz \emph{et al.} 2021};
\citeproc{ref-wager2018}{Wager and Athey 2018}). A review of these
promising developments would take us beyond the scope of this
discussion, however, readers should be aware that causal inference is
not bound to standard \(E[Y(1)] - E[Y(0)]\) estimands that require
simulating often implausible or even unhelpful counterfactual outcomes
for the entire population at two levels of a pre-specified intervention.

\paragraph{1.3.2 Target validity}\label{target-validity}

The question of whether one's results generalise as intended is at the
heart of science. Often the term `selection bias' is used to describe
threats from sample/target population mismatch. However, we shall limit
our use of the term `selection bias' because it is interpreted
differently across disciplines. In economics, it often corresponds to
what epidemiologists term `confounding' or `confounding
bias'(\citeproc{ref-angrist2009mostly}{Angrist and Pischke 2009}). In
epidemiology, confounding bias typically refers to a non-causal link
between the exposure, \(A\), and the outcome, \(Y\), leading to a
scenario where the potential outcomes are not independent of the
exposure: \(Y(a)\cancel\coprod A|L\). This concept incompletely overlaps
with threats to valid inference sample/target population mismatch.

Additionally, some epidemiologists use `selection bias' to include
\emph{collider stratification bias}. This bias occurs when conditioning
on a collider, \(C\), disrupts the conditional independence between
\(Y(a)\) and \(A\) given a set of covariates \(L\):
\(Y(a) \coprod A \mid L\), and this independence is violated with the
inclusion of \(C\), as in \(Y(a) \cancel\coprod A \mid L, C\)
(\citeproc{ref-greenland2003quantifying}{Greenland 2003};
\citeproc{ref-hernan2023}{Hernan and Robins 2023};
\citeproc{ref-hernuxe1n2004}{Hernán 2004}). However, other
epidemiologists use `selection bias' to refer to a mismatch between the
study population and the target population, leading to effect estimates
that do not necessarily deliver what investigators hope. Such
restriction can manifest as \emph{confounding bias}, where the
restriction event opens a backdoor path linking the exposure and
outcome. However, selection-restriction can also present as
\emph{effect-modifier-restriction bias}, where the distributions of
effect-modifiers differs between the restricted sample and the target
population. Both problems affect randomised experiments where there is
attrition (\citeproc{ref-lu2021revisiting}{Lu \emph{et al.} 2021},
\citeproc{ref-lu2022toward}{2022b},
\citeproc{ref-lu2023selection}{2023}). For this reason, the term
`selection bias' is ambiguous.

In this guide, we examine \emph{collider stratification bias} in
\(\S 2.7.3\). When describing this bias we avoid using the term
`selection bias.' We discuss the implications of sample restriction bias
in \(\S 3.1.6\). We describe how describe how this threat may arise
either from \emph{collider stratication bias} or \emph{effect-modifier
restriction bias} (or both). Thus, we avoid the term `selection bias'
and focus on the problem of interest. Causal diagrams help to avoid
terminological confusions by plaining stating the source of bias in
causal-effect estimation without relying on ambiguous terminology.

Setting terminology aside, it is crucial for investigators to recognise
that a mismatch between the sample and target population can invalidate
causal effect estimates. This is so, even if investigators consistently
estimate the average treatment effect for the sample population. If the
mismatch affects effect-modifiers of the treatment-effects there will be
no guarantee that effect-estimates for the sample will generalise to the
target population -- that is, no guarantee that the effect estimates
will achieve `target validity,' or equivalently `external validity.'
Worringly such threats cannot be fully evaluated from responses in the
restricted or censored sample. Special workflows are required for
addressing disparities in the distribution of effect modifiers
influencing the treatment's effect on an outcome in the sample, as
compared with the target population. Mismatch between the target and
sample populations in the distributions of effect modifiers may occur at
baseline, and mismatch may subsequently evolve over subsequent intervals
during which study units are observed. The problem of aligning target
and sample populations in these distributions is therefore a problem
where measures are repeated over time (see: \(\S 3.1.6\) and
\textbf{Appendix 4}).

\paragraph{1.3.3 Measurement error bias}\label{measurement-error-bias}

Measurement error bias originates from a discrepancy between a
variable's true value and its observed or recorded value. These errors
can arise from instrument calibration problems, respondent misreporting,
coding errors, and other factors. Measurement error is almost
inevitable, and measurement error bias can significantly distort causal
inferences (\citeproc{ref-blackwell2017}{Blackwell \emph{et al.} 2017};
\citeproc{ref-bulbulia2023e}{Bulbulia 2023a};
\citeproc{ref-hernan2023}{Hernan and Robins 2023};
\citeproc{ref-vanderweele2012}{VanderWeele 2012}).

\textbf{Random measurement error} arises from fluctuations in the
measurement process and does not consistently bias effect estimates in
any one direction. Although random measurement errors can increase data
variability and reduce statistical power, they typically do not
introduce bias in estimates of causal effects when there are no true
effects. However, they can lead to attenuated estimates of causal
effects, systematically bias the estimate of true causal effects towards
the null (\citeproc{ref-hernan2023}{Hernan and Robins 2023};
\citeproc{ref-hernuxe1n2009}{Hernán and Cole 2009};
\citeproc{ref-vanderweele2012b}{VanderWeele and Hernán 2012b}).

\textbf{Systematic measurement error} error occurs when either the
errors of the exposure and outcome are correlated, or when the exposure
affect the errors of the outcome (or both). Such structural sources of
error can lead to biased causal effect estimates by consistently
overestimating or underestimating the true causal magnitudes
(\citeproc{ref-hernuxe1n2009}{Hernán and Cole 2009};
\citeproc{ref-vanderweele2012b}{VanderWeele and Hernán 2012b}).

Sequential causal diagrams can be useful in assessing certain structural
sources of bias arising from measurement error
(\citeproc{ref-bulbulia2023e}{Bulbulia 2023a};
\citeproc{ref-hernan2023}{Hernan and Robins 2023};
\citeproc{ref-hernuxe1n2009}{Hernán and Cole 2009};
\citeproc{ref-vanderweele2012b}{VanderWeele and Hernán 2012b}). This is
partly because the flow of information from error terms in causal
diagrams mirrors the flow of confounding as it needs to be assessed to
validate causal inferences. However, it is important to note that
measurement error bias cannot be entirely encapsulated by confounding
bias. Thus, employing causal diagrams to clarify measurement error bias
represents another non-standard application. Although we do not have
space here to cover measurement error bias in detail, \(\S 2.8\)
describes a common scenario where causal diagrams do not adequately
elucidate structural sources of bias.

\paragraph{1.3.4 Model mis-specification
bias}\label{model-mis-specification-bias}

Human scientists predominantly employ parametric models for statistical
analysis. These models are characterised by user-defined functional
forms and distributional assumptions. However, our reliance on
parametric models carries the risk of biased inferences due to model
mis-specification. The negative effects of model mis-specification are
evident in three key areas:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Limitations in establishing causation:} Even if a parametric
  fits the data well, it may not accurately represent causal
  relationships, which require simulating counterfactual data.
\item
  \textbf{Regularisation bias:} even when causal effects are formally
  identifiable, the actual relationships between variables may be more
  intricate or differ from those assumed in one's statistical model.
  Parametric models, in particular, are susceptible to bias when
  capturing the inherent complexity of real-world phenomena
  (\citeproc{ref-vansteelandt2022}{Vansteelandt and Dukes 2022};
  \citeproc{ref-wager2018}{Wager and Athey 2018}).
\item
  \textbf{Overstated precision:} a mis-specified model may erroneously
  imply a higher degree of precision in its estimates than warranted.
  This can occur through inaccurate estimation of parameter values or
  their standard errors, leading to misguided confidence in the results.
  (\citeproc{ref-duxedaz2021}{Díaz \emph{et al.} 2021};
  \citeproc{ref-vansteelandt2022}{Vansteelandt and Dukes 2022}).
\end{enumerate}

Recent developments in cross-validated non-parametric inference that
rely on machine learning offer promise for addressing threats to valid
inference from model mis-specification (\citeproc{ref-athey2019}{Athey
\emph{et al.} 2019}; \citeproc{ref-duxedaz2021}{Díaz \emph{et al.}
2021}; \citeproc{ref-hahn2020}{Hahn \emph{et al.} 2020};
\citeproc{ref-kuxfcnzel2019}{Künzel \emph{et al.} 2019};
\citeproc{ref-vanderlaan2011}{Van Der Laan and Rose 2011},
\citeproc{ref-vanderlaan2018}{2018}; \citeproc{ref-wager2018}{Wager and
Athey 2018}; \citeproc{ref-williams2021}{Williams and Díaz 2021}).
Doubly robust versions of these estimation methods, which model both the
exposure and the outcome, are particularly attractive because they can
provide valid causal effect estimates even if only one of the two models
is correctly specified (\citeproc{ref-vanderlaan2011}{Van Der Laan and
Rose 2011}). Presently however state-of-the-art machine learning
techniques only offer convergence guarantees for large samples
(\citeproc{ref-vansteelandt2022}{Vansteelandt and Dukes 2022}). These
promising methods remain under active development. For the present
purposes, it is important that readers understand the risks for drawing
invalid conclusions because our statistical models are mis-specification
(\citeproc{ref-cui2020}{Cui \emph{et al.} 2020};
\citeproc{ref-duxedaz2021}{Díaz \emph{et al.} 2021};
\citeproc{ref-hoffman2022}{Hoffman \emph{et al.} 2022};
\citeproc{ref-muuxf1oz2012}{Muñoz and Laan 2012};
\citeproc{ref-vansteelandt2022}{Vansteelandt and Dukes 2022};
\citeproc{ref-wager2018}{Wager and Athey 2018};
\citeproc{ref-williams2021}{Williams and Díaz 2021}). Causal diagrams
serve as model-free, qualitative tools for identifying structural biases
but fall short in rectifying model mis-specification and other biases.
Employing them without a rigorous, multi-stepped workflow for
assumption-verification risks drawing erroneous inferences, see
discussion in Major-Smith (\citeproc{ref-major2023exploring}{2023}) and
Auspurg and Brüderl (\citeproc{ref-auspurg2021has}{2021}).

\subsection{Appendix 1: Causal Inference in History: The Difficulty In
Satisfying the Three Fundamental
Assumptions}\label{appendix-1-causal-inference-in-history-the-difficulty-in-satisfying-the-three-fundamental-assumptions}

Consider the Protestant Reformation of the 16\(^{th}\) century, which
initiated religious change throughout much of Europe. Historians have
argued that Protestantism caused social, cultural, and economic changes
in those societies where it took hold; see: Weber
(\citeproc{ref-weber1905}{1905}); Weber
(\citeproc{ref-weber1993}{1993}); Swanson
(\citeproc{ref-swanson1967}{1967}); Swanson
(\citeproc{ref-swanson1971}{1971}); Basten and Betz
(\citeproc{ref-basten2013}{2013}), and for an overview see: Becker
\emph{et al.} (\citeproc{ref-becker2016}{2016}).

Suppose we were interested in estimating the `Average Treatment Effect'
of the Protestant Reformation. Let \(A = a^*\) denote the adoption of
Protestantism. We compare this effect with that of remaining Catholic,
represented as \(A = a\). We assume that both the concepts of `adopting
Protestantism' and of `economic development' are well-defined (e.g.~GDP
+1 century after a country has a Protestant majority contrasted with
remaining Catholic). The causal effect for any individual country is
\(Y_i(a^*) - Y_i(a)\). Although we cannot identify this effect, if the
basic assumptions of causal inference are met, we can estimate the
average or marginal effect conditioning the confounding effects of \(L\)
gives us,

\[ATE_{\textnormal{economic~development}} = \mathbb{E}[Y(\textnormal{Became~Protestant}|L) - Y(\textnormal{Remained~Catholic}|L)]\]

When asking causal questions about the economic effect of adopting
Protestantism versus remaining Catholic, there are several challenges
that arise in relation to the three fundamental assumptions required for
causal inference.

\textbf{Causal Consistency}: requires the outcome under each level of
exposure is well-defined. In this context, defining what `adopting
Protestantism' and `remaining Catholic' mean may present challenges. The
practices and beliefs associated with each religion might vary
significantly across countries and time periods, and it may be difficult
to create a consistent, well-defined exposure. Furthermore, the outcome
- economic development - may also be challenging to measure consistently
across different countries and time periods.

There is undoubtedly considerable heterogeneity in the `Protestant
exposure.' In England, Protestantism was closely tied to the monarchy
(\citeproc{ref-collinson2003}{Collinson 2003}). In Germany, Martin
Luther's teachings emphasised individual faith in scripture, which, it
has been claimed, supported economic development by promoting literacy
(\citeproc{ref-gawthrop1984}{Gawthrop and Strauss 1984}). In England,
King Henry VIII abolished Catholicism
(\citeproc{ref-collinson2003}{Collinson 2003}). The Reformation, then,
occurred differently in different places. The exposure needs to be
better-defined.

There is also ample scope for interference: 16th century societies were
interconnected through trade, diplomacy, and warfare. Thus, the
religious decisions of one society were unlikely to have been
independent from those of other societies.

\textbf{Exchangeability}: requires that given the confounders, the
potential outcomes are independent of the treatment assignment. It might
be difficult to account for all possible confounders in this context.
For example, historical, political, social, and geographical factors
could influence both a country's religious affiliations and its economic
development.

\textbf{Positivity}: requires that there is a non-zero probability of
every level of exposure for every strata of confounders. If we consider
various confounding factors such as geographical location, historical
events, or political circumstances, some countries might only ever have
the possibility of either remaining Catholic or becoming Protestant, but
not both. For example, it is unclear under which conditions 16th century
Spain could have been randomly assigned to Protestantism
(\citeproc{ref-nalle1987}{Nalle 1987}).

Perhaps a more credible measure of effect in the region of our interests
is the Average Treatment Effect in the Treated (ATT) expressed:

\[ATT_{\textnormal{economic~development}} = \mathbb{E}[(Y(a*)- Y(a)|A = a*,L]\]

Where \(Y(a*)\) represents the potential outcome if treated. \(Y(a)\)
represents the potential outcome if not treated. The expectation is
taken over the distribution of the treated units (i.e., those for whom
\(A = a*\)). \(L\) is a set of covariates on which we condition to
ensure that the potential outcomes \(Y(a*)\) and \(Y(a)\) are
independent of the treatment assignment \(A\), given \(L\). This
accounts for any confounding factors that might bias the estimate of the
treatment effect.

Here, the ATT defines the expected difference in economic success for
cultures that became Protestant compared with the expected economic
success if those cultures had not become Protestant, conditional on
measured confounders \(L\), among the exposed (\(A = a^*\)). To estimate
this contrast, our models would need to match Protestant cultures with
comparable Catholic cultures effectively. By estimating the ATT, we
would avoid the assumption of non-deterministic positivity for the
untreated. However, whether matching is conceptually plausible remains
debatable. Ostensibly, it would seem that assigning a religion to a
culture is not as easy as administering a pill
(\citeproc{ref-watts2018}{Watts \emph{et al.} 2018}).

\newpage{}

\subsection{Appendix 2: Sketch of Three-Wave Panel Design for Obtaining
a Marginal Incident-Exposure
Effect}\label{appendix-2-sketch-of-three-wave-panel-design-for-obtaining-a-marginal-incident-exposure-effect}

Causal diagrams point the need for obtaining clearly defined time-series
data. How might we do it? Here, I sketch the outlines of a design for a
three-wave panel study that intends to estimate an \emph{incident
exposure effect}. I do not intend this advice to be more than a sketch.
However, I believe it is important to give readers a concrete example
for how data collection for causal inference might occur.

\subsubsection{Step 1. Ask a causal
question}\label{step-1.-ask-a-causal-question}

In a three-wave panel design, ensuring the relative timing of events
essential for valid causal inference
(\citeproc{ref-vanderweele2020}{VanderWeele \emph{et al.} 2020}).

Here is a causal question:

What is the causal effect of attending weekly religious services
compared to not attending services on charitable giving in the
population of New Zealanders who identify as Christian?

To answer this question we must assess how changes in religious service
attendance, measured from the beginning of the year (baseline) to
mid-year (wave 1), affect levels of charitable giving at the end of the
year (wave 2) In this design, the change in religious service attendance
is captured between the first and second waves, while the outcome,
charitable giving, is measured in the third wave. This establishes a
sequential order that mirrors the cause-and-effect relationship.
Ensuring such temporal ordering is crucial in any causal analysis. Note
additionally that we must obtain comparisons from continuous data for a
binary data. Depending on the data, such a contrast might not be well
supported. For example, change between these levels might occur only
rarely, in which case our inference might rely too heavily on parametric
model specifications. Focusing on the estimand:

\paragraph{Exposure:}\label{exposure}

\begin{itemize}
\tightlist
\item
  A = 0: Attends less than once per month
\item
  A = 1: Attends weekly
\end{itemize}

\paragraph{Outcome:}\label{outcome}

\begin{itemize}
\tightlist
\item
  Focus: One-year effect of shifting from A = 0 to A = 1.
\item
  Charitable giving as measured by self-reported giving
\end{itemize}

\paragraph{Scale of contrast:}\label{scale-of-contrast}

\begin{itemize}
\tightlist
\item
  ATE on the causal difference scale (per protocol).
\end{itemize}

\paragraph{Target population:}\label{target-population}

\begin{itemize}
\tightlist
\item
  Individuals in New Zealand who might attend religious service and
  identify as Christian.
\end{itemize}

\paragraph{Source population:}\label{source-population}

\begin{itemize}
\tightlist
\item
  National probability sample of New Zealanders (N = 34,000).
\end{itemize}

\paragraph{Baseline population:}\label{baseline-population}

\begin{itemize}
\tightlist
\item
  Defined by eligibility criteria (including religious affiliation). If
  the baseline population differs from the target population, if sample
  weights for the distribution of covariates are available for the
  \emph{target population}, these should be applied to the baseline
  population (although with caution, given potential for model
  mis-specification, see \citeproc{ref-stuart2015}{Stuart \emph{et al.}
  2015}.)
\end{itemize}

Let \(\widehat{ATE}_{target}\) denote the population average treatment
effect for the target population. Let
\(\widehat{ATE}_{\text{restricted}}\) denote the average treatment
effect at the end of treatment. Let \(W\) denote a set of variables upon
which the restricted and target populations structurally differ. We say
that results \emph{generalise} if we can guarantee that:

\[
\widehat{ATE}_{target} =  \widehat{ATE}_{restricted} 
\]

or if there is a known function such that:

\[
ATE_{target}\approx  f_W(ATE_{\text{restricted}}, W)
\]

In most cases, \(f_W\) will be unknown, as it must account for potential
heterogeneity of effects and unobserved sources of bias. For further
discussion on this topic, see: Imai \emph{et al.}
(\citeproc{ref-imai2008misunderstandings}{2008}); Cole and Stuart
(\citeproc{ref-cole2010generalizing}{2010}); Stuart \emph{et al.}
(\citeproc{ref-stuart2018generalizability}{2018}); Bulbulia
(\citeproc{ref-bulbulia2023c}{2023b}), and \(\S 3.1.6\)

\subsubsection{2. Ensure that the exposure is measured at wave 0
(baseline) and wave 1 (the exposure
interval)}\label{ensure-that-the-exposure-is-measured-at-wave-0-baseline-and-wave-1-the-exposure-interval}

Measuring the exposure at both baseline (wave 0) and the exposure
interval (wave 1) has the following benefits:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Enables estimation of incident exposure effect}: by including
  baseline observations, we can distinguish between incidence (new
  occurrences) and prevalence (existing states) exposure effects. For
  instance, in a study on religious service attendance, assessing the
  incident exposure effect allows us to differentiate the effect of
  starting to attend services regularly from the effect of ongoing
  attendance.
\item
  \textbf{Confounding control}: measuring the exposure at baseline helps
  control for time-invariant confounders. These are factors that do not
  change over time and might affect both the exposure and outcome. In
  the context of religious service attendance, personal attributes like
  inherent religiosity could influence both attendance and related
  outcomes.
\item
  \textbf{Sample adequacy}: for rare exposures, baseline measurements
  can assess sample size adequacy. If a change in exposure is infrequent
  (e.g., infrequent to weekly religious service attendance), a larger
  sample may be needed to satisfy the positivity assumption and detect
  causal effects. By measuring the exposure at baseline, we can better
  evaluate whether our sample is representative and large enough to
  detect such rare changes.
\end{enumerate}

\subsubsection{3. Ensure that the outcome is measured at wave 0
(baseline) and wave 2 (post-exposure wave
1)}\label{ensure-that-the-outcome-is-measured-at-wave-0-baseline-and-wave-2-post-exposure-wave-1}

Measuring the outcome at both wave 0 (baseline) and the post-exposure
outcome wave (wave 2) offers the following advantages:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Temporal ordering}: causes precede effects. We need this to
  avoid \emph{causal incoherence}. For example, ensuring order protects
  us from inadvertently estimating \(Y\rightarrowred A\).
\item
  \textbf{Confounding control}: including the baseline measure of both
  the exposure and outcome allows for better control of confounding.
  This approach helps to isolate the effect of the exposure on the
  outcome from the exposure wave (wave 1) to the outcome wave (wave 2),
  independent of their baseline levels. It reduces the risk of
  confounding, where unmeasured factors might influence both the
  exposure and the outcome, as shown in Figure~\ref{fig-dag-1}.
\end{enumerate}

\subsubsection{4. Measure observable common causes of the exposure and
outcome}\label{measure-observable-common-causes-of-the-exposure-and-outcome}

Next, we must identify and record at wave 0 (baseline) all potential
confounders that could influence both the exposure (e.g., frequency of
attending religious services) and the outcome (e.g., charitable giving).
Proper identification and adjustment for these confounders are crucial
for accurate causal inference. By obtaining measures of the confounders
at baseline we:

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\item
  \textbf{Minimise mediation bias}: by measuring confounders at
  baseline, it will be difficult to produce the \emph{causally
  incoherent} model: \(A_1\to \boxed{L_2} \rightarrowdotted Y_3\)
\item
  \textbf{Minimise collider bias}: by measuring confounders at baseline,
  it will be difficult to produce the \emph{causally incoherent} model:
  \(A_1\rightarrowred L_3 \leftarrowred Y_2\).
\end{enumerate}

The topic of measurement construction is vast. For now, it is worth
noting that measures should be obtained in consultation with locals and
domain experts (\citeproc{ref-vanderweele2022}{VanderWeele 2022}).

\subsubsection{5. Gather data for proxy variables of unmeasured common
causes at the baseline
wave}\label{gather-data-for-proxy-variables-of-unmeasured-common-causes-at-the-baseline-wave}

If any unmeasured confounders influence both the exposure and outcome,
but we lack direct measurements, we should make efforts to include
proxies for them at baseline. Even if this strategy cannot eliminate all
bias from unmeasured confounding, it will generally reduce bias.

\subsubsection{6. Retain sample}\label{retain-sample}

Censoring leads to bias. Strategies for sample retention are essential.

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\item
  \textbf{Developing tracking protocols}: establish robust systems for
  tracking participants over the study period. This involves keeping
  updated records of contact information such as addresses, emails,
  phone numbers, and names, and accounting for changes in name over
  time.
\item
  \textbf{Motivate retention}: implement strategies to encourage ongoing
  participation. These incentives should ideally not lead to bias in the
  distribution of effect-modifiers that might affect the outcome of
  interest. For example, retention should not appeal to trust in science
  if trust in science is the outcome of interest.
\item
  \textbf{Investigators should avoid acting in ways that lead to
  differential retention}: for example, stay out of the news.
\end{enumerate}

\begin{figure}

\centering{

\includegraphics[width=1\textwidth,height=\textheight]{short-causal-diagrams_files/figure-pdf/fig-dag-1-1.pdf}

}

\caption{\label{fig-dag-1}Three-wave panel design with
selection-restriction bias. Where the exposure affects attrition, and an
unmeasured confounder (U\_C=1) may affect both attrition and the
outcome, there is scope for restriction bias from censoring. Even if the
exposure does not affect censoring, if U\_C=1 leads to informative
censoring, the marginal effect in the censored may differ from the
marginal effect in the uncensored (Section 3.1.6). Note we seek
inference on the distribution of Y over C=0,1 (the target population.)
Post-treatment selection bias cannot be corrected by conditioning on
baseline co-variates. The best strategy is to minimise attrition and
non-response. However, because attrition is nearly inevitable, we apply
correction methods such as censoring weighting or multiple imputation.
These methods introduce scope for bias from model mis-specification.}

\end{figure}%

Although this article does not discuss estimation, it is worth
mentioning that we do not require a structural equation model or a
multi-level model to handle the repeated measures. We may estimate this
model using ordinary least squares. For example, in R we could write the
three-wave model that controls for baseline confounders, exposure, and
outcome as follows:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# load libraries}
\FunctionTok{library}\NormalTok{(tidyverse)}

\CommentTok{\# set a seed for reproducibility}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{1234}\NormalTok{)}
\CommentTok{\# number of observations}
\NormalTok{n }\OtherTok{\textless{}{-}} \DecValTok{100}
\CommentTok{\# simulating data}
\CommentTok{\# baseline religious service attendance (binary)}
\NormalTok{a0 }\OtherTok{\textless{}{-}} \FunctionTok{rbinom}\NormalTok{(n, }\DecValTok{1}\NormalTok{, }\FloatTok{0.5}\NormalTok{)}
\CommentTok{\# wave 1 religious service attendance (binary)}
\NormalTok{a1 }\OtherTok{\textless{}{-}} \FunctionTok{rbinom}\NormalTok{(n, }\DecValTok{1}\NormalTok{, }\FloatTok{0.5}\NormalTok{)}
\CommentTok{\# confounder (continuous)}
\NormalTok{l0 }\OtherTok{\textless{}{-}} \FunctionTok{rnorm}\NormalTok{(n, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{)}
\CommentTok{\# baseline donations (continuous)}
\NormalTok{y0 }\OtherTok{\textless{}{-}} \DecValTok{2} \SpecialCharTok{*}\NormalTok{ a0 }\SpecialCharTok{+}\NormalTok{ l0 }\SpecialCharTok{+} \FunctionTok{rnorm}\NormalTok{(n, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{)}
\CommentTok{\# wave 2 donations (continuous)}
\NormalTok{y2 }\OtherTok{\textless{}{-}} \DecValTok{3} \SpecialCharTok{*}\NormalTok{ a1 }\SpecialCharTok{+} \DecValTok{2} \SpecialCharTok{*}\NormalTok{ a0 }\SpecialCharTok{+}\NormalTok{ l0 }\SpecialCharTok{+}\NormalTok{ y0 }\SpecialCharTok{+} \FunctionTok{rnorm}\NormalTok{(n, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{)}

\CommentTok{\# create a dataframe}
\NormalTok{data }\OtherTok{\textless{}{-}} \FunctionTok{tibble}\NormalTok{(}\AttributeTok{a0 =}\NormalTok{ a0, }
               \AttributeTok{a1 =}\NormalTok{ a1, }
               \AttributeTok{l0 =}\NormalTok{ l0, }
               \AttributeTok{y0 =}\NormalTok{ y0, }
               \AttributeTok{y2 =}\NormalTok{ y2)}

\CommentTok{\# step 1 fit a linear model to data}
\CommentTok{\#  model the outcome variable y at wave 2 (y2) as a function of:}
\CommentTok{\# {-} religious service attendance at wave 1 (a1) and baseline (a0),}
\CommentTok{\# {-} outcome at baseline (y0), and}
\CommentTok{\# {-} a confounder measured at baseline (l0).}

\NormalTok{model }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{( }
\NormalTok{  y2 }\SpecialCharTok{\textasciitilde{}}\NormalTok{ a1 }\SpecialCharTok{+}\NormalTok{ a0 }\SpecialCharTok{+}\NormalTok{ y0 }\SpecialCharTok{+}\NormalTok{ l0, }
  \AttributeTok{data =}\NormalTok{ data}
\NormalTok{  )}

\CommentTok{\# step 2: predict the outcome (y2) assuming no religious service attendance at wave 1 (a1 = 0).}
\CommentTok{\# keeping other vars at their measured levels}

\NormalTok{pred\_a0 }\OtherTok{\textless{}{-}} \FunctionTok{predict}\NormalTok{(}
\NormalTok{  model, }\AttributeTok{newdata =}\NormalTok{ data }\SpecialCharTok{|\textgreater{}} \FunctionTok{mutate}\NormalTok{(}\AttributeTok{a1 =} \DecValTok{0}\NormalTok{), }\AttributeTok{se.fit =} \ConstantTok{TRUE}
\NormalTok{  )}

\CommentTok{\# step 3: predict the outcome (y2) assuming full religious service attendance at wave 1 (a1 = 1).}

\NormalTok{pred\_a1 }\OtherTok{\textless{}{-}} \FunctionTok{predict}\NormalTok{(}
\NormalTok{  model, }\AttributeTok{newdata =}\NormalTok{ data }\SpecialCharTok{|\textgreater{}} \FunctionTok{mutate}\NormalTok{(}\AttributeTok{a1 =} \DecValTok{1}\NormalTok{), }\AttributeTok{se.fit =} \ConstantTok{TRUE}
\NormalTok{  )}

\CommentTok{\# step 4. calculate the causal contrast (the average treatment effect).}
\CommentTok{\# this is the difference in predicted outcomes between attending and not attending}
\NormalTok{causal\_contrast }\OtherTok{\textless{}{-}} \FunctionTok{mean}\NormalTok{(pred\_a1}\SpecialCharTok{$}\NormalTok{fit) }\SpecialCharTok{{-}} \FunctionTok{mean}\NormalTok{(pred\_a0}\SpecialCharTok{$}\NormalTok{fit)}

\CommentTok{\# print causal contrast}
\NormalTok{causal\_contrast}

\CommentTok{\# step 5: compute variance and standard error of the causal contrast.}
\CommentTok{\# a. determine the sample size (N).}
\NormalTok{n }\OtherTok{\textless{}{-}} \FunctionTok{length}\NormalTok{(pred\_a1}\SpecialCharTok{$}\NormalTok{fit)}

\CommentTok{\# b. calculate the variance of the causal contrast using Delta method.}
\NormalTok{var\_contrast }\OtherTok{\textless{}{-}}\NormalTok{ (}\FunctionTok{var}\NormalTok{(pred\_a1}\SpecialCharTok{$}\NormalTok{fit) }\SpecialCharTok{+} \FunctionTok{var}\NormalTok{(pred\_a0}\SpecialCharTok{$}\NormalTok{fit)) }\SpecialCharTok{/}\NormalTok{ n}


\CommentTok{\# c. calculate the stand error for the mean difference.}
\NormalTok{se\_contrast }\OtherTok{\textless{}{-}} \FunctionTok{sqrt}\NormalTok{(var\_contrast)}

\CommentTok{\# d. output standard error.}
\FunctionTok{print}\NormalTok{(se\_contrast)}
\end{Highlighting}
\end{Shaded}

The method just described is called `parametric g-computation' or
(confusingly) `regression standardisation.' Note: `standardisation' has
a completely different meaning in many human sciences. Here, we did not
z-transform variables in our data. We simulated average causal effects
by setting coefficients to one and another level of treatment and
predicting outcomes for each condition across the entire population.
There are other methods for simulating potential outcomes, such as
inverse probability treatment weighting, and more sophisticated doubly
robust estimators, including those that rely on machine learning
described in our discussion of model mis-specification \(\S 1.3.4\).
Here, we simulate the counterfactual contrast using ordinary least
squares. In our simple three-wave model we recover an incident-exposure
effect, see: Hernán \emph{et al.}
(\citeproc{ref-hernuxe1n2016b}{2016b}); Hoffman \emph{et al.}
(\citeproc{ref-hoffman2022}{2022}); Hernán \emph{et al.}
(\citeproc{ref-hernuxe1n2008a}{2008}); Bulbulia
(\citeproc{ref-bulbulia2022}{2022}) VanderWeele \emph{et al.}
(\citeproc{ref-vanderweele2020}{2020}). Note the \texttt{clarify}
package compute treatment effects with standard errors, passing
\texttt{model} to its functions (\citeproc{ref-greifer2023}{Greifer
\emph{et al.} 2023}).

\newpage{}

\subsection{Appendix 3: Counterfactual
Reality}\label{appendix-3-counterfactual-reality}

A distinctive feature of causal data science is the premise that
potential outcomes, \(Y_i(1)\) and \(Y_i(0)\), although never jointly
realised, may be assumed to be real and to exist independently of data
collection (\citeproc{ref-hernan2023}{Hernan and Robins 2023 p. 6}). In
causal data science, we encounter a unique missing data problem: the
`full data' necessary to compute any causal contrast are missing at
least half of their values (\citeproc{ref-edwards2015}{Edwards \emph{et
al.} 2015}; \citeproc{ref-ogburn2021}{Ogburn and Shpitser 2021};
\citeproc{ref-westreich2015}{Westreich \emph{et al.} 2015}). This
challenge differs from typical missing data scenarios, where the data
could have been recorded but were not. The missing information crucial
for computing causal contrasts is inherently linked to the irreversible
nature of time. Sequentially ordered causal diagrams encode assumptions
about this structure.

There are implications for notation. As Hernán and Robins point out:
``Sometimes we abbreviate the expression 'individual \(i\) has outcome
\(Y^a = 1'\) by writing \(Y^a_i = 1\). Technically, when \(i\) refers to
a specific individual, such as Zeus, \(Y^a_i\) is not a random variable
because we are assuming that individual counterfactual outcomes are
deterministic. Causal effect for individual \(i\):
\(Y^{a=1}_i \neq Y^{a=0}_i\)'' (\citeproc{ref-hernan2023}{Hernan and
Robins 2023 p. 6}).

As the Hernán and Robins quotation illustrates, there are different
conventions for denoting potential (counterfactual) outcomes. The
following notations are equivalent: \(Y^a\), \(Y_a\), and \(Y(a)\).

\newpage{}

\subsection{Appendix 4: Explanation for the Difference in Marginal
Effects between Censored and Uncensored
Populations}\label{appendix-4-explanation-for-the-difference-in-marginal-effects-between-censored-and-uncensored-populations}

\paragraph{Definitions:}\label{definitions}

\begin{itemize}
\tightlist
\item
  \textbf{\(A\)}: Exposure variable
\item
  \textbf{\(Y\)}: Outcome variable
\item
  \textbf{\(Z\)}: Effect modifier
\item
  \textbf{\(C\)}: Represents the uncensored population
\item
  \textbf{\(C=1\)}: Represents the censored population
\end{itemize}

\paragraph{Average treatment effects for the uncensored and censored
populations defined
as:}\label{average-treatment-effects-for-the-uncensored-and-censored-populations-defined-as}

\[
\Delta_{\text{uncensored}} = \mathbb{E}[Y(a^*) - Y(a) | C]
\] \[
\Delta_{\text{censored}} = \mathbb{E}[Y(a^*) - Y(a) | C=1]
\]

\paragraph{By Causal Consistency, potential outcomes from observations
given
as:}\label{by-causal-consistency-potential-outcomes-from-observations-given-as}

\[
\Delta_{\text{uncensored}} = \mathbb{E}[Y|A=a^*,C] - \mathbb{E}[Y|A=a,C]
\] \[
\Delta_{\text{censored}} = \mathbb{E}[Y|A=a^*,C=1] - \mathbb{E}[Y|A=a,C=1]
\]

\paragraph{\texorpdfstring{By the Law of Total Probability, Average
Treatment Effects weighted by \(\Pr(Z|C)\) given
as:}{By the Law of Total Probability, Average Treatment Effects weighted by \textbackslash Pr(Z\textbar C) given as:}}\label{by-the-law-of-total-probability-average-treatment-effects-weighted-by-przc-given-as}

\[
\Delta_{\text{uncensored}} = \sum_{z} \bigg\{\mathbb{E}[Y|A=a^*,Z=z,C] - \mathbb{E}[Y|A=a,Z=z,C]\bigg\}\times \Pr(Z=z|C)
\] \[
\Delta_{\text{censored}} = \sum_{z} \bigg\{\mathbb{E}[Y|A=a^*,Z=z,C=1] - \mathbb{E}[Y|A=a,Z=z,C=1]\bigg\} \times \Pr(Z=z|C=1)
\]

\paragraph{Assume informative
censoring}\label{assume-informative-censoring}

We assume the effect modifier \(Z\) has a different distribution in the
censored and uncensored populations:

\[\Pr(Z=z|C) \neq \Pr(Z=z|C=1)\]

Under this assumption, the probability weights used to calculate the
marginal effects for the uncensored and censored populations differ.

\paragraph{Effect estimates for censored and uncensored will not be the
same on at least one effect measurement
scale}\label{effect-estimates-for-censored-and-uncensored-will-not-be-the-same-on-at-least-one-effect-measurement-scale}

Given that \(\Pr(Z=z|C) \neq \Pr(Z=z|C=1)\), we cannot guarantee that:

\[\Delta_{\text{uncensored}} = \Delta_{\text{censored}}\]

Indeed the equality of the marginal effects between the two populations
will only hold if there is a universal null effect across all units, by
chance, or under specific conditions discussed by VanderWeele and Robins
(\citeproc{ref-vanderweele2007}{2007}) and further elucidated by Suzuki
\emph{et al.} (\citeproc{ref-suzuki2013counterfactual}{2013}),
otherwise:

\[\Delta_{\text{uncensored}} \ne \Delta_{\text{censored}}\]

Futhermore, VanderWeele (\citeproc{ref-vanderweele2012}{2012}) proved
that if there is effect modification of \(A\) by \(Z\), there will be a
difference in at least one scale of causal contrast, such that

\[\Delta^{\text{risk ratio}}_{\text{uncensored }}  \ne \Delta^{\text{risk ratio}}_{\text{censored}}\]

or

\[\Delta^{\text{difference}}_{\text{uncensored }}  \ne \Delta^{\text{difference}}_{\text{censored}}\]

For comprehensive discussions on sampling and inference, refer to
Dahabreh and Hernán (\citeproc{ref-dahabreh2019}{2019}) and Dahabreh
\emph{et al.} (\citeproc{ref-dahabreh2021study}{2021}).



\end{document}
