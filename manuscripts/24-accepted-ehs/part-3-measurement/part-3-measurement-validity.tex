% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames,x11names}{xcolor}
%
\documentclass[
  single column]{article}

\usepackage{amsmath,amssymb}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage[]{libertinus}
\ifPDFTeX\else  
    % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\usepackage[top=30mm,left=25mm,heightrounded,headsep=22pt,headheight=11pt,footskip=33pt,ignorehead,ignorefoot]{geometry}
\setlength{\emergencystretch}{3em} % prevent overfull lines
\setcounter{secnumdepth}{-\maxdimen} % remove section numbering
% Make \paragraph and \subparagraph free-standing
\makeatletter
\ifx\paragraph\undefined\else
  \let\oldparagraph\paragraph
  \renewcommand{\paragraph}{
    \@ifstar
      \xxxParagraphStar
      \xxxParagraphNoStar
  }
  \newcommand{\xxxParagraphStar}[1]{\oldparagraph*{#1}\mbox{}}
  \newcommand{\xxxParagraphNoStar}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
  \let\oldsubparagraph\subparagraph
  \renewcommand{\subparagraph}{
    \@ifstar
      \xxxSubParagraphStar
      \xxxSubParagraphNoStar
  }
  \newcommand{\xxxSubParagraphStar}[1]{\oldsubparagraph*{#1}\mbox{}}
  \newcommand{\xxxSubParagraphNoStar}[1]{\oldsubparagraph{#1}\mbox{}}
\fi
\makeatother


\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
% definitions for citeproc citations
\NewDocumentCommand\citeproctext{}{}
\NewDocumentCommand\citeproc{mm}{%
  \begingroup\def\citeproctext{#2}\cite{#1}\endgroup}
\makeatletter
 % allow citations to break across lines
 \let\@cite@ofmt\@firstofone
 % avoid brackets around text for \cite:
 \def\@biblabel#1{}
 \def\@cite#1#2{{#1\if@tempswa , #2\fi}}
\makeatother
\newlength{\cslhangindent}
\setlength{\cslhangindent}{1.5em}
\newlength{\csllabelwidth}
\setlength{\csllabelwidth}{3em}
\newenvironment{CSLReferences}[2] % #1 hanging-indent, #2 entry-spacing
 {\begin{list}{}{%
  \setlength{\itemindent}{0pt}
  \setlength{\leftmargin}{0pt}
  \setlength{\parsep}{0pt}
  % turn on hanging indent if param 1 is 1
  \ifodd #1
   \setlength{\leftmargin}{\cslhangindent}
   \setlength{\itemindent}{-1\cslhangindent}
  \fi
  % set entry spacing
  \setlength{\itemsep}{#2\baselineskip}}}
 {\end{list}}
\usepackage{calc}
\newcommand{\CSLBlock}[1]{\hfill\break\parbox[t]{\linewidth}{\strut\ignorespaces#1\strut}}
\newcommand{\CSLLeftMargin}[1]{\parbox[t]{\csllabelwidth}{\strut#1\strut}}
\newcommand{\CSLRightInline}[1]{\parbox[t]{\linewidth - \csllabelwidth}{\strut#1\strut}}
\newcommand{\CSLIndent}[1]{\hspace{\cslhangindent}#1}

\usepackage{booktabs}
\usepackage{longtable}
\usepackage{array}
\usepackage{multirow}
\usepackage{wrapfig}
\usepackage{float}
\usepackage{colortbl}
\usepackage{pdflscape}
\usepackage{tabu}
\usepackage{threeparttable}
\usepackage{threeparttablex}
\usepackage[normalem]{ulem}
\usepackage{makecell}
\usepackage{xcolor}
\input{/Users/joseph/GIT/latex/latex-for-quarto.tex}
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\AtBeginDocument{%
\ifdefined\contentsname
  \renewcommand*\contentsname{Table of contents}
\else
  \newcommand\contentsname{Table of contents}
\fi
\ifdefined\listfigurename
  \renewcommand*\listfigurename{List of Figures}
\else
  \newcommand\listfigurename{List of Figures}
\fi
\ifdefined\listtablename
  \renewcommand*\listtablename{List of Tables}
\else
  \newcommand\listtablename{List of Tables}
\fi
\ifdefined\figurename
  \renewcommand*\figurename{Figure}
\else
  \newcommand\figurename{Figure}
\fi
\ifdefined\tablename
  \renewcommand*\tablename{Table}
\else
  \newcommand\tablename{Table}
\fi
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Listing}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}
\makeatother
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\@ifpackageloaded{subcaption}{}{\usepackage{subcaption}}
\makeatother
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\usepackage{bookmark}

\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same} % disable monospaced font for URLs
\hypersetup{
  pdftitle={Methods in Causal Inference Part 3: Measurement Error and External Validity Threats},
  pdfauthor={Joseph A. Bulbulia},
  colorlinks=true,
  linkcolor={blue},
  filecolor={Maroon},
  citecolor={Blue},
  urlcolor={Blue},
  pdfcreator={LaTeX via pandoc}}

\title{Methods in Causal Inference Part 3: Measurement Error and
External Validity Threats}

\usepackage{academicons}
\usepackage{xcolor}

  \author{Joseph A. Bulbulia}
            \affil{%
             \small{     Victoria University of Wellington, New Zealand
          ORCID \textcolor[HTML]{A6CE39}{\aiOrcid} ~0000-0002-5861-2056 }
              }
      


\date{2024-06-12}
\begin{document}
\maketitle
\begin{abstract}
The human sciences should seek generalisations wherever possible. For
ethical and scientific reasons, it is desirable to sample more broadly
than `Western, Educated, Industrialised, Rich, and Democratic' (WEIRD)
societies. However, restricting the target population is sometimes
necessary; for example, young children should not be recruited for
studies on elderly care. Under which conditions is unrestricted sampling
desirable or undesirable? Here, we use causal diagrams to clarify the
structural features of measurement error bias and target population
restriction bias (or `selection restriction'), focusing on issues in
comparative cultural research. We define any study exhibiting such
biases, or confounding biases, as \textbf{weird} (\textbf{w}rongly
\textbf{e}stimated inferences due to \textbf{i}nappropriate
\textbf{r}estriction and \textbf{d}istortion). We explain why
statistical tests such as configural, metric, and scalar invariance
cannot address the structural biases of \textbf{weird} studies. Overall,
we examine how the workflows for causal inference provide the necessary
preflight checklists for ambitious, effective, and safe comparative
cultural research.

\textbf{KEYWORDS}: \emph{Causal Inference}; \emph{Comparative};
\emph{Cross-Cultural}; \emph{DAGs}; \emph{Experiments};
\emph{Longitudinal}; \emph{Measurement Error Bias\textbf{; }Selection
Bias}; \emph{Single World Intervention Graphs}, \emph{SWIGs},
\emph{Target Validity}; \emph{WEIRD}
\end{abstract}

\subsection{Introduction}\label{id-sec-intro}

Human scientists ask and answer questions. To anchor answers in facts,
we collect data.

Most publishing human scientists work in what Joseph Henrich, Steven
Heine, and Ara Norenzayan have termed `WEIRD' societies: `Western,
Educated, Industrialised, Rich, and Democratic Societies'
(\citeproc{ref-henrich2010weirdest}{Henrich \emph{et al.} 2010}).
Unsurprisingly, WEIRD samples are over-represented in human science
datasets (\citeproc{ref-arnett2008neglected}{Arnett 2008};
\citeproc{ref-sears1986college}{Sears 1986}). Henrich et al.~illustrate
how WEIRD samples differ from non-WEIRD samples in areas such as spatial
cognition and perceptions of fairness, while showing continuities in
basic emotion recognition, positive self-views, and motivation to punish
anti-social behaviour. Because science seeks generalisation wherever it
can, Henrich et al.~urge that sampling from non-WEIRD populations is
desirable.

Recently, a host of institutional diversity and inclusion initiatives
have been developed that commend researchers to obtain data from global
samples. In my view, the motivation for these mission statements is
ethically laudable. The injunction for a broader science of humanity
also accords with institutional missions. For example, the scientific
mission of the American Psychological Association (APA) is `to promote
the advancement, communication, and application of psychological science
and knowledge to benefit society and improve lives.' The APA does not
state that it wants to understand and benefit only North Atlantic
Societies (https://www.apa.org/pubs/authors/equity-diversity-inclusion,
accessed March 2024). It is therefore tempting to use such a mission
statement as an ideal by which to evaluate the samples used in human
scientific research.

Suppose we agree that promoting a globally diverse science makes ethical
sense. Does the sampling of globally diverse populations always advance
this ideal? It is easy to find examples in which restricting our source
population makes better scientific sense. Suppose we are interested in
the psychological effects of restorative justice among victims of
violent crime. Here, it would make little scientific sense to sample
from a population that has not experienced violent crime. Nor would it
make ethical sense. The scientific question, which may have important
ethical implications, is not served by casting a wider net. Suppose we
want to investigate the health effects of calorie restriction. It might
be unethical to include children or the elderly. It makes little sense
to investigate the psychological impact of vasectomy in biological
females or hysterectomy in biological males.

In the cases we just considered, the scientific questions pertained to a
sub-sample of the human population and so could be sensibly restricted
(refer also to Gaechter (\citeproc{ref-gachter2010}{2010}), Machery
(\citeproc{ref-machery2010}{2010})). However, even for questions that
relate to all of humanity, sampling from all of humanity might be
undesirable. For example, if we were interested in the effects of a
vaccine on disease, sampling from one population might be as good as
sampling from all. Sampling from one population might spare time and
expense, which come with opportunity costs. We might conclude that
sampling universally, where unnecessary, is wasteful and unethical.

We might agree with our mission statements in judging that ethical
aspirations must guide research at every phase. More fundamentally, we
cannot assess the bandwidth of human diversity from the armchair,
without empirical study, and this is a motivation to investigate. Yet,
mistaking our aspirations for sampling directives risks wasteful
science. Because waste carries opportunity costs, wasteful science is
unethical science.

I present these examples to remind ourselves of the importance of
addressing questions of sampling in relation to the scientific question
at hand.

During the past twenty years, causal data science, also known as `causal
inference' or `CI', has enabled tremendous clarity for questions of
research design and analysis
(\citeproc{ref-richardson2014causal}{Richardson and Rotnitzky 2014}).
Here, we examine how the workflows developed for causal inference
clarify threats and opportunities for causal inference in comparative
human research. These workflows require that we state our causal
question in terms of well-defined counterfactual quantities, state the
population of interest, and evaluate assumptions under which it is
possible to obtain valid quantitative estimates of the counterfactual
quantities we seek from data. Application of these workflows to
comparative questions enables us to clarify when comparative research is
possible, and also whether it is desirable. Not all questions are
causal, of course. However, because manifest associations in a dataset
may not be evidence of \emph{association} in the world, even those who
seek comparative descriptive understanding may benefit from causal
inference workflows (\citeproc{ref-vansteelandt2022a}{Vansteelandt and
Dukes 2022a}).

In the remainder of the introduction, I review causal directed acyclic
graphs (causal DAGs). Readers familiar with causal directed acyclic
graphs may skip this section. I encourage readers unfamiliar with causal
directed acyclic graphs to develop familiarity before proceeding:
(\citeproc{ref-barrett2021}{Barrett 2021};
\citeproc{ref-bulbulia2023}{Bulbulia 2024b};
\citeproc{ref-hernan2024WHATIF}{Hernan and Robins 2024 Chapter 6};
\citeproc{ref-mcelreath2020}{McElreath 2020 Chapters 5, 6};
\citeproc{ref-neal2020introduction}{Neal 2020};
\citeproc{ref-pearl2009a}{Pearl 2009}). Because directed acyclic graphs
encode causal assumptions, we will use the terms `structural' and
`causal' synonymously.

\hyperref[id-sec-1]{Part 1} uses causal diagrams to clarify five
structural features of measurement-error bias. Understanding measurement
error bias is essential in all research, especially in comparative human
science, where it casts a long shadow.

\hyperref[id-sec-2]{Part 2} examines structural sources of bias arising
from attrition and non-response, also known as `right-censoring' or
simply `censoring'. Censoring may lead to restriction of the target
population at baseline. If the analytic sample population at baseline is
meant to be the target population, censoring at baseline may lead to
bias.

\hyperref[id-sec-3]{Part 3} addresses biases that arise at the start of
a study when there is a mismatch between the analytic sample population
and the target population. When the target population is restricted in
the analytic sample population at baseline, results may be biased. I
focus on structural threats to inference when the analytic sample
population is (1) too restrictive (e.g., too WEIRD - Western, Educated,
Industrialised, Rich, and Democratic) and (2) insufficiently restrictive
(leading to bias from WEIRD sampling). We find that
population-restriction biases are formally equivalent to certain
measurement error biases. This structural parallel is crucial because it
shows that many biases in comparative research can be treated as
measurement error biases. As these biases are structural---causal in
nature---they cannot be assessed using the statistical estimation
methods typically employed by comparative researchers.

\hyperref[id-sec-4]{Part 4} uses Single World Intervention Graphs
(SWIGs) to enhance understanding of measurement-error bias, which is not
easily conveyed through causal directed acyclic graphs (DAGs). Causal
DAGs are designed to evaluate assumptions of `no unmeasured
confounding'. Consequently, they do not fully elucidate
population-restriction and measurement-error biases that do not stem
from confounding. Although SWIGs are also built to evaluate `no
unmeasured confounding', they represent counterfactual dependencies
directly on a graph. By placing the measurements---or `reporters'---of
latent realities we aim to quantify, along with the variables that
perturb these reporters so that the reported quantities differ from the
latent realities, we can advance the structural understanding of
measurement problems. This approach better diagnoses threats to
comparative human science and elucidates their remedies.

The importance of causal inference for comparative research has been
highlighted in several recent studies
(\citeproc{ref-bulbulia2022}{Bulbulia 2022};
\citeproc{ref-deffner2022}{Deffner \emph{et al.} 2022}). Here, I focus
on challenges arising from structural features of (1) measurement error
bias, (2) target population restriction bias from censoring, and (3)
target population restriction bias at a study's baseline. I clarify that
the basis of these biases is causal, not statistical, by demonstrating
their roots in measurement error bias. This understanding is essential
because comparative researchers often rely on statistical methods, such
as configurable scalar and metric invariance, to address measurement
issues. However, if the problems are causal, such methods are
inadequate. They fail to clarify the dependencies between reality, its
measurements, and the contextual and cultural features that modify the
effects of reality on its measurements
(\citeproc{ref-vanderweele2022}{VanderWeele 2022};
\citeproc{ref-vanderweele2022a}{VanderWeele and Vansteelandt 2022}).

I begin with a brief overview of causal inference, causal directed
acyclic graphs (causal DAGs), and our terminology.

\subsubsection{What is Causality?}\label{what-is-causality}

To quantify a causal effect, we must contrast the world as it is -- in
principle, observable -- with the world as it might have been -- in
principle, not observable.

Consider a binary treatment variable \(A \in \{0,1\}\) representing the
randomised administration of a vaccine to individuals \(i\) in the set
\(\{1, 2, \ldots, n\}\). \(A_i = 1\) denotes vaccine administration, and
\(A_i = 0\) denotes no vaccine. The potential outcomes for each
individual are \(Y_i(0)\) and \(Y_i(1)\), representing outcomes yet to
be realised before administration. Thus, they are called `potential' or
`counterfactual' outcomes. For an individual \(i\), we define a causal
effect as the contrast between the outcome observed under one
intervention level and the outcome observed under another. This
contrast, for the \(i^{th}\) individual, can be expressed on the
difference scale as:

\[
\text{Individual Treatment Effect} = Y_i(1) - Y_i(0)
\]

where the `Individual Treatment Effect' is the difference in the
outcomes for an individual under two treatment conditions, where
\(Y_i(1) - Y_i(0) \neq 0\) denotes a causal effect of \(A\) on \(Y\) for
individual \(i\) on the difference scale. Similarly,
\(\frac{Y_i(1)}{Y_i(0)} \neq 1\) denotes a causal effect of treatment
\(A\) for individual \(i\) on the risk ratio scale. These quantities
cannot be computed from observational data for any individual \(i\). The
inability to observe individual-level causal effects is the
\emph{Fundamental Problem of Causal Inference}
(\citeproc{ref-holland1986}{Holland 1986};
\citeproc{ref-rubin1976}{Rubin 1976}). This problem has long puzzled
philosophers (\citeproc{ref-hume1902}{Hume 1902};
\citeproc{ref-lewis1973}{Lewis 1973}). However, although individual
causal effects are generally unobservable, we can sometimes recover
average causal effects by treatment group.

\subsubsection{How We Obtain Average Causal Effect Estimates from
Ideally Conducted Randomised
Experiments}\label{how-we-obtain-average-causal-effect-estimates-from-ideally-conducted-randomised-experiments}

The Average Treatment Effect (ATE) measures the difference in outcomes
between treated and control groups:

\[
\text{Average Treatment Effect} = \mathbb{E}[Y(1)] - \mathbb{E}[Y(0)]
\]

Here, \(\mathbb{E}[Y(1)]\) and \(\mathbb{E}[Y(0)]\) represent the
average outcome for the target population if \emph{everyone} in the
population were subjected to the treatment and control conditions,
respectively.

In a randomised experiment, we estimate these averages assuming that the
analytic sample population matches the target population. We do this by
considering the average observed and unobserved outcomes under the
treatment conditions:

\[
\text{ATE} = \left(\mathbb{E}[Y(1) \mid A = 1] + \underbrace{\mathbb{E}[Y(1) \mid A = 0]}_{\textcolor{red}{\text{unobserved}}}\right) - \left(\mathbb{E}[Y(0) \mid A = 0] + \underbrace{\mathbb{E}[Y(0) \mid A = 1]}_{\textcolor{red}{\text{unobserved}}}\right)
\]

Effective randomisation ensures that potential outcomes are similarly
distributed across both groups. Thus, any differences in the averages of
the treatment groups can be attributed to the treatment. Therefore, in
an ideally conducted randomised experiment, the average outcomes are
expected to be equal across different treatment conditions for the
population from which the sample is drawn:

\[
\underbrace{\overbrace{\left[\mathbb{E}[Y(0) \mid A = 1]\right]}^{\textcolor{red}{\text{unobserved}}} = \overbrace{\left[\mathbb{E}[Y(0) \mid A = 0]\right]}^{\text{observed}}}_{\text{Under } A = 0}, \quad \underbrace{\overbrace{\left[\mathbb{E}[Y(1) \mid A = 1]\right]}^{\text{observed}} = \overbrace{\left[\mathbb{E}[Y(1) \mid A = 0]\right]}^{\textcolor{red}{\text{unobserved}}}}_{\text{Under } A = 1}
\]

Because treatment groups are exchangeable -- by randomisation, it
follows that an ideally randomised controlled experiment provides an
unbiased estimate of the Average Treatment Effect:

\[
\widehat{\text{ATE}} = \widehat{\mathbb{E}}[Y \mid A = 1] - \widehat{\mathbb{E}}[Y \mid A = 0]
\]

Note that in the context of our imagined experiment,
\(\widehat{\text{ATE}}\) applies to the population from which the
experimental participants were drawn and is calculated on the difference
scale. A more explicit notation would define this effect estimate by
referencing its scale and population:
\(\widehat{\text{ATE}}^{a'-a}_{\text{S}}\), where \(a'-a\) denotes the
difference scale, and \(S\) denotes the source population. I will return
to this point in \hyperref[id-sec-2]{Part 2} and
\hyperref[id-sec-3]{Part 3}, but it is important to build intuition
early that in causal inference we must specify: 1. The causal effect of
interest. 2. A scale of contrast. 3. A target population for whom a
causal effect estimate is meant to generalise.

\subsubsection{Three Fundamental Assumptions For Causal
Inference}\label{three-fundamental-assumptions-for-causal-inference}

An observational study aims to estimate the average treatment effects
without researchers controlling treatments or randomising treatment
assignments. We can consistently estimate counterfactual contrasts only
under strict assumptions. Three fundamental assumptions are required to
obtain the counterfactual quantities required to compute causal
contrasts from observational data.

\paragraph{Assumption 1. Causal
Consistency}\label{assumption-1.-causal-consistency}

Causal consistency states that the observed outcome for each individual
under the treatment they actually received is equal to their potential
outcome under that treatment. This means if an individual \(i\) received
treatment \(A_i = 1\), their observed outcome \(Y_i\) is the same as
their potential outcome under treatment, denoted as \(Y_i(1)\).
Similarly, if they did not receive the treatment (\(A_i = 0\)), their
observed outcome is the same as their potential outcome without
treatment, denoted as \(Y_i(0)\), such that:

\[
Y_i = A_i \cdot Y_i(1) + (1 - A_i) \cdot Y_i(0)
\]

where:

\begin{itemize}
\tightlist
\item
  \(Y_i\) is the observed outcome for individual \(i\);
\item
  \(A_i\) is the treatment status for individual \(i\), with \(A_i = 1\)
  indicating treatment received and \(A_i = 0\) indicating no treatment;
\item
  \(Y_i(1)\) and \(Y_i(0)\) are the potential outcomes for individual
  \(i\) under treatment and no treatment, respectively (refer to Morgan
  and Winship (\citeproc{ref-morgan2014}{2014}); VanderWeele
  (\citeproc{ref-vanderweele2009}{2009})).
\end{itemize}

The causal consistency assumption is necessary to link the theoretical
concept of potential outcomes --- the target quantities of interest ---
with observable data (see Bulbulia \emph{et al.}
(\citeproc{ref-bulbulia2023a}{2023})).

\paragraph{Assumption 2. Conditional Exchangeability (or
Ignorability)}\label{assumption-2.-conditional-exchangeability-or-ignorability}

Conditional exchangeability states that given a set of measured
covariates \(L\), the potential outcomes are independent of the
treatment assignment. Once we control for \(L\), the treatment
assignment \(A\) is as good as random with respect to the potential
outcomes:

\[
Y(a) \coprod A | L
\]

where:

\begin{itemize}
\tightlist
\item
  \(Y(a)\) represents the potential outcomes for a particular treatment
  level \(a\).
\item
  \(\coprod\) denotes conditional independence.
\item
  \(A\) represents the treatment levels to be contrasted.
\item
  \(L\) represents the measured covariates.
\end{itemize}

Under the conditional exchangeability assumption, any differences in
outcomes between treatment groups can be attributed to the treatment.
This assumption requires that all confounding variables affecting both
the treatment assignment \(A\) and the potential outcomes \(Y(a)\) are
measured and included in \(L\).

\paragraph{Assumption 3. Positivity}\label{assumption-3.-positivity}

The positivity assumption requires that every individual in the
population has a non-zero probability of receiving each treatment level,
given their covariates. Formally,

\[
0 < Pr(A = a | L = l) < 1, \quad \forall a \in A, \, \forall l \in L \, \text{ such that } \, Pr(L = l) > 0
\]

where:

\begin{itemize}
\tightlist
\item
  \(A\) is the treatment or exposure variable.
\item
  \(L\) is a vector of covariates assumed sufficient for satisfying
  conditional exchangeability.
\end{itemize}

The positivity assumption requires that every individual has a non-zero
chance of receiving each treatment across all covariates in \(L\)
(\citeproc{ref-bulbulia2023a}{Bulbulia \emph{et al.} 2023};
\citeproc{ref-chatton2020}{Chatton \emph{et al.} 2020};
\citeproc{ref-hernan2024WHATIF}{Hernan and Robins 2024};
\citeproc{ref-westreich2010}{Westreich and Cole 2010}). For a discussion
of causal assumptions in relation to external validity, refer to Imai
\emph{et al.} (\citeproc{ref-imai2008misunderstandings}{2008}).

\subsubsection{Terminology}\label{terminology}

To avoid confusion, we define the meanings of our terms:

\begin{itemize}
\item
  \textbf{Unit/individual}: An entity, such as an object, person, or
  culture. We will use the term `individual' instead of the more general
  term `unit'. Think `row' in a dataset.
\item
  \textbf{Variable}: A feature of an individual, transient or permanent.
  For example, `Albert was sleepy but is no longer' or `Alice was born
  30 November'.
\item
  \textbf{Treatment}: Equivalent to `exposure', an event that might
  change a variable. For instance, `Albert was sleepy; we intervened
  with coffee; he is now wide awake' or `Alice was born in November;
  nothing can change that'. The `cause'.
\item
  \textbf{Outcome}: The response variable or `effect'. In causal
  inference, we contrast `potential' or `counterfactual outcomes'. In
  observational or `real-world' studies where treatments are not
  randomised, the assumptions for obtaining contrasts of counterfactual
  outcomes are typically much stronger than in randomised controlled
  experiments.
\item
  \textbf{Confounding}: A state where the treatment and outcome share a
  common cause and no adjustment is made to remove the non-causal
  association, or where the treatment and outcome share a common effect,
  and adjustment is made for this common effect, or when the effect of
  the treatment on the outcome is mediated by a variable which is
  conditioned upon. In each case, the observed association will not
  reflect a causal association. Causal directed acyclic graphs clarify
  strategies for confounding control.
\item
  \textbf{Measurement}: A recorded trace of a variable, such as a column
  in a dataset. When placing measurements within causal settings, we
  call measurements \textbf{reporters}.
\item
  \textbf{Measurement error}: A misalignment between the true state of a
  variable and its recorded state. For example, `Alice was born on 30
  November; records were lost, and her birthday was recorded as 1
  December'.
\item
  \textbf{Population}: An abstraction from statistics, denoting the set
  of all individuals defined by certain features. Albert belongs to the
  set of all individuals who ignore instructions.
\item
  \textbf{Super-population}: An abstraction, the population of all
  possible individuals of a given kind. Albert and Alice belong to a
  super-population of hominins.
\item
  \textbf{Restricted population}: Population \(p\) is restricted
  relative to another population \(P\) if the individuals \(p \in P\)
  share some but not all features of \(P\). `The living' is a
  restriction of hominins.
\item
  \textbf{Target population}: A restriction of the super-population
  whose features interest investigators. An investigator who defines
  their interests is a member of the population of `good investigators'.
\item
  \textbf{Source population}: The population from which the study's
  sample is drawn. Investigators wanted to recruit from a general
  population but recruited from the pool of first-year university
  psychology students.
\item
  \textbf{Sample population at baseline}: or equivalently the `analytic
  sample population.' The abstract set of individuals from which the
  units in a study at treatment assignment belong, e.g., `the set of all
  first-year university psychology students who might end up in this
  study'. Unless stated otherwise, we will consider the baseline
  analytic sample population to represent the \emph{source population};
  we will consider the \emph{analytic population} at baseline to be
  representative of the \emph{target population}.
\item
  \textbf{Selection into the analytic sample}: Selection occurs and is
  under investigator control when a target population is defined from a
  super-population or when investigators apply eligibility criteria for
  inclusion in the analytic sample. Selection into the sample is often
  out of the investigator's control. Investigators might aspire to
  answer questions about all of humanity but find themselves limited to
  undergraduate samples. Investigators might sample from a source
  population but recover an analytic sample that differs from it in ways
  they cannot measure, such as mistrust of scientists. There is
  typically attrition of an analytic sample over time, and this is not
  typically fully within investigator control. Because the term
  `selection' has different meanings in different areas of human
  science, we will speak of `target population restriction at the start
  of study'. Note that to evaluate this bias, it is important for
  investigators to state a causal effect of interest with respect to
  \emph{the full data} that includes the counterfactual quantities for
  the treatments to be compared in a clearly defined target population
  where all members of the target population are exposed to each level
  of treatment to be contrasted (\citeproc{ref-westreich2017}{Westreich
  \emph{et al.} 2017a}).
\item
  \textbf{(Right) censored analytic sample at the end of study}: Right
  censoring is generally uninformative if there is no treatment effect
  for everyone in the baseline population (the sharp causal null
  hypothesis). Censoring is informative if there is an effect of the
  treatment, and this effect varies in at least one stratum of the
  baseline population. If no correction is applied, unbiased effect
  estimates for the analytic sample will bias causal effect estimates
  for the target population in at least one measure of effect
  (\citeproc{ref-greenland2009commentary}{Greenland 2009};
  \citeproc{ref-lash2020}{Lash \emph{et al.} 2020};
  \citeproc{ref-vanderweele2012}{VanderWeele 2012}). We call such bias
  from right censoring `target population restriction at the end of
  study'. Note again that to evaluate this bias, the causal effect of
  interest must be stated with respect to \emph{the full data} that
  includes the counterfactual quantities for the treatments to be
  compared in a clearly defined target population where all members of
  the target population are exposed to each level of treatment to be
  contrasted (\citeproc{ref-westreich2017}{Westreich \emph{et al.}
  2017a}).
\item
  \textbf{Target population restriction bias}: Bias occurs when the
  distribution of effect modifiers in the analytic sample population
  differs from that in the target population, either at the start, at
  the end, or throughout the study. Here we consider: \emph{target
  population restriction bias at the start of study} and \emph{target
  population restriction bias at the end of study}. If this bias occurs
  at the start of the study, it will generally occur at the end of the
  study (and at intervals between), except by accident. We require
  validity to be non-accidental.
\item
  \textbf{Generalisability}: A study's findings generalise to a target
  population if the effects observed in the analytic sample at the end
  of study are also valid for the target population for structurally
  valid reasons (i.e., non-accidentally).
\item
  \textbf{Transportability}: When the analytic sample is not drawn from
  the target population, we cannot directly generalise the findings.
  However, we can transport the estimated causal effect from the source
  population to the target population under certain assumptions. This
  involves adjusting for differences in the distributions of effect
  modifiers between the two populations. The closer the source
  population is to the target population, the more plausible the
  transportability assumptions and the less we need to rely on complex
  adjustment methods see (Refer to supplementary materials \textbf{S2}).
\item
  \textbf{Marginal effect}: Typically a synonym for the average
  treatment effect --- always relative to some population specified by
  investigators.
\item
  \textbf{Intention-to-treat effect}: The marginal effect of random
  treatment assignment.
\item
  \textbf{Per-protocol effect}: The effect of adherence to a randomly
  assigned treatment if adherence were perfect
  (\citeproc{ref-hernan2017per}{Hern치n \emph{et al.} 2017}). We have no
  guarantee that the intention-to-treat effect will be the same as the
  per-protocol effect. A safe assumption is that: \[
  \widehat{ATE}_{\text{target}}^{\text{Per-Protocol}} \ne \widehat{ATE}_{\text{target}}^{\text{Intention-to-Treat}}
  \]
\end{itemize}

When evaluating evidence for causality, in addition to specifying their
causal contrast, effect measure, and target population, investigators
should specify whether they are estimating an intention-to-treat or
per-protocol effect (\citeproc{ref-hernuxe1n2004}{Hern치n 2004};
\citeproc{ref-tripepi2007}{Tripepi \emph{et al.} 2007}).

\begin{itemize}
\item
  \textbf{WEIRD}: A sample of `Western, Educated, Industrialised, Rich,
  and Democratic Societies' (\citeproc{ref-henrich2010weirdest}{Henrich
  \emph{et al.} 2010}).
\item
  \textbf{weird}: (\textbf{w}rongly \textbf{e}stimated inferences due to
  \textbf{i}nappropriate \textbf{r}estriction and \textbf{d}istortion) A
  causal effect estimate that is not valid for the target population,
  either from confounding bias, measurement error bias, target
  population restriction at the start of study, or target population
  restriction at the end of study.
\end{itemize}

For discussion of these concepts refer to Dahabreh \emph{et al.}
(\citeproc{ref-dahabreh2021study}{2021}); Imai \emph{et al.}
(\citeproc{ref-imai2008misunderstandings}{2008}); Cole and Stuart
(\citeproc{ref-cole2010generalizing}{2010}); Westreich \emph{et al.}
(\citeproc{ref-westreich2017transportability}{2017b}). A clear
decomposition of key concepts needed to external validity--- or what we
call `target validity' --- is given in Imai \emph{et al.}
(\citeproc{ref-imai2008misunderstandings}{2008}). For a less technical,
pragmatically useful discussion, refer to Stuart \emph{et al.}
(\citeproc{ref-stuart2018generalizability}{2018}). Note that terminology
differs across the causal inference literature. See supplementary
materials \textbf{S1} for a causal inference glossary.

\subsubsection{Graphical Conventions}\label{graphical-conventions}

\begin{itemize}
\item
  \textbf{\(A\)}: Denotes the `treatment' or `exposure' --- a random
  variable, `the cause'.
\item
  \textbf{\(Y\)}: Denotes the outcome or response, measured at the end
  of the study. \(Y\) is the `effect'.
\item
  \textbf{\(L\)}: Denotes a measured confounder or set of confounders.
\item
  \textbf{\(U\)}: Denotes an unmeasured confounder or confounders.
\item
  \textbf{\(\mathbf{\mathcal{R}}\)}: Denotes randomisation to treatment
  condition \(\big(\mathcal{R} \rightarrow A\big)\).
\item
  \textbf{Node}: Represents characteristics or features of units within
  a population on a causal diagram --- that is, a `variable'. In causal
  directed acyclic graphs, nodes are drawn with respect to the
  \emph{target population}, which is the population for whom
  investigators seek causal inferences (\citeproc{ref-suzuki2020}{Suzuki
  \emph{et al.} 2020}). Time-indexed nodes: \(X_t\) denotes relative
  chronology.
\item
  \textbf{Edge without an Arrow} (\(\association\)): Path of
  association, causality not asserted.
\item
  \textbf{Red Edge without an Arrow} (\(\associationred\)): Confounding
  path, ignoring arrows to clarify statistical dependencies.
\item
  \textbf{Arrow} (\(\rightarrow\)): Denotes a causal relationship from
  the node at the base of the arrow (a `parent') to the node at the tip
  of the arrow (a `child'). In causal directed acyclic graphs, it is
  conventional to refrain from drawing an arrow from treatment to
  outcome to avoid asserting a causal path from \(A\) to \(Y\) because
  we aim to ascertain whether causality can be identified for this path.
  All other nodes and paths --- including the absence of nodes and paths
  --- are typically assumed.
\item
  \textbf{Red Arrow} (\(\rightarrowred\)): Denotes a path of non-causal
  association between the treatment and outcome. Despite the arrows,
  this path is associational and may flow against time.
\item
  \textbf{Open Blue Arrow} (\(\rightarrowblue\)): Denotes effect
  modification, which occurs when the effect of treatment varies within
  levels of a covariate. We do not assess the causal effect of the
  effect modifier on the outcome, recognising that it may be incoherent
  to consider intervening on the effect modifier. However, if the
  distribution of effect modifiers in the analytic sample population
  differs from that in the target population, then at least one measure
  of causal effect will differ between the two populations.
\item
  \textbf{Boxed Variable} \(\big(\boxed{X}\big)\): Denotes conditioning
  or adjustment for \(X\).
\item
  \textbf{Red-Boxed Variable} \(\big(\boxedred{X}\big)\): Highlights the
  source of confounding bias from adjustment.
\item
  \textbf{Dashed Circle} \(\big(\circledotted{X}\big)\): Denotes no
  adjustment is made for a variable (implied for unmeasured
  confounders).
\item
  \textbf{\(\mathcal{G}\)}: Names a causal diagram.
\item
  \textbf{Split Node (SWIGs)} \(\nodesplit\): Convention used in Single
  World Intervention Graphs (SWIGs) that allows for factorisation of
  counterfactuals by splitting a node at intervention with
  post-intervention nodes relabeled to match the treatment. We introduce
  Single World Intervention Graphs in \hyperref[id-sec-4]{Part 4}.
\item
  \textbf{Unobserved Node (SWIGs)} \(\swiglatent\): Our convention when
  using Single World Intervention Graphs to denote an unobserved node
  (SWIGs): \(X\) unmeasured.
\end{itemize}

\subsubsection{Causal Directed Acyclic Graphs
(DAGs)}\label{causal-directed-acyclic-graphs-dags}

Judea Pearl proved that, based on assumptions about causal structure,
researchers can identify causal effects from joint distributions of
observed data (\citeproc{ref-pearl1995}{Pearl 1995},
\citeproc{ref-pearl2009a}{2009}). The rules of d-separation are given in
Table~\ref{tbl-terminologygeneral}.

\begin{table}

\caption{\label{tbl-terminologygeneral}Elements of Causal Graphs}

\centering{

\terminologydirectedgraph

}

\end{table}%

Pearl's rules of d-separation are as follows:

\begin{itemize}
\tightlist
\item
  \textbf{Fork rule} (\(B \leftarrowNEW \boxed{A} \rightarrowNEW C\)):
  \(B\) and \(C\) are independent when conditioned on \(A\)
  (\(B \coprod C \mid A\)).
\item
  \textbf{Chain rule} (\(A \rightarrowNEW \boxed{B} \rightarrowNEW C\)):
  Conditioning on \(B\) blocks the path between \(A\) and \(C\)
  (\(A \coprod C \mid B\)).
\item
  \textbf{Collider rule}
  (\(A \rightarrowNEW \boxed{C} \leftarrowNEW B\)): \(A\) and \(B\) are
  independent until conditioned on \(C\), which introduces dependence
  (\(A \cancel{\coprod} B \mid C\)).
\end{itemize}

Table~\ref{tbl-terminologygeneral} shows causal directed acyclic graphs
corresponding to these rules. Because all causal relationships can be
assembled from combinations of the five structures presented in
Table~\ref{tbl-terminologygeneral}, we can use causal graphs to evaluate
whether and how causal effects may be identified from data
(\citeproc{ref-bulbulia2023}{Bulbulia 2024b}).

Pearl's general identification algorithm is known as the `back door
adjustment theorem' (\citeproc{ref-pearl2009a}{Pearl 2009}).

\textbf{Backdoor Adjustment}: In a causal directed acyclic graph (DAG),
a set of variables \(L\) satisfies the backdoor adjustment theorem
relative to the treatment \(A\) and the outcome \(Y\) if \(L\) blocks
every path between \(A\) and \(Y\) that contains an arrow pointing into
\(A\) (a backdoor path). Formally, \(L\) must satisfy two conditions:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{No Path Condition}: No element of \(L\) is a descendant of
  \(A\).
\item
  \textbf{Blocking Condition}: \(L\) blocks all backdoor paths from
  \(A\) to \(Y\).
\end{enumerate}

If \(L\) satisfies these conditions, the causal effect of \(A\) on \(Y\)
can be estimated by conditioning on \(\boxed{L}\)
(\citeproc{ref-pearl2009a}{Pearl 2009}).

\subsubsection{Effect-Modification on Causal Directed Acyclic
Graphs}\label{effect-modification-on-causal-directed-acyclic-graphs}

The primary function of a causal directed acyclic graph is to apply the
Pearl's backdoor adjustment theorem to establish relations of
conditional independence Table~\ref{tbl-terminologygeneral} for the
purposes of causal identification. We have noted that modifying a causal
effect within one or more strata of the target population opens the
possibility for biased average treatment effect estimates when the
distribution of these effect modifiers differs in the analytic sample
population.

We do not generally represent non-linearities in causal directed acyclic
graphs, which are tools for obtaining relationships of conditional and
unconditional independence from assumed structural relationships encoded
in a causal diagram that may lead to a non-causal treatment/outcome
association.

Table~\ref{tbl-terminologygeneral} presents our convention for
highlighting a relationship of effect modification in settings where (1)
we assume no confounding of treatment and outcome and (2) there is
effect modification such that the effect of \(A\) on \(Y\) differs in at
least one stratum of the target population.

\begin{table}

\caption{\label{tbl-terminologygeneral}Elements of Causal Graphs}

\centering{

\terminologyeffectmodification

}

\end{table}%

To focus on effect modification, we do not draw a causal arrow from the
direct effect modifier \(F\) to the outcome \(Y\). This convention is
specific to this article (refer to Hernan and Robins
(\citeproc{ref-hernan2024WHATIF}{2024}), pp.~126-127, for a discussion
of `non-causal' arrows).

\subsection{\texorpdfstring{Part 1: How Measurement Error Bias Makes
Your Causal Inferences \textbf{weird} (\textbf{w}rongly
\textbf{e}stimated inferences due to \textbf{i}nappropriate
\textbf{r}estriction and
\textbf{d}istortion)}{Part 1: How Measurement Error Bias Makes Your Causal Inferences weird (wrongly estimated inferences due to inappropriate restriction and distortion)}}\label{id-sec-1}

Measurements record reality, but they are not always accurate. Whenever
variables are measured with error, our results can be misleading. Every
study must therefore consider how its measurements might mislead.

Causal graphs can deepen understanding because -- as implied by the
concept of `record' ---there are structural or causal properties that
give rise to measurement error. Measurement error can take various
forms, each with distinct implications for causal inference:

\begin{itemize}
\tightlist
\item
  \textbf{Independent (undirected) /uncorrelated}: Errors in different
  variables do not influence each other.
\item
  \textbf{Independent (undirected) and correlated}: Errors in different
  variables are related through a shared cause.
\item
  \textbf{Dependent (directed) and uncorrelated}: Errors in one variable
  influence the measurement of another, but these influences are not
  related through a shared cause.
\item
  \textbf{Dependent (directed) and correlated}: Errors in one variable
  influence the measurement of another, and these influences are related
  through a shared cause (\citeproc{ref-hernuxe1n2009}{Hern치n and Cole
  2009}; \citeproc{ref-vanderweele2012a}{VanderWeele and Hern치n 2012}).
\end{itemize}

The six causal diagrams presented in
Table~\ref{tbl-terminologymeasurementerror} illustrate structural
features of measurement error bias and clarify how these structural
features compromise causal inferences.

\begin{table}

\caption{\label{tbl-terminologymeasurementerror}Example of measurement
error bias}

\centering{

\terminologymeasurementerror

}

\end{table}%

Understanding the structural features of measurement error bias will
help us understand why measurement error bias cannot typically be
evaluated with statistical models and will prepare us to understand how
target-population restriction biases are linked to measurement error.

\subsubsection{Example 1: Uncorrelated Non-Differential Errors under
Sharp Null: No Treatment
Effect}\label{example-1-uncorrelated-non-differential-errors-under-sharp-null-no-treatment-effect}

Table~\ref{tbl-terminologymeasurementerror} \(\mathcal{G}_1\)
illustrates uncorrelated non-differential measurement error under the
`sharp-null,' which arises when the error terms in the exposure and
outcome are independent. In this setting, the measurement error
structure is not expected to produce bias.

For example, consider a study investigating the causal effect of beliefs
in big Gods on social complexity in ancient societies. Imagine that
societies either randomly omitted or inaccurately recorded details about
their beliefs in big Gods and their social complexities. This might
occur because of varying preservation in the records of cultures which
is unrelated to the actual beliefs or social complexity. In this
scenario, we imagine the errors in historical records for beliefs in big
Gods and for social complexity are independent. When the treatment is
randomised, uncorrelated and undirected errors will generally not
introduce bias \emph{under the sharp null of no treatment effect for any
unit} when all backdoor paths are closed. However, if confounders are
measured without error this may open a backdoor path from treatment to
outcome. For example, Robins \emph{et al.}
(\citeproc{ref-robins2004effects}{2004}) p.2216 discusses how in
non-experimental settings, mismeasured confounders can introduce bias
even when the measurement errors of the treatment and outcome are
uncorrelated and undirected and there is no treatment effect. This is
because mismeasured confounders will not control for confounding bias.
We present an illustration of this bias in
Table~\ref{tbl-terminologyselectionrestrictionbaseline}
\(\mathcal{G}_6\), where we discuss challenges to comparative research
in which the accuracy of confounder measurements varies across the sites
to be compared.

\subsubsection{Example 2: Uncorrelated Non-Differential Errors `Off The
Null' (True Treatment Effect) Biases True Effects toward the
Null}\label{example-2-uncorrelated-non-differential-errors-off-the-null-true-treatment-effect-biases-true-effects-toward-the-null}

Table~\ref{tbl-terminologymeasurementerror} \(\mathcal{G}_2\)
illustrates uncorrelated non-differential measurement error, which
arises when the error terms in the exposure and outcome are independent
This bias is also called information bias
(\citeproc{ref-lash2009applying}{Lash \emph{et al.} 2009}). In this
setting, bias will often attenuate a true treatment effect. However,
there are no guarantees that uncorrelated undirected measurement error
bias effect estimates toward the null
(\citeproc{ref-jurek2005proper}{Jurek \emph{et al.} 2005},
\citeproc{ref-jurek2008brief}{2008};
\citeproc{ref-jurek2006exposure}{Jurek \emph{et al.} 2006};
\citeproc{ref-lash2009applying}{Lash \emph{et al.} 2009 p. 93}).

Consider again the example of a study investigating a causal effect of
beliefs in big Gods on social complexity in ancient societies, where
there are uncorrelated errors in the treatment and outcome. In this
case, measurement error will often make it seem that the true causal
effects of beliefs in big Gods are smaller than they are, or perhaps
even that such an effect is absent. Often but not always: again,
attenuation of the effect estimate is not guaranteed, and mismeasured
confounders will open backdoor paths. We can, however, say this:
uncorrelated undirected measurement error in the presence of a true
effect leads to distortion of that effect, inviting \textbf{weird}
results (\textbf{w}rongly \textbf{e}stimated inferences due to
\textbf{i}nappropriate \textbf{r}estriction and \textbf{d}istortion).

Uncorrelated undirected measurement error in the presence of a true
effect leads to distortion of true causal effects, inviting
\textbf{weird} results (\textbf{w}rongly \textbf{e}stimated inferences
due to \textbf{i}nappropriate \textbf{r}estriction and
\textbf{d}istortion).

\subsubsection{Example 3: Correlated Errors Non-Differential
(Undirected) Measurement
Errors}\label{example-3-correlated-errors-non-differential-undirected-measurement-errors}

Table~\ref{tbl-terminologymeasurementerror} \(\mathcal{G}_3\)
illustrates the structure of correlated non-differential (undirected)
measurement error bias, which arises when the error terms of the
treatment and outcome share a common cause.

Consider an example: imagine that societies with more sophisticated
record-keeping systems tend to offer more precise and comprehensive
records of both beliefs in big Gods and of social complexity. In this
setting, it is the record-keeping systems that give the illusion of a
relationship between big Gods and social complexity. This might occur
without any causal effect of big-God beliefs on measuring social
complexity or vice versa. Nevertheless, the correlated sources of error
for both the exposure and outcome might suggest causation in its
absence.

Correlated non-differential measurement error invites \textbf{weird}
results (\textbf{w}rongly \textbf{e}stimated inferences due to
\textbf{i}nappropriate \textbf{r}estriction and \textbf{d}istortion).

\subsubsection{Example 4: Uncorrelated Differential Measurement Error:
Exposure Affects Error of
Outcome}\label{example-4-uncorrelated-differential-measurement-error-exposure-affects-error-of-outcome}

Table~\ref{tbl-terminologymeasurementerror} \(\mathcal{G}_4\)
illustrates the structure of uncorrelated differential (or directed)
measurement error, where a non-causal path is opened linking the
treatment, the outcome, or a common cause of the treatment and outcome.

Continuing with our previous example, imagine that beliefs in big Gods
lead to inflated records of social complexity in a culture's
record-keeping. This might happen because the record keepers in
societies that believe in big Gods prefer societies to reflect the
grandeur of their big Gods. Suppose further that cultures lacking
beliefs in big Gods prefer Bacchanalian-style feasting to
record-keeping. In this scenario, societies with record keepers who
believe in big Gods would appear to have more social complexity than
equally complex societies without such record keepers.

Uncorrelated directed measurement error bias also invites \textbf{weird}
results (\textbf{w}rongly \textbf{e}stimated inferences due to
\textbf{i}nappropriate \textbf{r}estriction and \textbf{d}istortion).

\subsubsection{Example 5: Uncorrelated Differential Measurement Error:
Outcome Affects Error of
Exposure}\label{example-5-uncorrelated-differential-measurement-error-outcome-affects-error-of-exposure}

Table~\ref{tbl-terminologymeasurementerror} \(\mathcal{G}_5\)
illustrates the structure of uncorrelated differential (or directed)
measurement error, this time when the outcome affects the recording of
the treatment that preceded the outcome.

Suppose that `history is written by the victors.' Can we give a
structural account of measurement error bias arising from such selective
retention of the past? Suppose that social complexity causes beliefs in
big Gods. Perhaps kings create big Gods after the image of kings. If the
kings prefer a history in which big Gods were historically present, this
might bias the historical record, opening a path of association that
reverses the order of causation. Such results would be \textbf{weird}:
(\textbf{w}rongly \textbf{e}stimated inferences due to
\textbf{i}nappropriate \textbf{r}estriction and \textbf{d}istortion).

\subsubsection{Example 6: Uncorrelated Differential Error: Outcome
Affects Error of
Exposure}\label{example-6-uncorrelated-differential-error-outcome-affects-error-of-exposure}

Table~\ref{tbl-terminologymeasurementerror} \(\mathcal{G}_6\)
illustrates the structure of correlated differential (directed)
measurement error, which occurs when the exposure affects levels of
already correlated error terms.

Suppose social complexity produces a flattering class of religious
elites who produce vainglorious depictions of kings and their dominions,
and also of the extent and scope of their society's beliefs in big Gods.
For example, such elites might downplay widespread cultural practices of
worshipping lesser gods, inflate population estimates, and overstate the
range of the king's political economy. In this scenario, the errors of
the exposure and of the outcome are both correlated and differential.

Results based on such measures might be \textbf{weird}:
(\textbf{w}rongly \textbf{e}stimated inferences due to
\textbf{i}nappropriate \textbf{r}estriction and \textbf{d}istortion).

\subsubsection{Summary}\label{summary}

In Part 1, we examined four types of measurement error bias:
independent, correlated, dependent, and correlated dependent. The
structural features of measurement error bias clarify how measurement
errors threaten causal inferences. Considerably more could be said about
this topic. For example, VanderWeele and Hern치n
(\citeproc{ref-vanderweele2012a}{2012}) demonstrate that, under specific
conditions, we can infer the direction of a causal effect from observed
associations. Specifically, if:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  The association between the measured variables \(A^{\prime}_{1}\) and
  \(Y^{\prime}_{2}\) is positive,
\item
  The measurement errors for these variables are not correlated, and
\item
  We assume distributional monotonicity for the effect of \(A\) on \(Y\)
  (applicable when both are binary),
\end{enumerate}

then a positive observed association implies a positive causal effect
from \(A\) to \(Y\). Conversely, a negative observed association
provides stronger evidence for a negative causal effect if the error
terms are positively correlated than if they are independent. This
conclusion relies on the assumption of distributional monotonicity for
the effect of \(A\) on \(Y\). For now, the four elementary structures of
measurement error bias will enable us to clarify the connections between
the structures of measurement error bias, target population restriction
bias at the end of a study, and target restriction bias at the start of
a study.

We will return to measurement error again in \hyperref[id-sec-4]{Part
4}. Next, we focus on structural features of bias when there is an
inappropriate restriction of the target population in the analytic
sample at the end of study.

\subsection{\texorpdfstring{Part 2: How Target Population Restriction
Bias At The End of Study Makes Your Causal Inferences weird
(\textbf{w}rongly \textbf{e}stimated inferences due to
\textbf{i}nappropriate \textbf{r}estriction and
\textbf{d}istortion)}{Part 2: How Target Population Restriction Bias At The End of Study Makes Your Causal Inferences weird (wrongly estimated inferences due to inappropriate restriction and distortion)}}\label{id-sec-2}

Suppose the analytic sample population at the start of a study matches
the source population from which it is drawn and that this source
population aligns with the target population. At the start of study, our
results may generalise as we hope. Right-censoring, typically
abbreviated to `censoring' and also known as `attrition and
non-response', may bias causal effect estimates, spoiling our hopes for
valid causal inferences, in one of two ways: by opening pathways of bias
(distortion) or by inappropriately restricting the analytic sample
population at the end of a study so that it is no longer representative
of the target population. Both forms of bias will make causal inferences
\textbf{weird}: (\textbf{w}rongly \textbf{e}stimated inferences due to
\textbf{i}nappropriate \textbf{r}estriction and \textbf{d}istortion).

\begin{table}

\caption{\label{tbl-terminologycensoring}Five examples of censoring
bias.}

\centering{

\terminologycensoring

}

\end{table}%

\subsubsection{Example 1: Confounding by common cause of treatment and
attrition}\label{example-1-confounding-by-common-cause-of-treatment-and-attrition}

Table~\ref{tbl-terminologycensoring} \(\mathcal{G}_1\) illustrates
confounding by common cause of treatment and outcome in the censored
such that the potential outcomes of the population at baseline \(Y(a)\)
may differ from those of the censored population at the end of study
\(Y'(a)\) such that \(Y'(a) \neq Y(a)\).

Suppose investigators are interested in whether religious service
attendance affects volunteering. Suppose that an unmeasured variable,
loyalty, affects religious service attendance, attrition, and
volunteering. The structure of this bias reveals an open backdoor path
from the treatment to the outcome.

We have encountered this bias before. The structure we observe here is
one of correlated measurement errors
(Table~\ref{tbl-terminologymeasurementerror} \(\mathcal{G}_3\)). In this
example, attrition may exacerbate measurement error bias by opening a
path from
\(A \associationred U \associationred U_{\Delta{A}}  \associationred Y'\)

The results obtained from such a study would be distorted -- that is,
\textbf{weird}: (\textbf{w}rongly \textbf{e}stimated inferences due to
\textbf{i}nappropriate \textbf{r}estriction and \textbf{d}istortion).
Here, distortion operates through the restriction of the target
population in the analytic sample population at the end of the study.

\subsubsection{Example 2: Treatment affects
censoring}\label{example-2-treatment-affects-censoring}

Table~\ref{tbl-terminologycensoring} \(\mathcal{G}_2\) illustrates bias
in which the treatment affects the censoring process. Here, the
treatment causally affects the outcome reporter but does not affect the
outcome itself.

Consider a study investigating the effects of mediation on well-being.
Suppose there is no treatment effect but that Buddha-like detachment
increases attrition. Suppose those with lower Buddha-like detachment
report well-being differently than those with higher Buddha-like
detachment. Buddha-like detachment is not a cause of well-being, we
suppose, however, we also suppose it is a cause of measurement error in
the reporting of well-being. In this setting, we discover a biasing path
that runs: \(A \associationred U_{\Delta{A\to Y}}  \associationred Y'\).
Note there is no confounding bias here because there is no common cause
of the treatment and the outcome.

We have encountered this structural bias before. The structure we
observe here is one of directed uncorrelated measurement error
(Table~\ref{tbl-terminologymeasurementerror} \(\mathcal{G}_4\)).
Randomisation ensures no backdoor paths. However, if the intervention
affects both attrition and bias in the outcome we may expect measurement
error bias.

The results obtained from this study risk distortation and as such
invite \textbf{weirdness}: (\textbf{w}rongly \textbf{e}stimated
inferences due to \textbf{i}nappropriate \textbf{r}estriction and
\textbf{d}istortion). Here, distortion operates through the restriction
of the target population at the end of the study, assuming the analytic
sample at the start of the study represented that target population (or
was weighted to represent it.)

\subsubsection{Example 3: No treatment effect when outcome causing
censoring}\label{example-3-no-treatment-effect-when-outcome-causing-censoring}

Table~\ref{tbl-terminologycensoring} \(\mathcal{G}_3\) illustrates the
structure of bias when there is no treatment effect yet the outcome
affects censoring.

If \(\mathcal{G}_3\) faithfully represents reality, under the sharp null
we would generally not expect bias in the average treatment effect
estimate from attrition. The structure we observe here is again
familiar: it is one of undirected uncorrelated measurement
error(Table~\ref{tbl-terminologymeasurementerror} \(\mathcal{G}_1\)).
However, as before, it is generally never clear whether the sharp null
holds. In theory, however, although the analytic sample population would
be restricted, such restriction of the target population should not bias
the null result. Again, this example is theoretical; no statistical test
could validate what amounts to a structural assumption of the sharp
null.

\subsubsection{Example 4: Treatment effect when outcome causes censoring
and there is a true treatment
effect}\label{example-4-treatment-effect-when-outcome-causes-censoring-and-there-is-a-true-treatment-effect}

Table~\ref{tbl-terminologycensoring} \(\mathcal{G}_4\) illustrates the
structure of bias when the outcome affects censoring in the presence of
a treatment effect. If the true outcome is an effect modifier of the
measured outcome, we can expect bias in at least one measure of effect
(e.g., the risk ratio or the causal difference scale). We return to this
form of bias with a worked example in \hyperref[id-sec-4]{Part 4}, where
we clarify how such bias may arise even without confounding. We shall
see that the bias described in Table~\ref{tbl-terminologycensoring}
\(\mathcal{G}_4\) is equivalent to measurement error bias.

Results would be \textbf{weird}: (\textbf{w}rongly \textbf{e}stimated
inferences due to \textbf{i}nappropriate \textbf{r}estriction and
\textbf{d}istortion).

\subsubsection{Example 5: Treatment effect and effect-modifiers differ
in censored (restriction bias without
confounding)}\label{example-5-treatment-effect-and-effect-modifiers-differ-in-censored-restriction-bias-without-confounding}

Table~\ref{tbl-terminologycensoring} \(\mathcal{G}_5\) represents a
setting in which there is a true treatment effect, but the distribution
of effect-modifiers -- variables that interact with the treatment --
differ among the sample at baseline and the sample at the end of the
study. Knowing nothing else, we might expect this setting to be
standard. Where measured variables are sufficient to predict attrition,
that is, where missingness is at random, we can obtain valid estimates
for a treatment effect by inverse probability of treatment weighting
(\citeproc{ref-cole2008}{Cole and Hern치n 2008};
\citeproc{ref-leyrat2021}{Leyrat \emph{et al.} 2021}) or by multiple
imputation -- on the assumption that our models are correctly specified
(\citeproc{ref-shiba2021}{Shiba and Kawahara 2021}). However, if
missingness is not completely at random, or if our models are otherwise
misspecified, then causal estimation is compromised
(\citeproc{ref-malinsky2022semiparametric}{Malinsky \emph{et al.} 2022};
\citeproc{ref-tchetgen2017general}{Tchetgen Tchetgen and Wirth 2017}).

Note that Table~\ref{tbl-terminologycensoring} \(\mathcal{G}_5\) closely
resembles a measurement structure we have considered before, in
\textbf{Part 1:} Table~\ref{tbl-terminologymeasurementerror}
\(\mathcal{G}_2\) Replacing the unmeasured effect modifiers
\(\circledotted{F}\) and \(U_{\Delta F}\) in
Table~\ref{tbl-terminologycensoring} \(\mathcal{G}_5\) for
\(\circledotted{U_Y}\) in Table~\ref{tbl-terminologymeasurementerror}
\(\mathcal{G}_2\) reveals that the unmeasured effect modification in the
present setting can be viewed as an example of uncorrelated independent
measurement error when there is a treatment effect (i.e.~off the null.)

In the setting we describe here, there is no distortion for the causal
effect estimate in the analytic sample population as it exists at the
end of study. However the end-of-study analytic sample population is an
undesirable restriction of the target population. We infer that our
results in this setting risk \textbf{weirdness}: (\textbf{w}rongly
\textbf{e}stimated inferences due to \textbf{i}nappropriate
\textbf{r}estriction and \textbf{d}istortion) because there is
\textbf{i}nappropriate \textbf{r}estriction.

\subsubsection{Summary}\label{summary-1}

In this section, we examined how right-censoring, or attrition, can lead
to biased causal effect estimates. Even without confounding bias,
wherever the distribution of variables that modify treatment effects
differs between the analytic sample population at the start and end of
the study, the average treatment effects may differ, leading to biased
estimates for the target population. To address such bias, investigators
must ensure that the distribution of potential outcomes at the end of
the study corresponds with that of the target population. Again, methods
such as inverse probability weighting and multiple imputation can help
mitigate this bias (refer to
\citeproc{ref-bulbulia2024PRACTICAL}{Bulbulia 2024a}).

The take-home message is this: attrition is nearly inevitable, and if
attrition cannot be checked it will make results \textbf{weird}:
(\textbf{w}rongly \textbf{e}stimated inferences due to
\textbf{i}nappropriate \textbf{r}estriction and \textbf{d}istortion).
Refer to supplementary materials \textbf{S3} for a mathematical
explanation of why effects differ when the distribution of effect
modifiers differs. Refer to supplementary materials \textbf{S4} for a
data simulation that makes the same point.

Next, we investigate target population restriction bias at the start of
the study (left-censoring). We shall discover that structural motifs of
measurement error bias reappear.

\newpage{}

\subsection{\texorpdfstring{Part 3: How Target Population Restriction
Bias At The Start of Study Makes Your Causal Inferences weird
(\textbf{w}rongly \textbf{e}stimated inferences due to
\textbf{i}nappropriate \textbf{r}estriction and
\textbf{d}istortion)}{Part 3: How Target Population Restriction Bias At The Start of Study Makes Your Causal Inferences weird (wrongly estimated inferences due to inappropriate restriction and distortion)}}\label{id-sec-3}

Consider target-restriction bias that occurs at the start of a study.
There are several failure modes. For example, the source population from
which participants are recruited might not align with the target
population. Moreover, even where there is such alignment, the
participants recruited into a study - the analytic sample -- might not
align with the source population. For simplicity, we imagine the
analytic sample population at the start of the study accurately aligns
with the source population. What constitutes `alignment'? We say the
sample is unrestrictive of the target population if there are no
differences between the sample and target population in the distribution
both of confounders (common causes of treatment and outcome) and of the
variables that modify treatment effects (effect modifiers). Proof of
alignment cannot be verified with data (refer to supplementary materials
\textbf{S3}).

\subsubsection{Target Population Restriction Bias at Baseline Can Be
Collider-Restriction
Bias}\label{target-population-restriction-bias-at-baseline-can-be-collider-restriction-bias}

\begin{table}

\caption{\label{tbl-terminologyselectionrestrictionclassic}Collider-Stratification
bias at the start of a study (`M-bias')}

\centering{

\terminologyselectionrestrictionclassic

}

\end{table}%

Table~\ref{tbl-terminologyselectionrestrictionclassic} \(\mathcal{G}_1\)
illustrates an example of target population restriction bias at baseline
in which there is collider-restriction bias.

Suppose investigators want to estimate the causal effects of regular
physical activity, \(A\), and heart health, \(Y\), among adults visiting
a network of community health centres for routine check-ups.

Suppose there are two unmeasured variables that affect selection into
the study \(S=1\)

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Health Awareness, \(U1\), an unmeasured variable that influences both
  the probability of participating in the study, \(\boxed{S = 1}\), and
  the probability of being physically active, \(A\). Perhaps people with
  higher health awareness are more likely to (1) engage in physical
  activity and (2) participate in health-related studies.
\item
  Socioeconomic Status (SES), \(U2\), an unmeasured variable that
  influences both the probability of participating in the study,
  \(\boxed{S = 1}\), and heart health, \(Y\). We assume that individuals
  with higher SES have better access to healthcare and are more likely
  to participate in health surveys; they also tend to have better heart
  health from healthy lifestyles: joining expensive gyms, juicing, long
  vacations, and the like.
\end{enumerate}

As presented in Table~\ref{tbl-terminologyselectionrestrictionclassic}
\(\mathcal{G}_1\), there is collider-restriction bias from conditioning
on \(S=1\):

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{\(U1\)}: Because individuals with higher health awareness are
  more likely to be both physically active and participate in the study,
  the subsample over-represents physically active individuals. This
  overestimates the prevalence of physical activity, setting up a bias
  in overstating the potential benefits of physical activity on heart
  health in the general population.
\item
  \textbf{\(U2\)}: Because individuals with higher SES may have better
  heart health from SES-related factors, this opens a confounding path
  from physical activity and heart health through the selected sample,
  setting up the investigators for the potentially erroneous inference
  that physical activity has a greater positive impact on heart health
  than it actually does in the general population. The actual effect of
  physical activity on heart health in the general population might be
  less pronounced than observed.
\end{enumerate}

It might seem that researchers would need to sample from the target
population. However, by adjusting for health awareness or SES or a proxy
for either, researchers may block the open path.
Table~\ref{tbl-terminologyselectionrestrictionclassic} \$ mathcalG\_2\$
presents this solution. This strategy will only provide an unbiased
effect estimate for the population if either there is no causal effect
for all strata of the selected sample (the sharp null hypothesis) or
there are no interactions between the distribution of effect modifiers
in the analytic sample population and the target population.

The next series of examples illustrate challenges to obtaining valid
causal effect estimates in the presence of interactions.

\subsubsection{Target Population Restriction Bias at Baseline Without
Collider-Restriction Bias at
Baseline}\label{target-population-restriction-bias-at-baseline-without-collider-restriction-bias-at-baseline}

\begin{table}

\caption{\label{tbl-terminologyselectionrestrictionbaseline}The
association in the population of selected individuals differs from the
causal association in the target population. Hern치n calls this scenario
`selection bias off the null' (\citeproc{ref-hernuxe1n2017}{Hern치n
2017}). Lu et al.~call this scenario `Type 2 selection bias'
(\citeproc{ref-lu2022}{Lu \emph{et al.} 2022}). We call this bias
`target population restriction bias at baseline'.}

\centering{

\terminologyselectionrestrictionbaseline

}

\end{table}%

\subsubsection{Problem 1: The target population is not WEIRD (Western,
Educated, Industrialised, Rich, and Democratic); the analytic sample
population is
WEIRD}\label{problem-1-the-target-population-is-not-weird-western-educated-industrialised-rich-and-democratic-the-analytic-sample-population-is-weird}

Table~\ref{tbl-terminologyselectionrestrictionbaseline}
\(\mathcal{G}_{1.1}\) presents a scenario with target population
restriction bias at baseline. When the sample obtained at baseline
differs from the target population in the distributions of variables
that modify treatment effects, effect estimates may be biased, even
without confounding bias. Results may be \textbf{weird} without arising
from confounding bias.

Suppose we are interested in the effects of political campaigning but
only sample from our preferred political party. Results for the general
population will be distorted if the distribution of effect modifiers of
the treatment varies by party. One such effect modifier might be `party
affiliation'. This valid concern underscores the call for broader
sampling in the human sciences. WEIRD samples will not be informative
for science generally whenever the distribution of effect modifiers
among humans differs from those of the restricted population of humans
from which WEIRD analytic samples are drawn.

Note that we have encountered
Table~\ref{tbl-terminologyselectionrestrictionbaseline}
\(\mathcal{G}_{1.1}\) \emph{twice} before. It is the same causal
directed acyclic graph as we found in
Table~\ref{tbl-terminologycensoring} \(\mathcal{G}_5\). As we did
before, we may replace the unmeasured effect modifiers
\(\circledotted{F}\) and \(U_{\Delta F}\) for \(\circledotted{U_Y}\) in
Table~\ref{tbl-terminologymeasurementerror} \(\mathcal{G}_2\) and
observe that we recover uncorrelated measurement error off the null
(i.e.~when there is a true treatment effect).

The structural similarity suggests options might be easily overlooked.
Where the distributions of treatment-effect modifiers are known and
measured and where census (or other) weights are available for the
distributions of effect modifiers in the target population, it may be
possible to weight the sample to more closely approximate the target
population parameters of interest (\citeproc{ref-stuart2015}{Stuart
\emph{et al.} 2015}).

Let \(\widehat{ATE}_{target}\) denote the population average treatment
effect for the target population. Let
\(\widehat{ATE}_{\text{restricted}}\) denote the average treatment
effect at the end of treatment. Let \(W\) denote a set of variables upon
which the restricted and target populations structurally differ. We say
that results \emph{generalise} if we can ensure that:

\[
\widehat{ATE}_{target} = \widehat{ATE}_{restricted}
\]

or if there is a known function such that:

\[
ATE_{target} \approx f_W(ATE_{\text{restricted}}, W)
\]

In most cases, \(f_W\) will be unknown, as it must account for potential
heterogeneity of effects and unobserved sources of bias. For further
discussion on this topic, see Imai \emph{et al.}
(\citeproc{ref-imai2008misunderstandings}{2008}); Cole and Stuart
(\citeproc{ref-cole2010generalizing}{2010}); Stuart \emph{et al.}
(\citeproc{ref-stuart2018generalizability}{2018}).

Table~\ref{tbl-terminologyselectionrestrictionbaseline}
\(\mathcal{G}_{1.2}\) provides a graphical representation of the
solution.

Importantly, if there is considerable heterogeneity across humans,
\textbf{then we might not know how to interpret the average treatment
effect for the target population of all humans even if this causal
effect can be estimated.} In comparative research, we are often
precisely interested in treatment heterogeneity. If we seek explicitly
comparative models, however, we will need to ensure the validity of
estimates for every sample that we compare. If one stratum in the
comparative study is \textbf{weird}: (\textbf{w}rongly
\textbf{e}stimated inferences due to \textbf{i}nappropriate
\textbf{r}estriction and \textbf{d}istortion), errors will propagate to
the remainder of the comparative study. To understand such propogation
consider scenarios where the target population is a deliberate
restriction of the the source population from which the analytic sample
at baseline is drawn. We deliberately seek restriction wherever
`eligibility criteria' are desirable for a study. Although this point is
perhaps obvious, it is less clear whether many studies should be
conducted without eligibility criteria.

\subsubsection{Example 2: The target population is a sub-sample of WEIRD
(Western, Educated, Industrialised, Rich, and Democratic); the analytic
sample population is not WEIRD
enough.}\label{example-2-the-target-population-is-a-sub-sample-of-weird-western-educated-industrialised-rich-and-democratic-the-analytic-sample-population-is-not-weird-enough.}

Table~\ref{tbl-terminologyselectionrestrictionbaseline}
\(\mathcal{G}_{2.1}\) presents a scenario where the source population
does not meet eligibility criteria. Consider again the question of
whether vasectomy affects a sense of meaning and purpose in life.
Suppose further we want to evaluate effects in New Zealand among men
over the age of 40 who have no prior history of vasectomy, and who are
in relationships with heterosexual partners. The target population is a
stratum of WEIRD population (Western, Educated, Industrialised, Rich,
and Democratic). That is, the WIERD population would be too broad for
scientific interest. We should not sample from young children, the
elderly, or any who do not qualify. Not only is it clear that a narrow
population is desirable for many scientific questions, but also it is
easy to imagine settings in comparative human science for which a fully
unrestricted human population would be undesirable. In causal inference,
we attempt to emulate ideal (although typically implausible) experiments
with `real world' data. Just as eligibility criteria are often useful
for isolating populations of interest in experimental designs, so too
are eligibility criteria often useful for isolating populations of
interest in real-world `target trials'
(\citeproc{ref-hernuxe1n2008a}{Hern치n \emph{et al.} 2008}).

Note again Table~\ref{tbl-terminologyselectionrestrictionbaseline}
\(\mathcal{G}_{2.1}\) is identical to
Table~\ref{tbl-terminologycensoring} \(\mathcal{G}_5\) ---
right-censoring bias with effect modifiers in an otherwise unconfounded
study. The structure is also similar to
Table~\ref{tbl-terminologymeasurementerror} \(\mathcal{G}_2\) the
problem is structurally that of uncorrelated measurement error `off the
null'. Where it is the defusion of the effect-modifiers that causes we
may fix the measurement error by restricting the sample.

Table~\ref{tbl-terminologyselectionrestrictionbaseline}
\(\mathcal{G}_{2.2}\) presents a solution. Ensure eligibility criteria
are scientifically relevant and feasible. Sample from this eligible
population. With caution, apply survey or other weights where these
weights enable a closer approximation to the distributions of
effect-modifiers in the target population. Here, we avoid
\textbf{weird}: (\textbf{w}rongly \textbf{e}stimated inferences due to
\textbf{i}nappropriate \textbf{r}estriction and \textbf{d}istortion) by
imposing greater restriction on what had been an inappropriately
unrestricted target population.

\subsubsection{Example 3: Correlated Measurement Error of Covariates and
Outcome in the Absence of a Treatment
Effect}\label{example-3-correlated-measurement-error-of-covariates-and-outcome-in-the-absence-of-a-treatment-effect}

Table~\ref{tbl-terminologyselectionrestrictionbaseline}
\(\mathcal{G}_{3.1}\) considers the threats to external validity from
correlated measurement errors in the target population arising from
structured errors across heterogeneous strata. For simplicity imagine
the groups with structured errors are cultures. Even if the treatment is
measured without error, multiple sources of error may lead to
association without causation.

Suppose we plan a cross-cultural investigation to clarify the
relationship between interventions on religious service attendance,
\(A\), and charitable giving, \(Y\). We plan to obtain measures of
covariates \(L\) sufficient to control for confounding. Suppose we
observe religious attendance so that it is not measured with error (as
did \citeproc{ref-shaver2021comparison}{Shaver \emph{et al.} 2021}), yet
there is heterogeneity in the measurement of covariates \(L\) and the
outcome \(Y\). For example, if charitable giving measures are included
among the baseline covariates in \(L\), measurement errors at baseline
will be correlated with outcome measures. Perhaps in certain cultures,
charitable giving is under-reported (perhaps charity is associated with
the vice of gullibility), while in others, it is over-reported (only the
charitable are hired and promoted). Suppose further that true covariates
affect the treatment and outcome. As shown in
Table~\ref{tbl-terminologyselectionrestrictionbaseline}
\(\mathcal{G}_{3.1}\), in this setting, multiple paths of confounding
bias are open.

Moreover, because measurements are causally related to the phenomena we
record, we cannot apply statistical tests to verify whether measures are
recorded with error (\citeproc{ref-vanderweele2022}{VanderWeele 2022};
\citeproc{ref-vansteelandt2022}{Vansteelandt and Dukes 2022b}). Whether
the phenomena that we hope to measure are functionally equivalent across
cultural settings remains unknown, and can generally only be discovered
slowly, through patient, careful work with local experts. Although big
cross-cultural projects are preferred in certain science journals,
including multiple cultures in a single analysis imposes considerable
burdens on investigators. All sources of error must be evaluated -- and
errors from one culture can poison the wells in the analysis of others.

Table~\ref{tbl-terminologyselectionrestrictionbaseline}
\(\mathcal{G}_{3.2}\) provides a sensible solution: restrict one's study
to those cultures where causality can be identified. Democritus wrote,
`I would rather discover one cause than gain the kingdom of Persia'
(\citeproc{ref-freeman1948ancilla}{Freeman 1948}). Paraphrasing
Democritus, we might say, `I should rather discover one WEIRD cause than
the kingdom of \textbf{weird} comparative research.'

\subsubsection{Example 4: Correlated Measurement Error of Effect
Modifiers for an Overly Ambitious Target
Population}\label{example-4-correlated-measurement-error-of-effect-modifiers-for-an-overly-ambitious-target-population}

Table~\ref{tbl-terminologyselectionrestrictionbaseline}
\(\mathcal{G}_{4.1}\) considers the threats to target validity from
correlated measurement errors in the target population arising from
structured errors linking measurements for the effect modifiers. Here,
we discover a familiar structural bias of correlated measurement error
bias Table~\ref{tbl-terminologymeasurementerror} \(\mathcal{G}_3\)

Even if the treatment is randomised so that there are no open backdoor
paths, and even if the treatment and outcome are measured without error,
we will not be able to obtain valid estimates for treatment-effect
heterogeneity from their data, nor will we be able to apply
target-sample weights (such as census weights) to obtain valid estimates
for the populations in which the measurement errors of effect modifiers
are manifest.

Table~\ref{tbl-terminologyselectionrestrictionbaseline}
\(\mathcal{G}_{4.2}\) suggests that where measures of effect
modification are uncertain, it is best to consider settings in which the
measurements are reliable --- whether or not the settings are WEIRD
(Western, Educated, Industrialised, Rich, and Democratic). Moreover, in
comparative settings where multiple cultures are measured, unless each
is proven innocent of structural measurement error bias, it is generally
best to report the results for each culture separately, without
attempting comparisons.

\subsection{Part 4 Measurement Error Bias Understood Through Single
World Intervention Graphs}\label{id-sec-4}

Thus far, we have repeatedly observed that all biases in causal
inference relate to confounding. In \hyperref[id-sec-1]{Part 1}, we
examined undirected/uncorrelated measurement error bias and found that
measurement bias can arise `off the null' without any confounding
(Table~\ref{tbl-terminologymeasurementerror} \(\mathcal{G}_2\)). In
\hyperref[id-sec-2]{Part 2}, we examined population-restriction bias at
the end of a study, finding it to be a variety of undirected
uncorrelated measurement error bias
(Table~\ref{tbl-terminologycensoring} \(\mathcal{G}_5\)). In
\hyperref[id-sec-3]{Part 3}, we examined population-restriction bias at
baseline; of the five biases considered, only one could be classified as
confounding bias.

Throughout this article, we encountered challenges in using causal
direct acyclic graphs to represent biases that arise from effect
modification. The blue arrows that we use to convey this bias in causal
dags might make it appear that bias occurs through action at a distance.
That causal directed acyclic graphs are limited in representing such
biases should come as no surprise because causal DAGs are designed to
clarify confounding bias and not other biases
(\citeproc{ref-hernan2024WHATIF}{Hernan and Robins 2024};
\citeproc{ref-pearl2009a}{Pearl 2009}).

To enrich our understanding of bias from measurement error bias and
target population restriction -- certain forms of which occur without
confounding bias -- we turn to Single World Intervention Graphs (SWIGs).
SWIGs are causal diagrams that allow us to read counterfactual
dependencies directly off a graph
(\citeproc{ref-richardson2013}{Richardson and Robins 2013a}). Similar to
causal DAGs, Single World Intervention Graphs are not purpose-built to
evaluate measurement error and restriction biases: they function to
factorise conditional probability distributions from assumed causal
structures so that investigators may evaluate identifiability conditions
-- or `no unmeasured confounding'. However, because SWIGs encode
assumptions about the relationships of treatments to the counterfactual
outcomes that arise after an intervention is made, SWIGs may help us to
better understand the causal mechanisms at work when there are
measurement error biases. We can demonstrate that there is no `action at
distance' in measurement error biases. Furthermore, because target
population restriction biases can be approached as measurement error
biases, our results extend to target restriction biases as well.

SWIGs operate by `node-splitting' at each intervention
(\citeproc{ref-bulbulia2024swigstime}{Bulbulia 2024c};
\citeproc{ref-richardson2013}{Richardson and Robins 2013a}), dividing
the intervention into a random component and a fixed component. Nodes
that follow a fixed intervention are relabeled with the value of the
intervention depicted in a SWIG. Importantly only one intervention is
represented in any given SWIG (we never observe the joint distribution
of more than one intervention at a time). Single World Intervention
Templates are functions that we may use to generate multiple Single
World Intervention Graphs
(\citeproc{ref-richardson2013swigsprimer}{Richardson and Robins 2013b}).
Whether we imagine a single-point or sequential series of interventions,
one reads a SWIG just as one would read a causal DAG, ensuring there are
no backdoor paths linking the random part of the node to the outcome.
The deterministic part of a node is fixed, preventing confounding in the
counterfactual future from the fixed portion of the node unless an open
backdoor path arises before a subsequent intervention that links the
subsequent intervention to the outcome in the absence of a causal
effect. Again, although SWIGs, like causal DAGs, are built to evaluate
identification (no unmeasured confounding) by factorising the
conditional and marginal distributions associated with a graph, the
explicit representation of counterfactual states makes it easier to
understand how bias arises in the absence of confounding, without
supposing action at distance.

\begin{table}

\caption{\label{tbl-tblme}Uncorrelated/Undirected Measurement Error in
Single-World Intervention Graph. There is no `action at a distance': all
measurement errors have causes; errors entering reporters of the
treatment and outcome clarify that the treatment reporter induces
collider bias, and the outcome reporter induces effect modification
during estimation.}

\centering{

\tblme

}

\end{table}%

\subsubsection{Measurement Error in the Treatment Biases Causal
Contrasts Because the Treatment Reporter is a Post-Treatment
Collider}\label{measurement-error-in-the-treatment-biases-causal-contrasts-because-the-treatment-reporter-is-a-post-treatment-collider}

Table~\ref{tbl-tblme}\(\mathcal{G}_{1.1}\) presents a Single World
Intervention Template from which we may generate two counterfactual
states of the world under two distinct interventions
\(A = \tilde{a} \in \{0,1\}\). We call the measurement of the
intervention a `reporter of A' and denote the state of the reporter
under \(A = \tilde{a}\) as \(B(\tilde{a})\). In our convention, if a
node in a Single World Intervention Graph (or Template) is unobserved,
we shade it in grey. \(E^A\) denotes an unmeasured variable or set of
variables that cause the reporter \(B(\tilde{a})\) to differ from
\(\tilde{a}\), the fixed state of the intervention when \(A\) is set to
\(\tilde{a}\). In template Table~\ref{tbl-tblme}\(\mathcal{G}_{1.1}\),
\(A = \tilde{a}\) remains unobserved. The only observed nodes are
\(B(\tilde{a})\) and \(Y(B(\tilde{a}))\), which is the potential outcome
for \(Y\) \emph{as reported} by \(B(\tilde{a})\). Note that here we
include reporters of the unobserved true state of the treatment directly
in our representation of the causal order as encoded in our Single World
Intervention Graph. Table~\ref{tbl-tblme}\(\mathcal{G}_{1.2}\)
corresponds to the assumed state of the world when the reporter of \(A\)
is set to \(B(0)\). Table~\ref{tbl-tblme}\(\mathcal{G}_{1.3}\)
corresponds to the assumed state of the world when the reporter of \(A\)
is set to \(B(1)\); in this world, investigators observe \(Y(B(1))\). We
assume that \(E^A\) is independent of both \(A\) and \(Y(\tilde{a})\).
However, we assume that \(E^A\) causes \(B(\tilde{a})\) to differ from
the true state \(A=\tilde{a}\). As a result of this misclassification,
we have no assurance whether
\(\mathbb{E}[Y(B(1)) - Y(B(0))] = \mathbb{E}[Y(1) - Y(0)]\). The SWIGs
make it apparent that although \(A\) is independent of \(E^A\),
\(A = \tilde{a}\) and \(E^A\) become statistically entangled in the
reporter \(B(\tilde{a})\), and it is this reporter, not the unobserved
true state of \(Y(\tilde{a})\), that investigators record.

Consider the following example. Coach Alice randomly assigns one of two
running programs to club runners: \(A = 1\) (train), \(A = 0\) (do not
train). Alice is not interested in estimating the effect of random
treatment assignment (the intent-to-treat effect). Rather, she wants to
understand the causal effect of training compared to rest---a
per-protocol effect. Unknown to Alice, 20\% do not follow the program.
Table~\ref{tbl-tblme}\(\mathcal{G}{1.1}\) is a SWIG template that
presents bias from measurement error in the treatment. The template
serves as a `graph value function' that generates SWIGs
Table~\ref{tbl-tblme}\(\mathcal{G}{1.1}\), in which all runners receive
\(A = 0\) (do not train), and
Table~\ref{tbl-tblme}\(\mathcal{G}_{1.2}\), in which all runners receive
\(A = 1\) (train). Here, \(B(0)\) and \(B(1)\) denote the reporters of
the level of the intervention.

Again, we note that the treatment recorded is not the per-protocol
effect \(\mathbb{E}[Y(1) - Y(0)]\) but rather the intention-to-treat
effect \(\mathbb{E}[Y(B(1)) - Y(B(0))]\). Generally, the effect we
obtain will understate the per-protocol effect of training both on the
difference scale and the risk ratio scale. Those who were assigned to
training but rest will dilute the effect of training:
\(\mathbb{E}[Y(1)] > \mathbb{E}[Y(B(1))]\). Those who were assigned to
rest but train will augment the rest effect:
\(\mathbb{E}[Y(B(1))] > \mathbb{E}[Y(1)]\). Hence:

\[
 \mathbb{E}[Y(1) - Y(0)] > \mathbb{E}[Y(B(1)) - Y(B(0))]
\]

Note that attenuation of a true treatment effect is not guaranteed
(\citeproc{ref-jurek2008brief}{Jurek \emph{et al.} 2008};
\citeproc{ref-lash2020}{Lash \emph{et al.} 2020}). The SWIGs in
Table~\ref{tbl-tblme}\(\mathcal{G}_{1.1-1.3}\) make the general
measurement bias problem clear: although the treatment that is estimated
remains \(d\)-separated from the potential outcomes, the causal contrast
that we obtain at the end of the study is not the treatment we seek and
will often (though not always) diminish a true treatment effect because
the reporter \emph{under treatment} is a common effect of the unmeasured
source of bias and the treatment that has been applied, and it is the
outcomes under mismeasured treatments that investigators contrast.

\subsubsection{Measurement Error in the Outcome Biases Causal Contrasts
Because The Unmeasured Error of the Outcome is an Effect Modifier of the
Outcome
Reporter}\label{measurement-error-in-the-outcome-biases-causal-contrasts-because-the-unmeasured-error-of-the-outcome-is-an-effect-modifier-of-the-outcome-reporter}

Table~\ref{tbl-tblme}\(\mathcal{G}_{2.1}\) presents a Single World
Intervention Template from which we may generate two counterfactual
states of the world under two distinct interventions
\(A = \tilde{a} \in \{0,1\}\). Here, the treatment is observed and
recorded without error. Hence we do not include a reporter of the
treatment. However, the true outcome is not observed, but only reported
with error. \(E^Y\) denotes the unmeasured source of error in the
reporting of \(Y(\tilde{a})\), which we assume to be independent of
\(A\) and of \(Y\). We shade these nodes in grey because both \(E^Y\)
and \(Y(\tilde{a})\) are not observed. The node \(V(Y(\tilde{a}))\)
denotes the observed state of \(Y\) when \(A = \tilde{a}\). In template
Table~\ref{tbl-tblme}\(\mathcal{G}_{2.1}\), \(A = \tilde{a}\) remains
unobserved. Table~\ref{tbl-tblme}\(\mathcal{G}_{2.2}\) corresponds to
the assumed state of the world when \(A=0\) and \(Y(0)\) is reported
with error as \(V(Y(0))\). Likewise,
Table~\ref{tbl-tblme}\(\mathcal{G}_{2.3}\) corresponds to the assumed
state of the world when \(A=1\) and \(Y(1)\) is reported with error as
\(V(Y(Y(1))\). We assume that \(E^Y\) is independent of \(A\) and of
\(Y\). Misclassification will tend to increase the variance of the
estimated treatment effect. If the outcome is continuous, the expected
difference in the mean of the outcome for the reported outcome may
differ from that for the true outcome. How bias affects the outcome will
vary depending on the scale we use to evaluate such bias.

Suppose that under training, the athlete runs a marathon in 3 hours, and
under rest, they run a marathon in 4 hours. To keep figures easy, we
will use round numbers. Suppose the bias in reporting is 1 hour. Thus,
we have \(\mathbb{E}[Y(1)] = 3\), \(\mathbb{E}[Y(0)] = 4\);
\(\mathbb{E}[V(Y(1))] = 2\) and \(\mathbb{E}[V(Y(0))] = 3\).

\[
\text{ATE Difference Scale: no measurement error} = \mathbb{E}[Y(1)] - \mathbb{E}[Y(0)] = 3 - 4 = -1
\]

\[
\text{ATE Difference Scale: measurement error} = \mathbb{E}[V(Y(1))] - \mathbb{E}[V(Y(0))] = 2 - 3 = -1
\]

The effect estimates do not differ:

\[
\text{ATE no measurement error} = \text{ATE measurement error}
\]

However, consider this bias on the risk ratio scale:

\[
\text{ATE Risk Ratio Scale: no measurement error} = \mathbb{E}[Y(1)] / \mathbb{E}[Y(0)] = 3 / 4 = 0.75
\]

\[
\text{ATE Risk Ratio Scale: measurement error} = \mathbb{E}[V(Y(1))] / \mathbb{E}[V(Y(0))] = 2 / 3 = 0.66
\]

These effect estimates differ:

\[\text{ATE RR no measurement error} \neq \text{ATE RR measurement error}\]

Imagine the bias was positive, such that runners added an hour to their
times---perhaps the runners do not want to stand out. The true risk
ratio for the treatment remains \(0.75\). However, the biased risk ratio
for the treatment would become:

\[
\text{ATE Risk Ratio Scale: measurement error} = \mathbb{E}[V(Y(1))] / \mathbb{E}[V(Y(0))] = 4 / 5 = 0.8
\]

Here we would understate the true treatment effect. The Single World
Intervention Graphs Table~\ref{tbl-tblme}\(\mathcal{G}_{2.1-2.3}\) make
clear the reason for the scale sensitivity of the bias. Although the
source of bias in the outcome (\(E^Y\)) is independent of the treatment
(\(A\)), \(E^Y\) functions as an effect modifier for the reported
outcome \(V(Y(\tilde{a}))\).

Consider: it has long been understood that where treatment effects vary
across different population strata, an estimate of the causal effect on
the risk difference scale will differ from the estimate on the risk
ratio scale (\citeproc{ref-greenland2003quantifying}{Greenland 2003}).
Here, we find that reporters of the outcome are subject to similar
relativity. For example, we might have constructed a multiplicative
error function for the outcome such that we subtract 1 hour if the
response is 3 and subtract 1.344 if the response is 4. Under this error
function, the risk ratio would remain stable at 0.75 irrespective of
whether the outcome was measured with error; however, the risk
difference would no longer be constant.

Note that we have encountered SWIGs
Table~\ref{tbl-tblme}\(\mathcal{G}_{1.2-1.3}\) and
Table~\ref{tbl-tblme}\(\mathcal{G}_{2.2-2.3}\) before: they are
structurally equivalent to the causal directed acyclic graph in
\hyperref[id-sec-1]{Part 1} Table~\ref{tbl-terminologymeasurementerror}
\(\mathcal{G}_2\), in which we considered uncorrelated independent
measurement error. Moreover, we
Table~\ref{tbl-tblme}\(\mathcal{G}_{2.2-2.3}\) is equivalent to
\hyperref[id-sec-2]{Part 2} Table~\ref{tbl-terminologycensoring}
\(\mathcal{G}_5\), in which we considered target-restriction bias
without confounding; and \hyperref[id-sec-3]{Part 3}
Table~\ref{tbl-terminologyselectionrestrictionclassic}
\(\mathcal{G}_1\), \(\mathcal{G}_2\), \(\mathcal{G}_4\) in which we
considered target population restriction biases in the analytic sample
at the start of the study. Single World Intervention Graphs are useful
in providing a more detailed representation of causality in which the
biases that give rise to biased causal effect estimates when there is
measurement error bias `off the null' -- such as when restricted
representation of the target population by the analytic sample at the
start or end invalidates the causal inferences we seek for the target
population.

\subsubsection{Measurement Error in The Treatment Or Outcome Will Not
Modify a Strictly `Null'
Effect}\label{measurement-error-in-the-treatment-or-outcome-will-not-modify-a-strictly-null-effect}

As shown in Table~\ref{tbl-tblme}\(\mathcal{G}_{1.4}\) and
Table~\ref{tbl-tblme}\(\mathcal{G}_{2.4}\), if we assume randomisation
into treatments, or equivalently if we assume no unmeasured confounding
conditional on perfectly measured covariates, we will no expect a
biasing path leading to an association between treatment and outcome.
However, a strict `null' effect cannot be assumed. Note that we do not
use `null' here in the sense of null hypothesis significance testing,
where there is no such thing as a `null' effect. Supplementary materials
\textbf{S5} uses Single World Intervention Graphs to describe correlated
and directed measurement error and consider how bias correction may be
interpreted mechanistically as interventions on reporters.

\subsubsection{Summary Part 4}\label{summary-part-4}

We have characterised target-population restriction bias, whether at the
start or the end of the study, as formally equivalent to undirected
uncorrelated measurement error. Here, Single World Intervention Graphs
(SWIGs) allow us to apply lessons from the study of effect modification
to the analysis of measurement error biases. SWIGs, as shown in
\(\mathcal{G}_{2.1-2.3}\), greatly clarify how measurement error bias
for the outcome arises in the absence of confounding bias: the
unmeasured causes of error function as effect modifiers of the outcome
reporters, such that causal contrasts will differ on at least one scale
of effect `off the null'. The mathematical explanation in supplementary
materials \textbf{S3} for threats to external validity from right
censoring applies equally to threats from left censoring, as does the
simulation in supplementary materials \textbf{S4}.

Note that all of the biases we have considered cannot be evaluated by
statistical tests. For example, even if researchers were to obtain
satisfactory test statistics for metric, configural, and scalar
equivalence, we would be unable to diagnose target population
restriction biases with these tests. Nor would we be able to diagnose
other forms of measurement error biases using statistical tests. Rather,
we can only evaluate evidence for bias by first representing the causal
structures we \textbf{assume} hold in the world, and investigating the
implications of each assumption one by one. Likewise, we cannot take the
invalidation of standard statistical tests as evidence that similar
causal effects underpin sample responses to the interventions of
interest. Assumptions alone do not clarify the causal realities that
give rise to them.

\subsection{Conclusions}\label{conclusions}

In causal inference, we begin by specifying clearly defined treatments
and outcomes, stating the contrasts to be made for interventions at
specific levels of treatment, and identifying a target population for
whom results generalise. Although causal inference is gaining
popularity, much work remains in considering its relevance to evaluating
common threats posed by measurement error bias and target
population-restriction bias to causal inferences. These threats are
particularly evident in the comparative human sciences, where both
incongruent measurements and incongruent human realities may imperil
causal inferences. Moreover, we have considered how biases arising from
target-population restriction at the beginning and end of a study may be
approached as varieties of measurement-error bias. Furthermore, using
Single World Intervention Graphs (SWIGs), we have clarified that
structural features of uncorrelated and undirected measurement error
bias arise either because treatment reporters are the common effects of
unmeasured treatments and (typically) unmeasured sources of treatment
measurement error, or because there is effect modification of the
unmeasured sources of measurement error on reporters of outcomes.
Lacking structural assumptions, statistical tests will not detect these
biases. However, it may be possible to correct for these biases, again
under structural assumptions that we do not obtain from the data alone,
see supplementary materials \textbf{S2} and \textbf{S5}.

Nothing I have said here should detract from the importance of seeking
species-level knowledge. We should seek such knowledge. Science should
seek generalisations where it can because generalisation is knowledge.
In my view, there are also ethical reasons -- a great many populations
remain understudied. However, before investigators venture into the vast
wildernesses of human existence, locally-understood gardens must be
cultivated. A long shadow of measurement error bias casts its shade over
nearly every aspect of the human condition that scientists hope to
understand. However, the standard workflow for causal inference offers
guidance for research design, whether or not the causal questions are
comparative because causality itself is inescapably general.

\subsubsection{Schematic Workflow For Avoiding `weird' Causal
Inferences}\label{schematic-workflow-for-avoiding-weird-causal-inferences}

We avoid \textbf{weird} (\textbf{w}rongly \textbf{e}stimated inferences
due to \textbf{i}nappropriate \textbf{r}estriction and
\textbf{d}istortion) inferences in comparative research in the same way
that we avoid \textbf{weird} inferences in any research by undertaking
the following steps:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{State a well-defined intervention.}
\end{enumerate}

Clearly define the treatment or exposure to be evaluated. Of which
events do we hope to infer consequences? Which levels of intervention
will be compared?

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  \textbf{State a well-defined outcome.}
\end{enumerate}

Clearly define the outcome to be evaluated. Which consequences are of
interest? Which comparisons will be made? At which time scale are we
interested? At which scale of causal contrast are we interested?

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  \textbf{Clarify the target population.}
\end{enumerate}

Define the population to whom the results will generalise, understanding
that causal contrasts may differ for different populations, even in the
absence of confounding or measurement error biases.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{3}
\tightlist
\item
  \textbf{Ensure treatments to be compared satisfy causal consistency.}
\end{enumerate}

Verify that the treatments correspond to interpretable interventions
(\citeproc{ref-hernan2024WHATIF}{Hernan and Robins 2024})

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{4}
\tightlist
\item
  \textbf{Evaluate whether treatment groups, conditional on measured
  covariates, are exchangeable.}
\end{enumerate}

Balancing confounding covariates across treatment levels ensures that
differences between groups are conditionally `ignorable', or
equivalently, are conditionally exchangeable, or equivalently, that all
backdoor paths have been closed.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{5}
\tightlist
\item
  \textbf{Check if the positivity assumption is satisfied.}
\end{enumerate}

Confirm that all individuals have a non-zero probability of receiving
each treatment level, given their covariates.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{6}
\tightlist
\item
  \textbf{Ensure that the measures relate to the scientific questions at
  hand.}
\end{enumerate}

Ensure that the data collected and the measures used directly relate to
the research question. As part of this, evaluate structural features of
measurement error bias. As we have considered, there are manifold
possibilities for measurement error bias to obscure the phenomena under
study, and these biases may interact with target-population-restriction
biases.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{7}
\tightlist
\item
  \textbf{Consider strategies to ensure the study group measured at the
  end of the study represents the target population.}
\end{enumerate}

If the study population at both the beginning and end of treatment
differs in the distribution of variables that modify the effect of a
treatment on the outcome, the study will be biased in at least one
measure of effect.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{8}
\tightlist
\item
  \textbf{Clearly communicate the reasoning, evidence, and
  decision-making that inform steps 1-8.}
\end{enumerate}

Provide transparent and thorough documentation of how steps 1-8 have
been made. This includes detailing the assumptions, methods, and
decisions. Prepare to conduct and report multiple analyses where
analystic decisions are ambiguous.

We have seen that the demands of following this workflow in comparative
research are more stringent because measurement error bias must be
evaluated at every site to be compared. Heterogeneity in measurement
error can open biasing paths, and the target populations may not be
easily defined, sampled, or---where the scientific question requires---
appropriately restricted. Methodologists broadly agree on these points,
but they can easily forget them. We have shown how workflows for causal
inference act as essential preflight checklists for ambitious,
effective, and safe comparative cultural research. These workflows help
propel science forward without overreaching.

\newpage{}

\subsection{Funding}\label{funding}

This work is supported by a grant from the Templeton Religion Trust
(TRT0418) and RSNZ Marsden 3721245, 20-UOA-123; RSNZ 19-UOO-090. I also
received support from the Max Planck Institute for the Science of Human
History. The Funders had no role in preparing the manuscript or deciding
to publish it.

\subsection{Acknowledgements}\label{acknowledgements}

I am grateful to Charles Efferson and Ruth Mace, for their constructive
feedback.

Any remaining errors are my own.

\newpage{}

\subsection{References}\label{references}

\phantomsection\label{refs}
\begin{CSLReferences}{1}{0}
\bibitem[\citeproctext]{ref-arnett2008neglected}
Arnett, JJ (2008) The neglected 95\%: Why american psychology needs to
become less american. \emph{American Psychologist}, \textbf{63}(7),
602--614.
doi:\href{https://doi.org/10.1037/0003-066X.63.7.602}{10.1037/0003-066X.63.7.602}.

\bibitem[\citeproctext]{ref-barrett2021}
Barrett, M (2021) \emph{Ggdag: Analyze and create elegant directed
acyclic graphs}. Retrieved from
\url{https://CRAN.R-project.org/package=ggdag}

\bibitem[\citeproctext]{ref-bulbulia2024PRACTICAL}
Bulbulia, J (2024a) A practical guide to causal inference in three-wave
panel studies. \emph{PsyArXiv Preprints}.
doi:\href{https://doi.org/10.31234/osf.io/uyg3d}{10.31234/osf.io/uyg3d}.

\bibitem[\citeproctext]{ref-bulbulia2022}
Bulbulia, JA (2022) A workflow for causal inference in cross-cultural
psychology. \emph{Religion, Brain \& Behavior}, \textbf{0}(0), 1--16.
doi:\href{https://doi.org/10.1080/2153599X.2022.2070245}{10.1080/2153599X.2022.2070245}.

\bibitem[\citeproctext]{ref-bulbulia2023a}
Bulbulia, JA, Afzali, MU, Yogeeswaran, K, and Sibley, CG (2023)
Long-term causal effects of far-right terrorism in {N}ew {Z}ealand.
\emph{PNAS Nexus}, \textbf{2}(8), pgad242.

\bibitem[\citeproctext]{ref-bulbulia2023}
Bulbulia, JosephA (2024b) Methods in causal inference part 1: Causal
diagrams and confounding. \emph{Evolutionary Human Sciences},
\textbf{6}.

\bibitem[\citeproctext]{ref-bulbulia2024swigstime}
Bulbulia, JosephA (2024c) Methods in causal inference part 2:
Interaction, mediation, and time-varying treatments. \emph{Evolutionary
Human Sciences}, \textbf{6}.

\bibitem[\citeproctext]{ref-chatton2020}
Chatton, A, Le Borgne, F, Leyrat, C, \ldots{} Foucher, Y (2020)
G-computation, propensity score-based methods, and targeted maximum
likelihood estimator for causal inference with different covariates
sets: a comparative simulation study. \emph{Scientific Reports},
\textbf{10}(1), 9219.
doi:\href{https://doi.org/10.1038/s41598-020-65917-x}{10.1038/s41598-020-65917-x}.

\bibitem[\citeproctext]{ref-cole2008}
Cole, SR, and Hern치n, MA (2008) Constructing inverse probability weights
for marginal structural models. \emph{American Journal of Epidemiology},
\textbf{168}(6), 656--664.

\bibitem[\citeproctext]{ref-cole2010generalizing}
Cole, SR, and Stuart, EA (2010) Generalizing evidence from randomized
clinical trials to target populations: The ACTG 320 trial.
\emph{American Journal of Epidemiology}, \textbf{172}(1), 107--115.

\bibitem[\citeproctext]{ref-dahabreh2021study}
Dahabreh, IJ, Haneuse, SJA, Robins, JM, \ldots{} Hern치n, MA (2021) Study
designs for extending causal inferences from a randomized trial to a
target population. \emph{American Journal of Epidemiology},
\textbf{190}(8), 1632--1642.

\bibitem[\citeproctext]{ref-deffner2022}
Deffner, D, Rohrer, JM, and McElreath, R (2022) A Causal Framework for
Cross-Cultural Generalizability. \emph{Advances in Methods and Practices
in Psychological Science}, \textbf{5}(3), 25152459221106366.
doi:\href{https://doi.org/10.1177/25152459221106366}{10.1177/25152459221106366}.

\bibitem[\citeproctext]{ref-freeman1948ancilla}
Freeman, K (1948) \emph{Ancilla to the pre-socratic philosophers},
Reprint edition, Cambridge, MA: Harvard University Press.

\bibitem[\citeproctext]{ref-gachter2010}
Gaechter, S (2010) (Dis)advantages of student subjects: What is your
research question? \emph{Behavioral and Brain Sciences},
\textbf{33}(2-3), 92--93.
doi:\href{https://doi.org/10.1017/S0140525X10000099}{10.1017/S0140525X10000099}.

\bibitem[\citeproctext]{ref-greenland2003quantifying}
Greenland, S (2003) Quantifying biases in causal models: Classical
confounding vs collider-stratification bias. \emph{Epidemiology},
300--306.

\bibitem[\citeproctext]{ref-greenland2009commentary}
Greenland, S (2009) Commentary: Interactions in epidemiology: Relevance,
identification, and estimation. \emph{Epidemiology}, \textbf{20}(1),
14--17.

\bibitem[\citeproctext]{ref-henrich2010weirdest}
Henrich, J, Heine, SJ, and Norenzayan, A (2010) The weirdest people in
the world? \emph{Behavioral and Brain Sciences}, \textbf{33}(2-3),
61--83.

\bibitem[\citeproctext]{ref-hernan2024WHATIF}
Hernan, MA, and Robins, JM (2024) \emph{Causal inference: What if?},
Taylor \& Francis. Retrieved from
\url{https://www.hsph.harvard.edu/miguel-hernan/causal-inference-book/}

\bibitem[\citeproctext]{ref-hernuxe1n2004}
Hern치n, MA (2004) A definition of causal effect for epidemiological
research. \emph{Journal of Epidemiology \& Community Health},
\textbf{58}(4), 265--271.
doi:\href{https://doi.org/10.1136/jech.2002.006361}{10.1136/jech.2002.006361}.

\bibitem[\citeproctext]{ref-hernuxe1n2017}
Hern치n, MA (2017) Invited commentary: Selection bias without colliders
\textbar{} american journal of epidemiology \textbar{} oxford academic.
\emph{American Journal of Epidemiology}, \textbf{185}(11), 1048--1050.
Retrieved from \url{https://doi.org/10.1093/aje/kwx077}

\bibitem[\citeproctext]{ref-hernuxe1n2008a}
Hern치n, MA, Alonso, A, Logan, R, \ldots{} Robins, JM (2008)
Observational studies analyzed like randomized experiments: An
application to postmenopausal hormone therapy and coronary heart
disease. \emph{Epidemiology}, \textbf{19}(6), 766.
doi:\href{https://doi.org/10.1097/EDE.0b013e3181875e61}{10.1097/EDE.0b013e3181875e61}.

\bibitem[\citeproctext]{ref-hernuxe1n2009}
Hern치n, MA, and Cole, SR (2009) Invited commentary: Causal diagrams and
measurement bias. \emph{American Journal of Epidemiology},
\textbf{170}(8), 959--962.
doi:\href{https://doi.org/10.1093/aje/kwp293}{10.1093/aje/kwp293}.

\bibitem[\citeproctext]{ref-hernan2017per}
Hern치n, MA, Robins, JM, et al. (2017) Per-protocol analyses of pragmatic
trials. \emph{N Engl J Med}, \textbf{377}(14), 1391--1398.

\bibitem[\citeproctext]{ref-holland1986}
Holland, PW (1986) Statistics and causal inference. \emph{Journal of the
American Statistical Association}, \textbf{81}(396), 945--960.

\bibitem[\citeproctext]{ref-hume1902}
Hume, D (1902) \emph{Enquiries Concerning the Human Understanding: And
Concerning the Principles of Morals}, Clarendon Press.

\bibitem[\citeproctext]{ref-imai2008misunderstandings}
Imai, K, King, G, and Stuart, EA (2008) Misunderstandings between
experimentalists and observationalists about causal inference.
\emph{Journal of the Royal Statistical Society Series A: Statistics in
Society}, \textbf{171}(2), 481--502.

\bibitem[\citeproctext]{ref-jurek2008brief}
Jurek, AM, Greenland, S, and Maldonado, G (2008) Brief report: How far
from non-differential does exposure or disease misclassification have to
be to bias measures of association away from the null?
\emph{International Journal of Epidemiology}, \textbf{37}(2), 382--385.

\bibitem[\citeproctext]{ref-jurek2005proper}
Jurek, AM, Greenland, S, Maldonado, G, and Church, TR (2005) Proper
interpretation of non-differential misclassification effects:
Expectations vs observations. \emph{International Journal of
Epidemiology}, \textbf{34}(3), 680--687.

\bibitem[\citeproctext]{ref-jurek2006exposure}
Jurek, AM, Maldonado, G, Greenland, S, and Church, TR (2006)
Exposure-measurement error is frequently ignored when interpreting
epidemiologic study results. \emph{European Journal of Epidemiology},
\textbf{21}(12), 871--876.
doi:\href{https://doi.org/10.1007/s10654-006-9083-0}{10.1007/s10654-006-9083-0}.

\bibitem[\citeproctext]{ref-lash2009applying}
Lash, TL, Fox, MP, and Fink, AK (2009) \emph{Applying quantitative bias
analysis to epidemiologic data}, Springer.

\bibitem[\citeproctext]{ref-lash2020}
Lash, TL, Rothman, KJ, VanderWeele, TJ, and Haneuse, S (2020)
\emph{Modern epidemiology}, Wolters Kluwer. Retrieved from
\url{https://books.google.co.nz/books?id=SiTSnQEACAAJ}

\bibitem[\citeproctext]{ref-lewis1973}
Lewis, D (1973) Causation. \emph{The Journal of Philosophy},
\textbf{70}(17), 556--567.
doi:\href{https://doi.org/10.2307/2025310}{10.2307/2025310}.

\bibitem[\citeproctext]{ref-leyrat2021}
Leyrat, C, Carpenter, JR, Bailly, S, and Williamson, EJ (2021) Common
methods for handling missing data in marginal structural models: What
works and why. \emph{American Journal of Epidemiology}, \textbf{190}(4),
663--672.

\bibitem[\citeproctext]{ref-lu2022}
Lu, H, Cole, SR, Howe, CJ, and Westreich, D (2022) Toward a Clearer
Definition of Selection Bias When Estimating Causal Effects.
\emph{Epidemiology (Cambridge, Mass.)}, \textbf{33}(5), 699--706.
doi:\href{https://doi.org/10.1097/EDE.0000000000001516}{10.1097/EDE.0000000000001516}.

\bibitem[\citeproctext]{ref-machery2010}
Machery, E (2010) Explaining why experimental behavior varies across
cultures: A missing step in "the weirdest people in the world?".
\emph{Behavioral and Brain Sciences}, \textbf{33}(2-3), 101--102.
doi:\href{https://doi.org/10.1017/S0140525X10000178}{10.1017/S0140525X10000178}.

\bibitem[\citeproctext]{ref-malinsky2022semiparametric}
Malinsky, D, Shpitser, I, and Tchetgen Tchetgen, EJ (2022)
Semiparametric inference for nonmonotone missing-not-at-random data: The
no self-censoring model. \emph{Journal of the American Statistical
Association}, \textbf{117}(539), 1415--1423.

\bibitem[\citeproctext]{ref-mcelreath2020}
McElreath, R (2020) \emph{Statistical rethinking: A {B}ayesian course
with examples in {R} and {S}tan}, CRC press.

\bibitem[\citeproctext]{ref-morgan2014}
Morgan, SL, and Winship, C (2014) \emph{Counterfactuals and causal
inference: Methods and principles for social research}, 2nd edn,
Cambridge: Cambridge University Press.
doi:\href{https://doi.org/10.1017/CBO9781107587991}{10.1017/CBO9781107587991}.

\bibitem[\citeproctext]{ref-neal2020introduction}
Neal, B (2020) Introduction to causal inference from a machine learning
perspective. \emph{Course Lecture Notes (Draft)}. Retrieved from
\url{https://www.bradyneal.com/Introduction_to_Causal_Inference-Dec17_2020-Neal.pdf}

\bibitem[\citeproctext]{ref-pearl1995}
Pearl, J (1995) Causal diagrams for empirical research.
\emph{Biometrika}, \textbf{82}(4), 669--688.

\bibitem[\citeproctext]{ref-pearl2009a}
Pearl, J (2009) \emph{Causality}, Cambridge University Press.

\bibitem[\citeproctext]{ref-richardson2013}
Richardson, TS, and Robins, JM (2013a) Single world intervention graphs:
A primer. In, Citeseer. Retrieved from
\url{https://core.ac.uk/display/102673558}

\bibitem[\citeproctext]{ref-richardson2013swigsprimer}
Richardson, TS, and Robins, JM (2013b) Single world intervention graphs:
A primer. In \emph{Second UAI workshop on causal structure learning,
{B}ellevue, {W}ashington}, Citeseer. Retrieved from
\url{https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=07bbcb458109d2663acc0d098e8913892389a2a7}

\bibitem[\citeproctext]{ref-richardson2014causal}
Richardson, TS, and Rotnitzky, A (2014) Causal etiology of the research
of {J}ames {M}. {R}obins. \emph{Statistical Science}, \textbf{29}(4),
459--484.

\bibitem[\citeproctext]{ref-robins2004effects}
Robins, JM, Hern치n, MA, and SiEBERT, U (2004) Effects of multiple
interventions. \emph{Comparative Quantification of Health Risks: Global
and Regional Burden of Disease Attributable to Selected Major Risk
Factors}, \textbf{1}, 2191--2230.

\bibitem[\citeproctext]{ref-rubin1976}
Rubin, DB (1976) Inference and missing data. \emph{Biometrika},
\textbf{63}(3), 581--592.
doi:\href{https://doi.org/10.1093/biomet/63.3.581}{10.1093/biomet/63.3.581}.

\bibitem[\citeproctext]{ref-sears1986college}
Sears, DO (1986) College sophomores in the laboratory: Influences of a
narrow data base on social psychology's view of human nature.
\emph{Journal of Personality and Social Psychology}, \textbf{51}(3),
515.

\bibitem[\citeproctext]{ref-shaver2021comparison}
Shaver, JH, White, TA, Vakaoti, P, and Lang, M (2021) A comparison of
self-report, systematic observation and third-party judgments of church
attendance in a rural fijian village. \emph{Plos One}, \textbf{16}(10),
e0257160.

\bibitem[\citeproctext]{ref-shiba2021}
Shiba, K, and Kawahara, T (2021) Using propensity scores for causal
inference: Pitfalls and tips. \emph{Journal of Epidemiology},
\textbf{31}(8), 457--463.

\bibitem[\citeproctext]{ref-stuart2018generalizability}
Stuart, EA, Ackerman, B, and Westreich, D (2018) Generalizability of
randomized trial results to target populations: Design and analysis
possibilities. \emph{Research on Social Work Practice}, \textbf{28}(5),
532--537.

\bibitem[\citeproctext]{ref-stuart2015}
Stuart, EA, Bradshaw, CP, and Leaf, PJ (2015) Assessing the
Generalizability of Randomized Trial Results to Target Populations.
\emph{Prevention Science}, \textbf{16}(3), 475--485.
doi:\href{https://doi.org/10.1007/s11121-014-0513-z}{10.1007/s11121-014-0513-z}.

\bibitem[\citeproctext]{ref-suzuki2020}
Suzuki, E, Shinozaki, T, and Yamamoto, E (2020) Causal Diagrams:
Pitfalls and Tips. \emph{Journal of Epidemiology}, \textbf{30}(4),
153--162.
doi:\href{https://doi.org/10.2188/jea.JE20190192}{10.2188/jea.JE20190192}.

\bibitem[\citeproctext]{ref-tchetgen2017general}
Tchetgen Tchetgen, EJ, and Wirth, KE (2017) A general instrumental
variable framework for regression analysis with outcome missing not at
random. \emph{Biometrics}, \textbf{73}(4), 1123--1131.

\bibitem[\citeproctext]{ref-tripepi2007}
Tripepi, G, Jager, KJ, Dekker, FW, Wanner, C, and Zoccali, C (2007)
Measures of effect: Relative risks, odds ratios, risk difference, and
{`}number needed to treat{'}. \emph{Kidney International},
\textbf{72}(7), 789--791.
doi:\href{https://doi.org/10.1038/sj.ki.5002432}{10.1038/sj.ki.5002432}.

\bibitem[\citeproctext]{ref-vanderweele2009}
VanderWeele, TJ (2009) Concerning the consistency assumption in causal
inference. \emph{Epidemiology}, \textbf{20}(6), 880.
doi:\href{https://doi.org/10.1097/EDE.0b013e3181bd5638}{10.1097/EDE.0b013e3181bd5638}.

\bibitem[\citeproctext]{ref-vanderweele2012}
VanderWeele, TJ (2012) Confounding and Effect Modification: Distribution
and Measure. \emph{Epidemiologic Methods}, \textbf{1}(1), 55--82.
doi:\href{https://doi.org/10.1515/2161-962X.1004}{10.1515/2161-962X.1004}.

\bibitem[\citeproctext]{ref-vanderweele2022}
VanderWeele, TJ (2022) Constructed measures and causal inference:
Towards a new model of measurement for psychosocial constructs.
\emph{Epidemiology}, \textbf{33}(1), 141.
doi:\href{https://doi.org/10.1097/EDE.0000000000001434}{10.1097/EDE.0000000000001434}.

\bibitem[\citeproctext]{ref-vanderweele2012a}
VanderWeele, TJ, and Hern치n, MA (2012) Results on differential and
dependent measurement error of the exposure and the outcome using signed
directed acyclic graphs. \emph{American Journal of Epidemiology},
\textbf{175}(12), 1303--1310.
doi:\href{https://doi.org/10.1093/aje/kwr458}{10.1093/aje/kwr458}.

\bibitem[\citeproctext]{ref-vanderweele2022a}
VanderWeele, TJ, and Vansteelandt, S (2022) A statistical test to reject
the structural interpretation of a latent factor model. \emph{Journal of
the Royal Statistical Society Series B: Statistical Methodology},
\textbf{84}(5), 2032--2054.

\bibitem[\citeproctext]{ref-vansteelandt2022}
Vansteelandt, S, and Dukes, O (2022b) Assumption-lean inference for
generalised linear model parameters. \emph{Journal of the Royal
Statistical Society Series B: Statistical Methodology}, \textbf{84}(3),
657--685.

\bibitem[\citeproctext]{ref-vansteelandt2022a}
Vansteelandt, S, and Dukes, O (2022a) Assumption-lean inference for
generalised linear model parameters. \emph{Journal of the Royal
Statistical Society Series B: Statistical Methodology}, \textbf{84}(3),
657--685.

\bibitem[\citeproctext]{ref-westreich2010}
Westreich, D, and Cole, SR (2010) Invited commentary: positivity in
practice. \emph{American Journal of Epidemiology}, \textbf{171}(6).
doi:\href{https://doi.org/10.1093/aje/kwp436}{10.1093/aje/kwp436}.

\bibitem[\citeproctext]{ref-westreich2017}
Westreich, D, Edwards, JK, Lesko, CR, Stuart, E, and Cole, SR (2017a)
Transportability of trial results using inverse odds of sampling
weights. \emph{American Journal of Epidemiology}, \textbf{186}(8),
1010--1014.
doi:\href{https://doi.org/10.1093/aje/kwx164}{10.1093/aje/kwx164}.

\bibitem[\citeproctext]{ref-westreich2017transportability}
Westreich, D, Edwards, JK, Lesko, CR, Stuart, E, and Cole, SR (2017b)
Transportability of trial results using inverse odds of sampling
weights. \emph{American Journal of Epidemiology}, \textbf{186}(8),
1010--1014.

\end{CSLReferences}



\end{document}
